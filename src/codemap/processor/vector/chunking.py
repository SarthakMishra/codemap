"""Module for chunking source code files using LODGenerator."""

import logging
from collections.abc import Generator
from pathlib import Path
from typing import TYPE_CHECKING, Any

from codemap.processor.lod import LODEntity, LODGenerator, LODLevel
from codemap.processor.tree_sitter.base import EntityType
from codemap.processor.utils.git_utils import GitRepoContext
from codemap.processor.vector.schema import (
	ChunkMetadataSchema,
	ChunkSchema,
	GitMetadataSchema,
)
from codemap.utils.file_utils import read_file_content

if TYPE_CHECKING:
	from codemap.config import ConfigLoader

logger = logging.getLogger(__name__)


class TreeSitterChunker:
	"""Chunks code files based on LODEntity structure generated by LODGenerator."""

	def __init__(
		self,
		lod_generator: LODGenerator | None = None,
		config_loader: "ConfigLoader | None" = None,
		git_context: GitRepoContext | None = None,
	) -> None:
		"""
		Initialize the chunker.

		Args:
		    lod_generator: An instance of LODGenerator. If None, creates a new one.
		    config_loader: Configuration loader instance.
		    git_context: Git repository context instance.

		"""
		self.lod_generator = lod_generator or LODGenerator()
		if config_loader:
			self.config_loader = config_loader
		else:
			from codemap.config import ConfigLoader

			self.config_loader = ConfigLoader()

		# Load configuration values
		embedding_config = self.config_loader.get.embedding
		chunking_config = embedding_config.chunking

		# Set constants from config with fallbacks
		self.max_hierarchy_depth = chunking_config.max_hierarchy_depth
		self.max_file_lines = chunking_config.max_file_lines

		self.git_context = git_context

	def _get_entity_code_content(self, entity: LODEntity, file_lines: list[str]) -> str | None:
		"""Extract the raw code content for an entity using its line numbers."""
		if entity.start_line is None or entity.end_line is None:
			return None

		start_idx = entity.start_line - 1
		end_idx = entity.end_line
		if 0 <= start_idx < end_idx <= len(file_lines):
			return "\n".join(file_lines[start_idx:end_idx])
		logger.warning(
			"Invalid line numbers for entity %s in %s: start=%s, end=%s, total_lines=%d",
			entity.name,
			entity.metadata.get("file_path"),
			entity.start_line,
			entity.end_line,
			len(file_lines),
		)
		return None

	def _build_hierarchy_path(self, entity: LODEntity, parent_path: str = "") -> str:
		"""
		Build a hierarchical path string representing the entity's position in the code.

		Args:
		        entity: The current entity
		        parent_path: Path of parent entities

		Returns:
		        String representation of the hierarchy path

		"""
		entity_name = entity.name or f"<{entity.entity_type.name.lower()}>"
		if not parent_path:
			return entity_name
		return f"{parent_path}.{entity_name}"

	def _extract_nested_entities(self, entity: LODEntity) -> list[dict[str, Any]]:
		"""
		Extract information about nested entities to enhance chunk context.

		Args:
		        entity: The current entity

		Returns:
		        List of dictionaries containing info about nested entities

		"""
		nested_info = []

		def process_nested(nested_entity: LODEntity, depth: int = 1) -> None:
			"""Process a nested entity and its children recursively to extract information.

			Args:
				nested_entity: The nested entity to process
				depth: Current depth in the hierarchy (default: 1)

			Returns:
				None: Modifies nested_info in place by appending entity information
			"""
			# Skip UNKNOWN entities
			if nested_entity.entity_type == EntityType.UNKNOWN:
				return

			entity_info = {
				"type": nested_entity.entity_type.name,
				"name": nested_entity.name or f"<{nested_entity.entity_type.name.lower()}>",
				"signature": nested_entity.signature or "",
				"depth": depth,
				"line_range": f"{nested_entity.start_line}-{nested_entity.end_line}"
				if nested_entity.start_line and nested_entity.end_line
				else "",
			}
			nested_info.append(entity_info)

			# Process children (limited by configured max hierarchy depth)
			if depth < self.max_hierarchy_depth:
				for child in nested_entity.children:
					process_nested(child, depth + 1)

		# Process all direct children
		for child in entity.children:
			process_nested(child)

		return nested_info

	def _chunk_entity_recursive(
		self,
		entity: LODEntity,
		file_path: Path,
		file_lines: list[str],
		git_hash: str | None,
		language: str,
		parent_hierarchy: str = "",
		file_entity: LODEntity | None = None,
	) -> Generator[ChunkSchema, None, None]:
		"""Recursive helper to generate chunks from the LODEntity tree with hierarchy context."""
		# Decide which entity types are significant enough to become their own chunk
		primary_chunkable_types = (
			EntityType.MODULE,
			EntityType.CLASS,
			EntityType.INTERFACE,
			EntityType.STRUCT,
		)

		secondary_chunkable_types = (
			EntityType.FUNCTION,
			EntityType.METHOD,
		)

		# Skip UNKNOWN entities entirely
		if entity.entity_type == EntityType.UNKNOWN:
			return

		# Build hierarchy path for this entity
		entity_hierarchy = self._build_hierarchy_path(entity, parent_hierarchy)

		# For primary entities (modules, classes), create full chunks with all their content
		if (
			entity.entity_type in primary_chunkable_types
			and entity.start_line is not None
			and entity.end_line is not None
		):
			try:
				# Get full content including all nested entities
				code_content = self._get_entity_code_content(entity, file_lines)
				if code_content:
					# Extract information about nested entities to enhance context
					nested_entities = self._extract_nested_entities(entity)

					# Construct rich chunk content with nested entity information
					content_parts = []
					content_parts.append(f"Type: {entity.entity_type.name}")
					content_parts.append(f"Path: {entity_hierarchy}")
					if entity.name:
						content_parts.append(f"Name: {entity.name}")
					if entity.signature:
						content_parts.append(f"Signature: {entity.signature}")
					if entity.docstring:
						content_parts.append(f"Docstring:\n{entity.docstring}")

					# Add structure overview
					if nested_entities:
						content_parts.append("Contains:")
						for ne in nested_entities:
							indent = "  " * ne["depth"]
							content_parts.append(
								f"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})"
							)

					# Add the full code
					content_parts.append(f"Code:\n```{language}\n{code_content}\n```")

					# Add raw unformatted code at the end
					content_parts.append(f"Raw:\n{code_content}")

					chunk_content = "\n\n".join(content_parts)

					# Reverted path logic: use original file_path
					# Removed relative path calculation
					chunk_id = f"{file_path!s}:{entity.start_line}-{entity.end_line}"
					metadata = self._make_chunk_metadata(
						chunk_id,
						str(file_path),
						entity.start_line,
						entity.end_line,
						entity.entity_type.name,
						entity.name or "",
						language,
						entity_hierarchy,
					)
					yield ChunkSchema(content=chunk_content, metadata=metadata)

			except (ValueError, TypeError, KeyError, AttributeError):
				logger.exception("Error processing LOD entity %s in %s", entity.name, file_path)

		# For secondary entities (functions, methods), create individual chunks
		elif (
			entity.entity_type in secondary_chunkable_types
			and entity.start_line is not None
			and entity.end_line is not None
		):
			try:
				code_content = self._get_entity_code_content(entity, file_lines)
				if code_content:
					# Use file entity if available (for better context)
					file_context = ""
					if file_entity and file_entity.entity_type == EntityType.MODULE:
						file_context = f"File: {file_entity.name or Path(str(file_path)).name}\n"

					# Construct rich chunk content
					content_parts = []
					content_parts.append(f"{file_context}Type: {entity.entity_type.name}")
					content_parts.append(f"Path: {entity_hierarchy}")
					if entity.name:
						content_parts.append(f"Name: {entity.name}")
					if entity.signature:
						content_parts.append(f"Signature: {entity.signature}")
					if entity.docstring:
						content_parts.append(f"Docstring:\n{entity.docstring}")

					# Add code with any dependencies visible in comments
					content_parts.append(f"Code:\n```{language}\n{code_content}\n```")

					# Add raw unformatted code at the end
					content_parts.append(f"Raw:\n{code_content}")

					chunk_content = "\n\n".join(content_parts)

					# Reverted path logic: use original file_path
					# Removed relative path calculation
					chunk_id = f"{file_path!s}:{entity.start_line}-{entity.end_line}"
					metadata = self._make_chunk_metadata(
						chunk_id,
						str(file_path),
						entity.start_line,
						entity.end_line,
						entity.entity_type.name,
						entity.name or "",
						language,
						entity_hierarchy,
					)
					yield ChunkSchema(content=chunk_content, metadata=metadata)

			except (ValueError, TypeError, KeyError, AttributeError):
				logger.exception("Error processing LOD entity %s in %s", entity.name, file_path)

		# Recursively process children, remove repo_path pass
		for child in entity.children:
			yield from self._chunk_entity_recursive(
				child,
				file_path,
				file_lines,
				git_hash,
				language,
				entity_hierarchy,
				file_entity=file_entity,
			)

	def chunk_file(
		self,
		file_path: Path,
		git_hash: str | None = None,
		lod_level: LODLevel = LODLevel.FULL,  # Use FULL for max info, not DETAIL
	) -> Generator[ChunkSchema, None, None]:
		"""
		Generates code chunks for a given file using LODGenerator.

		Args:
		    file_path: The path to the file to chunk.
		    git_hash: Optional Git hash of the file content.
		    lod_level: The level of detail to request from LODGenerator.

		Yields:
		    CodeChunk dictionaries, each representing a semantically rich code chunk.

		"""
		content: str | None = None
		file_lines: list[str] = []

		try:
			# Generate the LODEntity tree for the file using the specified level of detail
			root_entity = self.lod_generator.generate_lod(file_path, lod_level)

			if not root_entity:
				logger.debug("LODGenerator returned no entity for %s, skipping chunking", file_path)
				return

			# Try to get full_content_str from root_entity metadata (set by LODGenerator)
			content = root_entity.metadata.get("full_content_str")

			if content is None:  # Fallback if not provided by LODGenerator
				logger.debug("full_content_str not in root_entity metadata for %s. Reading file directly.", file_path)
				content = read_file_content(file_path)
				if content is None:
					logger.debug("Skipping file %s - could not obtain content via LOD or direct read", file_path)
					return

			# Language should be available in the root entity metadata now
			resolved_language = root_entity.metadata.get("language", "unknown")
			file_lines = content.splitlines()

			# First, create a chunk for the entire file if it's small enough
			if len(file_lines) < self.max_file_lines:
				# Create a chunk for the entire file
				try:
					whole_file_content = "\n".join(file_lines)

					# Information about the file as a whole
					content_parts = []
					content_parts.append("Type: FILE")
					file_name = Path(file_path).name
					content_parts.append(f"Path: {file_name}")
					content_parts.append(f"Name: {file_name}")

					# Add docstring if the file has one (module docstring)
					if root_entity.docstring:
						content_parts.append(f"Docstring:\n{root_entity.docstring}")

					# Get structure overview
					nested_entities = self._extract_nested_entities(root_entity)
					if nested_entities:
						content_parts.append("Contains:")
						for ne in nested_entities:
							indent = "  " * ne["depth"]
							content_parts.append(
								f"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})"
							)

					# Add the full code
					content_parts.append(f"Code:\n```{resolved_language}\n{whole_file_content}\n```")

					# Add raw unformatted code at the end
					content_parts.append(f"Raw:\n{whole_file_content}")

					chunk_content = "\n\n".join(content_parts)

					chunk_id = f"{file_path!s}:1-{len(file_lines)}"
					metadata = self._make_chunk_metadata(
						chunk_id,
						str(file_path),
						1,
						len(file_lines),
						"FILE",
						file_name,
						resolved_language,
						file_name,
					)
					yield ChunkSchema(content=chunk_content, metadata=metadata)
				except (ValueError, TypeError, KeyError, AttributeError) as e:
					logger.warning("Error creating whole-file chunk for %s: %s", file_path, e)

			# Then create more specific chunks for the individual entities
			yield from self._chunk_entity_recursive(
				root_entity, file_path, file_lines, git_hash, resolved_language, file_entity=root_entity
			)

		except (OSError, ValueError, TypeError, KeyError, AttributeError) as e:
			logger.debug("Failed to chunk file %s: %s", file_path, str(e))
			return

	def _make_git_metadata(self, file_path_for_git: str, start_line: int, end_line: int) -> GitMetadataSchema:
		if self.git_context:
			# file_path_for_git is now expected to be relative by GitRepoContext
			return self.git_context.get_metadata_schema(file_path_for_git, start_line, end_line)
		# fallback: return empty/default metadata
		return GitMetadataSchema(
			git_hash="",
			tracked=False,
			branch="",
			blame=[],
		)

	def _make_chunk_metadata(
		self,
		chunk_id: str,
		file_path: str,
		start_line: int,
		end_line: int,
		entity_type: str,
		entity_name: str,
		language: str,
		hierarchy_path: str,
	) -> ChunkMetadataSchema:
		"""
		Create a ChunkMetadataSchema for the chunk.

		Args:
			chunk_id (str): The chunk ID.
			file_path (str): The file path.
			start_line (int): Start line.
			end_line (int): End line.
			entity_type (str): Entity type.
			entity_name (str): Entity name.
			language (str): Language.
			hierarchy_path (str): Hierarchy path.

		Returns:
			ChunkMetadataSchema: The chunk metadata.
		"""
		git_path_for_context = file_path  # Default to original path

		current_repo_root: Path | None = None
		if self.git_context:
			current_repo_root = self.git_context.repo_root  # Access the initialized repo_root

		if current_repo_root:  # Check if git_context was available and its repo_root is set
			try:
				abs_file_path = Path(file_path)
				if not abs_file_path.is_absolute():
					logger.warning(
						f"File path {file_path} provided to _make_chunk_metadata was not absolute. "
						"Ensure chunk_file receives absolute paths for correct relative path calculation."
					)

				# current_repo_root is already a Path object if not None
				relative_path = abs_file_path.relative_to(current_repo_root)
				git_path_for_context = str(relative_path)
			except ValueError:
				logger.warning(
					f"File path {file_path} could not be made relative to "
					f"git repo root {current_repo_root}. Using original path "
					"for Git metadata. This might lead to issues if the "
					"path is not understood by Git operations."
				)
			except Exception:  # General catch-all, ensure logger.exception is used
				logger.exception(
					f"Unexpected error when trying to make path relative {file_path} to {current_repo_root}"
				)

		return ChunkMetadataSchema(
			chunk_id=chunk_id,
			file_path=file_path,  # Original file_path for general metadata
			start_line=start_line,
			end_line=end_line,
			entity_type=entity_type,
			entity_name=entity_name or "",
			language=language,
			hierarchy_path=hierarchy_path,
			git_metadata=self._make_git_metadata(git_path_for_context, start_line, end_line),
		)
