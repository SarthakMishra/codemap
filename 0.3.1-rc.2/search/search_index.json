{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CodeMap: AI-Powered Developer Toolkit","text":"<p>CodeMap is an AI-powered developer toolkit designed to enhance your development workflow. It offers features like token-optimized documentation generation, semantic code analysis, and streamlined Git operations with AI assistance.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Tip</p> <p>After installation, you can use either <code>codemap</code> or the shorter alias <code>cm</code> to run the commands.</p> <p>Warning</p> <p>CodeMap currently only supports Unix-based platforms (macOS, Linux). For Windows users, we recommend using Windows Subsystem for Linux (WSL).</p>"},{"location":"#installation","title":"Installation","text":"<p>Using <code>uv</code> is recommended as it installs the package in an isolated environment and automatically manages the PATH.</p> <pre><code># Stable version:\nuv tool install codemap\n</code></pre>"},{"location":"#key-commands","title":"Key Commands","text":"<ul> <li>Generate Documentation: Create optimized markdown documentation and visualize repository structures.     <pre><code>cm gen path/to/source\n</code></pre></li> <li>Smart Commits: Get AI-generated commit messages based on semantic analysis of your changes.     <pre><code>cm commit\n</code></pre></li> <li>AI-Powered PRs: Streamline pull request creation and management with intelligent suggestions.     <pre><code>cm pr\n</code></pre></li> <li>AI Chat: Ask questions about your codebase.     <pre><code>cm ask \"Which module is responsible for managing auth tokens?\"\n</code></pre></li> <li>LLM Support: Integrate with various LLM providers supported by PydanticAi.     <pre><code># CodeMap Environment Variables Example\n# Copy this file to .env or .env.local and add your API keys\n# IMPORTANT: Make sure .env and .env.local are in your .gitignore file!\n\n# LLM Provider API Keys - Uncomment and add your actual keys\n# OPENAI_API_KEY=sk-...\n# ANTHROPIC_API_KEY=sk-ant-...\n# GROQ_API_KEY=gsk_...\n# AZURE_API_KEY=...\n# MISTRAL_API_KEY=...\n# TOGETHER_API_KEY=...\n# GOOGLE_API_KEY=...\n# OPENROUTER_API_KEY=...\n</code></pre></li> </ul>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<p>CodeMap relies on these excellent open-source libraries and models:</p>"},{"location":"acknowledgements/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>PydanticAI - Unified interface for LLM providers</li> <li>Pydantic - Data validation library for Python</li> <li>Questionary - Interactive user prompts</li> <li>Rich - Beautiful terminal formatting and output</li> <li>Typer - Modern CLI framework for Python</li> <li>Model2Vec - Text embeddings for semantic code analysis</li> <li>Tree-sitter - Robust parsing system for code analysis</li> <li>SQLModel - SQL database integration with Python</li> <li>Qdrant - Vector search engine for semantic analysis</li> <li>PyGit2 - Git repository manipulation</li> <li>Scikit-learn - Machine learning utilities</li> <li>PyGithub - GitHub API integration</li> <li>Docker SDK - Docker container management</li> <li>Watchdog - Filesystem event monitoring</li> </ul>"},{"location":"acknowledgements/#special-thanks","title":"Special Thanks","text":"<ul> <li>Cursor</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Warning</p> <p>CodeMap currently only supports Unix-based platforms (macOS, Linux). For Windows users, we recommend using Windows Subsystem for Linux (WSL).</p> <p>Tip</p> <p>After installation, you can use either <code>codemap</code> or the shorter alias <code>cm</code> to run the commands.</p>"},{"location":"installation/#installation-using-uv-recommended","title":"Installation using uv (Recommended)","text":"<p>Using <code>uv</code> is recommended as it installs the package in an isolated environment and automatically manages the PATH.</p> <pre><code># Stable version:\nuv tool install codemap\n</code></pre> <pre><code># Development Version:\nuv tool install codemap --prerelease allow\n</code></pre>"},{"location":"installation/#updating-codemap","title":"Updating CodeMap","text":"<p>To update CodeMap to the latest version:</p> <pre><code>uv tool upgrade codemap\n</code></pre>"},{"location":"installation/#uninstalling","title":"Uninstalling","text":"<pre><code>uv tool uninstall codemap\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>CodeMap - AI-powered developer toolkit.</p>"},{"location":"api/#main-modules","title":"Main Modules","text":"<ul> <li>Cli - Command-line interface package for CodeMap.</li> <li>Config - Configuration for the CodeMap project.</li> <li>Db - Database management utilities using SQLModel.</li> <li>Gen - Code documentation generation package for CodeMap.</li> <li>Git - Git utilities for CodeMap.</li> <li>Llm - LLM module for CodeMap.</li> <li>Processor - CodeMap processor module.</li> <li>Utils - Utility module for CodeMap package.</li> <li>Watcher - Watcher module for CodeMap.</li> </ul>"},{"location":"api/cli/","title":"Cli Overview","text":"<p>Command-line interface package for CodeMap.</p> <ul> <li>Ask Cmd - CLI command for asking questions about the codebase using RAG.</li> <li>Commit Cmd - Command for generating conventional commit messages from Git diffs.</li> <li>Conf Cmd - Configuration management commands.</li> <li>Gen Cmd - CLI command for generating code documentation.</li> <li>Index Cmd - CLI command for indexing repositories.</li> <li>Pr Cmd - CLI command for generating pull requests using the refactored lazy-loading pattern.</li> </ul>"},{"location":"api/cli/ask_cmd/","title":"Ask Cmd","text":"<p>CLI command for asking questions about the codebase using RAG.</p>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.QuestionArg","title":"QuestionArg  <code>module-attribute</code>","text":"<pre><code>QuestionArg = Annotated[\n\tstr | None,\n\tArgument(\n\t\thelp=\"Your question about the codebase (omit for interactive mode).\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.InteractiveFlag","title":"InteractiveFlag  <code>module-attribute</code>","text":"<pre><code>InteractiveFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--interactive\",\n\t\t\"-i\",\n\t\thelp=\"Start an interactive chat session.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the ask command with the CLI app.</p> Source code in <code>src/codemap/cli/ask_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the ask command with the CLI app.\"\"\"\n\n\t@app.command(name=\"ask\")\n\t@asyncer.runnify\n\tasync def ask_command(\n\t\tquestion: QuestionArg = None,\n\t\tinteractive: InteractiveFlag = False,\n\t) -&gt; None:\n\t\t\"\"\"Ask questions about the codebase using Retrieval-Augmented Generation (RAG).\"\"\"\n\t\t# Defer heavy imports and logic to the implementation function\n\t\tawait _ask_command_impl(\n\t\t\tquestion=question,\n\t\t\tinteractive=interactive,\n\t\t)\n</code></pre>"},{"location":"api/cli/commit_cmd/","title":"Commit Cmd","text":"<p>Command for generating conventional commit messages from Git diffs.</p>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.NonInteractiveFlag","title":"NonInteractiveFlag  <code>module-attribute</code>","text":"<pre><code>NonInteractiveFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--non-interactive\",\n\t\t\"-y\",\n\t\thelp=\"Run in non-interactive mode\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.BypassHooksFlag","title":"BypassHooksFlag  <code>module-attribute</code>","text":"<pre><code>BypassHooksFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--bypass-hooks\",\n\t\t\"--no-verify\",\n\t\thelp=\"Bypass git hooks with --no-verify\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the commit commands with the CLI app.</p> Source code in <code>src/codemap/cli/commit_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the commit commands with the CLI app.\"\"\"\n\n\t@app.command(name=\"commit\")\n\t@asyncer.runnify\n\tasync def semantic_commit_command(\n\t\tnon_interactive: NonInteractiveFlag = False,\n\t\tbypass_hooks: BypassHooksFlag = False,\n\t\tpathspecs: list[str] | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGenerate semantic commits by grouping related changes.\n\n\t\tThis command analyzes your changes, groups them semantically, and\n\t\tcreates multiple focused commits with AI-generated messages.\n\n\t\t\"\"\"\n\t\t# Defer heavy imports and logic to the implementation function\n\t\tawait _semantic_commit_command_impl(\n\t\t\tnon_interactive=non_interactive,\n\t\t\tbypass_hooks=bypass_hooks,\n\t\t\tpathspecs=pathspecs,\n\t\t)\n</code></pre>"},{"location":"api/cli/conf_cmd/","title":"Conf Cmd","text":"<p>Configuration management commands.</p>"},{"location":"api/cli/conf_cmd/#codemap.cli.conf_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/conf_cmd/#codemap.cli.conf_cmd.ForceOpt","title":"ForceOpt  <code>module-attribute</code>","text":"<pre><code>ForceOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--force\",\n\t\t\"-f\",\n\t\thelp=\"Overwrite existing configuration file.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/conf_cmd/#codemap.cli.conf_cmd.PathOpt","title":"PathOpt  <code>module-attribute</code>","text":"<pre><code>PathOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--path\",\n\t\t\"-p\",\n\t\thelp=\"Path to the configuration file. Defaults to .codemap.yml in the repo root.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/conf_cmd/#codemap.cli.conf_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the configuration commands with the main app.</p> Source code in <code>src/codemap/cli/conf_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the configuration commands with the main app.\"\"\"\n\n\t@app.command(\"conf\")\n\tdef conf_command(\n\t\tforce: ForceOpt = False,\n\t) -&gt; None:\n\t\t\"\"\"Create a default .codemap.yml configuration file in the project root.\"\"\"\n\t\t# Defer heavy imports and logic to the implementation function\n\t\t_conf_command_impl(force=force)\n</code></pre>"},{"location":"api/cli/gen_cmd/","title":"Gen Cmd","text":"<p>CLI command for generating code documentation.</p>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.PathArg","title":"PathArg  <code>module-attribute</code>","text":"<pre><code>PathArg = Annotated[\n\tPath,\n\tArgument(\n\t\texists=True,\n\t\thelp=\"Path to the codebase to analyze\",\n\t\tshow_default=True,\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.OutputOpt","title":"OutputOpt  <code>module-attribute</code>","text":"<pre><code>OutputOpt = Annotated[\n\tPath | None,\n\tOption(\n\t\t\"--output\",\n\t\t\"-o\",\n\t\thelp=\"Output file path (overrides config)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.ConfigOpt","title":"ConfigOpt  <code>module-attribute</code>","text":"<pre><code>ConfigOpt = Annotated[\n\tPath | None,\n\tOption(\"--config\", \"-c\", help=\"Path to config file\"),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MaxContentLengthOpt","title":"MaxContentLengthOpt  <code>module-attribute</code>","text":"<pre><code>MaxContentLengthOpt = Annotated[\n\tint | None,\n\tOption(\n\t\t\"--max-content-length\",\n\t\thelp=\"Maximum content length for file display (set to 0 for unlimited)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.TreeOpt","title":"TreeOpt  <code>module-attribute</code>","text":"<pre><code>TreeOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--tree/--no-tree\",\n\t\t\"-t\",\n\t\thelp=\"Include directory tree in output\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.EntityGraphOpt","title":"EntityGraphOpt  <code>module-attribute</code>","text":"<pre><code>EntityGraphOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--entity-graph/--no-entity-graph\",\n\t\t\"-e\",\n\t\thelp=\"Include entity relationship graph in output\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.LODLevelOpt","title":"LODLevelOpt  <code>module-attribute</code>","text":"<pre><code>LODLevelOpt = Annotated[\n\tstr,\n\tOption(\n\t\t\"--lod\",\n\t\thelp=\"Level of Detail for code analysis (e.g., 'full', 'docs', 'signatures')\",\n\t\tcase_sensitive=False,\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidEntitiesOpt","title":"MermaidEntitiesOpt  <code>module-attribute</code>","text":"<pre><code>MermaidEntitiesOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--mermaid-entities\",\n\t\thelp=\"Comma-separated list of entity types to include in Mermaid graph (e.g., 'module,class,function')\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidRelationshipsOpt","title":"MermaidRelationshipsOpt  <code>module-attribute</code>","text":"<pre><code>MermaidRelationshipsOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--mermaid-relationships\",\n\t\thelp=\"Comma-separated list of relationship types to include in Mermaid graph (e.g., 'declares,imports,calls')\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidLegendOpt","title":"MermaidLegendOpt  <code>module-attribute</code>","text":"<pre><code>MermaidLegendOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--mermaid-legend/--no-mermaid-legend\",\n\t\thelp=\"Show/hide the legend in the Mermaid diagram\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidUnconnectedOpt","title":"MermaidUnconnectedOpt  <code>module-attribute</code>","text":"<pre><code>MermaidUnconnectedOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--mermaid-unconnected/--no-mermaid-unconnected\",\n\t\thelp=\"Remove/keep nodes with no connections in the Mermaid diagram\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.SemanticAnalysisOpt","title":"SemanticAnalysisOpt  <code>module-attribute</code>","text":"<pre><code>SemanticAnalysisOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--semantic/--no-semantic\",\n\t\thelp=\"Enable/disable semantic analysis\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the gen command with the CLI app.</p> Source code in <code>src/codemap/cli/gen_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the gen command with the CLI app.\"\"\"\n\n\t@app.command(name=\"gen\")\n\tdef gen_command(\n\t\tpath: PathArg = Path(),\n\t\toutput: OutputOpt = None,\n\t\tmax_content_length: MaxContentLengthOpt = None,\n\t\tlod_level_str: LODLevelOpt = \"docs\",\n\t\tsemantic_analysis: SemanticAnalysisOpt = True,\n\t\ttree: TreeOpt = None,\n\t\tentity_graph: EntityGraphOpt = None,\n\t\tmermaid_entities_str: MermaidEntitiesOpt = None,\n\t\tmermaid_relationships_str: MermaidRelationshipsOpt = None,\n\t\tmermaid_show_legend_flag: MermaidLegendOpt = None,\n\t\tmermaid_remove_unconnected_flag: MermaidUnconnectedOpt = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGenerate code documentation.\n\n\t\tThis command processes a codebase and generates Markdown documentation\n\t\twith configurable level of detail.\n\n\t\tExamples:\n\t\t        codemap gen                      # Generate docs for current directory\n\t\t        codemap gen --lod full           # Generate full implementation docs\n\t\t        codemap gen --lod signatures     # Generate docs with signatures only\n\t\t        codemap gen --no-semantic        # Generate without semantic analysis\n\n\t\t\"\"\"\n\t\t# Defer all heavy imports by calling implementation function\n\t\t_gen_command_impl(\n\t\t\tpath=path,\n\t\t\toutput=output,\n\t\t\tmax_content_length=max_content_length,\n\t\t\tlod_level_str=lod_level_str,\n\t\t\tsemantic_analysis=semantic_analysis,\n\t\t\ttree=tree,\n\t\t\tentity_graph=entity_graph,\n\t\t\tmermaid_entities_str=mermaid_entities_str,\n\t\t\tmermaid_relationships_str=mermaid_relationships_str,\n\t\t\tmermaid_show_legend_flag=mermaid_show_legend_flag,\n\t\t\tmermaid_remove_unconnected_flag=mermaid_remove_unconnected_flag,\n\t\t)\n</code></pre>"},{"location":"api/cli/index_cmd/","title":"Index Cmd","text":"<p>CLI command for indexing repositories.</p>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.PathArg","title":"PathArg  <code>module-attribute</code>","text":"<pre><code>PathArg = Annotated[\n\tPath,\n\tArgument(\n\t\thelp=\"Path to the repository root directory.\",\n\t\texists=True,\n\t\tfile_okay=False,\n\t\tdir_okay=True,\n\t\treadable=True,\n\t\tresolve_path=True,\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.SyncOpt","title":"SyncOpt  <code>module-attribute</code>","text":"<pre><code>SyncOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--sync/--no-sync\",\n\t\thelp=\"Synchronize the vector database with the current Git state on startup.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.WatchOpt","title":"WatchOpt  <code>module-attribute</code>","text":"<pre><code>WatchOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--watch\",\n\t\t\"-w\",\n\t\thelp=\"Keep running and watch for file changes, automatically syncing the index.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the index command with the CLI app.</p> Source code in <code>src/codemap/cli/index_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the index command with the CLI app.\"\"\"\n\n\t@app.command(name=\"index\")\n\tdef index_command(\n\t\tpath: PathArg = Path(),\n\t\tsync: SyncOpt = True,\n\t\twatch: WatchOpt = False,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tIndex the repository: Process files, generate embeddings, and store in the vector database.\n\n\t\tOptionally, use --sync (default) to synchronize with the Git state on startup,\n\t\tand --watch (-w) to keep running and sync automatically on file changes.\n\t\t\"\"\"\n\t\t# Defer heavy imports and logic to the implementation function\n\t\t_index_command_impl(\n\t\t\tpath=path,\n\t\t\tsync=sync,\n\t\t\twatch=watch,\n\t\t)\n</code></pre>"},{"location":"api/cli/pr_cmd/","title":"Pr Cmd","text":"<p>CLI command for generating pull requests using the refactored lazy-loading pattern.</p>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRAction","title":"PRAction","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Actions for PR command.</p> Source code in <code>src/codemap/cli/pr_cmd.py</code> <pre><code>class PRAction(str, Enum):\n\t\"\"\"Actions for PR command.\"\"\"\n\n\tCREATE = \"create\"\n\tUPDATE = \"update\"\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRAction.CREATE","title":"CREATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CREATE = 'create'\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRAction.UPDATE","title":"UPDATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UPDATE = 'update'\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.validate_workflow_strategy","title":"validate_workflow_strategy","text":"<pre><code>validate_workflow_strategy(value: str | None) -&gt; str | None\n</code></pre> <p>Validate workflow strategy - lightweight callback for typer.</p> Source code in <code>src/codemap/cli/pr_cmd.py</code> <pre><code>def validate_workflow_strategy(value: str | None) -&gt; str | None:\n\t\"\"\"Validate workflow strategy - lightweight callback for typer.\"\"\"\n\t# Avoid heavy imports like Console here\n\tvalid_strategies = [\"github-flow\", \"gitflow\", \"trunk-based\"]\n\tif value is None or value in valid_strategies:\n\t\treturn value\n\tmsg = f\"Invalid workflow strategy: {value}. Must be one of: {', '.join(valid_strategies)}\"\n\traise typer.BadParameter(msg)\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ActionArg","title":"ActionArg  <code>module-attribute</code>","text":"<pre><code>ActionArg = Annotated[\n\tPRAction,\n\tArgument(help=\"Action to perform: create or update\"),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.BranchNameOpt","title":"BranchNameOpt  <code>module-attribute</code>","text":"<pre><code>BranchNameOpt = Annotated[\n\tstr | None,\n\tOption(\"--branch\", \"-b\", help=\"Target branch name\"),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.BranchTypeOpt","title":"BranchTypeOpt  <code>module-attribute</code>","text":"<pre><code>BranchTypeOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--type\",\n\t\t\"-t\",\n\t\thelp=\"Branch type (feature, release, hotfix, bugfix)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.BaseBranchOpt","title":"BaseBranchOpt  <code>module-attribute</code>","text":"<pre><code>BaseBranchOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--base\",\n\t\thelp=\"Base branch for the PR (defaults to repo default)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.TitleOpt","title":"TitleOpt  <code>module-attribute</code>","text":"<pre><code>TitleOpt = Annotated[\n\tstr | None, Option(\"--title\", help=\"Pull request title\")\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.DescriptionOpt","title":"DescriptionOpt  <code>module-attribute</code>","text":"<pre><code>DescriptionOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--desc\",\n\t\t\"-d\",\n\t\thelp=\"Pull request description (file path or text)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.NoCommitOpt","title":"NoCommitOpt  <code>module-attribute</code>","text":"<pre><code>NoCommitOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--no-commit\",\n\t\thelp=\"Skip the commit process before creating PR\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ForcePushOpt","title":"ForcePushOpt  <code>module-attribute</code>","text":"<pre><code>ForcePushOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--force-push\", \"-f\", help=\"Force push the branch\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRNumberOpt","title":"PRNumberOpt  <code>module-attribute</code>","text":"<pre><code>PRNumberOpt = Annotated[\n\tint | None,\n\tOption(\n\t\t\"--pr\",\n\t\thelp=\"PR number to update (required for update action)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.WorkflowOpt","title":"WorkflowOpt  <code>module-attribute</code>","text":"<pre><code>WorkflowOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--workflow\",\n\t\t\"-w\",\n\t\thelp=\"Git workflow strategy (github-flow, gitflow, trunk-based)\",\n\t\tcallback=validate_workflow_strategy,\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.NonInteractiveOpt","title":"NonInteractiveOpt  <code>module-attribute</code>","text":"<pre><code>NonInteractiveOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--non-interactive\",\n\t\thelp=\"Run in non-interactive mode\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ModelOpt","title":"ModelOpt  <code>module-attribute</code>","text":"<pre><code>ModelOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--model\",\n\t\t\"-m\",\n\t\thelp=\"LLM model for content generation\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ApiBaseOpt","title":"ApiBaseOpt  <code>module-attribute</code>","text":"<pre><code>ApiBaseOpt = Annotated[\n\tstr | None,\n\tOption(\"--api-base\", help=\"API base URL for LLM\"),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ApiKeyOpt","title":"ApiKeyOpt  <code>module-attribute</code>","text":"<pre><code>ApiKeyOpt = Annotated[\n\tstr | None, Option(\"--api-key\", help=\"API key for LLM\")\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.BypassHooksFlag","title":"BypassHooksFlag  <code>module-attribute</code>","text":"<pre><code>BypassHooksFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--bypass-hooks\",\n\t\t\"--no-verify\",\n\t\thelp=\"Bypass git hooks with --no-verify\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the pr command with the Typer app.</p> Source code in <code>src/codemap/cli/pr_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the pr command with the Typer app.\"\"\"\n\n\t@app.command(\"pr\")\n\t@asyncer.runnify\n\tasync def pr_command_entrypoint(\n\t\taction: ActionArg = PRAction.CREATE,\n\t\tbranch_name: BranchNameOpt = None,\n\t\tbranch_type: BranchTypeOpt = None,\n\t\tbase_branch: BaseBranchOpt = None,\n\t\ttitle: TitleOpt = None,\n\t\tdescription: DescriptionOpt = None,\n\t\tno_commit: NoCommitOpt = False,\n\t\tforce_push: ForcePushOpt = False,\n\t\tpr_number: PRNumberOpt = None,\n\t\tworkflow: WorkflowOpt = None,\n\t\tnon_interactive: NonInteractiveOpt = False,\n\t\tbypass_hooks: BypassHooksFlag = False,\n\t) -&gt; None:\n\t\t\"\"\"Create or update a GitHub/GitLab pull request with generated content.\"\"\"\n\t\tawait _pr_command_impl(\n\t\t\taction=action,\n\t\t\tbranch_name=branch_name,\n\t\t\tbranch_type=branch_type,\n\t\t\tbase_branch=base_branch,\n\t\t\ttitle=title,\n\t\t\tdescription=description,\n\t\t\tno_commit=no_commit,\n\t\t\tforce_push=force_push,\n\t\t\tpr_number=pr_number,\n\t\t\tworkflow=workflow,\n\t\t\tnon_interactive=non_interactive,\n\t\t\tbypass_hooks=bypass_hooks,\n\t\t)\n</code></pre>"},{"location":"api/config/","title":"Config Overview","text":"<p>Configuration for the CodeMap project.</p> <ul> <li>Config Loader - Configuration loader for CodeMap.</li> <li>Config Schema - Schemas for the CodeMap configuration.</li> <li>Default Config Template - Default configuration template for CodeMap.</li> </ul>"},{"location":"api/config/config_loader/","title":"Config Loader","text":"<p>Configuration loader for CodeMap.</p> <p>This module provides functionality for loading and managing configuration settings.</p>"},{"location":"api/config/config_loader/#codemap.config.config_loader.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigValue","title":"ConfigValue  <code>module-attribute</code>","text":"<pre><code>ConfigValue = (\n\tBaseModel\n\t| dict[str, Any]\n\t| list[Any]\n\t| str\n\t| int\n\t| float\n\t| bool\n\t| None\n)\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigError","title":"ConfigError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for configuration errors.</p> Source code in <code>src/codemap/config/config_loader.py</code> <pre><code>class ConfigError(Exception):\n\t\"\"\"Exception raised for configuration errors.\"\"\"\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigFileNotFoundError","title":"ConfigFileNotFoundError","text":"<p>               Bases: <code>ConfigError</code></p> <p>Exception raised when configuration file is not found.</p> Source code in <code>src/codemap/config/config_loader.py</code> <pre><code>class ConfigFileNotFoundError(ConfigError):\n\t\"\"\"Exception raised when configuration file is not found.\"\"\"\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigParsingError","title":"ConfigParsingError","text":"<p>               Bases: <code>ConfigError</code></p> <p>Exception raised when configuration file cannot be parsed.</p> Source code in <code>src/codemap/config/config_loader.py</code> <pre><code>class ConfigParsingError(ConfigError):\n\t\"\"\"Exception raised when configuration file cannot be parsed.\"\"\"\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigLoader","title":"ConfigLoader","text":"<p>Loads and manages configuration for CodeMap using Pydantic schemas.</p> <p>This class handles loading configuration from files, applying defaults from Pydantic models, with proper error handling and path resolution.</p> Source code in <code>src/codemap/config/config_loader.py</code> <pre><code>class ConfigLoader:\n\t\"\"\"\n\tLoads and manages configuration for CodeMap using Pydantic schemas.\n\n\tThis class handles loading configuration from files, applying defaults\n\tfrom Pydantic models, with proper error handling and path\n\tresolution.\n\n\t\"\"\"\n\n\t_instance: \"ConfigLoader | None\" = None\n\n\t@classmethod\n\tdef get_instance(\n\t\tcls,\n\t) -&gt; \"ConfigLoader\":\n\t\t\"\"\"\n\t\tGet the singleton instance of ConfigLoader.\n\n\t\tReturns:\n\t\t\tConfigLoader: Singleton instance\n\n\t\t\"\"\"\n\t\tif cls._instance is None:\n\t\t\tcls._instance = cls()\n\t\treturn cls._instance\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the configuration loader.\n\n\t\tArgs:\n\t\t\tconfig_file: Path to configuration file (optional)\n\t\t\trepo_root: Repository root path (optional)\n\n\t\t\"\"\"\n\t\t# Initialize repo_root attribute\n\t\tself.repo_root: Path | None = None\n\t\t# Load configuration eagerly during initialization instead of lazy loading\n\t\tself._app_config = self._load_config()\n\t\tlogger.debug(\"ConfigLoader initialized with eager configuration loading\")\n\n\tdef _get_config_file(self) -&gt; Path | None:\n\t\t\"\"\"\n\t\tResolve the configuration file path.\n\n\t\tlook in standard locations:\n\t\t1. ./.codemap.yml in the current directory\n\t\t2. ./.codemap.yml in the repository root\n\n\t\tArgs:\n\t\t\tconfig_file: Explicitly provided config file path (optional)\n\n\t\tReturns:\n\t\t\tOptional[Path]: Resolved config file path or None if no suitable file found\n\n\t\t\"\"\"\n\t\t# Import GitRepoContext here to avoid circular imports\n\t\tfrom codemap.git.utils import GitRepoContext\n\n\t\t# Try current directory\n\t\tlocal_config = Path(\".codemap.yml\")\n\t\tif local_config.exists():\n\t\t\tself.repo_root = local_config.parent\n\t\t\treturn local_config\n\n\t\t# Try repository root\n\t\trepo_root = GitRepoContext().get_repo_root()\n\t\tself.repo_root = repo_root\n\t\trepo_config = repo_root / \".codemap.yml\"\n\t\tif repo_config.exists():\n\t\t\treturn repo_config\n\n\t\t# If we get here, no config file was found\n\t\treturn None\n\n\t@staticmethod\n\tdef _parse_yaml_file(file_path: Path) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tParse a YAML file with caching for better performance.\n\n\t\tArgs:\n\t\t\tfile_path: Path to the YAML file to parse\n\n\t\tReturns:\n\t\t\tParsed YAML content as a dictionary\n\n\t\tRaises:\n\t\t\tyaml.YAMLError: If the file cannot be parsed as valid YAML\n\t\t\"\"\"\n\t\twith file_path.open(encoding=\"utf-8\") as f:\n\t\t\tcontent = yaml.safe_load(f)\n\t\t\tif content is None:  # Empty file\n\t\t\t\treturn {}\n\t\t\tif not isinstance(content, dict):\n\t\t\t\tmsg = f\"File {file_path} does not contain a valid YAML dictionary\"\n\t\t\t\traise yaml.YAMLError(msg)\n\t\t\treturn content\n\n\tdef _load_config(self) -&gt; AppConfigSchema:\n\t\t\"\"\"\n\t\tLoad configuration from file and parse it into AppConfigSchema.\n\n\t\tReturns:\n\t\t\tAppConfigSchema: Loaded and parsed configuration.\n\n\t\tRaises:\n\t\t\tConfigFileNotFoundError: If specified configuration file doesn't exist\n\t\t\tConfigParsingError: If configuration file exists but cannot be loaded or parsed.\n\n\t\t\"\"\"\n\t\t# Lazy imports\n\t\timport yaml\n\n\t\tfrom codemap.config import AppConfigSchema\n\n\t\tfile_config_dict: dict[str, Any] = {}\n\t\tconfig_file = self._get_config_file()\n\n\t\tif config_file:\n\t\t\ttry:\n\t\t\t\tif config_file.exists():\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfile_config_dict = self._parse_yaml_file(config_file)\n\t\t\t\t\t\tlogger.info(\"Loaded configuration from %s\", config_file)\n\t\t\t\t\texcept yaml.YAMLError as e:\n\t\t\t\t\t\tmsg = f\"Configuration file {config_file} does not contain a valid YAML dictionary.\"\n\t\t\t\t\t\tlogger.exception(msg)\n\t\t\t\t\t\traise ConfigParsingError(msg) from e\n\t\t\t\telse:\n\t\t\t\t\tmsg = f\"Configuration file not found: {config_file}.\"\n\t\t\t\t\tlogger.info(\"%s Using default configuration.\", msg)\n\t\t\texcept OSError as e:\n\t\t\t\terror_msg = f\"Error accessing configuration file {config_file}: {e}\"\n\t\t\t\tlogger.exception(error_msg)\n\t\t\t\traise ConfigParsingError(error_msg) from e\n\t\telse:\n\t\t\tlogger.info(\"No configuration file specified or found. Using default configuration.\")\n\n\t\tfile_config_dict[\"repo_root\"] = self.repo_root\n\n\t\ttry:\n\t\t\t# Initialize AppConfigSchema. If file_config_dict is empty, defaults will be used.\n\t\t\tconfig = AppConfigSchema(**file_config_dict)\n\t\t\t# Override github.token with env var if set\n\t\t\tenv_token = os.environ.get(\"GITHUB_TOKEN\") or os.environ.get(\"CODEMAP_GITHUB_TOKEN\")\n\t\t\tif env_token:\n\t\t\t\tconfig.github.token = env_token\n\t\t\treturn config\n\t\texcept Exception as e:  # Catch Pydantic validation errors etc.\n\t\t\terror_msg = f\"Error parsing configuration into schema: {e}\"\n\t\t\tlogger.exception(error_msg)\n\t\t\traise ConfigParsingError(error_msg) from e\n\n\tdef _merge_configs(self, base: dict[str, Any], override: dict[str, Any]) -&gt; None:\n\t\t\"\"\"\n\t\tRecursively merge two configuration dictionaries.\n\n\t\tArgs:\n\t\t\tbase: Base configuration dictionary to merge into\n\t\t\toverride: Override configuration to apply\n\n\t\t\"\"\"\n\t\tfor key, value in override.items():\n\t\t\tif isinstance(value, dict) and key in base and isinstance(base[key], dict):\n\t\t\t\tself._merge_configs(base[key], value)\n\t\t\telse:\n\t\t\t\tbase[key] = value\n\n\t@property\n\tdef get(self) -&gt; AppConfigSchema:\n\t\t\"\"\"\n\t\tGet the current application configuration.\n\n\t\tReturns:\n\t\t\tAppConfigSchema: The current configuration\n\t\t\"\"\"\n\t\t# Configuration is now loaded during initialization, no need for lazy loading\n\t\treturn self._app_config\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigLoader.get_instance","title":"get_instance  <code>classmethod</code>","text":"<pre><code>get_instance() -&gt; ConfigLoader\n</code></pre> <p>Get the singleton instance of ConfigLoader.</p> <p>Returns:</p> Name Type Description <code>ConfigLoader</code> <code>ConfigLoader</code> <p>Singleton instance</p> Source code in <code>src/codemap/config/config_loader.py</code> <pre><code>@classmethod\ndef get_instance(\n\tcls,\n) -&gt; \"ConfigLoader\":\n\t\"\"\"\n\tGet the singleton instance of ConfigLoader.\n\n\tReturns:\n\t\tConfigLoader: Singleton instance\n\n\t\"\"\"\n\tif cls._instance is None:\n\t\tcls._instance = cls()\n\treturn cls._instance\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigLoader.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the configuration loader.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <p>Path to configuration file (optional)</p> required <code>repo_root</code> <p>Repository root path (optional)</p> required Source code in <code>src/codemap/config/config_loader.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"\n\tInitialize the configuration loader.\n\n\tArgs:\n\t\tconfig_file: Path to configuration file (optional)\n\t\trepo_root: Repository root path (optional)\n\n\t\"\"\"\n\t# Initialize repo_root attribute\n\tself.repo_root: Path | None = None\n\t# Load configuration eagerly during initialization instead of lazy loading\n\tself._app_config = self._load_config()\n\tlogger.debug(\"ConfigLoader initialized with eager configuration loading\")\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigLoader.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root: Path | None = None\n</code></pre>"},{"location":"api/config/config_loader/#codemap.config.config_loader.ConfigLoader.get","title":"get  <code>property</code>","text":"<pre><code>get: AppConfigSchema\n</code></pre> <p>Get the current application configuration.</p> <p>Returns:</p> Name Type Description <code>AppConfigSchema</code> <code>AppConfigSchema</code> <p>The current configuration</p>"},{"location":"api/config/config_schema/","title":"Config Schema","text":"<p>Schemas for the CodeMap configuration.</p>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LLMSchema","title":"LLMSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the LLM.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class LLMSchema(BaseModel):\n\t\"\"\"Configuration for the LLM.\"\"\"\n\n\tmodel: str = \"openai:gpt-4o-mini\"\n\tbase_url: str | None = None\n\ttemperature: float = 0.5\n\tmax_output_tokens: int = 1024\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LLMSchema.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'openai:gpt-4o-mini'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LLMSchema.base_url","title":"base_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_url: str | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LLMSchema.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.5\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LLMSchema.max_output_tokens","title":"max_output_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_output_tokens: int = 1024\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingChunkingSchema","title":"EmbeddingChunkingSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the embedding chunking.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class EmbeddingChunkingSchema(BaseModel):\n\t\"\"\"Configuration for the embedding chunking.\"\"\"\n\n\tmax_hierarchy_depth: int = 2\n\tmax_file_lines: int = 1000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingChunkingSchema.max_hierarchy_depth","title":"max_hierarchy_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_hierarchy_depth: int = 2\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingChunkingSchema.max_file_lines","title":"max_file_lines  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_file_lines: int = 1000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AgglomerativeClusteringSchema","title":"AgglomerativeClusteringSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the agglomerative clustering.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class AgglomerativeClusteringSchema(BaseModel):\n\t\"\"\"Configuration for the agglomerative clustering.\"\"\"\n\n\tmetric: Literal[\"cosine\", \"euclidean\", \"manhattan\", \"l1\", \"l2\", \"precomputed\"] = \"precomputed\"\n\tdistance_threshold: float = 0.3\n\tlinkage: Literal[\"ward\", \"complete\", \"average\", \"single\"] = \"complete\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AgglomerativeClusteringSchema.metric","title":"metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metric: Literal[\n\t\"cosine\",\n\t\"euclidean\",\n\t\"manhattan\",\n\t\"l1\",\n\t\"l2\",\n\t\"precomputed\",\n] = \"precomputed\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AgglomerativeClusteringSchema.distance_threshold","title":"distance_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distance_threshold: float = 0.3\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AgglomerativeClusteringSchema.linkage","title":"linkage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>linkage: Literal[\n\t\"ward\", \"complete\", \"average\", \"single\"\n] = \"complete\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DBSCANSchema","title":"DBSCANSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the DBSCAN clustering.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class DBSCANSchema(BaseModel):\n\t\"\"\"Configuration for the DBSCAN clustering.\"\"\"\n\n\teps: float = 0.3\n\tmin_samples: int = 2\n\talgorithm: Literal[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"] = \"auto\"\n\tmetric: Literal[\"cityblock\", \"cosine\", \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"precomputed\"] = \"precomputed\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DBSCANSchema.eps","title":"eps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eps: float = 0.3\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DBSCANSchema.min_samples","title":"min_samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_samples: int = 2\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DBSCANSchema.algorithm","title":"algorithm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>algorithm: Literal[\n\t\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"\n] = \"auto\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DBSCANSchema.metric","title":"metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metric: Literal[\n\t\"cityblock\",\n\t\"cosine\",\n\t\"euclidean\",\n\t\"l1\",\n\t\"l2\",\n\t\"manhattan\",\n\t\"precomputed\",\n] = \"precomputed\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingClusteringSchema","title":"EmbeddingClusteringSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the embedding clustering.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class EmbeddingClusteringSchema(BaseModel):\n\t\"\"\"Configuration for the embedding clustering.\"\"\"\n\n\tmethod: Literal[\"agglomerative\", \"dbscan\"] = \"agglomerative\"\n\tagglomerative: AgglomerativeClusteringSchema = AgglomerativeClusteringSchema()\n\tdbscan: DBSCANSchema = DBSCANSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingClusteringSchema.method","title":"method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>method: Literal[\"agglomerative\", \"dbscan\"] = \"agglomerative\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingClusteringSchema.agglomerative","title":"agglomerative  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agglomerative: AgglomerativeClusteringSchema = (\n\tAgglomerativeClusteringSchema()\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingClusteringSchema.dbscan","title":"dbscan  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dbscan: DBSCANSchema = DBSCANSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema","title":"EmbeddingSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the embedding.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class EmbeddingSchema(BaseModel):\n\t\"\"\"Configuration for the embedding.\"\"\"\n\n\tmodel_name: str = \"minishlab/potion-base-8M\"\n\tdimension: int = 256\n\tdimension_metric: str = \"cosine\"\n\tmax_content_length: int = 5000\n\tqdrant_batch_size: int = 1000\n\turl: str = \"http://localhost:6333\"\n\tapi_key: str | None = None\n\ttimeout: int = 120\n\tprefer_grpc: bool = True\n\tchunking: EmbeddingChunkingSchema = EmbeddingChunkingSchema()\n\tclustering: EmbeddingClusteringSchema = EmbeddingClusteringSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str = 'minishlab/potion-base-8M'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.dimension","title":"dimension  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dimension: int = 256\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.dimension_metric","title":"dimension_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dimension_metric: str = 'cosine'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.max_content_length","title":"max_content_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_content_length: int = 5000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.qdrant_batch_size","title":"qdrant_batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>qdrant_batch_size: int = 1000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.url","title":"url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>url: str = 'http://localhost:6333'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: str | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: int = 120\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.prefer_grpc","title":"prefer_grpc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prefer_grpc: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.chunking","title":"chunking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>chunking: EmbeddingChunkingSchema = (\n\tEmbeddingChunkingSchema()\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.EmbeddingSchema.clustering","title":"clustering  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clustering: EmbeddingClusteringSchema = (\n\tEmbeddingClusteringSchema()\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.RAGSchema","title":"RAGSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the RAG.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class RAGSchema(BaseModel):\n\t\"\"\"Configuration for the RAG.\"\"\"\n\n\tmax_context_length: int = 10000\n\tmax_context_results: int = 10\n\tsimilarity_threshold: float = 0.75\n\tsystem_prompt: str | None = None\n\tinclude_file_content: bool = True\n\tinclude_metadata: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.RAGSchema.max_context_length","title":"max_context_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_context_length: int = 10000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.RAGSchema.max_context_results","title":"max_context_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_context_results: int = 10\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.RAGSchema.similarity_threshold","title":"similarity_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>similarity_threshold: float = 0.75\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.RAGSchema.system_prompt","title":"system_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>system_prompt: str | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.RAGSchema.include_file_content","title":"include_file_content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_file_content: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.RAGSchema.include_metadata","title":"include_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_metadata: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.SyncSchema","title":"SyncSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the sync.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class SyncSchema(BaseModel):\n\t\"\"\"Configuration for the sync.\"\"\"\n\n\texclude_patterns: list[str] = Field(\n\t\tdefault_factory=lambda: [\n\t\t\tr\"^node_modules/\",\n\t\t\tr\"^\\.venv/\",\n\t\t\tr\"^venv/\",\n\t\t\tr\"^env/\",\n\t\t\tr\"^__pycache__/\",\n\t\t\tr\"^\\.mypy_cache/\",\n\t\t\tr\"^\\.pytest_cache/\",\n\t\t\tr\"^\\.ruff_cache/\",\n\t\t\tr\"^dist/\",\n\t\t\tr\"^build/\",\n\t\t\tr\"^\\.git/\",\n\t\t\tr\"\\\\.pyc$\",\n\t\t\tr\"\\\\.pyo$\",\n\t\t\tr\"\\\\.so$\",\n\t\t\tr\"\\\\.dll$\",\n\t\t]\n\t)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.SyncSchema.exclude_patterns","title":"exclude_patterns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exclude_patterns: list[str] = Field(\n\tdefault_factory=lambda: [\n\t\t\"^node_modules/\",\n\t\t\"^\\\\.venv/\",\n\t\t\"^venv/\",\n\t\t\"^env/\",\n\t\t\"^__pycache__/\",\n\t\t\"^\\\\.mypy_cache/\",\n\t\t\"^\\\\.pytest_cache/\",\n\t\t\"^\\\\.ruff_cache/\",\n\t\t\"^dist/\",\n\t\t\"^build/\",\n\t\t\"^\\\\.git/\",\n\t\t\"\\\\\\\\.pyc$\",\n\t\t\"\\\\\\\\.pyo$\",\n\t\t\"\\\\\\\\.so$\",\n\t\t\"\\\\\\\\.dll$\",\n\t]\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema","title":"GenSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the gen command.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class GenSchema(BaseModel):\n\t\"\"\"Configuration for the gen command.\"\"\"\n\n\tmax_content_length: int = 5000\n\tuse_gitignore: bool = True\n\toutput_dir: str = \"documentation\"\n\tinclude_tree: bool = True\n\tinclude_entity_graph: bool = True\n\tsemantic_analysis: bool = True\n\tlod_level: LODLevel = LODLevel.DOCS\n\tmermaid_entities: list[str] = Field(\n\t\tdefault_factory=lambda: [\"module\", \"class\", \"function\", \"method\", \"constant\", \"variable\", \"import\"]\n\t)\n\tmermaid_relationships: list[str] = Field(default_factory=lambda: [\"declares\", \"imports\", \"calls\"])\n\tmermaid_show_legend: bool = True\n\tmermaid_remove_unconnected: bool = False\n\tmermaid_styled: bool = True\n\n\t@field_validator(\"lod_level\", mode=\"before\")\n\t@classmethod\n\tdef validate_lod_level(cls, v: LODLevel | str | int) -&gt; LODLevel:\n\t\t\"\"\"Convert string values to LODLevel enum.\"\"\"\n\t\tif isinstance(v, LODLevel):\n\t\t\treturn v\n\t\tif isinstance(v, str):\n\t\t\t# Direct mapping approach to avoid enum access issues\n\t\t\tlevel_map = {\n\t\t\t\t\"signatures\": LODLevel.SIGNATURES,\n\t\t\t\t\"structure\": LODLevel.STRUCTURE,\n\t\t\t\t\"docs\": LODLevel.DOCS,\n\t\t\t\t\"skeleton\": LODLevel.SKELETON,\n\t\t\t\t\"full\": LODLevel.FULL,\n\t\t\t\t\"1\": LODLevel.SIGNATURES,\n\t\t\t\t\"2\": LODLevel.STRUCTURE,\n\t\t\t\t\"3\": LODLevel.DOCS,\n\t\t\t\t\"4\": LODLevel.SKELETON,\n\t\t\t\t\"5\": LODLevel.FULL,\n\t\t\t}\n\t\t\t# Try lowercase first, then original case\n\t\t\tif v.lower() in level_map:\n\t\t\t\treturn level_map[v.lower()]\n\t\t\tif v in level_map:\n\t\t\t\treturn level_map[v]\n\t\t\tvalid_values = list(level_map.keys())\n\t\t\tmsg = f\"Invalid lod_level '{v}'. Valid values are: {valid_values}\"\n\t\t\traise ValueError(msg)\n\t\tif isinstance(v, int):\n\t\t\t# Handle numeric values\n\t\t\ttry:\n\t\t\t\treturn LODLevel(v)\n\t\t\texcept ValueError:\n\t\t\t\tvalid_numbers = [1, 2, 3, 4, 5]\n\t\t\t\tmsg = f\"Invalid lod_level number '{v}'. Valid numbers are: {valid_numbers}\"\n\t\t\t\traise ValueError(msg) from None\n\t\tmsg = f\"lod_level must be a string, number, or LODLevel enum, got {type(v)}\"\n\t\traise ValueError(msg)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.max_content_length","title":"max_content_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_content_length: int = 5000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.use_gitignore","title":"use_gitignore  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_gitignore: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.output_dir","title":"output_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_dir: str = 'documentation'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.include_tree","title":"include_tree  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_tree: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.include_entity_graph","title":"include_entity_graph  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_entity_graph: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.semantic_analysis","title":"semantic_analysis  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>semantic_analysis: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.lod_level","title":"lod_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lod_level: LODLevel = DOCS\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.mermaid_entities","title":"mermaid_entities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_entities: list[str] = Field(\n\tdefault_factory=lambda: [\n\t\t\"module\",\n\t\t\"class\",\n\t\t\"function\",\n\t\t\"method\",\n\t\t\"constant\",\n\t\t\"variable\",\n\t\t\"import\",\n\t]\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.mermaid_relationships","title":"mermaid_relationships  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_relationships: list[str] = Field(\n\tdefault_factory=lambda: [\"declares\", \"imports\", \"calls\"]\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.mermaid_show_legend","title":"mermaid_show_legend  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_show_legend: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.mermaid_remove_unconnected","title":"mermaid_remove_unconnected  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_remove_unconnected: bool = False\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.mermaid_styled","title":"mermaid_styled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_styled: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GenSchema.validate_lod_level","title":"validate_lod_level  <code>classmethod</code>","text":"<pre><code>validate_lod_level(v: LODLevel | str | int) -&gt; LODLevel\n</code></pre> <p>Convert string values to LODLevel enum.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>@field_validator(\"lod_level\", mode=\"before\")\n@classmethod\ndef validate_lod_level(cls, v: LODLevel | str | int) -&gt; LODLevel:\n\t\"\"\"Convert string values to LODLevel enum.\"\"\"\n\tif isinstance(v, LODLevel):\n\t\treturn v\n\tif isinstance(v, str):\n\t\t# Direct mapping approach to avoid enum access issues\n\t\tlevel_map = {\n\t\t\t\"signatures\": LODLevel.SIGNATURES,\n\t\t\t\"structure\": LODLevel.STRUCTURE,\n\t\t\t\"docs\": LODLevel.DOCS,\n\t\t\t\"skeleton\": LODLevel.SKELETON,\n\t\t\t\"full\": LODLevel.FULL,\n\t\t\t\"1\": LODLevel.SIGNATURES,\n\t\t\t\"2\": LODLevel.STRUCTURE,\n\t\t\t\"3\": LODLevel.DOCS,\n\t\t\t\"4\": LODLevel.SKELETON,\n\t\t\t\"5\": LODLevel.FULL,\n\t\t}\n\t\t# Try lowercase first, then original case\n\t\tif v.lower() in level_map:\n\t\t\treturn level_map[v.lower()]\n\t\tif v in level_map:\n\t\t\treturn level_map[v]\n\t\tvalid_values = list(level_map.keys())\n\t\tmsg = f\"Invalid lod_level '{v}'. Valid values are: {valid_values}\"\n\t\traise ValueError(msg)\n\tif isinstance(v, int):\n\t\t# Handle numeric values\n\t\ttry:\n\t\t\treturn LODLevel(v)\n\t\texcept ValueError:\n\t\t\tvalid_numbers = [1, 2, 3, 4, 5]\n\t\t\tmsg = f\"Invalid lod_level number '{v}'. Valid numbers are: {valid_numbers}\"\n\t\t\traise ValueError(msg) from None\n\tmsg = f\"lod_level must be a string, number, or LODLevel enum, got {type(v)}\"\n\traise ValueError(msg)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.WatcherSchema","title":"WatcherSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the watcher.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class WatcherSchema(BaseModel):\n\t\"\"\"Configuration for the watcher.\"\"\"\n\n\tenabled: bool = True\n\tdebounce_delay: float = 1.0\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.WatcherSchema.enabled","title":"enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>enabled: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.WatcherSchema.debounce_delay","title":"debounce_delay  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debounce_delay: float = 1.0\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.ProcessorSchema","title":"ProcessorSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the processor.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class ProcessorSchema(BaseModel):\n\t\"\"\"Configuration for the processor.\"\"\"\n\n\tenabled: bool = True\n\tmax_workers: int = 4\n\tignored_patterns: list[str] = Field(\n\t\tdefault_factory=lambda: [\n\t\t\t\"**/.git/**\",\n\t\t\t\"**/__pycache__/**\",\n\t\t\t\"**/.venv/**\",\n\t\t\t\"**/node_modules/**\",\n\t\t\t\"**/*.pyc\",\n\t\t\t\"**/dist/**\",\n\t\t\t\"**/build/**\",\n\t\t]\n\t)\n\twatcher: WatcherSchema = WatcherSchema()\n\tdefault_lod_level: str = \"signatures\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.ProcessorSchema.enabled","title":"enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>enabled: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.ProcessorSchema.max_workers","title":"max_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_workers: int = 4\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.ProcessorSchema.ignored_patterns","title":"ignored_patterns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ignored_patterns: list[str] = Field(\n\tdefault_factory=lambda: [\n\t\t\"**/.git/**\",\n\t\t\"**/__pycache__/**\",\n\t\t\"**/.venv/**\",\n\t\t\"**/node_modules/**\",\n\t\t\"**/*.pyc\",\n\t\t\"**/dist/**\",\n\t\t\"**/build/**\",\n\t]\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.ProcessorSchema.watcher","title":"watcher  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>watcher: WatcherSchema = WatcherSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.ProcessorSchema.default_lod_level","title":"default_lod_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_lod_level: str = 'signatures'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema","title":"DiffSplitterSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the diff splitter.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class DiffSplitterSchema(BaseModel):\n\t\"\"\"Configuration for the diff splitter.\"\"\"\n\n\tsimilarity_threshold: float = 0.6\n\tdirectory_similarity_threshold: float = 0.3\n\tfile_move_similarity_threshold: float = 0.85\n\tmin_chunks_for_consolidation: int = 2\n\tmax_chunks_before_consolidation: int = 20\n\tmax_file_size_for_llm: int = 50000\n\tmax_log_diff_size: int = 1000\n\tdefault_code_extensions: list[str] = Field(\n\t\tdefault_factory=lambda: [\n\t\t\t\"js\",\n\t\t\t\"jsx\",\n\t\t\t\"ts\",\n\t\t\t\"tsx\",\n\t\t\t\"py\",\n\t\t\t\"java\",\n\t\t\t\"c\",\n\t\t\t\"cpp\",\n\t\t\t\"h\",\n\t\t\t\"hpp\",\n\t\t\t\"cc\",\n\t\t\t\"cs\",\n\t\t\t\"go\",\n\t\t\t\"rb\",\n\t\t\t\"php\",\n\t\t\t\"rs\",\n\t\t\t\"swift\",\n\t\t\t\"scala\",\n\t\t\t\"kt\",\n\t\t\t\"sh\",\n\t\t\t\"pl\",\n\t\t\t\"pm\",\n\t\t]\n\t)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.similarity_threshold","title":"similarity_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>similarity_threshold: float = 0.6\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.directory_similarity_threshold","title":"directory_similarity_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>directory_similarity_threshold: float = 0.3\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.file_move_similarity_threshold","title":"file_move_similarity_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_move_similarity_threshold: float = 0.85\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.min_chunks_for_consolidation","title":"min_chunks_for_consolidation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_chunks_for_consolidation: int = 2\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.max_chunks_before_consolidation","title":"max_chunks_before_consolidation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_chunks_before_consolidation: int = 20\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.max_file_size_for_llm","title":"max_file_size_for_llm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_file_size_for_llm: int = 50000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.max_log_diff_size","title":"max_log_diff_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_log_diff_size: int = 1000\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.DiffSplitterSchema.default_code_extensions","title":"default_code_extensions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_code_extensions: list[str] = Field(\n\tdefault_factory=lambda: [\n\t\t\"js\",\n\t\t\"jsx\",\n\t\t\"ts\",\n\t\t\"tsx\",\n\t\t\"py\",\n\t\t\"java\",\n\t\t\"c\",\n\t\t\"cpp\",\n\t\t\"h\",\n\t\t\"hpp\",\n\t\t\"cc\",\n\t\t\"cs\",\n\t\t\"go\",\n\t\t\"rb\",\n\t\t\"php\",\n\t\t\"rs\",\n\t\t\"swift\",\n\t\t\"scala\",\n\t\t\"kt\",\n\t\t\"sh\",\n\t\t\"pl\",\n\t\t\"pm\",\n\t]\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitConventionSchema","title":"CommitConventionSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the commit convention.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class CommitConventionSchema(BaseModel):\n\t\"\"\"Configuration for the commit convention.\"\"\"\n\n\ttypes: list[str] = Field(\n\t\tdefault_factory=lambda: [\n\t\t\t\"feat\",\n\t\t\t\"fix\",\n\t\t\t\"docs\",\n\t\t\t\"style\",\n\t\t\t\"refactor\",\n\t\t\t\"perf\",\n\t\t\t\"test\",\n\t\t\t\"build\",\n\t\t\t\"ci\",\n\t\t\t\"chore\",\n\t\t]\n\t)\n\tscopes: list[str] = Field(default_factory=list)\n\tmax_length: int = 72\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitConventionSchema.types","title":"types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>types: list[str] = Field(\n\tdefault_factory=lambda: [\n\t\t\"feat\",\n\t\t\"fix\",\n\t\t\"docs\",\n\t\t\"style\",\n\t\t\"refactor\",\n\t\t\"perf\",\n\t\t\"test\",\n\t\t\"build\",\n\t\t\"ci\",\n\t\t\"chore\",\n\t]\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitConventionSchema.scopes","title":"scopes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scopes: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitConventionSchema.max_length","title":"max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_length: int = 72\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LintRuleSchema","title":"LintRuleSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the lint rule.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class LintRuleSchema(BaseModel):\n\t\"\"\"Configuration for the lint rule.\"\"\"\n\n\tlevel: str  # \"ERROR\", \"WARNING\", \"DISABLED\"\n\trule: str  # \"always\", \"never\"\n\tvalue: Any | None = None  # Can be int, str, list of str\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LintRuleSchema.level","title":"level  <code>instance-attribute</code>","text":"<pre><code>level: str\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LintRuleSchema.rule","title":"rule  <code>instance-attribute</code>","text":"<pre><code>rule: str\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.LintRuleSchema.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value: Any | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema","title":"CommitLintSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the commit lint.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class CommitLintSchema(BaseModel):\n\t\"\"\"Configuration for the commit lint.\"\"\"\n\n\theader_max_length: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"always\", value=100)\n\t)\n\theader_case: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"DISABLED\", rule=\"always\", value=\"lower-case\")\n\t)\n\theader_full_stop: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"never\", value=\".\")\n\t)\n\ttype_enum: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"always\"))\n\ttype_case: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"always\", value=\"lower-case\")\n\t)\n\ttype_empty: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"never\"))\n\tscope_case: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"always\", value=\"lower-case\")\n\t)\n\tscope_empty: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"DISABLED\", rule=\"never\"))\n\tscope_enum: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"DISABLED\", rule=\"always\"))\n\tsubject_case: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(\n\t\t\tlevel=\"ERROR\", rule=\"never\", value=[\"sentence-case\", \"start-case\", \"pascal-case\", \"upper-case\"]\n\t\t)\n\t)\n\tsubject_empty: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"never\"))\n\tsubject_full_stop: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"never\", value=\".\")\n\t)\n\tsubject_exclamation_mark: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"DISABLED\", rule=\"never\")\n\t)\n\tbody_leading_blank: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"WARNING\", rule=\"always\"))\n\tbody_empty: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"DISABLED\", rule=\"never\"))\n\tbody_max_line_length: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"always\", value=100)\n\t)\n\tfooter_leading_blank: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"WARNING\", rule=\"always\"))\n\tfooter_empty: LintRuleSchema = Field(default_factory=lambda: LintRuleSchema(level=\"DISABLED\", rule=\"never\"))\n\tfooter_max_line_length: LintRuleSchema = Field(\n\t\tdefault_factory=lambda: LintRuleSchema(level=\"ERROR\", rule=\"always\", value=100)\n\t)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.header_max_length","title":"header_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_max_length: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"always\", value=100\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.header_case","title":"header_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_case: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"DISABLED\", rule=\"always\", value=\"lower-case\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.header_full_stop","title":"header_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_full_stop: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"never\", value=\".\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.type_enum","title":"type_enum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_enum: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"always\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.type_case","title":"type_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_case: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"always\", value=\"lower-case\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.type_empty","title":"type_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_empty: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"never\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.scope_case","title":"scope_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_case: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"always\", value=\"lower-case\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.scope_empty","title":"scope_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_empty: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"DISABLED\", rule=\"never\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.scope_enum","title":"scope_enum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_enum: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"DISABLED\", rule=\"always\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.subject_case","title":"subject_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_case: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\",\n\t\trule=\"never\",\n\t\tvalue=[\n\t\t\t\"sentence-case\",\n\t\t\t\"start-case\",\n\t\t\t\"pascal-case\",\n\t\t\t\"upper-case\",\n\t\t],\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.subject_empty","title":"subject_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_empty: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"never\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.subject_full_stop","title":"subject_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_full_stop: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"never\", value=\".\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.subject_exclamation_mark","title":"subject_exclamation_mark  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_exclamation_mark: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"DISABLED\", rule=\"never\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.body_leading_blank","title":"body_leading_blank  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_leading_blank: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"WARNING\", rule=\"always\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.body_empty","title":"body_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_empty: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"DISABLED\", rule=\"never\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.body_max_line_length","title":"body_max_line_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_max_line_length: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"always\", value=100\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.footer_leading_blank","title":"footer_leading_blank  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_leading_blank: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"WARNING\", rule=\"always\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.footer_empty","title":"footer_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_empty: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"DISABLED\", rule=\"never\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitLintSchema.footer_max_line_length","title":"footer_max_line_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_max_line_length: LintRuleSchema = Field(\n\tdefault_factory=lambda: LintRuleSchema(\n\t\tlevel=\"ERROR\", rule=\"always\", value=100\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema","title":"CommitSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the commit.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class CommitSchema(BaseModel):\n\t\"\"\"Configuration for the commit.\"\"\"\n\n\tstrategy: str = \"file\"\n\tbypass_hooks: bool = False\n\tuse_lod_context: bool = True\n\tis_non_interactive: bool = False\n\tdiff_splitter: DiffSplitterSchema = DiffSplitterSchema()\n\tconvention: CommitConventionSchema = CommitConventionSchema()\n\tlint: CommitLintSchema = CommitLintSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: str = 'file'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema.bypass_hooks","title":"bypass_hooks  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bypass_hooks: bool = False\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema.use_lod_context","title":"use_lod_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_lod_context: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema.is_non_interactive","title":"is_non_interactive  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_non_interactive: bool = False\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema.diff_splitter","title":"diff_splitter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>diff_splitter: DiffSplitterSchema = DiffSplitterSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema.convention","title":"convention  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>convention: CommitConventionSchema = (\n\tCommitConventionSchema()\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.CommitSchema.lint","title":"lint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lint: CommitLintSchema = CommitLintSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRDefaultsSchema","title":"PRDefaultsSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the pull request defaults.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class PRDefaultsSchema(BaseModel):\n\t\"\"\"Configuration for the pull request defaults.\"\"\"\n\n\tbase_branch: str | None = None\n\tfeature_prefix: str = \"feature/\"\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRDefaultsSchema.base_branch","title":"base_branch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_branch: str | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRDefaultsSchema.feature_prefix","title":"feature_prefix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_prefix: str = 'feature/'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingDetailSchema","title":"PRBranchMappingDetailSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the pull request branch mapping detail.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class PRBranchMappingDetailSchema(BaseModel):\n\t\"\"\"Configuration for the pull request branch mapping detail.\"\"\"\n\n\tbase: str\n\tprefix: str\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingDetailSchema.base","title":"base  <code>instance-attribute</code>","text":"<pre><code>base: str\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingDetailSchema.prefix","title":"prefix  <code>instance-attribute</code>","text":"<pre><code>prefix: str\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingSchema","title":"PRBranchMappingSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the pull request branch mapping.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class PRBranchMappingSchema(BaseModel):\n\t\"\"\"Configuration for the pull request branch mapping.\"\"\"\n\n\tfeature: PRBranchMappingDetailSchema = Field(\n\t\tdefault_factory=lambda: PRBranchMappingDetailSchema(base=\"develop\", prefix=\"feature/\")\n\t)\n\trelease: PRBranchMappingDetailSchema = Field(\n\t\tdefault_factory=lambda: PRBranchMappingDetailSchema(base=\"main\", prefix=\"release/\")\n\t)\n\thotfix: PRBranchMappingDetailSchema = Field(\n\t\tdefault_factory=lambda: PRBranchMappingDetailSchema(base=\"main\", prefix=\"hotfix/\")\n\t)\n\tbugfix: PRBranchMappingDetailSchema = Field(\n\t\tdefault_factory=lambda: PRBranchMappingDetailSchema(base=\"develop\", prefix=\"bugfix/\")\n\t)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingSchema.feature","title":"feature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature: PRBranchMappingDetailSchema = Field(\n\tdefault_factory=lambda: PRBranchMappingDetailSchema(\n\t\tbase=\"develop\", prefix=\"feature/\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingSchema.release","title":"release  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>release: PRBranchMappingDetailSchema = Field(\n\tdefault_factory=lambda: PRBranchMappingDetailSchema(\n\t\tbase=\"main\", prefix=\"release/\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingSchema.hotfix","title":"hotfix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hotfix: PRBranchMappingDetailSchema = Field(\n\tdefault_factory=lambda: PRBranchMappingDetailSchema(\n\t\tbase=\"main\", prefix=\"hotfix/\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRBranchMappingSchema.bugfix","title":"bugfix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bugfix: PRBranchMappingDetailSchema = Field(\n\tdefault_factory=lambda: PRBranchMappingDetailSchema(\n\t\tbase=\"develop\", prefix=\"bugfix/\"\n\t)\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRGenerateSchema","title":"PRGenerateSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the pull request generate.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class PRGenerateSchema(BaseModel):\n\t\"\"\"Configuration for the pull request generate.\"\"\"\n\n\ttitle_strategy: Literal[\"commits\", \"llm\"] = \"llm\"\n\tdescription_strategy: Literal[\"commits\", \"llm\"] = \"llm\"\n\tdescription_template: str = Field(\n\t\tdefault_factory=lambda: dedent(\n\t\t\t\"\"\"\n            ## Changes\n            {changes}\n\n            ## Testing\n            {testing_instructions}\n\n            ## Screenshots\n            {screenshots}\n            \"\"\"\n\t\t).strip()\n\t)\n\tuse_workflow_templates: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRGenerateSchema.title_strategy","title":"title_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>title_strategy: Literal['commits', 'llm'] = 'llm'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRGenerateSchema.description_strategy","title":"description_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description_strategy: Literal['commits', 'llm'] = 'llm'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRGenerateSchema.description_template","title":"description_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description_template: str = Field(\n\tdefault_factory=lambda: strip()\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRGenerateSchema.use_workflow_templates","title":"use_workflow_templates  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_workflow_templates: bool = True\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRSchema","title":"PRSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the pull request.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class PRSchema(BaseModel):\n\t\"\"\"Configuration for the pull request.\"\"\"\n\n\tdefaults: PRDefaultsSchema = Field(default_factory=PRDefaultsSchema)\n\tstrategy: str = \"github-flow\"\n\tbranch_mapping: PRBranchMappingSchema = Field(default_factory=PRBranchMappingSchema)\n\tgenerate: PRGenerateSchema = Field(default_factory=PRGenerateSchema)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRSchema.defaults","title":"defaults  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>defaults: PRDefaultsSchema = Field(\n\tdefault_factory=PRDefaultsSchema\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRSchema.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: str = 'github-flow'\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRSchema.branch_mapping","title":"branch_mapping  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>branch_mapping: PRBranchMappingSchema = Field(\n\tdefault_factory=PRBranchMappingSchema\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.PRSchema.generate","title":"generate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>generate: PRGenerateSchema = Field(\n\tdefault_factory=PRGenerateSchema\n)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AskSchema","title":"AskSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the ask command.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class AskSchema(BaseModel):\n\t\"\"\"Configuration for the ask command.\"\"\"\n\n\tinteractive_chat: bool = False\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AskSchema.interactive_chat","title":"interactive_chat  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interactive_chat: bool = False\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GitHubConfigSchema","title":"GitHubConfigSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for GitHub integration (PRs, API, etc).</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class GitHubConfigSchema(BaseModel):\n\t\"\"\"Configuration for GitHub integration (PRs, API, etc).\"\"\"\n\n\ttoken: str | None = None  # OAuth token for GitHub API\n\trepo: str | None = None  # Optional: default repo (e.g., user/repo)\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GitHubConfigSchema.token","title":"token  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>token: str | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.GitHubConfigSchema.repo","title":"repo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repo: str | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema","title":"AppConfigSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the application.</p> Source code in <code>src/codemap/config/config_schema.py</code> <pre><code>class AppConfigSchema(BaseModel):\n\t\"\"\"Configuration for the application.\"\"\"\n\n\tllm: LLMSchema = LLMSchema()\n\tembedding: EmbeddingSchema = EmbeddingSchema()\n\trag: RAGSchema = RAGSchema()\n\tsync: SyncSchema = SyncSchema()\n\tgen: GenSchema = GenSchema()\n\tprocessor: ProcessorSchema = ProcessorSchema()\n\tcommit: CommitSchema = CommitSchema()\n\tpr: PRSchema = PRSchema()\n\task: AskSchema = AskSchema()\n\tgithub: GitHubConfigSchema = GitHubConfigSchema()\n\trepo_root: Path | None = None\n\n\tmodel_config = {\n\t\t\"validate_assignment\": True  # Useful for ensuring type checks on assignment if loaded config is modified\n\t}\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.llm","title":"llm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>llm: LLMSchema = LLMSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.embedding","title":"embedding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>embedding: EmbeddingSchema = EmbeddingSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.rag","title":"rag  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rag: RAGSchema = RAGSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.sync","title":"sync  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sync: SyncSchema = SyncSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.gen","title":"gen  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gen: GenSchema = GenSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.processor","title":"processor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>processor: ProcessorSchema = ProcessorSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.commit","title":"commit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>commit: CommitSchema = CommitSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.pr","title":"pr  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pr: PRSchema = PRSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.ask","title":"ask  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ask: AskSchema = AskSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.github","title":"github  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>github: GitHubConfigSchema = GitHubConfigSchema()\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.repo_root","title":"repo_root  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repo_root: Path | None = None\n</code></pre>"},{"location":"api/config/config_schema/#codemap.config.config_schema.AppConfigSchema.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = {'validate_assignment': True}\n</code></pre>"},{"location":"api/config/default_config_template/","title":"Default Config Template","text":"<p>Default configuration template for CodeMap.</p>"},{"location":"api/config/default_config_template/#codemap.config.default_config_template.DEFAULT_CONFIG_TEMPLATE","title":"DEFAULT_CONFIG_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONFIG_TEMPLATE = '# CodeMap Configuration File\\n# -------------------------\\n# This file configures CodeMap\\'s behavior. Uncomment and modify settings as needed.\\n\\n# LLM Configuration - Controls which model is used for AI operations\\nllm:\\n  # Format: \"provider:model-name\", e.g., \"openai:gpt-4o\", \"anthropic:claude-3-opus\"\\n  model: \"openai:gpt-4o-mini\"\\n  temperature: 0.5  # Lower for more deterministic outputs, higher for creativity\\n  max_output_tokens: 1024  # Maximum tokens in responses\\n\\n# Embedding Configuration - Controls vector embedding behavior\\nembedding:\\n  # Recommended models: \"minishlab/potion-base-8M3\", Only Model2Vec static models are supported\\n  model_name: \"minishlab/potion-base-8M\"\\n  # dimension: 256\\n  # dimension_metric: \"cosine\" # Metric for dimension calculation (e.g., \"cosine\", \"euclidean\")\\n  # max_retries: 3 # Maximum retries for embedding requests\\n  # retry_delay: 5 # Delay in seconds between retries\\n  # max_content_length: 5000  # Maximum characters per file chunk\\n  # Qdrant (Vector DB) settings\\n  # qdrant_batch_size: 100 # Batch size for Qdrant uploads\\n  # url: \"http://localhost:6333\" # Qdrant server URL\\n  # timeout: 30 # Qdrant client timeout in seconds\\n  # prefer_grpc: true # Prefer gRPC for Qdrant communication\\n\\n  # Advanced chunking settings - controls how code is split\\n  # chunking:\\n  #   max_hierarchy_depth: 2  # Maximum depth of code hierarchy to consider\\n  #   max_file_lines: 1000  # Maximum lines per file before splitting\\n\\n  # Clustering settings for embeddings\\n  # clustering:\\n  #   method: \"agglomerative\"  # Clustering method: \"agglomerative\", \"dbscan\"\\n  #   agglomerative: # Settings for Agglomerative Clustering\\n  #     metric: \"precomputed\" # Metric: \"cosine\", \"euclidean\", \"manhattan\", \"l1\", \"l2\", \"precomputed\"\\n  #     distance_threshold: 0.3 # Distance threshold for forming clusters\\n  #     linkage: \"complete\" # Linkage criterion: \"ward\", \"complete\", \"average\", \"single\"\\n  #   dbscan: # Settings for DBSCAN Clustering\\n  #     eps: 0.3 # The maximum distance between two samples for one to be considered as in the neighborhood of the other\\n  #     min_samples: 2 # The number of samples in a neighborhood for a point to be considered as a core point\\n  #     algorithm: \"auto\" # Algorithm to compute pointwise distances: \"auto\", \"ball_tree\", \"kd_tree\", \"brute\"\\n  #     metric: \"precomputed\" # Metric for distance computation: \"cityblock\", \"cosine\", \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"precomputed\"\\n\\n# RAG (Retrieval Augmented Generation) Configuration\\nrag:\\n  max_context_length: 8000  # Maximum context length for the LLM\\n  max_context_results: 10  # Maximum number of context results to return\\n  similarity_threshold: 0.75  # Minimum similarity score (0-1) for relevance\\n  # system_prompt: null # Optional system prompt to guide the RAG model (leave commented or set if needed)\\n  include_file_content: true  # Include file content in context\\n  include_metadata: true  # Include file metadata in context\\n\\n# Sync Configuration - Controls which files are excluded from processing\\nsync:\\n  exclude_patterns:\\n    - \"^node_modules/\"\\n    - \"^\\\\\\\\.venv/\"\\n    - \"^venv/\"\\n    - \"^env/\"\\n    - \"^__pycache__/\"\\n    - \"^\\\\\\\\.mypy_cache/\"\\n    - \"^\\\\\\\\.pytest_cache/\"\\n    - \"^\\\\\\\\.ruff_cache/\"\\n    - \"^dist/\"\\n    - \"^build/\"\\n    - \"^\\\\\\\\.git/\"\\n    - \"\\\\\\\\\\\\\\\\.pyc$\"\\n    - \"\\\\\\\\\\\\\\\\.pyo$\"\\n    - \"\\\\\\\\\\\\\\\\.so$\"\\n    - \"\\\\\\\\\\\\\\\\.dll$\"\\n\\n# Generation Configuration - Controls documentation generation\\ngen:\\n  max_content_length: 5000  # Maximum content length per file for generation\\n  use_gitignore: true  # Use .gitignore patterns to exclude files\\n  output_dir: \"documentation\"  # Directory to store generated documentation\\n  include_tree: true  # Include directory tree in output\\n  include_entity_graph: true  # Include entity relationship graph\\n  semantic_analysis: true  # Enable semantic analysis\\n  lod_level: \"docs\"  # Level of detail: \"signatures\", \"structure\", \"docs\", \"skeleton\", \"full\"\\n\\n  # Mermaid diagram configuration for entity graphs\\n  # mermaid_entities:\\n  #   - \"module\"\\n  #   - \"class\"\\n  #   - \"function\"\\n  #   - \"method\"\\n  #   - \"constant\"\\n  #   - \"variable\"\\n  #   - \"import\"\\n  # mermaid_relationships:\\n  #   - \"declares\"\\n  #   - \"imports\"\\n  #   - \"calls\"\\n  # mermaid_show_legend: true\\n  # mermaid_remove_unconnected: false  # Show isolated nodes\\n\\n# Processor Configuration - Controls code processing behavior\\nprocessor:\\n  enabled: true  # Enable the processor\\n  max_workers: 4  # Maximum number of parallel workers\\n  ignored_patterns:  # Patterns to ignore during processing\\n    - \"**/.git/**\"\\n    - \"**/__pycache__/**\"\\n    - \"**/.venv/**\"\\n    - \"**/node_modules/**\"\\n    - \"**/*.pyc\"\\n    - \"**/dist/**\"\\n    - \"**/build/**\"\\n  default_lod_level: \"signatures\"  # Default level of detail: \"signatures\", \"structure\", \"docs\", \"full\"\\n\\n  # File watcher configuration\\n  # watcher:\\n  #   enabled: true  # Enable file watching\\n  #   debounce_delay: 1.0  # Delay in seconds before processing changes\\n\\n# Commit Command Configuration\\ncommit:\\n  strategy: \"semantic\"  # Strategy for splitting diffs: \"file\", \"hunk\", \"semantic\"\\n  bypass_hooks: false  # Whether to bypass git hooks\\n  use_lod_context: true  # Use level of detail context\\n  is_non_interactive: false  # Run in non-interactive mode\\n\\n  # Diff splitter configuration\\n  # diff_splitter:\\n  #   similarity_threshold: 0.6  # Similarity threshold for grouping related changes\\n  #   directory_similarity_threshold: 0.3 # Threshold for considering directories similar (e.g., for renames)\\n  #   file_move_similarity_threshold: 0.85 # Threshold for detecting file moves/renames based on content\\n  #   min_chunks_for_consolidation: 2 # Minimum number of small chunks to consider for consolidation\\n  #   max_chunks_before_consolidation: 20 # Maximum number of chunks before forcing consolidation\\n  #   max_file_size_for_llm: 50000  # Maximum file size (bytes) for LLM processing of individual files\\n  #   max_log_diff_size: 1000 # Maximum size (lines) of diff log to pass to LLM for context\\n  #   default_code_extensions: # File extensions considered as code for semantic splitting\\n  #     - \"js\"\\n  #     - \"jsx\"\\n  #     - \"ts\"\\n  #     - \"tsx\"\\n  #     - \"py\"\\n  #     - \"java\"\\n  #     - \"c\"\\n  #     - \"cpp\"\\n  #     - \"h\"\\n  #     - \"hpp\"\\n  #     - \"cc\"\\n  #     - \"cs\"\\n  #     - \"go\"\\n  #     - \"rb\"\\n  #     - \"php\"\\n  #     - \"rs\"\\n  #     - \"swift\"\\n  #     - \"scala\"\\n  #     - \"kt\"\\n  #     - \"sh\"\\n  #     - \"pl\"\\n  #     - \"pm\"\\n\\n  # Commit convention configuration (Conventional Commits)\\n  convention:\\n    types: # Allowed commit types\\n      - \"feat\"\\n      - \"fix\"\\n      - \"docs\"\\n      - \"style\"\\n      - \"refactor\"\\n      - \"perf\"\\n      - \"test\"\\n      - \"build\"\\n      - \"ci\"\\n      - \"chore\"\\n    scopes: []  # Add project-specific scopes here, e.g., [\"api\", \"ui\", \"db\"]\\n    max_length: 72  # Maximum length of commit message header\\n\\n  # Commit linting configuration (based on conventional-changelog-lint rules)\\n  # lint:\\n  #   # Rules are defined as: {level: \"ERROR\"|\"WARNING\"|\"DISABLED\", rule: \"always\"|\"never\", value: &lt;specific_value_if_any&gt;}\\n  #   header_max_length:\\n  #     level: \"ERROR\"\\n  #     rule: \"always\"\\n  #     value: 100\\n  #   header_case: # e.g., \\'lower-case\\', \\'upper-case\\', \\'camel-case\\', etc.\\n  #     level: \"DISABLED\"\\n  #     rule: \"always\"\\n  #     value: \"lower-case\"\\n  #   header_full_stop:\\n  #     level: \"ERROR\"\\n  #     rule: \"never\"\\n  #     value: \".\"\\n  #   type_enum: # Types must be from the \\'convention.types\\' list\\n  #     level: \"ERROR\"\\n  #     rule: \"always\"\\n  #   type_case:\\n  #     level: \"ERROR\"\\n  #     rule: \"always\"\\n  #     value: \"lower-case\"\\n  #   type_empty:\\n  #     level: \"ERROR\"\\n  #     rule: \"never\"\\n  #   scope_case:\\n  #     level: \"ERROR\"\\n  #     rule: \"always\"\\n  #     value: \"lower-case\"\\n  #   scope_empty: # Set to \"ERROR\" if scopes are mandatory\\n  #     level: \"DISABLED\"\\n  #     rule: \"never\"\\n  #   scope_enum: # Scopes must be from the \\'convention.scopes\\' list if enabled\\n  #     level: \"DISABLED\"\\n  #     rule: \"always\"\\n  #     # value: [] # Add allowed scopes here if rule is \"always\" and level is not DISABLED\\n  #   subject_case: # Forbids specific cases in the subject\\n  #     level: \"ERROR\"\\n  #     rule: \"never\"\\n  #     value: [\"sentence-case\", \"start-case\", \"pascal-case\", \"upper-case\"]\\n  #   subject_empty:\\n  #     level: \"ERROR\"\\n  #     rule: \"never\"\\n  #   subject_full_stop:\\n  #     level: \"ERROR\"\\n  #     rule: \"never\"\\n  #     value: \".\"\\n  #   subject_exclamation_mark:\\n  #     level: \"DISABLED\"\\n  #     rule: \"never\"\\n  #   body_leading_blank: # Body must start with a blank line after subject\\n  #     level: \"WARNING\"\\n  #     rule: \"always\"\\n  #   body_empty:\\n  #     level: \"DISABLED\"\\n  #     rule: \"never\"\\n  #   body_max_line_length:\\n  #     level: \"ERROR\"\\n  #     rule: \"always\"\\n  #     value: 100\\n  #   footer_leading_blank: # Footer must start with a blank line after body\\n  #     level: \"WARNING\"\\n  #     rule: \"always\"\\n  #   footer_empty:\\n  #     level: \"DISABLED\"\\n  #     rule: \"never\"\\n  #   footer_max_line_length:\\n  #     level: \"ERROR\"\\n  #     rule: \"always\"\\n  #     value: 100\\n\\n# Pull Request Configuration\\npr:\\n  defaults:\\n    base_branch: null  # Default base branch (null = auto-detect, e.g., main, master, develop)\\n    feature_prefix: \"feature/\"  # Default feature branch prefix\\n\\n  strategy: \"github-flow\"  # Git workflow: \"github-flow\", \"gitflow\", \"trunk-based\"\\n\\n  # Branch mapping for different PR types (primarily used in gitflow strategy)\\n  # branch_mapping:\\n  #   feature:\\n  #     base: \"develop\"\\n  #     prefix: \"feature/\"\\n  #   release:\\n  #     base: \"main\"\\n  #     prefix: \"release/\"\\n  #   hotfix:\\n  #     base: \"main\"\\n  #     prefix: \"hotfix/\"\\n  #   bugfix:\\n  #     base: \"develop\"\\n  #     prefix: \"bugfix/\"\\n\\n  # PR generation configuration\\n  generate:\\n    title_strategy: \"llm\"  # Strategy for generating PR titles: \"commits\" (from commit messages), \"llm\" (AI generated)\\n    description_strategy: \"llm\"  # Strategy for descriptions: \"commits\", \"llm\"\\n    # description_template: | # Template for PR description when using \\'llm\\' strategy. Placeholders: {changes}, {testing_instructions}, {screenshots}\\n    #   ## Changes\\n    #   {changes}\\n    #\\n    #   ## Testing\\n    #   {testing_instructions}\\n    #\\n    #   ## Screenshots\\n    #   {screenshots}\\n    use_workflow_templates: true  # Use workflow-specific templates if available (e.g., for GitHub PR templates)\\n\\n# Ask Command Configuration\\nask:\\n  interactive_chat: false  # Enable interactive chat mode for the \\'ask\\' command\\n'\n</code></pre>"},{"location":"api/db/","title":"Db Overview","text":"<p>Database management utilities using SQLModel.</p> <ul> <li>Client - Client interface for interacting with the database in CodeMap.</li> <li>Engine - Handles SQLModel engine creation and session management for CodeMap.</li> <li>Models - Database models for CodeMap using SQLModel.</li> </ul>"},{"location":"api/db/client/","title":"Client","text":"<p>Client interface for interacting with the database in CodeMap.</p>"},{"location":"api/db/client/#codemap.db.client.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient","title":"DatabaseClient","text":"<p>Provides high-level methods to interact with the CodeMap database.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>class DatabaseClient:\n\t\"\"\"Provides high-level methods to interact with the CodeMap database.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"\n\t\tInitializes the database client.\n\n\t\tIt sets up the client in an uninitialized state. The actual\n\t\tinitialization needs to be performed by calling the async initialize()\n\t\tmethod or waiting for _initialize_engine_if_needed to run when\n\t\trequired.\n\n\t\t\"\"\"\n\t\tself.engine: Engine | None = None  # Initialize engine as None\n\t\tself.initialized = False  # Flag to track initialization status\n\t\tself._init_task: asyncio.Task[None] | None = None  # Store reference to initialization task\n\n\t\t# Initialize engine in event loop if possible\n\t\ttry:\n\t\t\tif asyncio.get_event_loop().is_running():\n\t\t\t\tself._init_task = asyncio.create_task(self.initialize())\n\t\t\telse:\n\t\t\t\t# In sync context, create a new event loop to initialize\n\t\t\t\tloop = asyncio.new_event_loop()\n\t\t\t\tloop.run_until_complete(self.initialize())\n\t\t\t\tloop.close()\n\t\texcept RuntimeError:\n\t\t\t# No event loop available, will initialize on first use\n\t\t\tlogger.debug(\"No event loop available during DatabaseClient init, will initialize on demand\")\n\n\tasync def initialize(self) -&gt; None:\n\t\t\"\"\"\n\t\tAsynchronously initialize the database client.\n\n\t\tThis should be called after creating the client and before using it.\n\n\t\t\"\"\"\n\t\tawait self._initialize_engine()\n\t\tself.initialized = True\n\t\tlogger.info(\"Database client successfully initialized\")\n\n\tasync def _initialize_engine(self) -&gt; None:\n\t\t\"\"\"Asynchronously gets the engine and creates tables.\"\"\"\n\t\tif self.engine is None:\n\t\t\ttry:\n\t\t\t\tself.engine = await get_engine()  # Await the async function\n\t\t\t\t# create_db_and_tables is synchronous, run it after engine is ready\n\t\t\t\tcreate_db_and_tables(self.engine)\n\t\t\t\tlogger.info(\"Database client initialized with PostgreSQL engine.\")\n\t\t\texcept RuntimeError:\n\t\t\t\tlogger.exception(\"Failed to initialize database engine\")\n\t\t\t\t# Decide how to handle this error - maybe raise, maybe set engine to None?\n\t\t\t\t# For now, re-raising to make the failure explicit.\n\t\t\t\traise\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"An unexpected error occurred during database initialization\")\n\t\t\t\traise\n\n\tasync def _initialize_engine_if_needed(self) -&gt; None:\n\t\t\"\"\"Ensure engine is initialized before use.\"\"\"\n\t\tif not self.initialized or self.engine is None:\n\t\t\tawait self.initialize()\n\n\tasync def cleanup(self) -&gt; None:\n\t\t\"\"\"\n\t\tAsynchronously cleanup the database client resources.\n\n\t\tThis should be called before discarding the client.\n\n\t\t\"\"\"\n\t\tif self.engine:\n\t\t\t# No need to close Engine in SQLAlchemy 2.0, but dispose will close connections\n\t\t\tif hasattr(self.engine, \"dispose\"):\n\t\t\t\tself.engine.dispose()\n\t\t\tself.engine = None\n\t\tself.initialized = False\n\t\tlogger.info(\"Database client cleaned up\")\n\n\t# Ensure engine is initialized before DB operations\n\tasync def _ensure_engine_initialized(self) -&gt; None:\n\t\t\"\"\"Ensures the database engine is properly initialized.\n\n\t\tThis method checks if the engine is initialized and attempts to initialize it if not.\n\t\tIf initialization fails, it logs an error and raises a RuntimeError.\n\n\t\tRaises:\n\t\t    RuntimeError: If database client initialization fails after attempting to initialize.\n\n\t\tReturns:\n\t\t    None: This method doesn't return anything but ensures engine is ready for use.\n\t\t\"\"\"\n\t\tif not self.initialized or self.engine is None:\n\t\t\tawait self._initialize_engine_if_needed()\n\t\t\tif not self.initialized or self.engine is None:\n\t\t\t\tmsg = \"Database client initialization failed.\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise RuntimeError(msg)\n\n\tdef add_chat_message(\n\t\tself,\n\t\tsession_id: str,\n\t\tuser_query: str,\n\t\tai_response: str | None = None,\n\t\tcontext: str | None = None,\n\t\ttool_calls: str | None = None,\n\t) -&gt; ChatHistory:\n\t\t\"\"\"\n\t\tAdds a chat message to the history.\n\n\t\tArgs:\n\t\t    session_id (str): The session identifier.\n\t\t    user_query (str): The user's query.\n\t\t    ai_response (Optional[str]): The AI's response.\n\t\t    context (Optional[str]): JSON string of context used.\n\t\t    tool_calls (Optional[str]): JSON string of tool calls made.\n\n\t\tReturns:\n\t\t    ChatHistory: The newly created chat history record.\n\n\t\t\"\"\"\n\t\t# Ensure engine is initialized - run in a new event loop if needed\n\t\tif not self.initialized or self.engine is None:\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\ttry:\n\t\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\t\tfinally:\n\t\t\t\tloop.close()\n\n\t\tif self.engine is None:\n\t\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\t\tmsg = \"Database engine is not initialized after check.\"\n\t\t\tlogger.error(msg)\n\t\t\traise RuntimeError(msg)\n\n\t\tchat_entry = ChatHistory(\n\t\t\tsession_id=session_id,\n\t\t\tuser_query=user_query,\n\t\t\tai_response=ai_response,\n\t\t\tcontext=context,\n\t\t\ttool_calls=tool_calls,\n\t\t)\n\t\ttry:\n\t\t\twith get_session(self.engine) as session:\n\t\t\t\tsession.add(chat_entry)\n\t\t\t\tsession.commit()\n\t\t\t\tsession.refresh(chat_entry)\n\t\t\t\tlogger.debug(f\"Added chat message for session {session_id} to DB (ID: {chat_entry.id}).\")\n\t\t\t\treturn chat_entry\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error adding chat message\")\n\t\t\traise  # Re-raise after logging\n\n\tdef get_chat_history(self, session_id: str, limit: int = 50) -&gt; Sequence[ChatHistory]:\n\t\t\"\"\"\n\t\tRetrieves chat history for a session, ordered chronologically.\n\n\t\tArgs:\n\t\t    session_id (str): The session identifier.\n\t\t    limit (int): The maximum number of messages to return.\n\n\t\tReturns:\n\t\t    Sequence[ChatHistory]: A sequence of chat history records.\n\n\t\t\"\"\"\n\t\t# Ensure engine is initialized - run in a new event loop if needed\n\t\tif not self.initialized or self.engine is None:\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\ttry:\n\t\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\t\tfinally:\n\t\t\t\tloop.close()\n\n\t\tif self.engine is None:\n\t\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\t\tmsg = \"Database engine is not initialized after check.\"\n\t\t\tlogger.error(msg)\n\t\t\traise RuntimeError(msg)\n\n\t\tstatement = (\n\t\t\tselect(ChatHistory)\n\t\t\t.where(ChatHistory.session_id == session_id)\n\t\t\t# Using type ignore as the linter incorrectly flags the type\n\t\t\t.order_by(asc(ChatHistory.timestamp))  # type: ignore[arg-type]\n\t\t\t.limit(limit)\n\t\t)\n\t\ttry:\n\t\t\twith get_session(self.engine) as session:\n\t\t\t\tresults: Sequence[ChatHistory] = session.exec(statement).all()  # type: ignore[assignment]\n\t\t\t\tlogger.debug(f\"Retrieved {len(results)} messages for session {session_id}.\")\n\t\t\t\treturn results\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error retrieving chat history\")\n\t\t\traise\n\n\tdef update_chat_response(self, message_id: int, ai_response: str) -&gt; bool:\n\t\t\"\"\"\n\t\tUpdates the AI response for a specific chat message.\n\n\t\tArgs:\n\t\t        message_id (int): The primary key ID of the chat message to update.\n\t\t        ai_response (str): The new AI response text.\n\n\t\tReturns:\n\t\t        bool: True if the update was successful, False otherwise.\n\n\t\t\"\"\"\n\t\t# Ensure engine is initialized - run in a new event loop if needed\n\t\tif not self.initialized or self.engine is None:\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\ttry:\n\t\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\t\tfinally:\n\t\t\t\tloop.close()\n\n\t\tif self.engine is None:\n\t\t\tlogger.error(\"Cannot update chat response: Database engine not initialized.\")\n\t\t\treturn False\n\n\t\ttry:\n\t\t\twith get_session(self.engine) as session:\n\t\t\t\tdb_entry = session.get(ChatHistory, message_id)  # type: ignore[arg-type]\n\t\t\t\tif db_entry:\n\t\t\t\t\tdb_entry.ai_response = ai_response\n\t\t\t\t\tsession.commit()\n\t\t\t\t\tlogger.debug(f\"Updated DB entry {message_id} with AI response.\")\n\t\t\t\t\treturn True\n\t\t\t\tlogger.warning(f\"Chat message with ID {message_id} not found for update.\")\n\t\t\t\treturn False\n\t\texcept SQLAlchemyError:\n\t\t\tlogger.exception(f\"Database error updating chat response for message ID {message_id}\")\n\t\t\treturn False\n\t\texcept Exception:\n\t\t\tlogger.exception(f\"Unexpected error updating chat response for message ID {message_id}\")\n\t\t\treturn False\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initializes the database client.</p> <p>It sets up the client in an uninitialized state. The actual initialization needs to be performed by calling the async initialize() method or waiting for _initialize_engine_if_needed to run when required.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"\n\tInitializes the database client.\n\n\tIt sets up the client in an uninitialized state. The actual\n\tinitialization needs to be performed by calling the async initialize()\n\tmethod or waiting for _initialize_engine_if_needed to run when\n\trequired.\n\n\t\"\"\"\n\tself.engine: Engine | None = None  # Initialize engine as None\n\tself.initialized = False  # Flag to track initialization status\n\tself._init_task: asyncio.Task[None] | None = None  # Store reference to initialization task\n\n\t# Initialize engine in event loop if possible\n\ttry:\n\t\tif asyncio.get_event_loop().is_running():\n\t\t\tself._init_task = asyncio.create_task(self.initialize())\n\t\telse:\n\t\t\t# In sync context, create a new event loop to initialize\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\tloop.run_until_complete(self.initialize())\n\t\t\tloop.close()\n\texcept RuntimeError:\n\t\t# No event loop available, will initialize on first use\n\t\tlogger.debug(\"No event loop available during DatabaseClient init, will initialize on demand\")\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.engine","title":"engine  <code>instance-attribute</code>","text":"<pre><code>engine: Engine | None = None\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.initialized","title":"initialized  <code>instance-attribute</code>","text":"<pre><code>initialized = False\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize() -&gt; None\n</code></pre> <p>Asynchronously initialize the database client.</p> <p>This should be called after creating the client and before using it.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>async def initialize(self) -&gt; None:\n\t\"\"\"\n\tAsynchronously initialize the database client.\n\n\tThis should be called after creating the client and before using it.\n\n\t\"\"\"\n\tawait self._initialize_engine()\n\tself.initialized = True\n\tlogger.info(\"Database client successfully initialized\")\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Asynchronously cleanup the database client resources.</p> <p>This should be called before discarding the client.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>async def cleanup(self) -&gt; None:\n\t\"\"\"\n\tAsynchronously cleanup the database client resources.\n\n\tThis should be called before discarding the client.\n\n\t\"\"\"\n\tif self.engine:\n\t\t# No need to close Engine in SQLAlchemy 2.0, but dispose will close connections\n\t\tif hasattr(self.engine, \"dispose\"):\n\t\t\tself.engine.dispose()\n\t\tself.engine = None\n\tself.initialized = False\n\tlogger.info(\"Database client cleaned up\")\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.add_chat_message","title":"add_chat_message","text":"<pre><code>add_chat_message(\n\tsession_id: str,\n\tuser_query: str,\n\tai_response: str | None = None,\n\tcontext: str | None = None,\n\ttool_calls: str | None = None,\n) -&gt; ChatHistory\n</code></pre> <p>Adds a chat message to the history.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The session identifier.</p> required <code>user_query</code> <code>str</code> <p>The user's query.</p> required <code>ai_response</code> <code>Optional[str]</code> <p>The AI's response.</p> <code>None</code> <code>context</code> <code>Optional[str]</code> <p>JSON string of context used.</p> <code>None</code> <code>tool_calls</code> <code>Optional[str]</code> <p>JSON string of tool calls made.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChatHistory</code> <code>ChatHistory</code> <p>The newly created chat history record.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def add_chat_message(\n\tself,\n\tsession_id: str,\n\tuser_query: str,\n\tai_response: str | None = None,\n\tcontext: str | None = None,\n\ttool_calls: str | None = None,\n) -&gt; ChatHistory:\n\t\"\"\"\n\tAdds a chat message to the history.\n\n\tArgs:\n\t    session_id (str): The session identifier.\n\t    user_query (str): The user's query.\n\t    ai_response (Optional[str]): The AI's response.\n\t    context (Optional[str]): JSON string of context used.\n\t    tool_calls (Optional[str]): JSON string of tool calls made.\n\n\tReturns:\n\t    ChatHistory: The newly created chat history record.\n\n\t\"\"\"\n\t# Ensure engine is initialized - run in a new event loop if needed\n\tif not self.initialized or self.engine is None:\n\t\tloop = asyncio.new_event_loop()\n\t\ttry:\n\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\tfinally:\n\t\t\tloop.close()\n\n\tif self.engine is None:\n\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\tmsg = \"Database engine is not initialized after check.\"\n\t\tlogger.error(msg)\n\t\traise RuntimeError(msg)\n\n\tchat_entry = ChatHistory(\n\t\tsession_id=session_id,\n\t\tuser_query=user_query,\n\t\tai_response=ai_response,\n\t\tcontext=context,\n\t\ttool_calls=tool_calls,\n\t)\n\ttry:\n\t\twith get_session(self.engine) as session:\n\t\t\tsession.add(chat_entry)\n\t\t\tsession.commit()\n\t\t\tsession.refresh(chat_entry)\n\t\t\tlogger.debug(f\"Added chat message for session {session_id} to DB (ID: {chat_entry.id}).\")\n\t\t\treturn chat_entry\n\texcept Exception:\n\t\tlogger.exception(\"Error adding chat message\")\n\t\traise  # Re-raise after logging\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.get_chat_history","title":"get_chat_history","text":"<pre><code>get_chat_history(\n\tsession_id: str, limit: int = 50\n) -&gt; Sequence[ChatHistory]\n</code></pre> <p>Retrieves chat history for a session, ordered chronologically.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The session identifier.</p> required <code>limit</code> <code>int</code> <p>The maximum number of messages to return.</p> <code>50</code> <p>Returns:</p> Type Description <code>Sequence[ChatHistory]</code> <p>Sequence[ChatHistory]: A sequence of chat history records.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def get_chat_history(self, session_id: str, limit: int = 50) -&gt; Sequence[ChatHistory]:\n\t\"\"\"\n\tRetrieves chat history for a session, ordered chronologically.\n\n\tArgs:\n\t    session_id (str): The session identifier.\n\t    limit (int): The maximum number of messages to return.\n\n\tReturns:\n\t    Sequence[ChatHistory]: A sequence of chat history records.\n\n\t\"\"\"\n\t# Ensure engine is initialized - run in a new event loop if needed\n\tif not self.initialized or self.engine is None:\n\t\tloop = asyncio.new_event_loop()\n\t\ttry:\n\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\tfinally:\n\t\t\tloop.close()\n\n\tif self.engine is None:\n\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\tmsg = \"Database engine is not initialized after check.\"\n\t\tlogger.error(msg)\n\t\traise RuntimeError(msg)\n\n\tstatement = (\n\t\tselect(ChatHistory)\n\t\t.where(ChatHistory.session_id == session_id)\n\t\t# Using type ignore as the linter incorrectly flags the type\n\t\t.order_by(asc(ChatHistory.timestamp))  # type: ignore[arg-type]\n\t\t.limit(limit)\n\t)\n\ttry:\n\t\twith get_session(self.engine) as session:\n\t\t\tresults: Sequence[ChatHistory] = session.exec(statement).all()  # type: ignore[assignment]\n\t\t\tlogger.debug(f\"Retrieved {len(results)} messages for session {session_id}.\")\n\t\t\treturn results\n\texcept Exception:\n\t\tlogger.exception(\"Error retrieving chat history\")\n\t\traise\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.update_chat_response","title":"update_chat_response","text":"<pre><code>update_chat_response(\n\tmessage_id: int, ai_response: str\n) -&gt; bool\n</code></pre> <p>Updates the AI response for a specific chat message.</p> <p>Parameters:</p> Name Type Description Default <code>message_id</code> <code>int</code> <p>The primary key ID of the chat message to update.</p> required <code>ai_response</code> <code>str</code> <p>The new AI response text.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the update was successful, False otherwise.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def update_chat_response(self, message_id: int, ai_response: str) -&gt; bool:\n\t\"\"\"\n\tUpdates the AI response for a specific chat message.\n\n\tArgs:\n\t        message_id (int): The primary key ID of the chat message to update.\n\t        ai_response (str): The new AI response text.\n\n\tReturns:\n\t        bool: True if the update was successful, False otherwise.\n\n\t\"\"\"\n\t# Ensure engine is initialized - run in a new event loop if needed\n\tif not self.initialized or self.engine is None:\n\t\tloop = asyncio.new_event_loop()\n\t\ttry:\n\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\tfinally:\n\t\t\tloop.close()\n\n\tif self.engine is None:\n\t\tlogger.error(\"Cannot update chat response: Database engine not initialized.\")\n\t\treturn False\n\n\ttry:\n\t\twith get_session(self.engine) as session:\n\t\t\tdb_entry = session.get(ChatHistory, message_id)  # type: ignore[arg-type]\n\t\t\tif db_entry:\n\t\t\t\tdb_entry.ai_response = ai_response\n\t\t\t\tsession.commit()\n\t\t\t\tlogger.debug(f\"Updated DB entry {message_id} with AI response.\")\n\t\t\t\treturn True\n\t\t\tlogger.warning(f\"Chat message with ID {message_id} not found for update.\")\n\t\t\treturn False\n\texcept SQLAlchemyError:\n\t\tlogger.exception(f\"Database error updating chat response for message ID {message_id}\")\n\t\treturn False\n\texcept Exception:\n\t\tlogger.exception(f\"Unexpected error updating chat response for message ID {message_id}\")\n\t\treturn False\n</code></pre>"},{"location":"api/db/engine/","title":"Engine","text":"<p>Handles SQLModel engine creation and session management for CodeMap.</p>"},{"location":"api/db/engine/#codemap.db.engine.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/db/engine/#codemap.db.engine.get_engine","title":"get_engine  <code>async</code>","text":"<pre><code>get_engine(echo: bool = False) -&gt; Engine\n</code></pre> <p>Gets or creates the SQLAlchemy engine for the database.</p> <p>Tries to use PostgreSQL first, falling back to SQLite in-memory if PostgreSQL is not available. Ensures the PostgreSQL Docker container is running before creating the engine.</p> <p>Parameters:</p> Name Type Description Default <code>echo</code> <code>bool</code> <p>Whether to echo SQL statements to the log.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Engine</code> <code>Engine</code> <p>The SQLAlchemy Engine instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If neither PostgreSQL nor SQLite can be initialized.</p> Source code in <code>src/codemap/db/engine.py</code> <pre><code>async def get_engine(echo: bool = False) -&gt; Engine:\n\t\"\"\"\n\tGets or creates the SQLAlchemy engine for the database.\n\n\tTries to use PostgreSQL first, falling back to SQLite in-memory if PostgreSQL is not available.\n\tEnsures the PostgreSQL Docker container is running before creating the engine.\n\n\tArgs:\n\t        echo (bool): Whether to echo SQL statements to the log.\n\n\tReturns:\n\t        Engine: The SQLAlchemy Engine instance.\n\n\tRaises:\n\t        RuntimeError: If neither PostgreSQL nor SQLite can be initialized.\n\n\t\"\"\"\n\t# Check if we should skip Docker and use SQLite (for testing or non-Docker environments)\n\tuse_sqlite = os.environ.get(\"CODEMAP_USE_SQLITE\", \"\").lower() in (\"true\", \"1\", \"yes\")\n\n\tif use_sqlite:\n\t\treturn _create_sqlite_engine(echo)\n\n\t# Try PostgreSQL first\n\ttry:\n\t\treturn await _create_postgres_engine(echo)\n\texcept (RuntimeError, ConnectionError, TimeoutError, OSError) as e:\n\t\tlogger.warning(f\"PostgreSQL connection failed: {e}. Falling back to SQLite in-memory database.\")\n\t\treturn _create_sqlite_engine(echo)\n</code></pre>"},{"location":"api/db/engine/#codemap.db.engine.create_db_and_tables","title":"create_db_and_tables","text":"<pre><code>create_db_and_tables(engine_instance: Engine) -&gt; None\n</code></pre> <p>Creates the database and all tables defined in SQLModel models.</p> Source code in <code>src/codemap/db/engine.py</code> <pre><code>def create_db_and_tables(engine_instance: Engine) -&gt; None:\n\t\"\"\"Creates the database and all tables defined in SQLModel models.\"\"\"\n\tlogger.info(\"Ensuring database tables exist...\")\n\ttry:\n\t\tSQLModel.metadata.create_all(engine_instance)\n\t\tlogger.info(\"Database tables ensured.\")\n\texcept Exception:\n\t\tlogger.exception(\"Error creating database tables\")\n\t\traise\n</code></pre>"},{"location":"api/db/engine/#codemap.db.engine.get_session","title":"get_session","text":"<pre><code>get_session(\n\tengine_instance: Engine,\n) -&gt; Generator[Session, None, None]\n</code></pre> <p>Provides a context-managed SQLModel session from a given engine.</p> Source code in <code>src/codemap/db/engine.py</code> <pre><code>@contextmanager\ndef get_session(engine_instance: Engine) -&gt; Generator[Session, None, None]:\n\t\"\"\"Provides a context-managed SQLModel session from a given engine.\"\"\"\n\tsession = Session(engine_instance)\n\ttry:\n\t\tyield session\n\texcept Exception:\n\t\tlogger.exception(\"Session rollback due to exception\")\n\t\tsession.rollback()\n\t\traise\n\tfinally:\n\t\tsession.close()\n</code></pre>"},{"location":"api/db/models/","title":"Models","text":"<p>Database models for CodeMap using SQLModel.</p>"},{"location":"api/db/models/#codemap.db.models.ChatHistory","title":"ChatHistory","text":"<p>               Bases: <code>SQLModel</code></p> <p>Represents a single entry in the chat history table.</p> Source code in <code>src/codemap/db/models.py</code> <pre><code>class ChatHistory(SQLModel, table=True):\n\t\"\"\"Represents a single entry in the chat history table.\"\"\"\n\n\t__tablename__: str = \"chat_history\"  # type: ignore[assignment]\n\n\tid: int | None = Field(default=None, primary_key=True)\n\tsession_id: str = Field(index=True)\n\ttimestamp: datetime = Field(default_factory=lambda: datetime.now(UTC), index=True)\n\tuser_query: str\n\tai_response: str | None = Field(default=None)\n\tcontext: str | None = Field(default=None)  # JSON string or similar\n\ttool_calls: str | None = Field(default=None)  # JSON string\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.__tablename__","title":"__tablename__  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>__tablename__: str = 'chat_history'\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: int | None = Field(default=None, primary_key=True)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.session_id","title":"session_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_id: str = Field(index=True)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: datetime = Field(\n\tdefault_factory=lambda: now(UTC), index=True\n)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.user_query","title":"user_query  <code>instance-attribute</code>","text":"<pre><code>user_query: str\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.ai_response","title":"ai_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ai_response: str | None = Field(default=None)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.context","title":"context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>context: str | None = Field(default=None)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.tool_calls","title":"tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_calls: str | None = Field(default=None)\n</code></pre>"},{"location":"api/gen/","title":"Gen Overview","text":"<p>Code documentation generation package for CodeMap.</p> <ul> <li>Command - Command implementation for code documentation generation.</li> <li>Generator - Code documentation generator implementation.</li> <li>Utils - Utility functions for the gen command.</li> </ul>"},{"location":"api/gen/command/","title":"Command","text":"<p>Command implementation for code documentation generation.</p>"},{"location":"api/gen/command/#codemap.gen.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.show_error","title":"show_error","text":"<pre><code>show_error(message: str) -&gt; None\n</code></pre> <p>Display an error message using the console.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The error message to display</p> required Source code in <code>src/codemap/gen/command.py</code> <pre><code>def show_error(message: str) -&gt; None:\n\t\"\"\"\n\tDisplay an error message using the console.\n\n\tArgs:\n\t\tmessage: The error message to display\n\t\"\"\"\n\tconsole.print(f\"[red]Error:[/red] {message}\")\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand","title":"GenCommand","text":"<p>Main implementation of the gen command.</p> Source code in <code>src/codemap/gen/command.py</code> <pre><code>class GenCommand:\n\t\"\"\"Main implementation of the gen command.\"\"\"\n\n\tdef __init__(self, config: GenSchema, config_loader: ConfigLoader | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the gen command.\n\n\t\tArgs:\n\t\t    config: Generation configuration\n\t\t    config_loader: Optional ConfigLoader instance to use\n\n\t\t\"\"\"\n\t\tself.config = config\n\t\tself.config_loader = config_loader or ConfigLoader()\n\t\tlogger.debug(\"GenCommand initialized with ConfigLoader\")\n\n\tdef execute(self, target_path: Path, output_path: Path) -&gt; bool:\n\t\t\"\"\"\n\t\tExecute the gen command.\n\n\t\tArgs:\n\t\t    target_path: Path to the target codebase\n\t\t    output_path: Path to write the output\n\n\t\tReturns:\n\t\t    True if successful, False otherwise\n\n\t\t\"\"\"\n\t\tfrom .generator import CodeMapGenerator\n\t\tfrom .utils import write_documentation\n\n\t\tstart_time = time.time()\n\n\t\ttry:\n\t\t\t# Create generator\n\t\t\tgenerator = CodeMapGenerator(self.config)\n\n\t\t\t# Process codebase with progress tracking\n\t\t\twith progress_indicator(\"Processing codebase...\"):\n\t\t\t\t# Process the codebase\n\t\t\t\tentities, metadata = process_codebase(target_path, self.config, config_loader=self.config_loader)\n\n\t\t\t# Generate documentation\n\t\t\tconsole.print(\"[green]Processing complete. Generating documentation...\")\n\t\t\tcontent = generator.generate_documentation(entities, metadata)\n\n\t\t\t# Write documentation to output file\n\t\t\twrite_documentation(output_path, content)\n\n\t\t\t# Show completion message with timing\n\t\t\telapsed = time.time() - start_time\n\t\t\tconsole.print(f\"[green]Generation completed in {elapsed:.2f} seconds.\")\n\n\t\t\treturn True\n\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Error during gen command execution\")\n\t\t\tshow_error(f\"Generation failed: {e}\")\n\t\t\treturn False\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig: GenSchema,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the gen command.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GenSchema</code> <p>Generation configuration</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> Source code in <code>src/codemap/gen/command.py</code> <pre><code>def __init__(self, config: GenSchema, config_loader: ConfigLoader | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize the gen command.\n\n\tArgs:\n\t    config: Generation configuration\n\t    config_loader: Optional ConfigLoader instance to use\n\n\t\"\"\"\n\tself.config = config\n\tself.config_loader = config_loader or ConfigLoader()\n\tlogger.debug(\"GenCommand initialized with ConfigLoader\")\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader or ConfigLoader()\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.execute","title":"execute","text":"<pre><code>execute(target_path: Path, output_path: Path) -&gt; bool\n</code></pre> <p>Execute the gen command.</p> <p>Parameters:</p> Name Type Description Default <code>target_path</code> <code>Path</code> <p>Path to the target codebase</p> required <code>output_path</code> <code>Path</code> <p>Path to write the output</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/gen/command.py</code> <pre><code>def execute(self, target_path: Path, output_path: Path) -&gt; bool:\n\t\"\"\"\n\tExecute the gen command.\n\n\tArgs:\n\t    target_path: Path to the target codebase\n\t    output_path: Path to write the output\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\tfrom .generator import CodeMapGenerator\n\tfrom .utils import write_documentation\n\n\tstart_time = time.time()\n\n\ttry:\n\t\t# Create generator\n\t\tgenerator = CodeMapGenerator(self.config)\n\n\t\t# Process codebase with progress tracking\n\t\twith progress_indicator(\"Processing codebase...\"):\n\t\t\t# Process the codebase\n\t\t\tentities, metadata = process_codebase(target_path, self.config, config_loader=self.config_loader)\n\n\t\t# Generate documentation\n\t\tconsole.print(\"[green]Processing complete. Generating documentation...\")\n\t\tcontent = generator.generate_documentation(entities, metadata)\n\n\t\t# Write documentation to output file\n\t\twrite_documentation(output_path, content)\n\n\t\t# Show completion message with timing\n\t\telapsed = time.time() - start_time\n\t\tconsole.print(f\"[green]Generation completed in {elapsed:.2f} seconds.\")\n\n\t\treturn True\n\n\texcept Exception as e:\n\t\tlogger.exception(\"Error during gen command execution\")\n\t\tshow_error(f\"Generation failed: {e}\")\n\t\treturn False\n</code></pre>"},{"location":"api/gen/generator/","title":"Generator","text":"<p>Code documentation generator implementation.</p>"},{"location":"api/gen/generator/#codemap.gen.generator.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.SMALL_RANGE_THRESHOLD","title":"SMALL_RANGE_THRESHOLD  <code>module-attribute</code>","text":"<pre><code>SMALL_RANGE_THRESHOLD = 5\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.MAX_SKELETON_EARLY_CALLS","title":"MAX_SKELETON_EARLY_CALLS  <code>module-attribute</code>","text":"<pre><code>MAX_SKELETON_EARLY_CALLS = 5\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator","title":"CodeMapGenerator","text":"<p>Generates code documentation based on LOD (Level of Detail).</p> Source code in <code>src/codemap/gen/generator.py</code> <pre><code>class CodeMapGenerator:\n\t\"\"\"Generates code documentation based on LOD (Level of Detail).\"\"\"\n\n\tdef __init__(self, config: GenSchema) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the code map generator.\n\n\t\tArgs:\n\t\t    config: Generation configuration settings\n\n\t\t\"\"\"\n\t\tself.config = config\n\n\tdef _generate_mermaid_diagram(self, entities: list[LODEntity]) -&gt; str:\n\t\t\"\"\"Generate a Mermaid diagram string for entity relationships using subgraphs.\"\"\"\n\t\t# Convert config strings to lower case for case-insensitive comparison\n\t\tallowed_entities = {e.lower() for e in self.config.mermaid_entities} if self.config.mermaid_entities else None\n\t\tallowed_relationships = (\n\t\t\t{r.lower() for r in self.config.mermaid_relationships} if self.config.mermaid_relationships else None\n\t\t)\n\n\t\t# Helper to check if an entity type should be included\n\t\tdef should_include_entity(entity_type: EntityType) -&gt; bool:\n\t\t\t\"\"\"Determines if an entity type should be included in the diagram based on allowed entities.\n\n\t\t\tIf no allowed entities are specified in the config, all entities will be included. Otherwise,\n\t\t\tonly entities whose type matches one of the allowed entity types will be included.\n\n\t\t\tArgs:\n\t\t\t\tentity_type: The type of entity to check for inclusion.\n\n\t\t\tReturns:\n\t\t\t\tbool: True if the entity should be included, False otherwise.\n\t\t\t\"\"\"\n\t\t\tif not allowed_entities:\n\t\t\t\treturn True  # Include all if not specified\n\t\t\treturn entity_type.name.lower() in allowed_entities\n\n\t\tdef should_include_relationship(relationship_type: str) -&gt; bool:\n\t\t\t\"\"\"Determines if a relationship type should be included in the diagram based on allowed relationships.\n\n\t\t\tIf no allowed relationships are specified in the config, all relationships will be included. Otherwise,\n\t\t\tonly relationships whose type matches one of the allowed relationship types will be included.\n\n\t\t\tArgs:\n\t\t\t\trelationship_type: The type of relationship to check for inclusion.\n\n\t\t\tReturns:\n\t\t\t\tbool: True if the relationship should be included, False otherwise.\n\t\t\t\"\"\"\n\t\t\tif not allowed_relationships:\n\t\t\t\treturn True  # Include all if not specified\n\t\t\treturn relationship_type.lower() in allowed_relationships\n\n\t\t# --- Data Structures --- #\n\t\t# node_id -&gt; (definition_line, class_name) for regular nodes\n\t\tnode_definitions: dict[str, tuple[str, str]] = {}\n\t\t# subgraph_id -&gt; (label, type, list_of_contained_node_ids)\n\t\tsubgraph_definitions: dict[str, tuple[str, str, list[str]]] = {}\n\t\t# subgraph_id -&gt; parent_subgraph_id (for nesting)\n\t\tsubgraph_hierarchy: dict[str, str] = {}\n\t\t# Edges (parent_id, child_id, label, type)\n\t\tedges: list[tuple[str, str, str, str]] = []\n\t\t# Track processed entities/subgraphs to avoid duplicates\n\t\tprocessed_ids = set()\n\t\t# Map entity ID to entity object\n\t\tentity_map: dict[str, LODEntity] = {}\n\t\t# Track which nodes/subgraphs are connected by edges\n\t\tconnected_ids = set()\n\t\t# Map simple function/method names to their full node IDs\n\t\tname_to_node_ids: dict[str, list[str]] = {}\n\t\t# Keep track of nodes defined outside any subgraph (like external imports)\n\t\tglobal_nodes: set[str] = set()\n\t\t# Node ID counter for generating short, sequential IDs\n\t\tnode_id_counter = {\"count\": 0}\n\t\t# Subgraph ID counter for generating short subgraph IDs\n\t\tsubgraph_id_counter = {\"count\": 0}\n\t\t# Mapping from short node ID to entity info for debugging/comments\n\t\tnode_id_to_info: dict[str, tuple[str, str, str]] = {}\n\t\t# Map entity unique IDs to their assigned node IDs to prevent duplicates\n\t\tentity_to_node_id: dict[str, str] = {}  # entity_unique_id -&gt; node_id\n\n\t\tinternal_paths = {str(e.metadata.get(\"file_path\")) for e in entities if e.metadata.get(\"file_path\")}\n\n\t\tdef get_entity_unique_id(entity: LODEntity) -&gt; str:\n\t\t\t\"\"\"Generate a unique identifier for an entity based on its properties.\"\"\"\n\t\t\tfile_path = entity.metadata.get(\"file_path\", \"unknown\")\n\t\t\tname = entity.name or \"\"\n\t\t\tentity_type = entity.entity_type.name\n\t\t\tstart_line = entity.start_line\n\t\t\tend_line = entity.end_line\n\t\t\treturn f\"{file_path}:{start_line}-{end_line}:{entity_type}:{name}\"\n\n\t\tdef get_node_id(entity: LODEntity) -&gt; str:\n\t\t\t\"\"\"Generates a short, unique node ID for an entity in Mermaid diagram format.\n\n\t\t\tUses sequential numbering (n1, n2, n3, etc.) to keep node IDs clean and readable.\n\t\t\tThe full path and entity information is preserved in comments and labels.\n\t\t\tReturns the existing ID if the entity has already been assigned one.\n\n\t\t\tArgs:\n\t\t\t\tentity: The entity to generate an ID for.\n\n\t\t\tReturns:\n\t\t\t\tstr: A short Mermaid-compatible node ID string like 'n1', 'n2', etc.\n\t\t\t\"\"\"\n\t\t\t# Check if this entity already has an assigned node ID\n\t\t\tentity_unique_id = get_entity_unique_id(entity)\n\t\t\tif entity_unique_id in entity_to_node_id:\n\t\t\t\treturn entity_to_node_id[entity_unique_id]\n\n\t\t\tnode_id_counter[\"count\"] += 1\n\t\t\tshort_id = f\"n{node_id_counter['count']}\"\n\n\t\t\t# Store the mapping to prevent duplicates\n\t\t\tentity_to_node_id[entity_unique_id] = short_id\n\n\t\t\t# Store mapping for debugging/comments\n\t\t\tfile_path = entity.metadata.get(\"file_path\", \"unknown_file\")\n\t\t\tname = entity.name or entity.entity_type.name\n\t\t\tnode_id_to_info[short_id] = (file_path, str(entity.start_line), name)\n\n\t\t\treturn short_id\n\n\t\tdef get_subgraph_id(entity: LODEntity) -&gt; str:\n\t\t\t\"\"\"Generates a short, unique subgraph ID for modules and classes.\n\n\t\t\tReturns the existing ID if the entity has already been assigned one.\n\n\t\t\tArgs:\n\t\t\t\tentity: The entity to generate a subgraph ID for.\n\n\t\t\tReturns:\n\t\t\t\tstr: A short subgraph ID like 'sg1', 'sg2', etc.\n\t\t\t\"\"\"\n\t\t\t# Check if this entity already has an assigned node ID\n\t\t\tentity_unique_id = get_entity_unique_id(entity)\n\t\t\tif entity_unique_id in entity_to_node_id:\n\t\t\t\treturn entity_to_node_id[entity_unique_id]\n\n\t\t\tsubgraph_id_counter[\"count\"] += 1\n\t\t\tshort_id = f\"sg{subgraph_id_counter['count']}\"\n\n\t\t\t# Store the mapping to prevent duplicates\n\t\t\tentity_to_node_id[entity_unique_id] = short_id\n\n\t\t\treturn short_id\n\n\t\tdef process_entity_recursive(entity: LODEntity, current_subgraph_id: str | None = None) -&gt; None:\n\t\t\t\"\"\"Recursively processes an entity to build Mermaid diagram components.\n\n\t\t\tThis function handles:\n\t\t\t- Creating subgraphs for modules and classes\n\t\t\t- Creating nodes for functions, methods, variables and constants\n\t\t\t- Processing dependencies (imports)\n\t\t\t- Establishing relationships between entities\n\t\t\t- Recursively processing child entities\n\n\t\t\tArgs:\n\t\t\t\tentity: The entity to process\n\t\t\t\tcurrent_subgraph_id: The ID of the current subgraph context (if any)\n\n\t\t\tReturns:\n\t\t\t\tNone: Modifies various data structures in the closure:\n\t\t\t\t\t- node_definitions: Dictionary of node definitions\n\t\t\t\t\t- subgraph_definitions: Dictionary of subgraph definitions\n\t\t\t\t\t- subgraph_hierarchy: Dictionary of subgraph parent-child relationships\n\t\t\t\t\t- edges: List of edges between nodes/subgraphs\n\t\t\t\t\t- processed_ids: Set of processed entity IDs\n\t\t\t\t\t- entity_map: Dictionary mapping node IDs to entities\n\t\t\t\t\t- connected_ids: Set of connected node/subgraph IDs\n\t\t\t\t\t- name_to_node_ids: Dictionary mapping names to node IDs\n\t\t\t\t\t- global_nodes: Set of nodes defined outside subgraphs\n\t\t\t\"\"\"\n\t\t\tnonlocal processed_ids, connected_ids, global_nodes\n\n\t\t\t# Use different ID generation for subgraphs vs nodes\n\t\t\tif entity.entity_type in (EntityType.MODULE, EntityType.CLASS):\n\t\t\t\tentity_node_id = get_subgraph_id(entity)\n\t\t\telse:\n\t\t\t\tentity_node_id = get_node_id(entity)\n\n\t\t\t# Skip if already processed, but allow UNKNOWN entities to process their children\n\t\t\tif entity_node_id in processed_ids:\n\t\t\t\treturn\n\n\t\t\t# For UNKNOWN entities, skip creating nodes but still process children\n\t\t\tif entity.entity_type == EntityType.UNKNOWN:\n\t\t\t\tprocessed_ids.add(entity_node_id)\n\t\t\t\t# Process children recursively even for UNKNOWN entities\n\t\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t\tprocess_entity_recursive(child, current_subgraph_id)\n\t\t\t\treturn\n\n\t\t\tprocessed_ids.add(entity_node_id)\n\t\t\tentity_map[entity_node_id] = entity\n\t\t\tinclude_this_entity = should_include_entity(entity.entity_type)\n\n\t\t\tnext_subgraph_id = current_subgraph_id\n\n\t\t\t# --- Handle Subgraphs (Module, Class) --- #\n\t\t\tif entity.entity_type in (EntityType.MODULE, EntityType.CLASS) and include_this_entity:\n\t\t\t\tsubgraph_label = _escape_mermaid_label(\n\t\t\t\t\tentity.name or Path(entity.metadata.get(\"file_path\", \"unknown\")).name\n\t\t\t\t)\n\t\t\t\tsubgraph_type = \"moduleSubgraph\" if entity.entity_type == EntityType.MODULE else \"classSubgraph\"\n\t\t\t\tif entity.entity_type == EntityType.MODULE and current_subgraph_id:  # Nested module\n\t\t\t\t\tsubgraph_type = \"submoduleSubgraph\"\n\n\t\t\t\tsubgraph_definitions[entity_node_id] = (subgraph_label, subgraph_type, [])\n\t\t\t\tif current_subgraph_id:\n\t\t\t\t\tsubgraph_hierarchy[entity_node_id] = current_subgraph_id\n\t\t\t\t# Mark the container as potentially connected if it has children or dependencies\n\t\t\t\tconnected_ids.add(entity_node_id)\n\t\t\t\tnext_subgraph_id = entity_node_id  # Children belong to this new subgraph\n\n\t\t\t# --- Handle Nodes (Functions, Methods, Vars, Consts, Imports) --- #\n\t\t\telif include_this_entity and entity.entity_type != EntityType.IMPORT:  # Imports handled separately\n\t\t\t\tnode_definition = \"\"\n\t\t\t\tnode_class = \"\"\n\t\t\t\tentity_type_name = str(entity.entity_type.name).lower()\n\t\t\t\tlabel = _escape_mermaid_label(entity.name or f\"({entity_type_name})\")\n\n\t\t\t\tif entity.entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\t\t\t\tnode_definition = f'{entity_node_id}(\"{label}\")'\n\t\t\t\t\tnode_class = \"funcNode\"\n\t\t\t\telif entity.entity_type == EntityType.CONSTANT:\n\t\t\t\t\tnode_definition = f'{entity_node_id}[\"{label}\"]'\n\t\t\t\t\tnode_class = \"constNode\"\n\t\t\t\telif entity.entity_type == EntityType.VARIABLE:\n\t\t\t\t\tnode_definition = f'{entity_node_id}[\"{label}\"]'\n\t\t\t\t\tnode_class = \"varNode\"\n\t\t\t\t# Add other types if needed\n\n\t\t\t\tif node_definition and entity_node_id not in node_definitions:\n\t\t\t\t\tnode_definitions[entity_node_id] = (node_definition, node_class)\n\t\t\t\t\tif current_subgraph_id:\n\t\t\t\t\t\tsubgraph_definitions[current_subgraph_id][2].append(entity_node_id)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Should not happen often if root is module, but handle just in case\n\t\t\t\t\t\tglobal_nodes.add(entity_node_id)\n\n\t\t\t# --- Add to Name Map (for call edges) --- #\n\t\t\tif entity.entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\t\t\tname = entity.name\n\t\t\t\tif name:\n\t\t\t\t\tif name not in name_to_node_ids:\n\t\t\t\t\t\tname_to_node_ids[name] = []\n\t\t\t\t\tname_to_node_ids[name].append(entity_node_id)\n\n\t\t\t# --- Process Dependencies (Imports) --- #\n\t\t\tdependencies = entity.metadata.get(\"dependencies\", [])\n\t\t\t# Imports are associated with the module/class they are in, or globally if nowhere else\n\n\t\t\tif dependencies and should_include_relationship(\"imports\"):\n\t\t\t\tfor dep in dependencies:\n\t\t\t\t\tis_external = not dep.startswith(\".\") and not any(\n\t\t\t\t\t\tdep.startswith(str(p).replace(\"\\\\\", \"/\"))\n\t\t\t\t\t\tfor p in internal_paths\n\t\t\t\t\t\tif p  # Handle path separators\n\t\t\t\t\t)\n\t\t\t\t\tdep_id = \"dep_\" + \"\".join(c if c.isalnum() else \"_\" for c in dep)\n\t\t\t\t\tdep_label = _escape_mermaid_label(dep)\n\n\t\t\t\t\tif dep_id not in node_definitions and dep_id not in processed_ids:\n\t\t\t\t\t\tprocessed_ids.add(dep_id)  # Mark as processed to avoid duplicate definitions\n\t\t\t\t\t\tdep_class = \"externalImportNode\" if is_external else \"internalImportNode\"\n\t\t\t\t\t\tnode_shape = f'((\"{dep_label}\"))' if is_external else f'[\"{dep_label}\"]'\n\t\t\t\t\t\tnode_definitions[dep_id] = (f\"{dep_id}{node_shape}\", dep_class)\n\t\t\t\t\t\tglobal_nodes.add(dep_id)  # Imports are defined globally\n\n\t\t\t\t\t# Add edge from the importing *container* (subgraph) to the dependency\n\t\t\t\t\t# Use the entity_node_id of the *module* or *class* for the source of the import edge\n\t\t\t\t\tsource_node_id = (\n\t\t\t\t\t\tentity_node_id\n\t\t\t\t\t\tif entity.entity_type in (EntityType.MODULE, EntityType.CLASS)\n\t\t\t\t\t\telse current_subgraph_id\n\t\t\t\t\t)\n\t\t\t\t\tif source_node_id and source_node_id != dep_id:  # Check source_node_id validity\n\t\t\t\t\t\tedge_tuple = (source_node_id, dep_id, \"imports\", \"import\")\n\t\t\t\t\t\tif edge_tuple not in edges:\n\t\t\t\t\t\t\tedges.append(edge_tuple)\n\t\t\t\t\t\t\tconnected_ids.add(source_node_id)\n\t\t\t\t\t\t\tconnected_ids.add(dep_id)\n\n\t\t\t# --- Process Children Recursively and track their node IDs --- #\n\t\t\tchild_node_ids = []  # Track child node IDs for declare edges\n\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t# Determine what the child's node ID would be before processing\n\t\t\t\tif child.entity_type in (EntityType.MODULE, EntityType.CLASS):\n\t\t\t\t\tchild_node_id = get_subgraph_id(child)\n\t\t\t\telse:\n\t\t\t\t\tchild_node_id = get_node_id(child)\n\n\t\t\t\t# Store the child node ID for declare edges (before processing to avoid duplicates)\n\t\t\t\tchild_node_ids.append((child, child_node_id))\n\n\t\t\t\t# Now process the child recursively\n\t\t\t\tprocess_entity_recursive(child, next_subgraph_id)\n\n\t\t\t# --- Define Parent Edges (Declares) using tracked child node IDs --- #\n\t\t\tfor child, child_node_id in child_node_ids:\n\t\t\t\tif (\n\t\t\t\t\tnext_subgraph_id  # Ensure there is a parent subgraph\n\t\t\t\t\tand child.entity_type != EntityType.UNKNOWN\n\t\t\t\t\tand child_node_id in processed_ids  # Ensure child was processed (not filtered out)\n\t\t\t\t\tand should_include_relationship(\"declares\")\n\t\t\t\t):\n\t\t\t\t\t# Only add edge if child is *directly* contained (node or subgraph)\n\t\t\t\t\tis_child_node = child_node_id in node_definitions\n\t\t\t\t\tis_child_subgraph = child_node_id in subgraph_definitions\n\n\t\t\t\t\tif is_child_node or is_child_subgraph:\n\t\t\t\t\t\tedge_tuple = (next_subgraph_id, child_node_id, \"declares\", \"declare\")\n\t\t\t\t\t\tif edge_tuple not in edges:\n\t\t\t\t\t\t\tedges.append(edge_tuple)\n\t\t\t\t\t\t\tconnected_ids.add(next_subgraph_id)\n\t\t\t\t\t\t\tconnected_ids.add(child_node_id)\n\n\t\t# --- Main Processing Loop --- #\n\t\tfor entity in entities:\n\t\t\t# Start processing from top-level modules\n\t\t\tif entity.entity_type == EntityType.MODULE and entity.metadata.get(\"file_path\"):\n\t\t\t\tprocess_entity_recursive(entity, current_subgraph_id=None)\n\n\t\t# --- Define Call Edges --- #\n\t\tif should_include_relationship(\"calls\"):\n\t\t\tfor caller_node_id, caller_entity in entity_map.items():\n\t\t\t\t# Check if the caller is a function/method node that was actually defined\n\t\t\t\tif caller_node_id in node_definitions and caller_entity.entity_type in (\n\t\t\t\t\tEntityType.FUNCTION,\n\t\t\t\t\tEntityType.METHOD,\n\t\t\t\t):\n\t\t\t\t\tcalls = caller_entity.metadata.get(\"calls\", [])\n\t\t\t\t\tfor called_name in calls:\n\t\t\t\t\t\t# Try matching full name first, then simple name\n\t\t\t\t\t\tpossible_target_ids = []\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\tcalled_name in name_to_node_ids\n\t\t\t\t\t\t):  # Full name match? (e.g., class.method) - Less likely with simple parsing\n\t\t\t\t\t\t\tpossible_target_ids.extend(name_to_node_ids[called_name])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsimple_called_name = called_name.split(\".\")[-1]\n\t\t\t\t\t\t\tif simple_called_name in name_to_node_ids:\n\t\t\t\t\t\t\t\tpossible_target_ids.extend(name_to_node_ids[simple_called_name])\n\n\t\t\t\t\t\tfor target_node_id in possible_target_ids:\n\t\t\t\t\t\t\t# Ensure target is also a defined node and not the caller itself\n\t\t\t\t\t\t\tif target_node_id in node_definitions and caller_node_id != target_node_id:\n\t\t\t\t\t\t\t\tedge_tuple = (caller_node_id, target_node_id, \"calls\", \"call\")\n\t\t\t\t\t\t\t\tif edge_tuple not in edges:\n\t\t\t\t\t\t\t\t\tedges.append(edge_tuple)\n\t\t\t\t\t\t\t\t\tconnected_ids.add(caller_node_id)\n\t\t\t\t\t\t\t\t\tconnected_ids.add(target_node_id)\n\n\t\t# --- Assemble Final Mermaid String --- #\n\t\tmermaid_lines = [\"graph LR\"]  # Or TD for Top-Down if preferred\n\n\t\t# --- Define Style Classes ---\n\t\tstyle_map = {\n\t\t\t# Node Styles\n\t\t\t\"funcNode\": \"fill:#007bff,stroke:#FFF,stroke-width:1px,color:white\",  # Blue\n\t\t\t\"constNode\": \"fill:#6f42c1,stroke:#FFF,stroke-width:1px,color:white\",  # Purple\n\t\t\t\"varNode\": \"fill:#fd7e14,stroke:#FFF,stroke-width:1px,color:white\",  # Orange\n\t\t\t\"internalImportNode\": \"fill:#20c997,stroke:#FFF,stroke-width:1px,color:white\",  # Teal\n\t\t\t\"externalImportNode\": \"fill:#ffc107,stroke:#333,stroke-width:1px,color:#333\",  # Yellow\n\t\t\t# Subgraph Styles\n\t\t\t\"moduleSubgraph\": \"fill:#121630,color:#FFF\",  # Dark Grey BG\n\t\t\t\"submoduleSubgraph\": \"fill:#2a122e,color:#FFF\",  # Lighter Grey BG\n\t\t\t\"classSubgraph\": \"fill:#100f5e,color:#FFF\",  # Light Green BG\n\t\t}\n\n\t\t# Check if styling is enabled\n\t\tstyling_enabled = self.config.mermaid_styled\n\n\t\t# Track nodes by their style class for bulk application\n\t\tnodes_by_class: dict[str, list[str]] = {}\n\n\t\t# --- Render Logic --- #\n\t\trendered_elements = set()  # Track IDs of things actually rendered\n\t\toutput_lines = []\n\t\tclass_def_lines = []  # Collect classDef commands\n\t\tstyle_lines = []  # Collect individual style commands for subgraphs\n\t\tused_style_keys = set()  # Track which styles (funcNode, classSubgraph etc.) are used\n\n\t\t# Helper function to check if a subgraph has any content after filtering\n\t\tdef subgraph_has_content(subgraph_id: str) -&gt; bool:\n\t\t\t\"\"\"Check if a subgraph has any content (nodes or nested subgraphs) after filtering.\n\n\t\t\tArgs:\n\t\t\t\tsubgraph_id: The ID of the subgraph to check\n\n\t\t\tReturns:\n\t\t\t\tTrue if the subgraph has any content that would be rendered, False otherwise\n\t\t\t\"\"\"\n\t\t\tif subgraph_id not in subgraph_definitions:\n\t\t\t\treturn False\n\n\t\t\t_, _, contained_node_ids = subgraph_definitions[subgraph_id]\n\n\t\t\t# Check if any nodes in this subgraph would be rendered\n\t\t\tfor node_id in contained_node_ids:\n\t\t\t\tif node_id in node_definitions:\n\t\t\t\t\t# If filtering is disabled, all nodes are rendered\n\t\t\t\t\tif not self.config.mermaid_remove_unconnected:\n\t\t\t\t\t\treturn True\n\t\t\t\t\t# If filtering is enabled, check if node is connected\n\t\t\t\t\tif node_id in connected_ids:\n\t\t\t\t\t\treturn True\n\n\t\t\t# Check if any nested subgraphs have content\n\t\t\tnested_subgraphs = [sid for sid, parent_id in subgraph_hierarchy.items() if parent_id == subgraph_id]\n\t\t\treturn any(subgraph_has_content(nested_id) for nested_id in nested_subgraphs)\n\n\t\t# Function to recursively render subgraphs and their nodes\n\t\tdef render_subgraph(subgraph_id: str, indent: str = \"\") -&gt; None:\n\t\t\t\"\"\"Recursively renders a Mermaid subgraph and its contents.\n\n\t\t\tArgs:\n\t\t\t\tsubgraph_id: The ID of the subgraph to render\n\t\t\t\tindent: String used for indentation in the output (default: \"\")\n\n\t\t\tReturns:\n\t\t\t\tNone: Output is written to output_lines and style_lines lists\n\n\t\t\tSide Effects:\n\t\t\t\t- Adds to rendered_elements set to track rendered items\n\t\t\t\t- Appends lines to output_lines for Mermaid graph definition\n\t\t\t\t- Appends style commands to style_lines\n\t\t\t\t- Updates used_style_keys with any styles actually used\n\t\t\t\"\"\"\n\t\t\tif subgraph_id in rendered_elements:\n\t\t\t\treturn\n\n\t\t\t# Skip empty subgraphs when filtering is enabled\n\t\t\tif self.config.mermaid_remove_unconnected and not subgraph_has_content(subgraph_id):\n\t\t\t\treturn\n\n\t\t\trendered_elements.add(subgraph_id)\n\n\t\t\tlabel, sg_type, contained_node_ids = subgraph_definitions[subgraph_id]\n\t\t\tsubgraph_comment = get_subgraph_comment(subgraph_id)\n\t\t\tif subgraph_comment:\n\t\t\t\toutput_lines.append(f\"{indent}{subgraph_comment}\")\n\t\t\toutput_lines.append(f'{indent}subgraph {subgraph_id}[\"{label}\"]')\n\t\t\toutput_lines.append(f\"{indent}  direction LR\")  # Or TD\n\n\t\t\t# Render nodes inside this subgraph\n\t\t\tfor node_id in contained_node_ids:\n\t\t\t\tif node_id in node_definitions:\n\t\t\t\t\t# Apply filtering if enabled\n\t\t\t\t\tif self.config.mermaid_remove_unconnected and node_id not in connected_ids:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif node_id in rendered_elements:\n\t\t\t\t\t\tcontinue  # Should not happen, but safeguard\n\t\t\t\t\trendered_elements.add(node_id)\n\n\t\t\t\t\tdefinition, node_class = node_definitions[node_id]\n\t\t\t\t\tinline_comment = get_inline_comment(node_id)\n\t\t\t\t\tif inline_comment:\n\t\t\t\t\t\toutput_lines.append(f\"{indent}  {inline_comment}\")\n\t\t\t\t\toutput_lines.append(f\"{indent}  {definition}\")\n\t\t\t\t\tif node_class in style_map and styling_enabled:\n\t\t\t\t\t\t# Track nodes by class for bulk application\n\t\t\t\t\t\tif node_class not in nodes_by_class:\n\t\t\t\t\t\t\tnodes_by_class[node_class] = []\n\t\t\t\t\t\tnodes_by_class[node_class].append(node_id)\n\t\t\t\t\t\tused_style_keys.add(node_class)  # Track used style\n\n\t\t\t# Render nested subgraphs\n\t\t\tnested_subgraphs = [sid for sid, parent_id in subgraph_hierarchy.items() if parent_id == subgraph_id]\n\t\t\tfor nested_id in sorted(nested_subgraphs):  # Sort for consistent output\n\t\t\t\t# Apply filtering if enabled - use the comprehensive content check\n\t\t\t\tif self.config.mermaid_remove_unconnected and not subgraph_has_content(nested_id):\n\t\t\t\t\tcontinue\n\t\t\t\trender_subgraph(nested_id, indent + \"  \")\n\n\t\t\toutput_lines.append(f\"{indent}end\")\n\t\t\t# Apply style definition to subgraph *after* end (subgraphs still use individual styles)\n\t\t\tif sg_type in style_map:\n\t\t\t\tstyle_lines.append(\n\t\t\t\t\tf\"  style {subgraph_id} {style_map[sg_type]}\"\n\t\t\t\t)  # Always use 2-space indent for styles\n\t\t\t\tused_style_keys.add(sg_type)  # Track used style\n\n\t\t# Helper function to create standalone comments for nodes\n\t\tdef get_inline_comment(node_id: str) -&gt; str:\n\t\t\t\"\"\"Generate a standalone comment for a node ID.\"\"\"\n\t\t\tif node_id in node_id_to_info:\n\t\t\t\tfile_path, line, name = node_id_to_info[node_id]\n\t\t\t\ttry:\n\t\t\t\t\tfrom pathlib import Path\n\n\t\t\t\t\tfilename = Path(file_path).name\n\t\t\t\texcept (ValueError, AttributeError, OSError):\n\t\t\t\t\tfilename = \"unknown\"\n\t\t\t\treturn f\"%% {filename}:{line}\"\n\t\t\treturn \"\"\n\n\t\t# Helper function to create standalone comments for subgraphs\n\t\tdef get_subgraph_comment(subgraph_id: str) -&gt; str:\n\t\t\t\"\"\"Generate a standalone comment for a subgraph ID.\"\"\"\n\t\t\tif subgraph_id in entity_map:\n\t\t\t\tentity = entity_map[subgraph_id]\n\t\t\t\tfile_path = entity.metadata.get(\"file_path\", \"unknown\")\n\t\t\t\ttry:\n\t\t\t\t\tfrom pathlib import Path\n\n\t\t\t\t\tfilename = Path(file_path).name\n\t\t\t\texcept (ValueError, AttributeError, OSError):\n\t\t\t\t\tfilename = \"unknown\"\n\t\t\t\treturn f\"%% {filename}:{entity.start_line}\"\n\t\t\treturn \"\"\n\n\t\t# --- Define Global Nodes (Imports primarily) ---\n\t\toutput_lines.append(\"\\n  %% Global Nodes\")\n\t\tfor node_id in sorted(global_nodes):\n\t\t\tif node_id in node_definitions:\n\t\t\t\t# Apply filtering if enabled\n\t\t\t\tif self.config.mermaid_remove_unconnected and node_id not in connected_ids:\n\t\t\t\t\tcontinue\n\t\t\t\tif node_id in rendered_elements:\n\t\t\t\t\tcontinue\n\t\t\t\trendered_elements.add(node_id)\n\n\t\t\t\tdefinition, node_class = node_definitions[node_id]\n\t\t\t\tinline_comment = get_inline_comment(node_id)\n\t\t\t\tif inline_comment:\n\t\t\t\t\toutput_lines.append(f\"  {inline_comment}\")\n\t\t\t\toutput_lines.append(f\"  {definition}\")\n\t\t\t\tif node_class in style_map and getattr(self.config, \"mermaid_styled\", True):\n\t\t\t\t\t# Track nodes by class for bulk application\n\t\t\t\t\tif nodes_by_class is not None:\n\t\t\t\t\t\tif node_class not in nodes_by_class:\n\t\t\t\t\t\t\tnodes_by_class[node_class] = []\n\t\t\t\t\t\tnodes_by_class[node_class].append(node_id)\n\t\t\t\t\tused_style_keys.add(node_class)  # Track used style\n\n\t\t# --- Render Top-Level Subgraphs ---\n\t\toutput_lines.append(\"\\n  %% Subgraphs\")\n\t\ttop_level_subgraphs = [sg_id for sg_id in subgraph_definitions if sg_id not in subgraph_hierarchy]\n\t\tfor sg_id in sorted(top_level_subgraphs):\n\t\t\t# Apply filtering if enabled - use the comprehensive content check\n\t\t\tif self.config.mermaid_remove_unconnected and not subgraph_has_content(sg_id):\n\t\t\t\tcontinue\n\t\t\trender_subgraph(sg_id, \"  \")  # Use 2-space base indentation for top-level subgraphs\n\n\t\t# --- Render Edges --- #\n\t\toutput_lines.append(\"\\n  %% Edges\")\n\t\tlink_styles = []\n\t\tcall_edge_indices = []\n\t\timport_edge_indices = []\n\t\tdeclare_edge_indices = []\n\n\t\t# Group edges by type for better organization\n\t\tcall_edges = []\n\t\timport_edges = []\n\t\tdeclare_edges = []\n\t\tother_edges = []\n\n\t\tfor _i, (source_id, target_id, label, edge_type) in enumerate(edges):\n\t\t\t# Ensure both source and target were actually rendered (or are subgraphs that contain rendered nodes)\n\t\t\tsource_exists = source_id in rendered_elements or source_id in subgraph_definitions\n\t\t\ttarget_exists = target_id in rendered_elements or target_id in subgraph_definitions\n\n\t\t\tif source_exists and target_exists:\n\t\t\t\tif edge_type == \"call\":\n\t\t\t\t\tedge_str = f\"  {source_id} --&gt;|{label}| {target_id}\"\n\t\t\t\t\tcall_edges.append(edge_str)\n\t\t\t\telif edge_type == \"import\":\n\t\t\t\t\tedge_str = f\"  {source_id} -.-&gt;|{label}| {target_id}\"\n\t\t\t\t\timport_edges.append(edge_str)\n\t\t\t\telif edge_type == \"declare\":\n\t\t\t\t\t# Make declare edges less prominent\n\t\t\t\t\tedge_str = f\"  {source_id} --- {target_id}\"  # Simple line, no label needed visually\n\t\t\t\t\tdeclare_edges.append(edge_str)\n\t\t\t\telse:  # Default or unknown edge type\n\t\t\t\t\tedge_str = f\"  {source_id} --&gt; {target_id}\"\n\t\t\t\t\tother_edges.append(edge_str)\n\n\t\t# Track starting indices for link styles\n\t\tedge_counter = 0\n\n\t\t# Render call edges\n\t\tif call_edges:\n\t\t\toutput_lines.append(\"  %% Call edges\")\n\t\t\tcall_edge_indices = list(range(edge_counter, edge_counter + len(call_edges)))\n\t\t\tedge_counter += len(call_edges)\n\t\t\toutput_lines.extend(sorted(call_edges))\n\n\t\t# Render import edges\n\t\tif import_edges:\n\t\t\toutput_lines.append(\"  %% Import edges\")\n\t\t\timport_edge_indices = list(range(edge_counter, edge_counter + len(import_edges)))\n\t\t\tedge_counter += len(import_edges)\n\t\t\toutput_lines.extend(sorted(import_edges))\n\n\t\t# Render declare edges\n\t\tif declare_edges:\n\t\t\toutput_lines.append(\"  %% Declaration edges\")\n\t\t\tdeclare_edge_indices = list(range(edge_counter, edge_counter + len(declare_edges)))\n\t\t\tedge_counter += len(declare_edges)\n\t\t\toutput_lines.extend(sorted(declare_edges))\n\n\t\t# Render other edges (if any)\n\t\tif other_edges:\n\t\t\toutput_lines.append(\"  %% Other edges\")\n\t\t\toutput_lines.extend(sorted(other_edges))\n\n\t\t# --- Apply Link Styles (Optimized with Comments for Large Ranges) --- #\n\t\tif styling_enabled and (call_edge_indices or import_edge_indices or declare_edge_indices):\n\t\t\toutput_lines.append(\"\\n  %% Link Styles\")\n\n\t\t\t# Use helper function to add styles with range comments for better readability\n\t\t\tif call_edge_indices:\n\t\t\t\tlink_styles.append(\"  %% Call edges (green)\")\n\t\t\t\t_add_range_styles(link_styles, call_edge_indices, \"stroke:#28a745,stroke-width:2px\")\n\n\t\t\tif import_edge_indices:\n\t\t\t\tlink_styles.append(\"  %% Import edges (yellow dashed)\")\n\t\t\t\t_add_range_styles(\n\t\t\t\t\tlink_styles, import_edge_indices, \"stroke:#ffc107,stroke-width:1px,stroke-dasharray: 5 5\"\n\t\t\t\t)\n\n\t\t\tif declare_edge_indices:\n\t\t\t\tlink_styles.append(\"  %% Declaration edges (gray)\")\n\t\t\t\t_add_range_styles(link_styles, declare_edge_indices, \"stroke:#adb5bd,stroke-width:1px\")\n\n\t\t\toutput_lines.extend(link_styles)\n\n\t\t# --- Generate Dynamic Legend (if enabled) ---\n\t\tlegend_lines = []\n\t\tlegend_style_lines = []\n\t\tif self.config.mermaid_show_legend and used_style_keys and styling_enabled:\n\t\t\tlegend_lines.append(\"\\n  %% Legend\")\n\t\t\tlegend_lines.append(\"  subgraph Legend\")\n\t\t\tlegend_lines.append(\"    direction LR\")\n\n\t\t\t# Define all possible legend items and their corresponding style keys\n\t\t\tlegend_item_definitions = {\n\t\t\t\t\"legend_module\": (\"moduleSubgraph\", '[\"Module/File\"]'),\n\t\t\t\t\"legend_submodule\": (\"submoduleSubgraph\", '[\"Sub-Module\"]'),\n\t\t\t\t\"legend_class\": (\"classSubgraph\", '[\"Class\"]'),\n\t\t\t\t\"legend_func\": (\"funcNode\", '(\"Function/Method\")'),\n\t\t\t\t\"legend_const\": (\"constNode\", '[\"Constant\"]'),\n\t\t\t\t\"legend_var\": (\"varNode\", '[\"Variable\"]'),\n\t\t\t\t\"legend_import_int\": (\"internalImportNode\", '[\"Internal Import\"]'),\n\t\t\t\t\"legend_import_ext\": (\"externalImportNode\", '((\"External Import\"))'),\n\t\t\t}\n\n\t\t\tfor legend_id, (style_key, definition) in legend_item_definitions.items():\n\t\t\t\t# Only add legend item if its corresponding style was actually used in the graph\n\t\t\t\tif style_key in used_style_keys:\n\t\t\t\t\tlegend_lines.append(f\"    {legend_id}{definition}\")\n\t\t\t\t\t# Track legend nodes by class for bulk application (except subgraph styles)\n\t\t\t\t\tif style_key in [\"funcNode\", \"constNode\", \"varNode\", \"internalImportNode\", \"externalImportNode\"]:\n\t\t\t\t\t\tif style_key not in nodes_by_class:\n\t\t\t\t\t\t\tnodes_by_class[style_key] = []\n\t\t\t\t\t\tnodes_by_class[style_key].append(legend_id)\n\t\t\t\t\t# Subgraph-style legend items still use individual styles\n\t\t\t\t\telif style_key in style_map:\n\t\t\t\t\t\tlegend_style_lines.append(f\"  style {legend_id} {style_map[style_key]}\")\n\n\t\t\tlegend_lines.append(\"  end\")\n\t\t\tlegend_lines.append(\"\")  # Add a blank line after legend\n\n\t\t# --- Assemble Final Output --- #\n\t\tmermaid_lines.extend(legend_lines)  # Add legend definitions (if any)\n\t\tmermaid_lines.extend(output_lines)  # Add main graph structure and edges\n\n\t\t# --- Add Class Definitions (Consolidated Style Application) --- #\n\t\tif styling_enabled and nodes_by_class:\n\t\t\tmermaid_lines.append(\"\\n  %% Class Definitions\")\n\t\t\tfor style_class, node_ids in sorted(nodes_by_class.items()):\n\t\t\t\tif node_ids and style_class in style_map:\n\t\t\t\t\tclass_def_lines.append(f\"  classDef {style_class} {style_map[style_class]}\")\n\t\t\t\t\t# Apply class to all nodes of this type\n\t\t\t\t\tnode_list = \",\".join(sorted(node_ids))\n\t\t\t\t\tclass_def_lines.append(f\"  class {node_list} {style_class}\")\n\n\t\t\tmermaid_lines.extend(class_def_lines)\n\n\t\t# Append individual style commands for subgraphs and legend subgraph-style items\n\t\tall_style_lines = style_lines + legend_style_lines\n\t\tif styling_enabled and all_style_lines:\n\t\t\tmermaid_lines.append(\"\\n  %% Individual Styles\")\n\t\t\tmermaid_lines.extend(sorted(all_style_lines))\n\n\t\treturn \"\\n\".join(mermaid_lines)\n\n\tdef generate_documentation(self, entities: list[LODEntity], metadata: dict) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate markdown documentation from the processed LOD entities.\n\n\t\tArgs:\n\t\t    entities: List of LOD entities\n\t\t    metadata: Repository metadata\n\n\t\tReturns:\n\t\t    Generated documentation as string\n\n\t\t\"\"\"\n\t\tcontent = []\n\n\t\t# Add header with repository information\n\t\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\t\toriginal_path = metadata.get(\"original_path\", \"\")\n\t\tcommand_arg = metadata.get(\"command_arg\", \"\")\n\n\t\t# Debug logging to see what values we're receiving\n\t\tlogger.debug(\n\t\t\tf\"Metadata values for heading: \"\n\t\t\tf\"command_arg='{command_arg}', \"\n\t\t\tf\"original_path='{original_path}', \"\n\t\t\tf\"target_path='{target_path_str}'\"\n\t\t)\n\n\t\t# Use the exact command argument if available\n\t\tif command_arg:\n\t\t\trepo_name = command_arg\n\t\t# Fall back to original path if available\n\t\telif original_path:\n\t\t\trepo_name = original_path\n\t\t# Further fallback to just the directory name\n\t\telif target_path_str:\n\t\t\trepo_name = Path(target_path_str).name\n\t\telse:\n\t\t\trepo_name = metadata.get(\"name\", \"Repository\")\n\n\t\tcontent.append(f\"# `{repo_name}` Documentation\")\n\t\tcontent.append(f\"\\nGenerated on: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\t\tif \"description\" in metadata:\n\t\t\tcontent.append(\"\\n\" + metadata.get(\"description\", \"\"))\n\n\t\t# Add repository statistics\n\t\tif \"stats\" in metadata:\n\t\t\tstats = metadata[\"stats\"]\n\t\t\tcontent.append(\"\\n## Document Statistics\")\n\t\t\tcontent.append(f\"- Total files scanned: {stats.get('total_files_scanned', 0)}\")\n\t\t\tcontent.append(f\"- Total lines of code: {stats.get('total_lines', 0)}\")\n\t\t\tcontent.append(f\"- Languages: {', '.join(stats.get('languages', []))}\")\n\n\t\t# Add directory structure if requested\n\t\tif self.config.include_tree and \"tree\" in metadata:\n\t\t\tcontent.append(\"\\n## Directory Structure\")\n\t\t\tcontent.append(\"```\")\n\t\t\tcontent.append(metadata[\"tree\"])\n\t\t\tcontent.append(\"```\")\n\n\t\t# Add Mermaid diagram if entities exist and config enables it\n\t\tif entities and self.config.include_entity_graph:\n\t\t\tcontent.append(\"\\n## Entity Relationships\")\n\t\t\tcontent.append(\"```mermaid\")\n\t\t\tmermaid_diagram = self._generate_mermaid_diagram(entities)\n\t\t\tcontent.append(mermaid_diagram)\n\t\t\tcontent.append(\"```\")\n\n\t\t# Add table of contents for the scanned files\n\t\tcontent.append(\"\\n## Scanned Files\")\n\n\t\t# Group entities by file\n\t\tfiles: dict[Path, list[LODEntity]] = {}\n\n\t\t# Get the target path from metadata\n\t\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\t\ttarget_path = Path(target_path_str) if target_path_str else None\n\n\t\tfor entity in entities:\n\t\t\tfile_path = Path(entity.metadata.get(\"file_path\", \"\"))\n\t\t\tif not file_path.name:\n\t\t\t\tcontinue\n\n\t\t\tif file_path not in files:\n\t\t\t\tfiles[file_path] = []\n\t\t\tfiles[file_path].append(entity)\n\n\t\t# Create TOC entries with properly formatted relative paths\n\t\tfor i, file_path in enumerate(sorted(files.keys()), 1):\n\t\t\t# Get path relative to the target directory\n\t\t\ttry:\n\t\t\t\tif target_path and target_path.exists():\n\t\t\t\t\t# Get the relative path from the target directory\n\t\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\t\tfilename = file_path.name\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t\t# Handle paths with subdirectories\n\t\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\t\tanchor = clean_filename\n\t\t\t\telse:\n\t\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\t\tanchor = clean_filename\n\n\t\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{anchor})\")\n\t\t\texcept ValueError:\n\t\t\t\t# If relative_to fails, just use the filename\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{clean_filename})\")\n\n\t\t# Add code documentation grouped by file\n\t\tcontent.append(\"\\n## Code Documentation\")\n\n\t\t# Helper function to get comment syntax for a language\n\t\tdef get_comment_syntax(language: str) -&gt; str:\n\t\t\t\"\"\"Get the appropriate comment syntax for a programming language.\n\n\t\t\tArgs:\n\t\t\t\tlanguage: The programming language name\n\n\t\t\tReturns:\n\t\t\t\tThe comment prefix for that language\n\t\t\t\"\"\"\n\t\t\tlanguage_lower = language.lower() if language else \"\"\n\n\t\t\t# Languages using // for comments\n\t\t\tif language_lower in (\n\t\t\t\t\"javascript\",\n\t\t\t\t\"java\",\n\t\t\t\t\"c\",\n\t\t\t\t\"cpp\",\n\t\t\t\t\"c++\",\n\t\t\t\t\"csharp\",\n\t\t\t\t\"c#\",\n\t\t\t\t\"go\",\n\t\t\t\t\"rust\",\n\t\t\t\t\"php\",\n\t\t\t\t\"kotlin\",\n\t\t\t\t\"swift\",\n\t\t\t\t\"typescript\",\n\t\t\t):\n\t\t\t\treturn \"//\"\n\t\t\t# Languages using -- for comments\n\t\t\tif language_lower in (\"sql\", \"haskell\", \"lua\"):\n\t\t\t\treturn \"--\"\n\t\t\t# Languages using % for comments\n\t\t\tif language_lower in (\"matlab\", \"octave\"):\n\t\t\t\treturn \"%\"\n\t\t\t# Languages using ; for comments\n\t\t\tif language_lower in (\"assembly\", \"asm\"):\n\t\t\t\treturn \";\"\n\t\t\t# Default to # for Python, Ruby, Shell, Perl, etc.\n\t\t\treturn \"#\"\n\n\t\t# Helper function to reconstruct code with LOD filtering\n\t\tdef reconstruct_code_with_lod(entity: LODEntity, current_indent: str = \"\") -&gt; list[str]:\n\t\t\t\"\"\"Reconstructs code content based on LOD level.\n\n\t\t\tArgs:\n\t\t\t\tentity: The entity to process\n\t\t\t\tcurrent_indent: Current indentation level\n\n\t\t\tReturns:\n\t\t\t\tList of code lines with appropriate LOD filtering\n\t\t\t\"\"\"\n\t\t\tlines = []\n\n\t\t\t# Skip certain entity types based on LOD level\n\t\t\tif entity.entity_type == EntityType.IMPORT and self.config.lod_level.value &lt; LODLevel.FULL.value:\n\t\t\t\treturn []  # Skip imports except at FULL level\n\n\t\t\tif entity.entity_type == EntityType.COMMENT and self.config.lod_level.value &lt; LODLevel.FULL.value:\n\t\t\t\treturn []  # Skip comments except at FULL level\n\n\t\t\t# Handle different entity types\n\t\t\tif entity.entity_type == EntityType.MODULE:\n\t\t\t\t# For modules, process children without adding module declaration\n\t\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\t\tchild_lines = reconstruct_code_with_lod(child, current_indent)\n\t\t\t\t\t\tlines.extend(child_lines)\n\t\t\t\t\t\tif child_lines:  # Add spacing between entities\n\t\t\t\t\t\t\tlines.append(\"\")\n\t\t\t\t\telse:\n\t\t\t\t\t\t# For UNKNOWN entities, check if they contain constants/variables as direct children\n\t\t\t\t\t\t# This handles cases where expression_statement wrappers contain assignment nodes\n\t\t\t\t\t\tfor grandchild in sorted(child.children, key=lambda e: e.start_line):\n\t\t\t\t\t\t\tif grandchild.entity_type in (EntityType.CONSTANT, EntityType.VARIABLE):\n\t\t\t\t\t\t\t\tgrandchild_lines = reconstruct_code_with_lod(grandchild, current_indent)\n\t\t\t\t\t\t\t\tlines.extend(grandchild_lines)\n\t\t\t\t\t\t\t\tif grandchild_lines:  # Add spacing between entities\n\t\t\t\t\t\t\t\t\tlines.append(\"\")\n\n\t\t\telif entity.entity_type in (EntityType.CLASS, EntityType.FUNCTION, EntityType.METHOD):\n\t\t\t\t# Add signature/declaration\n\t\t\t\tif entity.signature:\n\t\t\t\t\tlines.append(f\"{current_indent}{entity.signature}:\")\n\t\t\t\telif entity.content:\n\t\t\t\t\t# Extract first line as signature\n\t\t\t\t\tfirst_line = entity.content.split(\"\\n\")[0].strip()\n\t\t\t\t\tlines.append(f\"{current_indent}{first_line}:\")\n\t\t\t\telse:\n\t\t\t\t\t# Fallback\n\t\t\t\t\tentity_keyword = \"class\" if entity.entity_type == EntityType.CLASS else \"def\"\n\t\t\t\t\tlines.append(f\"{current_indent}{entity_keyword} {entity.name}:\")\n\n\t\t\t\t# Add docstring if available and level permits (but not at SKELETON/FULL level)\n\t\t\t\tif (\n\t\t\t\t\tentity.docstring\n\t\t\t\t\tand self.config.lod_level.value &gt;= LODLevel.DOCS.value\n\t\t\t\t\tand self.config.lod_level.value &lt; LODLevel.SKELETON.value\n\t\t\t\t):\n\t\t\t\t\tlines.append(f'{current_indent}    \"\"\"')\n\t\t\t\t\tlines.extend(f\"{current_indent}    {doc_line}\" for doc_line in entity.docstring.strip().split(\"\\n\"))\n\t\t\t\t\tlines.append(f'{current_indent}    \"\"\"')\n\n\t\t\t\t# Determine indentation for children\n\t\t\t\tchild_indent = current_indent + \"    \"\n\n\t\t\t\tif self.config.lod_level.value &gt;= LODLevel.FULL.value:\n\t\t\t\t\t# FULL level: include full implementation\n\t\t\t\t\tif entity.content and entity.entity_type != EntityType.MODULE:\n\t\t\t\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\t\t\t# Skip the first line (signature) as we already added it\n\t\t\t\t\t\tlines.extend(f\"{current_indent}{content_line}\" for content_line in content_lines[1:])\n\t\t\t\telif self.config.lod_level.value &gt;= LODLevel.SKELETON.value:\n\t\t\t\t\t# SKELETON level: show full content but filter out comments\n\t\t\t\t\tif entity.content and entity.entity_type != EntityType.MODULE:\n\t\t\t\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\t\t\t# Skip the first line (signature) as we already added it\n\t\t\t\t\t\tfor content_line in content_lines[1:]:\n\t\t\t\t\t\t\tstripped = content_line.strip()\n\n\t\t\t\t\t\t\t# Skip lines that are entirely comments\n\t\t\t\t\t\t\tif stripped and any(stripped.startswith(prefix) for prefix in (\"#\", \"//\", \"/*\")):\n\t\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t\t\t# Handle inline comments by removing them\n\t\t\t\t\t\t\tif stripped:\n\t\t\t\t\t\t\t\tcleaned_line = content_line\n\t\t\t\t\t\t\t\t# Remove inline comments (simple approach - look for comment markers)\n\t\t\t\t\t\t\t\tfor comment_prefix in [\"#\", \"//\"]:\n\t\t\t\t\t\t\t\t\tif comment_prefix in content_line:\n\t\t\t\t\t\t\t\t\t\t# Find the comment position (avoiding strings)\n\t\t\t\t\t\t\t\t\t\tcomment_pos = content_line.find(comment_prefix)\n\t\t\t\t\t\t\t\t\t\tif comment_pos != -1:\n\t\t\t\t\t\t\t\t\t\t\t# Simple check: if not in quotes, it's likely a comment\n\t\t\t\t\t\t\t\t\t\t\tbefore_comment = content_line[:comment_pos]\n\t\t\t\t\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t\t\t\t\tbefore_comment.count('\"') % 2 == 0\n\t\t\t\t\t\t\t\t\t\t\t\tand before_comment.count(\"'\") % 2 == 0\n\t\t\t\t\t\t\t\t\t\t\t):\n\t\t\t\t\t\t\t\t\t\t\t\tcleaned_line = before_comment.rstrip()\n\t\t\t\t\t\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\t\t\tif cleaned_line.strip():  # Only add if there's actual code content\n\t\t\t\t\t\t\t\t\tlines.append(f\"{current_indent}{cleaned_line}\")\n\t\t\t\t\t\t\telse:  # Keep empty lines for structure\n\t\t\t\t\t\t\t\tlines.append(f\"{current_indent}{content_line}\")\n\t\t\t\telse:\n\t\t\t\t\t# SIGNATURES/STRUCTURE/DOCS: process children as signatures only\n\t\t\t\t\thas_children = False\n\t\t\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\t\t\tchild_lines = reconstruct_code_with_lod(child, child_indent)\n\t\t\t\t\t\t\tif child_lines:\n\t\t\t\t\t\t\t\tlines.extend(child_lines)\n\t\t\t\t\t\t\t\thas_children = True\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t# For UNKNOWN entities, check if they contain constants/variables as direct children\n\t\t\t\t\t\t\tfor grandchild in sorted(child.children, key=lambda e: e.start_line):\n\t\t\t\t\t\t\t\tif grandchild.entity_type in (EntityType.CONSTANT, EntityType.VARIABLE):\n\t\t\t\t\t\t\t\t\tgrandchild_lines = reconstruct_code_with_lod(grandchild, child_indent)\n\t\t\t\t\t\t\t\t\tif grandchild_lines:\n\t\t\t\t\t\t\t\t\t\tlines.extend(grandchild_lines)\n\t\t\t\t\t\t\t\t\t\thas_children = True\n\n\t\t\t\t\t# If we have content but not showing full, add truncation comment\n\t\t\t\t\tcomment_prefix = get_comment_syntax(entity.language or \"\")\n\t\t\t\t\tif entity.content and not has_children:\n\t\t\t\t\t\tlines.append(f\"{child_indent}{comment_prefix} truncated for brevity\")\n\t\t\t\t\telif not has_children and entity.entity_type != EntityType.CLASS:\n\t\t\t\t\t\t# For functions/methods without children, add truncation comment\n\t\t\t\t\t\tlines.append(f\"{child_indent}{comment_prefix} hidden for brevity\")\n\n\t\t\telif entity.entity_type in (EntityType.VARIABLE, EntityType.CONSTANT):\n\t\t\t\t# At FULL level, show everything; at SKELETON+ levels, show the declaration\n\t\t\t\tif entity.content:\n\t\t\t\t\tif self.config.lod_level.value &gt;= LODLevel.FULL.value:\n\t\t\t\t\t\t# Show complete content at FULL level\n\t\t\t\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\t\t\tlines.extend(f\"{current_indent}{content_line}\" for content_line in content_lines)\n\t\t\t\t\telif self.config.lod_level.value &gt;= LODLevel.SKELETON.value:\n\t\t\t\t\t\t# Show just the first line (declaration) at SKELETON and higher levels\n\t\t\t\t\t\tfirst_line = entity.content.strip().split(\"\\n\")[0]\n\t\t\t\t\t\tlines.append(f\"{current_indent}{first_line}\")\n\t\t\t\t# If no content is available, skip the entity entirely\n\n\t\t\telif entity.entity_type == EntityType.IMPORT:\n\t\t\t\tif entity.content:\n\t\t\t\t\tlines.append(f\"{current_indent}{entity.content.strip()}\")\n\t\t\t\telse:\n\t\t\t\t\tlines.append(f\"{current_indent}import {entity.name}\")\n\n\t\t\telif entity.entity_type == EntityType.COMMENT and entity.content:\n\t\t\t\tlines.extend(f\"{current_indent}{comment_line}\" for comment_line in entity.content.strip().split(\"\\n\"))\n\n\t\t\treturn lines\n\n\t\tfirst_file = True\n\t\tfor i, (file_path, file_entities) in enumerate(sorted(files.items()), 1):\n\t\t\t# Add a divider before each file section except the first one\n\t\t\tif not first_file:\n\t\t\t\tcontent.append(\"\\n---\")  # Horizontal rule\n\t\t\tfirst_file = False\n\n\t\t\t# Get path relative to the target directory\n\t\t\ttry:\n\t\t\t\tif target_path and target_path.exists():\n\t\t\t\t\t# Get the relative path from the target directory\n\t\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\t\tfilename = file_path.name\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t\t# Handle paths with subdirectories\n\t\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\t\tanchor = clean_filename\n\t\t\t\telse:\n\t\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\t\tanchor = clean_filename\n\n\t\t\t\t# Add a custom ID to the heading to match our anchor\n\t\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\t\t\texcept ValueError:\n\t\t\t\t# If relative_to fails, just use the filename\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\n\t\t\t# Sort top-level entities by line number\n\t\t\tsorted_entities = sorted(file_entities, key=lambda e: e.start_line)\n\n\t\t\t# Generate code content using LOD-based reconstruction\n\t\t\tcode_lines = []\n\t\t\tfor entity in sorted_entities:\n\t\t\t\t# Process all entities and collect code lines\n\t\t\t\tentity_lines = reconstruct_code_with_lod(entity)\n\t\t\t\tcode_lines.extend(entity_lines)\n\n\t\t\t# Remove trailing empty lines\n\t\t\twhile code_lines and not code_lines[-1].strip():\n\t\t\t\tcode_lines.pop()\n\n\t\t\t# Add the code block with proper language detection\n\t\t\tif code_lines:\n\t\t\t\t# Get language from the first entity with language info\n\t\t\t\tlang = \"\"\n\t\t\t\tfor entity in sorted_entities:\n\t\t\t\t\tif entity.language:\n\t\t\t\t\t\tlang = entity.language\n\t\t\t\t\t\tbreak\n\n\t\t\t\tcontent.append(f\"\\n```{lang}\")\n\t\t\t\tcontent.extend(code_lines)\n\t\t\t\tcontent.append(\"```\")\n\n\t\treturn \"\\n\".join(content)\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator.__init__","title":"__init__","text":"<pre><code>__init__(config: GenSchema) -&gt; None\n</code></pre> <p>Initialize the code map generator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GenSchema</code> <p>Generation configuration settings</p> required Source code in <code>src/codemap/gen/generator.py</code> <pre><code>def __init__(self, config: GenSchema) -&gt; None:\n\t\"\"\"\n\tInitialize the code map generator.\n\n\tArgs:\n\t    config: Generation configuration settings\n\n\t\"\"\"\n\tself.config = config\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator.generate_documentation","title":"generate_documentation","text":"<pre><code>generate_documentation(\n\tentities: list[LODEntity], metadata: dict\n) -&gt; str\n</code></pre> <p>Generate markdown documentation from the processed LOD entities.</p> <p>Parameters:</p> Name Type Description Default <code>entities</code> <code>list[LODEntity]</code> <p>List of LOD entities</p> required <code>metadata</code> <code>dict</code> <p>Repository metadata</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated documentation as string</p> Source code in <code>src/codemap/gen/generator.py</code> <pre><code>def generate_documentation(self, entities: list[LODEntity], metadata: dict) -&gt; str:\n\t\"\"\"\n\tGenerate markdown documentation from the processed LOD entities.\n\n\tArgs:\n\t    entities: List of LOD entities\n\t    metadata: Repository metadata\n\n\tReturns:\n\t    Generated documentation as string\n\n\t\"\"\"\n\tcontent = []\n\n\t# Add header with repository information\n\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\toriginal_path = metadata.get(\"original_path\", \"\")\n\tcommand_arg = metadata.get(\"command_arg\", \"\")\n\n\t# Debug logging to see what values we're receiving\n\tlogger.debug(\n\t\tf\"Metadata values for heading: \"\n\t\tf\"command_arg='{command_arg}', \"\n\t\tf\"original_path='{original_path}', \"\n\t\tf\"target_path='{target_path_str}'\"\n\t)\n\n\t# Use the exact command argument if available\n\tif command_arg:\n\t\trepo_name = command_arg\n\t# Fall back to original path if available\n\telif original_path:\n\t\trepo_name = original_path\n\t# Further fallback to just the directory name\n\telif target_path_str:\n\t\trepo_name = Path(target_path_str).name\n\telse:\n\t\trepo_name = metadata.get(\"name\", \"Repository\")\n\n\tcontent.append(f\"# `{repo_name}` Documentation\")\n\tcontent.append(f\"\\nGenerated on: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\tif \"description\" in metadata:\n\t\tcontent.append(\"\\n\" + metadata.get(\"description\", \"\"))\n\n\t# Add repository statistics\n\tif \"stats\" in metadata:\n\t\tstats = metadata[\"stats\"]\n\t\tcontent.append(\"\\n## Document Statistics\")\n\t\tcontent.append(f\"- Total files scanned: {stats.get('total_files_scanned', 0)}\")\n\t\tcontent.append(f\"- Total lines of code: {stats.get('total_lines', 0)}\")\n\t\tcontent.append(f\"- Languages: {', '.join(stats.get('languages', []))}\")\n\n\t# Add directory structure if requested\n\tif self.config.include_tree and \"tree\" in metadata:\n\t\tcontent.append(\"\\n## Directory Structure\")\n\t\tcontent.append(\"```\")\n\t\tcontent.append(metadata[\"tree\"])\n\t\tcontent.append(\"```\")\n\n\t# Add Mermaid diagram if entities exist and config enables it\n\tif entities and self.config.include_entity_graph:\n\t\tcontent.append(\"\\n## Entity Relationships\")\n\t\tcontent.append(\"```mermaid\")\n\t\tmermaid_diagram = self._generate_mermaid_diagram(entities)\n\t\tcontent.append(mermaid_diagram)\n\t\tcontent.append(\"```\")\n\n\t# Add table of contents for the scanned files\n\tcontent.append(\"\\n## Scanned Files\")\n\n\t# Group entities by file\n\tfiles: dict[Path, list[LODEntity]] = {}\n\n\t# Get the target path from metadata\n\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\ttarget_path = Path(target_path_str) if target_path_str else None\n\n\tfor entity in entities:\n\t\tfile_path = Path(entity.metadata.get(\"file_path\", \"\"))\n\t\tif not file_path.name:\n\t\t\tcontinue\n\n\t\tif file_path not in files:\n\t\t\tfiles[file_path] = []\n\t\tfiles[file_path].append(entity)\n\n\t# Create TOC entries with properly formatted relative paths\n\tfor i, file_path in enumerate(sorted(files.keys()), 1):\n\t\t# Get path relative to the target directory\n\t\ttry:\n\t\t\tif target_path and target_path.exists():\n\t\t\t\t# Get the relative path from the target directory\n\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\tfilename = file_path.name\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t# Handle paths with subdirectories\n\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\telse:\n\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\tanchor = clean_filename\n\t\t\telse:\n\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tanchor = clean_filename\n\n\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{anchor})\")\n\t\texcept ValueError:\n\t\t\t# If relative_to fails, just use the filename\n\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{clean_filename})\")\n\n\t# Add code documentation grouped by file\n\tcontent.append(\"\\n## Code Documentation\")\n\n\t# Helper function to get comment syntax for a language\n\tdef get_comment_syntax(language: str) -&gt; str:\n\t\t\"\"\"Get the appropriate comment syntax for a programming language.\n\n\t\tArgs:\n\t\t\tlanguage: The programming language name\n\n\t\tReturns:\n\t\t\tThe comment prefix for that language\n\t\t\"\"\"\n\t\tlanguage_lower = language.lower() if language else \"\"\n\n\t\t# Languages using // for comments\n\t\tif language_lower in (\n\t\t\t\"javascript\",\n\t\t\t\"java\",\n\t\t\t\"c\",\n\t\t\t\"cpp\",\n\t\t\t\"c++\",\n\t\t\t\"csharp\",\n\t\t\t\"c#\",\n\t\t\t\"go\",\n\t\t\t\"rust\",\n\t\t\t\"php\",\n\t\t\t\"kotlin\",\n\t\t\t\"swift\",\n\t\t\t\"typescript\",\n\t\t):\n\t\t\treturn \"//\"\n\t\t# Languages using -- for comments\n\t\tif language_lower in (\"sql\", \"haskell\", \"lua\"):\n\t\t\treturn \"--\"\n\t\t# Languages using % for comments\n\t\tif language_lower in (\"matlab\", \"octave\"):\n\t\t\treturn \"%\"\n\t\t# Languages using ; for comments\n\t\tif language_lower in (\"assembly\", \"asm\"):\n\t\t\treturn \";\"\n\t\t# Default to # for Python, Ruby, Shell, Perl, etc.\n\t\treturn \"#\"\n\n\t# Helper function to reconstruct code with LOD filtering\n\tdef reconstruct_code_with_lod(entity: LODEntity, current_indent: str = \"\") -&gt; list[str]:\n\t\t\"\"\"Reconstructs code content based on LOD level.\n\n\t\tArgs:\n\t\t\tentity: The entity to process\n\t\t\tcurrent_indent: Current indentation level\n\n\t\tReturns:\n\t\t\tList of code lines with appropriate LOD filtering\n\t\t\"\"\"\n\t\tlines = []\n\n\t\t# Skip certain entity types based on LOD level\n\t\tif entity.entity_type == EntityType.IMPORT and self.config.lod_level.value &lt; LODLevel.FULL.value:\n\t\t\treturn []  # Skip imports except at FULL level\n\n\t\tif entity.entity_type == EntityType.COMMENT and self.config.lod_level.value &lt; LODLevel.FULL.value:\n\t\t\treturn []  # Skip comments except at FULL level\n\n\t\t# Handle different entity types\n\t\tif entity.entity_type == EntityType.MODULE:\n\t\t\t# For modules, process children without adding module declaration\n\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\tchild_lines = reconstruct_code_with_lod(child, current_indent)\n\t\t\t\t\tlines.extend(child_lines)\n\t\t\t\t\tif child_lines:  # Add spacing between entities\n\t\t\t\t\t\tlines.append(\"\")\n\t\t\t\telse:\n\t\t\t\t\t# For UNKNOWN entities, check if they contain constants/variables as direct children\n\t\t\t\t\t# This handles cases where expression_statement wrappers contain assignment nodes\n\t\t\t\t\tfor grandchild in sorted(child.children, key=lambda e: e.start_line):\n\t\t\t\t\t\tif grandchild.entity_type in (EntityType.CONSTANT, EntityType.VARIABLE):\n\t\t\t\t\t\t\tgrandchild_lines = reconstruct_code_with_lod(grandchild, current_indent)\n\t\t\t\t\t\t\tlines.extend(grandchild_lines)\n\t\t\t\t\t\t\tif grandchild_lines:  # Add spacing between entities\n\t\t\t\t\t\t\t\tlines.append(\"\")\n\n\t\telif entity.entity_type in (EntityType.CLASS, EntityType.FUNCTION, EntityType.METHOD):\n\t\t\t# Add signature/declaration\n\t\t\tif entity.signature:\n\t\t\t\tlines.append(f\"{current_indent}{entity.signature}:\")\n\t\t\telif entity.content:\n\t\t\t\t# Extract first line as signature\n\t\t\t\tfirst_line = entity.content.split(\"\\n\")[0].strip()\n\t\t\t\tlines.append(f\"{current_indent}{first_line}:\")\n\t\t\telse:\n\t\t\t\t# Fallback\n\t\t\t\tentity_keyword = \"class\" if entity.entity_type == EntityType.CLASS else \"def\"\n\t\t\t\tlines.append(f\"{current_indent}{entity_keyword} {entity.name}:\")\n\n\t\t\t# Add docstring if available and level permits (but not at SKELETON/FULL level)\n\t\t\tif (\n\t\t\t\tentity.docstring\n\t\t\t\tand self.config.lod_level.value &gt;= LODLevel.DOCS.value\n\t\t\t\tand self.config.lod_level.value &lt; LODLevel.SKELETON.value\n\t\t\t):\n\t\t\t\tlines.append(f'{current_indent}    \"\"\"')\n\t\t\t\tlines.extend(f\"{current_indent}    {doc_line}\" for doc_line in entity.docstring.strip().split(\"\\n\"))\n\t\t\t\tlines.append(f'{current_indent}    \"\"\"')\n\n\t\t\t# Determine indentation for children\n\t\t\tchild_indent = current_indent + \"    \"\n\n\t\t\tif self.config.lod_level.value &gt;= LODLevel.FULL.value:\n\t\t\t\t# FULL level: include full implementation\n\t\t\t\tif entity.content and entity.entity_type != EntityType.MODULE:\n\t\t\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\t\t# Skip the first line (signature) as we already added it\n\t\t\t\t\tlines.extend(f\"{current_indent}{content_line}\" for content_line in content_lines[1:])\n\t\t\telif self.config.lod_level.value &gt;= LODLevel.SKELETON.value:\n\t\t\t\t# SKELETON level: show full content but filter out comments\n\t\t\t\tif entity.content and entity.entity_type != EntityType.MODULE:\n\t\t\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\t\t# Skip the first line (signature) as we already added it\n\t\t\t\t\tfor content_line in content_lines[1:]:\n\t\t\t\t\t\tstripped = content_line.strip()\n\n\t\t\t\t\t\t# Skip lines that are entirely comments\n\t\t\t\t\t\tif stripped and any(stripped.startswith(prefix) for prefix in (\"#\", \"//\", \"/*\")):\n\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t\t# Handle inline comments by removing them\n\t\t\t\t\t\tif stripped:\n\t\t\t\t\t\t\tcleaned_line = content_line\n\t\t\t\t\t\t\t# Remove inline comments (simple approach - look for comment markers)\n\t\t\t\t\t\t\tfor comment_prefix in [\"#\", \"//\"]:\n\t\t\t\t\t\t\t\tif comment_prefix in content_line:\n\t\t\t\t\t\t\t\t\t# Find the comment position (avoiding strings)\n\t\t\t\t\t\t\t\t\tcomment_pos = content_line.find(comment_prefix)\n\t\t\t\t\t\t\t\t\tif comment_pos != -1:\n\t\t\t\t\t\t\t\t\t\t# Simple check: if not in quotes, it's likely a comment\n\t\t\t\t\t\t\t\t\t\tbefore_comment = content_line[:comment_pos]\n\t\t\t\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t\t\t\tbefore_comment.count('\"') % 2 == 0\n\t\t\t\t\t\t\t\t\t\t\tand before_comment.count(\"'\") % 2 == 0\n\t\t\t\t\t\t\t\t\t\t):\n\t\t\t\t\t\t\t\t\t\t\tcleaned_line = before_comment.rstrip()\n\t\t\t\t\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\t\tif cleaned_line.strip():  # Only add if there's actual code content\n\t\t\t\t\t\t\t\tlines.append(f\"{current_indent}{cleaned_line}\")\n\t\t\t\t\t\telse:  # Keep empty lines for structure\n\t\t\t\t\t\t\tlines.append(f\"{current_indent}{content_line}\")\n\t\t\telse:\n\t\t\t\t# SIGNATURES/STRUCTURE/DOCS: process children as signatures only\n\t\t\t\thas_children = False\n\t\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\t\tchild_lines = reconstruct_code_with_lod(child, child_indent)\n\t\t\t\t\t\tif child_lines:\n\t\t\t\t\t\t\tlines.extend(child_lines)\n\t\t\t\t\t\t\thas_children = True\n\t\t\t\t\telse:\n\t\t\t\t\t\t# For UNKNOWN entities, check if they contain constants/variables as direct children\n\t\t\t\t\t\tfor grandchild in sorted(child.children, key=lambda e: e.start_line):\n\t\t\t\t\t\t\tif grandchild.entity_type in (EntityType.CONSTANT, EntityType.VARIABLE):\n\t\t\t\t\t\t\t\tgrandchild_lines = reconstruct_code_with_lod(grandchild, child_indent)\n\t\t\t\t\t\t\t\tif grandchild_lines:\n\t\t\t\t\t\t\t\t\tlines.extend(grandchild_lines)\n\t\t\t\t\t\t\t\t\thas_children = True\n\n\t\t\t\t# If we have content but not showing full, add truncation comment\n\t\t\t\tcomment_prefix = get_comment_syntax(entity.language or \"\")\n\t\t\t\tif entity.content and not has_children:\n\t\t\t\t\tlines.append(f\"{child_indent}{comment_prefix} truncated for brevity\")\n\t\t\t\telif not has_children and entity.entity_type != EntityType.CLASS:\n\t\t\t\t\t# For functions/methods without children, add truncation comment\n\t\t\t\t\tlines.append(f\"{child_indent}{comment_prefix} hidden for brevity\")\n\n\t\telif entity.entity_type in (EntityType.VARIABLE, EntityType.CONSTANT):\n\t\t\t# At FULL level, show everything; at SKELETON+ levels, show the declaration\n\t\t\tif entity.content:\n\t\t\t\tif self.config.lod_level.value &gt;= LODLevel.FULL.value:\n\t\t\t\t\t# Show complete content at FULL level\n\t\t\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\t\tlines.extend(f\"{current_indent}{content_line}\" for content_line in content_lines)\n\t\t\t\telif self.config.lod_level.value &gt;= LODLevel.SKELETON.value:\n\t\t\t\t\t# Show just the first line (declaration) at SKELETON and higher levels\n\t\t\t\t\tfirst_line = entity.content.strip().split(\"\\n\")[0]\n\t\t\t\t\tlines.append(f\"{current_indent}{first_line}\")\n\t\t\t# If no content is available, skip the entity entirely\n\n\t\telif entity.entity_type == EntityType.IMPORT:\n\t\t\tif entity.content:\n\t\t\t\tlines.append(f\"{current_indent}{entity.content.strip()}\")\n\t\t\telse:\n\t\t\t\tlines.append(f\"{current_indent}import {entity.name}\")\n\n\t\telif entity.entity_type == EntityType.COMMENT and entity.content:\n\t\t\tlines.extend(f\"{current_indent}{comment_line}\" for comment_line in entity.content.strip().split(\"\\n\"))\n\n\t\treturn lines\n\n\tfirst_file = True\n\tfor i, (file_path, file_entities) in enumerate(sorted(files.items()), 1):\n\t\t# Add a divider before each file section except the first one\n\t\tif not first_file:\n\t\t\tcontent.append(\"\\n---\")  # Horizontal rule\n\t\tfirst_file = False\n\n\t\t# Get path relative to the target directory\n\t\ttry:\n\t\t\tif target_path and target_path.exists():\n\t\t\t\t# Get the relative path from the target directory\n\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\tfilename = file_path.name\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t# Handle paths with subdirectories\n\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\telse:\n\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\tanchor = clean_filename\n\t\t\telse:\n\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tanchor = clean_filename\n\n\t\t\t# Add a custom ID to the heading to match our anchor\n\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\t\texcept ValueError:\n\t\t\t# If relative_to fails, just use the filename\n\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\n\t\t# Sort top-level entities by line number\n\t\tsorted_entities = sorted(file_entities, key=lambda e: e.start_line)\n\n\t\t# Generate code content using LOD-based reconstruction\n\t\tcode_lines = []\n\t\tfor entity in sorted_entities:\n\t\t\t# Process all entities and collect code lines\n\t\t\tentity_lines = reconstruct_code_with_lod(entity)\n\t\t\tcode_lines.extend(entity_lines)\n\n\t\t# Remove trailing empty lines\n\t\twhile code_lines and not code_lines[-1].strip():\n\t\t\tcode_lines.pop()\n\n\t\t# Add the code block with proper language detection\n\t\tif code_lines:\n\t\t\t# Get language from the first entity with language info\n\t\t\tlang = \"\"\n\t\t\tfor entity in sorted_entities:\n\t\t\t\tif entity.language:\n\t\t\t\t\tlang = entity.language\n\t\t\t\t\tbreak\n\n\t\t\tcontent.append(f\"\\n```{lang}\")\n\t\t\tcontent.extend(code_lines)\n\t\t\tcontent.append(\"```\")\n\n\treturn \"\\n\".join(content)\n</code></pre>"},{"location":"api/gen/utils/","title":"Utils","text":"<p>Utility functions for the gen command.</p>"},{"location":"api/gen/utils/#codemap.gen.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.process_files_for_lod","title":"process_files_for_lod","text":"<pre><code>process_files_for_lod(\n\tpaths: Sequence[Path],\n\tlod_level: LODLevel,\n\tmax_workers: int = 4,\n) -&gt; list[LODEntity]\n</code></pre> <p>Process a list of file paths to generate LOD entities in parallel.</p> <p>Bypasses the main ProcessingPipeline and uses LODGenerator directly.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Sequence[Path]</code> <p>Sequence of file paths to process.</p> required <code>lod_level</code> <code>LODLevel</code> <p>The level of detail required.</p> required <code>max_workers</code> <code>int</code> <p>Maximum number of parallel worker threads.</p> <code>4</code> <p>Returns:</p> Type Description <code>list[LODEntity]</code> <p>A list of successfully generated LODEntity objects.</p> Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def process_files_for_lod(\n\tpaths: Sequence[Path],\n\tlod_level: LODLevel,\n\tmax_workers: int = 4,\n) -&gt; list[LODEntity]:\n\t\"\"\"\n\tProcess a list of file paths to generate LOD entities in parallel.\n\n\tBypasses the main ProcessingPipeline and uses LODGenerator directly.\n\n\tArgs:\n\t        paths: Sequence of file paths to process.\n\t        lod_level: The level of detail required.\n\t        max_workers: Maximum number of parallel worker threads.\n\n\tReturns:\n\t        A list of successfully generated LODEntity objects.\n\n\t\"\"\"\n\tlod_generator = LODGenerator()\n\tlod_entities = []\n\tfutures: list[Future[LODEntity | None]] = []\n\tfiles_to_process = [p for p in paths if p.is_file()]\n\ttask_description = \"\"\n\n\ttask_description = f\"Processing {len(files_to_process)} files for LOD...\"\n\tcurrent_progress = 0\n\n\twith (\n\t\tprogress_indicator(task_description, style=\"progress\", total=len(files_to_process)) as update_progress,\n\t\tThreadPoolExecutor(max_workers=max_workers) as executor,\n\t):\n\t\tfor file_path in files_to_process:\n\t\t\tfuture = executor.submit(_process_single_file_lod, file_path, lod_level, lod_generator)\n\t\t\tfutures.append(future)\n\n\t\tfor future in as_completed(futures):\n\t\t\tresult = future.result()\n\t\t\tif result:\n\t\t\t\tlod_entities.append(result)\n\t\t\tcurrent_progress += 1\n\t\t\tupdate_progress(task_description, current_progress, len(files_to_process))\n\n\tlogger.info(\"Finished LOD processing. Generated %d entities.\", len(lod_entities))\n\treturn lod_entities\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.generate_tree","title":"generate_tree","text":"<pre><code>generate_tree(\n\ttarget_path: Path, filtered_paths: Sequence[Path]\n) -&gt; str\n</code></pre> <p>Generate a directory tree representation.</p> <p>Parameters:</p> Name Type Description Default <code>target_path</code> <code>Path</code> <p>Root path</p> required <code>filtered_paths</code> <code>Sequence[Path]</code> <p>List of filtered absolute paths within target_path</p> required <p>Returns:</p> Type Description <code>str</code> <p>Tree representation as string</p> Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def generate_tree(target_path: Path, filtered_paths: Sequence[Path]) -&gt; str:\n\t\"\"\"\n\tGenerate a directory tree representation.\n\n\tArgs:\n\t    target_path: Root path\n\t    filtered_paths: List of filtered **absolute** paths within target_path\n\n\tReturns:\n\t    Tree representation as string\n\n\t\"\"\"\n\t# Build a nested dictionary representing the file structure\n\ttree: dict[str, Any] = {}\n\n\t# Process directories first to ensure complete structure\n\t# Sort paths to process directories first, then files, all in alphabetical order\n\tsorted_paths = sorted(filtered_paths, key=lambda p: (p.is_file(), str(p).lower()))\n\n\t# Ensure target_path itself is in the structure if it's not already\n\tdir_paths = [p for p in sorted_paths if p.is_dir()]\n\n\t# Add all directories first to ensure complete structure\n\tfor abs_path in dir_paths:\n\t\t# Ensure we only process paths within the target_path\n\t\ttry:\n\t\t\trel_path = abs_path.relative_to(target_path)\n\t\t\tdir_parts = rel_path.parts\n\n\t\t\tcurrent_level: dict[str, Any] = tree\n\t\t\tfor _i, part in enumerate(dir_parts):\n\t\t\t\tif part not in current_level:\n\t\t\t\t\tcurrent_level[part] = {}\n\t\t\t\tcurrent_level = current_level[part]\n\t\texcept ValueError:\n\t\t\tcontinue  # Skip paths not under target_path\n\n\t# Then add files to the structure\n\tfile_paths = [p for p in sorted_paths if p.is_file()]\n\tfor abs_path in file_paths:\n\t\ttry:\n\t\t\trel_path = abs_path.relative_to(target_path)\n\t\t\tparts = rel_path.parts\n\n\t\t\tcurrent_level = tree\n\t\t\tfor i, part in enumerate(parts):\n\t\t\t\tif i == len(parts) - 1:  # Last part (file)\n\t\t\t\t\tcurrent_level[part] = \"file\"\n\t\t\t\telse:\n\t\t\t\t\t# Create directory levels if they don't exist\n\t\t\t\t\tif part not in current_level:\n\t\t\t\t\t\tcurrent_level[part] = {}\n\n\t\t\t\t\t# Get reference to the next level\n\t\t\t\t\tnext_level = current_level[part]\n\n\t\t\t\t\t# Handle case where a file might exist with the same name as a directory part\n\t\t\t\t\tif not isinstance(next_level, dict):\n\t\t\t\t\t\t# This shouldn't happen with proper directory structure, but handle just in case\n\t\t\t\t\t\tlogger.warning(f\"Name conflict: {part} is both a file and a directory in path {rel_path}\")\n\t\t\t\t\t\t# Convert to dictionary with special file marker\n\t\t\t\t\t\tcurrent_level[part] = {\"__file__\": True}\n\n\t\t\t\t\tcurrent_level = current_level[part]\n\t\texcept ValueError:\n\t\t\tcontinue  # Skip paths not under target_path\n\n\t# Get just the target directory name to display at the root of the tree\n\t# rather than the full absolute path\n\ttarget_dir_name = target_path.name\n\n\t# Initialize tree_lines with just the directory name at the root\n\ttree_lines = [target_dir_name]\n\n\t# Recursive function to generate formatted tree lines\n\tdef format_level(level: dict[str, Any], prefix: str = \"\") -&gt; None:\n\t\t\"\"\"Recursively formats a directory tree level into ASCII tree representation.\n\n\t\tArgs:\n\t\t    level: Dictionary representing the current directory level, where keys are names\n\t\t        and values are either subdirectories (dicts) or files (strings).\n\t\t    prefix: String used for indentation and tree connectors in the output. Defaults to \"\".\n\n\t\tReturns:\n\t\t    None: Modifies the tree_lines list in the closure by appending formatted lines.\n\n\t\tNote:\n\t\t    - Directories are sorted before files\n\t\t    - Items are sorted alphabetically within their type group\n\t\t    - Special markers (like \"__file__\") are skipped\n\t\t\"\"\"\n\t\t# Sort items: directories first (dictionaries), then files (strings)\n\t\tsorted_items = sorted(level.items(), key=lambda x: (not isinstance(x[1], dict), x[0].lower()))\n\n\t\tfor i, (name, item_type) in enumerate(sorted_items):\n\t\t\tis_last_item = i == len(sorted_items) - 1\n\t\t\tconnector = \"\u2514\u2500\u2500 \" if is_last_item else \"\u251c\u2500\u2500 \"\n\n\t\t\tif name == \"__file__\":\n\t\t\t\t# Skip special markers\n\t\t\t\tcontinue\n\n\t\t\tif isinstance(item_type, dict):  # It's a directory\n\t\t\t\ttree_lines.append(f\"{prefix}{connector}{name}/\")\n\t\t\t\tnew_prefix = prefix + (\"    \" if is_last_item else \"\u2502   \")\n\t\t\t\tformat_level(item_type, new_prefix)\n\t\t\telse:  # It's a file\n\t\t\t\ttree_lines.append(f\"{prefix}{connector}{name}\")\n\n\t# Start formatting from the root\n\tformat_level(tree)\n\n\t# If tree only contains the target path (no files/directories under it)\n\tif len(tree_lines) == 1:\n\t\treturn tree_lines[0] + \"/\"\n\n\treturn \"\\n\".join(tree_lines)\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.determine_output_path","title":"determine_output_path","text":"<pre><code>determine_output_path(\n\tproject_root: Path,\n\tconfig_loader: ConfigLoader,\n\toutput: Path | None,\n) -&gt; Path\n</code></pre> <p>Determine the output path for documentation.</p> <p>Parameters:</p> Name Type Description Default <code>project_root</code> <code>Path</code> <p>Root directory of the project</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance</p> required <code>output</code> <code>Path | None</code> <p>Optional output path from command line</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The determined output path</p> Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def determine_output_path(project_root: Path, config_loader: ConfigLoader, output: Path | None) -&gt; Path:\n\t\"\"\"\n\tDetermine the output path for documentation.\n\n\tArgs:\n\t    project_root: Root directory of the project\n\t    config_loader: ConfigLoader instance\n\t    output: Optional output path from command line\n\n\tReturns:\n\t    The determined output path\n\n\t\"\"\"\n\tfrom datetime import UTC, datetime\n\n\t# If output is provided, use it directly\n\tif output:\n\t\treturn output.resolve()\n\n\t# Get output directory from config\n\toutput_dir = config_loader.get.gen.output_dir\n\n\t# If output_dir is absolute, use it directly\n\toutput_dir_path = Path(output_dir)\n\tif not output_dir_path.is_absolute():\n\t\t# Otherwise, create the output directory in the project root\n\t\toutput_dir_path = project_root / output_dir\n\n\toutput_dir_path.mkdir(parents=True, exist_ok=True)\n\n\t# Generate a filename with timestamp\n\ttimestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n\tfilename = f\"documentation_{timestamp}.md\"\n\n\treturn output_dir_path / filename\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.process_codebase","title":"process_codebase","text":"<pre><code>process_codebase(\n\ttarget_path: Path,\n\tconfig: GenSchema,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; tuple[list[LODEntity], dict]\n</code></pre> <p>Process a codebase using the LOD pipeline architecture.</p> <p>Parameters:</p> Name Type Description Default <code>target_path</code> <code>Path</code> <p>Path to the target codebase</p> required <code>config</code> <code>GenSchema</code> <p>Generation configuration</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[LODEntity], dict]</code> <p>Tuple of (list of LOD entities, metadata dict)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If processing fails</p> Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def process_codebase(\n\ttarget_path: Path,\n\tconfig: GenSchema,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; tuple[list[LODEntity], dict]:\n\t\"\"\"\n\tProcess a codebase using the LOD pipeline architecture.\n\n\tArgs:\n\t    target_path: Path to the target codebase\n\t    config: Generation configuration\n\t    config_loader: Optional ConfigLoader instance to use\n\n\tReturns:\n\t    Tuple of (list of LOD entities, metadata dict)\n\n\tRaises:\n\t    RuntimeError: If processing fails\n\n\t\"\"\"\n\tlogger.info(\"Starting codebase processing for: %s\", target_path)\n\n\t# Get processor configuration from ConfigLoader\n\tif config_loader is None:\n\t\tconfig_loader = ConfigLoader().get_instance()\n\t\tlogger.debug(\"Created new ConfigLoader instance in process_codebase\")\n\n\tprocessor_config = config_loader.get.processor\n\tmax_workers = processor_config.max_workers\n\tlogger.debug(f\"Using max_workers: {max_workers} from configuration\")\n\n\ttry:\n\t\t# Handle both directories and individual files\n\t\tif target_path.is_file():\n\t\t\t# Single file processing\n\t\t\tif not is_binary_file(target_path):\n\t\t\t\tprocessable_paths = [target_path]\n\t\t\telse:\n\t\t\t\tlogger.debug(f\"Skipping binary file: {target_path}\")\n\t\t\t\tprocessable_paths = []\n\t\telif target_path.is_dir():\n\t\t\t# Directory processing (existing logic)\n\t\t\tproject_root = Path.cwd()  # Assuming CWD is project root\n\t\t\tall_paths = list(target_path.rglob(\"*\"))\n\n\t\t\t# Filter paths based on .gitignore patterns found in project_root\n\t\t\tfiltered_paths: Sequence[Path] = filter_paths_by_gitignore(all_paths, project_root)\n\n\t\t\t# Filter out binary files\n\t\t\tprocessable_paths = []\n\t\t\tfor path in filtered_paths:\n\t\t\t\tif path.is_file():\n\t\t\t\t\tif not is_binary_file(path):\n\t\t\t\t\t\tprocessable_paths.append(path)\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.debug(f\"Skipping binary file: {path}\")\n\t\telse:\n\t\t\tlogger.warning(f\"Target path does not exist or is not a file/directory: {target_path}\")\n\t\t\tprocessable_paths = []\n\n\t\t# Use the new utility function to process files and generate LOD entities\n\t\t# The utility function will handle parallel processing and progress updates\n\t\tentities = process_files_for_lod(\n\t\t\tpaths=processable_paths,\n\t\t\tlod_level=config.lod_level,\n\t\t\tmax_workers=max_workers,  # Get from configuration\n\t\t)\n\texcept Exception as e:\n\t\tlogger.exception(\"Error during LOD file processing\")\n\t\terror_msg = f\"LOD processing failed: {e}\"\n\t\traise RuntimeError(error_msg) from e\n\n\t# Update counts based on actual processed entities\n\tprocessed_files = len(entities)\n\tlogger.info(f\"LOD processing complete. Generated {processed_files} entities.\")\n\t# total_files count is now handled within process_files_for_lod for progress\n\n\t# Generate repository metadata\n\tlanguages = {entity.language for entity in entities if entity.language}\n\t# Get total file count accurately from the filtered list *before* processing\n\ttotal_files_scanned = len(processable_paths)\n\n\tmetadata: dict[str, Any] = {\n\t\t\"name\": target_path.name,\n\t\t\"target_path\": str(target_path.resolve()),  # Keep absolute target path for file path resolution\n\t\t\"original_path\": str(target_path),  # Store original path as provided in the command (could be relative)\n\t\t\"stats\": {\n\t\t\t\"total_files_scanned\": total_files_scanned,  # Total files scanned matching criteria\n\t\t\t\"processed_files\": processed_files,  # Files successfully processed for LOD\n\t\t\t\"total_lines\": sum(entity.end_line - entity.start_line + 1 for entity in entities),\n\t\t\t\"languages\": list(languages),\n\t\t},\n\t}\n\n\t# Generate directory tree if requested\n\tif config.include_tree:\n\t\tmetadata[\"tree\"] = generate_tree(target_path, filtered_paths)\n\n\treturn entities, metadata\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.write_documentation","title":"write_documentation","text":"<pre><code>write_documentation(\n\toutput_path: Path, documentation: str\n) -&gt; None\n</code></pre> <p>Write documentation to the specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path</code> <p>Path to write documentation to</p> required <code>documentation</code> <code>str</code> <p>Documentation content to write</p> required Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def write_documentation(output_path: Path, documentation: str) -&gt; None:\n\t\"\"\"\n\tWrite documentation to the specified output path.\n\n\tArgs:\n\t    output_path: Path to write documentation to\n\t    documentation: Documentation content to write\n\n\t\"\"\"\n\tfrom codemap.utils.cli_utils import console\n\tfrom codemap.utils.file_utils import ensure_directory_exists\n\n\ttry:\n\t\t# Ensure parent directory exists\n\t\tensure_directory_exists(output_path.parent)\n\t\toutput_path.write_text(documentation)\n\t\tconsole.print(f\"[green]Documentation written to {output_path}\")\n\texcept (PermissionError, OSError):\n\t\tlogger.exception(f\"Error writing documentation to {output_path}\")\n\t\traise\n</code></pre>"},{"location":"api/git/","title":"Git Overview","text":"<p>Git utilities for CodeMap.</p> <ul> <li>Commit Generator - Commit message generation package for CodeMap.</li> <li>Commit Linter - Commit linter package for validating git commit messages according to conventional commits.</li> <li>Diff Splitter - Diff splitting package for CodeMap.</li> <li>Interactive - Interactive commit interface for CodeMap.</li> <li>Pr Generator - PR generation package for CodeMap.</li> <li>Semantic Grouping - Semantic grouping implementation for the CodeMap project.</li> <li>Utils - Git utilities for CodeMap.</li> </ul>"},{"location":"api/git/interactive/","title":"Interactive","text":"<p>Interactive commit interface for CodeMap.</p>"},{"location":"api/git/interactive/#codemap.git.interactive.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.MAX_PREVIEW_LENGTH","title":"MAX_PREVIEW_LENGTH  <code>module-attribute</code>","text":"<pre><code>MAX_PREVIEW_LENGTH = 200\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.MAX_PREVIEW_LINES","title":"MAX_PREVIEW_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_PREVIEW_LINES = 10\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction","title":"ChunkAction","text":"<p>               Bases: <code>Enum</code></p> <p>Possible actions for a diff chunk.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>class ChunkAction(Enum):\n\t\"\"\"Possible actions for a diff chunk.\"\"\"\n\n\tCOMMIT = auto()\n\tEDIT = auto()\n\tSKIP = auto()\n\tABORT = auto()\n\tREGENERATE = auto()\n\tEXIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.COMMIT","title":"COMMIT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMMIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.EDIT","title":"EDIT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EDIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.SKIP","title":"SKIP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKIP = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.ABORT","title":"ABORT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ABORT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.REGENERATE","title":"REGENERATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REGENERATE = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.EXIT","title":"EXIT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult","title":"ChunkResult  <code>dataclass</code>","text":"<p>Result of processing a diff chunk.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>@dataclass\nclass ChunkResult:\n\t\"\"\"Result of processing a diff chunk.\"\"\"\n\n\taction: ChunkAction\n\tmessage: str | None = None\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult.__init__","title":"__init__","text":"<pre><code>__init__(\n\taction: ChunkAction, message: str | None = None\n) -&gt; None\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: ChunkAction\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult.message","title":"message  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>message: str | None = None\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI","title":"CommitUI","text":"<p>Interactive UI for the commit process.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>class CommitUI:\n\t\"\"\"Interactive UI for the commit process.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the commit UI.\"\"\"\n\t\tself.console = Console()\n\n\tdef display_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a diff chunk to the user.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk to display\n\t\t    index: The 0-based index of the current chunk\n\t\t    total: The total number of chunks\n\n\t\t\"\"\"\n\t\t# Build file information\n\t\tfile_info = Text(\"Files: \", style=\"blue\")\n\t\tfile_info.append(\", \".join(chunk.files))\n\n\t\t# Calculate changes\n\t\tadded = len(\n\t\t\t[line for line in chunk.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t\t)\n\t\tremoved = len(\n\t\t\t[line for line in chunk.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t\t)\n\t\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\t\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t\t# Prepare diff content\n\t\tpanel_content = chunk.content\n\t\tif not panel_content.strip():\n\t\t\tpanel_content = \"No content diff available (e.g., new file or mode change)\"\n\n\t\t# Truncate to maximum of MAX_PREVIEW_LINES lines\n\t\tcontent_lines = panel_content.splitlines()\n\t\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\t\tpanel_content = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\n\t\tdiff_content = Text(\"\\n\" + panel_content)\n\n\t\t# Determine title for the panel - use provided index and total\n\t\tpanel_title = f\"[bold]Commit {index + 1} of {total}[/bold]\"\n\n\t\t# Create content for the panel conditionally\n\t\tif getattr(chunk, \"description\", None):\n\t\t\t# If there's a description, create a combined panel\n\t\t\tif getattr(chunk, \"is_llm_generated\", False):\n\t\t\t\tmessage_title = \"[bold blue]Proposed message (AI)[/]\"\n\t\t\t\tmessage_style = \"blue\"\n\t\t\telse:\n\t\t\t\tmessage_title = \"[bold yellow]Proposed message (Simple)[/]\"\n\t\t\t\tmessage_style = \"yellow\"\n\n\t\t\t# Create separate panels and print them\n\t\t\t# First, print the diff panel\n\t\t\tdiff_panel = Panel(\n\t\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\t\ttitle=panel_title,\n\t\t\t\tborder_style=\"cyan\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(diff_panel)\n\n\t\t\t# Print divider\n\t\t\tself.console.print(Rule(style=\"dim\"))\n\n\t\t\t# Then print the message panel\n\t\t\tmessage_panel = Panel(\n\t\t\t\tText(str(chunk.description), style=\"green\"),\n\t\t\t\ttitle=message_title,\n\t\t\t\tborder_style=message_style,\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(message_panel)\n\t\telse:\n\t\t\t# If no description, just print the diff panel\n\t\t\tpanel = Panel(\n\t\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\t\ttitle=panel_title,\n\t\t\t\tborder_style=\"cyan\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print()\n\t\t\tself.console.print(panel)\n\t\t\tself.console.print()\n\n\tdef display_group(self, group: SemanticGroup, index: int = 0, total: int = 1) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a semantic group to the user.\n\n\t\tArgs:\n\t\t        group: SemanticGroup to display\n\t\t        index: The 0-based index of the current group\n\t\t        total: The total number of groups\n\n\t\t\"\"\"\n\t\t# Build file information\n\t\tfile_list = \"\\n\".join([f\"  - {file}\" for file in group.files])\n\t\tfile_info = Text(f\"Files ({len(group.files)}):\\n\", style=\"blue\")\n\t\tfile_info.append(file_list)\n\n\t\t# Prepare diff preview - show first few lines of diff content\n\t\tdiff_preview = group.content\n\t\tcontent_lines = diff_preview.splitlines()\n\t\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\t\tdiff_preview = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\t\tdiff_content = Text(\"\\n\\nDiff Preview:\\n\", style=\"blue\")\n\t\tdiff_content.append(diff_preview)\n\n\t\t# Calculate changes\n\t\tadded = len(\n\t\t\t[line for line in group.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t\t)\n\t\tremoved = len(\n\t\t\t[line for line in group.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t\t)\n\t\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\t\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t\t# Determine title for the panel\n\t\tpanel_title = f\"[bold]Group {index + 1} of {total}[/bold]\"\n\n\t\t# Create diff panel\n\t\tdiff_panel = Panel(\n\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\ttitle=panel_title,\n\t\t\tborder_style=\"cyan\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(diff_panel)\n\n\t\t# Print divider\n\t\tself.console.print(Rule(style=\"dim\"))\n\n\t\t# Create message panel if message exists\n\t\tif hasattr(group, \"message\") and group.message:\n\t\t\t# Create message panel\n\t\t\tmessage_panel = Panel(\n\t\t\t\tText(str(group.message), style=\"green\"),\n\t\t\t\ttitle=\"[bold blue]Generated message[/]\",\n\t\t\t\tborder_style=\"green\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(message_panel)\n\t\telse:\n\t\t\tself.console.print(\n\t\t\t\tPanel(\n\t\t\t\t\tText(\"No message generated yet\", style=\"dim\"),\n\t\t\t\t\ttitle=\"[bold]Message[/]\",\n\t\t\t\t\tborder_style=\"yellow\",\n\t\t\t\t\texpand=True,\n\t\t\t\t\twidth=self.console.width,\n\t\t\t\t\tpadding=(1, 2),\n\t\t\t\t)\n\t\t\t)\n\n\tdef display_message(self, message: str, is_llm_generated: bool = False) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a commit message to the user.\n\n\t\tArgs:\n\t\t    message: The commit message to display\n\t\t    is_llm_generated: Whether the message was generated by an LLM\n\n\t\t\"\"\"\n\t\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\t\tmessage_panel = Panel(\n\t\t\tText(message, style=\"green\"),\n\t\t\ttitle=f\"[bold {'blue' if is_llm_generated else 'yellow'}]Proposed message ({tag})[/]\",\n\t\t\tborder_style=\"blue\" if is_llm_generated else \"yellow\",\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\n\tdef get_user_action(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tGet the user's desired action for the current chunk.\n\n\t\tReturns:\n\t\t    ChunkAction indicating what to do with the chunk\n\n\t\t\"\"\"\n\t\t# Define options with their display text and corresponding action\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Commit with this message\", ChunkAction.COMMIT),\n\t\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\n\t\t# Use questionary to get the user's choice\n\t\tresult = questionary.select(\n\t\t\t\"What would you like to do?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tdefault=options[0][0],  # Set \"Commit with this message\" as default\n\t\t\tqmark=\"\u00bb\",\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\n\t\t# Map the result back to the ChunkAction\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\n\t\t# Fallback (should never happen)\n\t\treturn ChunkAction.EXIT\n\n\tdef get_user_action_on_lint_failure(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tGet the user's desired action when linting fails.\n\n\t\tReturns:\n\t\t    ChunkAction indicating what to do.\n\n\t\t\"\"\"\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t\t(\"Bypass linter and commit with --no-verify\", ChunkAction.COMMIT),\n\t\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\t\tresult = questionary.select(\n\t\t\t\"Linting failed. What would you like to do?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tqmark=\"?\u00bb\",  # Use a different qmark to indicate failure state\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\t\treturn ChunkAction.EXIT  # Fallback\n\n\tdef edit_message(self, current_message: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet an edited commit message from the user.\n\n\t\tArgs:\n\t\t    current_message: Current commit message\n\n\t\tReturns:\n\t\t    Edited commit message\n\n\t\t\"\"\"\n\t\tself.console.print(\"\\n[bold blue]Edit commit message:[/]\")\n\t\tself.console.print(\"[dim]Press Enter to keep current message[/]\")\n\t\treturn Prompt.ask(\"Message\", default=current_message)\n\n\tdef process_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; ChunkResult:\n\t\t\"\"\"\n\t\tProcess a single diff chunk interactively.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk to process\n\t\t    index: The 0-based index of the current chunk\n\t\t    total: The total number of chunks\n\n\t\tReturns:\n\t\t    ChunkResult with the user's action and any modified message\n\n\t\t\"\"\"\n\t\t# Display the combined diff and message panel\n\t\tself.display_chunk(chunk, index, total)\n\n\t\t# Now get the user's action through questionary (without displaying another message panel)\n\t\taction = self.get_user_action()\n\n\t\tif action == ChunkAction.EDIT:\n\t\t\tmessage = self.edit_message(chunk.description or \"\")\n\t\t\treturn ChunkResult(ChunkAction.COMMIT, message)\n\n\t\tif action == ChunkAction.COMMIT:\n\t\t\treturn ChunkResult(action, chunk.description)\n\n\t\treturn ChunkResult(action)\n\n\tdef confirm_abort(self) -&gt; bool:\n\t\t\"\"\"\n\t\tAsk the user to confirm aborting the commit process.\n\n\t\tReturns:\n\t\t    True if the user confirms, False otherwise\n\n\t\tRaises:\n\t\t    typer.Exit: When the user confirms exiting\n\n\t\t\"\"\"\n\t\tconfirmed = Confirm.ask(\n\t\t\t\"\\n[bold yellow]Are you sure you want to exit without committing?[/]\",\n\t\t\tdefault=False,\n\t\t)\n\n\t\tif confirmed:\n\t\t\tself.console.print(\"[yellow]Exiting commit process...[/yellow]\")\n\t\t\t# Use a zero exit code to indicate a successful (intended) exit\n\t\t\t# This prevents error messages from showing when exiting\n\t\t\traise typer.Exit(code=0)\n\n\t\treturn False\n\n\tdef confirm_bypass_hooks(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tAsk the user what to do when git hooks fail.\n\n\t\tReturns:\n\t\t    ChunkAction indicating what to do next\n\n\t\t\"\"\"\n\t\tself.console.print(\"\\n[bold yellow]Git hooks failed.[/]\")\n\t\tself.console.print(\"[yellow]This may be due to linting or other pre-commit checks.[/]\")\n\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Force commit and bypass hooks\", ChunkAction.COMMIT),\n\t\t\t(\"Regenerate message and try again\", ChunkAction.REGENERATE),\n\t\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\n\t\tresult = questionary.select(\n\t\t\t\"What would you like to do?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tqmark=\"\u00bb\",\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\n\t\t# Fallback (should never happen)\n\t\treturn ChunkAction.EXIT\n\n\tdef show_success(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow a success message.\n\n\t\tArgs:\n\t\t    message: Message to display\n\n\t\t\"\"\"\n\t\tself.console.print(f\"\\n[bold green]\u2713[/] {message}\")\n\n\tdef show_warning(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow a warning message to the user.\n\n\t\tArgs:\n\t\t    message: Warning message to display\n\n\t\t\"\"\"\n\t\tself.console.print(f\"\\n[bold yellow]\u26a0[/] {message}\")\n\n\tdef show_error(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow an error message to the user.\n\n\t\tArgs:\n\t\t    message: Error message to display\n\n\t\t\"\"\"\n\t\tif \"No changes to commit\" in message:\n\t\t\t# This is an informational message, not an error\n\t\t\tself.console.print(f\"[yellow]{message}[/yellow]\")\n\t\telse:\n\t\t\t# This is a real error\n\t\t\tself.console.print(f\"[red]Error:[/red] {message}\")\n\n\tdef show_skipped(self, files: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tShow which files were skipped.\n\n\t\tArgs:\n\t\t    files: List of skipped files\n\n\t\t\"\"\"\n\t\tif files:\n\t\t\tself.console.print(\"\\n[yellow]Skipped changes in:[/]\")\n\t\t\tfor file in files:\n\t\t\t\tself.console.print(f\"  \u2022 {file}\")\n\n\tdef show_message(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow a general informational message.\n\n\t\tArgs:\n\t\t    message: Message to display\n\n\t\t\"\"\"\n\t\tself.console.print(f\"\\n{message}\")\n\n\tdef show_regenerating(self) -&gt; None:\n\t\t\"\"\"Show message indicating message regeneration.\"\"\"\n\t\tself.console.print(\"\\n[yellow]Regenerating commit message...[/yellow]\")\n\n\tdef show_all_committed(self) -&gt; None:\n\t\t\"\"\"Show message indicating all changes are committed.\"\"\"\n\t\tself.console.print(\"[green]\u2713[/green] All changes committed!\")\n\n\tdef show_all_done(self) -&gt; None:\n\t\t\"\"\"\n\t\tShow a final success message when the process completes.\n\n\t\tThis is an alias for show_all_committed for now, but could be\n\t\tcustomized.\n\n\t\t\"\"\"\n\t\tself.show_all_committed()\n\n\tdef show_lint_errors(self, errors: list[str]) -&gt; None:\n\t\t\"\"\"Display linting errors to the user.\"\"\"\n\t\tself.console.print(\"[bold red]Commit message failed linting:[/bold red]\")\n\t\tfor error in errors:\n\t\t\tself.console.print(f\"  - {error}\")\n\n\tdef confirm_commit_with_lint_errors(self) -&gt; bool:\n\t\t\"\"\"Ask the user if they want to commit despite lint errors.\"\"\"\n\t\treturn questionary.confirm(\"Commit message has lint errors. Commit anyway?\", default=False).ask()\n\n\tdef confirm_exit(self) -&gt; bool:\n\t\t\"\"\"Ask the user to confirm exiting without committing.\"\"\"\n\t\treturn questionary.confirm(\"Are you sure you want to exit without committing?\", default=False).ask()\n\n\tdef display_failed_lint_message(self, message: str, lint_errors: list[str], is_llm_generated: bool = False) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a commit message that failed linting, along with the errors.\n\n\t\tArgs:\n\t\t    message: The commit message to display.\n\t\t    lint_errors: List of linting error messages.\n\t\t    is_llm_generated: Whether the message was generated by an LLM.\n\n\t\t\"\"\"\n\t\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\t\tmessage_panel = Panel(\n\t\t\tText(message, style=\"yellow\"),  # Use yellow style for the message text\n\t\t\ttitle=f\"[bold yellow]Proposed message ({tag}) - LINTING FAILED[/]\",\n\t\t\tborder_style=\"yellow\",  # Yellow border to indicate warning/failure\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\n\t\t# Display lint errors below\n\t\tif lint_errors:\n\t\t\terror_text = Text(\"\\n\".join([f\"- {err}\" for err in lint_errors]), style=\"red\")\n\t\t\terror_panel = Panel(\n\t\t\t\terror_text,\n\t\t\t\ttitle=\"[bold red]Linting Errors[/]\",\n\t\t\t\tborder_style=\"red\",\n\t\t\t\texpand=False,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(error_panel)\n\n\tdef display_failed_json_message(\n\t\tself, raw_content: str, json_errors: list[str], is_llm_generated: bool = True\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a raw response that failed JSON validation, along with the errors.\n\n\t\tArgs:\n\t\t    raw_content: The raw string content that failed JSON validation.\n\t\t    json_errors: List of JSON validation/formatting error messages.\n\t\t    is_llm_generated: Whether the message was generated by an LLM (usually True here).\n\t\t\"\"\"\n\t\ttag = \"AI\" if is_llm_generated else \"Manual\"\n\t\tmessage_panel = Panel(\n\t\t\tText(raw_content, style=\"dim yellow\"),  # Use dim yellow for the raw content\n\t\t\ttitle=f\"[bold yellow]Invalid JSON Response ({tag}) - VALIDATION FAILED[/]\",\n\t\t\tborder_style=\"yellow\",  # Yellow border to indicate JSON warning\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\n\t\t# Display JSON errors below\n\t\tif json_errors:\n\t\t\terror_text = Text(\"\\n\".join([f\"- {err}\" for err in json_errors]), style=\"red\")\n\t\t\terror_panel = Panel(\n\t\t\t\terror_text,\n\t\t\t\ttitle=\"[bold red]JSON Validation Errors[/]\",\n\t\t\t\tborder_style=\"red\",\n\t\t\t\texpand=False,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(error_panel)\n\n\tdef get_group_action(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tGet the user's desired action for the current semantic group.\n\n\t\tReturns:\n\t\t        ChunkAction indicating what to do with the group\n\n\t\t\"\"\"\n\t\t# Define options with their display text and corresponding action\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Commit this group\", ChunkAction.COMMIT),\n\t\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\n\t\t# Use questionary to get the user's choice\n\t\tresult = questionary.select(\n\t\t\t\"What would you like to do with this group?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tdefault=options[0][0],  # Set \"Commit this group\" as default\n\t\t\tqmark=\"\u00bb\",\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\n\t\t# Map the result back to the ChunkAction\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\n\t\t# Fallback (should never happen)\n\t\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the commit UI.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the commit UI.\"\"\"\n\tself.console = Console()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.console","title":"console  <code>instance-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_chunk","title":"display_chunk","text":"<pre><code>display_chunk(\n\tchunk: DiffChunk, index: int = 0, total: int = 1\n) -&gt; None\n</code></pre> <p>Display a diff chunk to the user.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>DiffChunk to display</p> required <code>index</code> <code>int</code> <p>The 0-based index of the current chunk</p> <code>0</code> <code>total</code> <code>int</code> <p>The total number of chunks</p> <code>1</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; None:\n\t\"\"\"\n\tDisplay a diff chunk to the user.\n\n\tArgs:\n\t    chunk: DiffChunk to display\n\t    index: The 0-based index of the current chunk\n\t    total: The total number of chunks\n\n\t\"\"\"\n\t# Build file information\n\tfile_info = Text(\"Files: \", style=\"blue\")\n\tfile_info.append(\", \".join(chunk.files))\n\n\t# Calculate changes\n\tadded = len(\n\t\t[line for line in chunk.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t)\n\tremoved = len(\n\t\t[line for line in chunk.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t)\n\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t# Prepare diff content\n\tpanel_content = chunk.content\n\tif not panel_content.strip():\n\t\tpanel_content = \"No content diff available (e.g., new file or mode change)\"\n\n\t# Truncate to maximum of MAX_PREVIEW_LINES lines\n\tcontent_lines = panel_content.splitlines()\n\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\tpanel_content = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\n\tdiff_content = Text(\"\\n\" + panel_content)\n\n\t# Determine title for the panel - use provided index and total\n\tpanel_title = f\"[bold]Commit {index + 1} of {total}[/bold]\"\n\n\t# Create content for the panel conditionally\n\tif getattr(chunk, \"description\", None):\n\t\t# If there's a description, create a combined panel\n\t\tif getattr(chunk, \"is_llm_generated\", False):\n\t\t\tmessage_title = \"[bold blue]Proposed message (AI)[/]\"\n\t\t\tmessage_style = \"blue\"\n\t\telse:\n\t\t\tmessage_title = \"[bold yellow]Proposed message (Simple)[/]\"\n\t\t\tmessage_style = \"yellow\"\n\n\t\t# Create separate panels and print them\n\t\t# First, print the diff panel\n\t\tdiff_panel = Panel(\n\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\ttitle=panel_title,\n\t\t\tborder_style=\"cyan\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(diff_panel)\n\n\t\t# Print divider\n\t\tself.console.print(Rule(style=\"dim\"))\n\n\t\t# Then print the message panel\n\t\tmessage_panel = Panel(\n\t\t\tText(str(chunk.description), style=\"green\"),\n\t\t\ttitle=message_title,\n\t\t\tborder_style=message_style,\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\telse:\n\t\t# If no description, just print the diff panel\n\t\tpanel = Panel(\n\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\ttitle=panel_title,\n\t\t\tborder_style=\"cyan\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print()\n\t\tself.console.print(panel)\n\t\tself.console.print()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_group","title":"display_group","text":"<pre><code>display_group(\n\tgroup: SemanticGroup, index: int = 0, total: int = 1\n) -&gt; None\n</code></pre> <p>Display a semantic group to the user.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>SemanticGroup</code> <p>SemanticGroup to display</p> required <code>index</code> <code>int</code> <p>The 0-based index of the current group</p> <code>0</code> <code>total</code> <code>int</code> <p>The total number of groups</p> <code>1</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_group(self, group: SemanticGroup, index: int = 0, total: int = 1) -&gt; None:\n\t\"\"\"\n\tDisplay a semantic group to the user.\n\n\tArgs:\n\t        group: SemanticGroup to display\n\t        index: The 0-based index of the current group\n\t        total: The total number of groups\n\n\t\"\"\"\n\t# Build file information\n\tfile_list = \"\\n\".join([f\"  - {file}\" for file in group.files])\n\tfile_info = Text(f\"Files ({len(group.files)}):\\n\", style=\"blue\")\n\tfile_info.append(file_list)\n\n\t# Prepare diff preview - show first few lines of diff content\n\tdiff_preview = group.content\n\tcontent_lines = diff_preview.splitlines()\n\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\tdiff_preview = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\tdiff_content = Text(\"\\n\\nDiff Preview:\\n\", style=\"blue\")\n\tdiff_content.append(diff_preview)\n\n\t# Calculate changes\n\tadded = len(\n\t\t[line for line in group.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t)\n\tremoved = len(\n\t\t[line for line in group.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t)\n\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t# Determine title for the panel\n\tpanel_title = f\"[bold]Group {index + 1} of {total}[/bold]\"\n\n\t# Create diff panel\n\tdiff_panel = Panel(\n\t\tGroup(file_info, changes_info, diff_content),\n\t\ttitle=panel_title,\n\t\tborder_style=\"cyan\",\n\t\texpand=True,\n\t\twidth=self.console.width,\n\t\tpadding=(1, 2),\n\t)\n\tself.console.print(diff_panel)\n\n\t# Print divider\n\tself.console.print(Rule(style=\"dim\"))\n\n\t# Create message panel if message exists\n\tif hasattr(group, \"message\") and group.message:\n\t\t# Create message panel\n\t\tmessage_panel = Panel(\n\t\t\tText(str(group.message), style=\"green\"),\n\t\t\ttitle=\"[bold blue]Generated message[/]\",\n\t\t\tborder_style=\"green\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\telse:\n\t\tself.console.print(\n\t\t\tPanel(\n\t\t\t\tText(\"No message generated yet\", style=\"dim\"),\n\t\t\t\ttitle=\"[bold]Message[/]\",\n\t\t\t\tborder_style=\"yellow\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_message","title":"display_message","text":"<pre><code>display_message(\n\tmessage: str, is_llm_generated: bool = False\n) -&gt; None\n</code></pre> <p>Display a commit message to the user.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to display</p> required <code>is_llm_generated</code> <code>bool</code> <p>Whether the message was generated by an LLM</p> <code>False</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_message(self, message: str, is_llm_generated: bool = False) -&gt; None:\n\t\"\"\"\n\tDisplay a commit message to the user.\n\n\tArgs:\n\t    message: The commit message to display\n\t    is_llm_generated: Whether the message was generated by an LLM\n\n\t\"\"\"\n\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\tmessage_panel = Panel(\n\t\tText(message, style=\"green\"),\n\t\ttitle=f\"[bold {'blue' if is_llm_generated else 'yellow'}]Proposed message ({tag})[/]\",\n\t\tborder_style=\"blue\" if is_llm_generated else \"yellow\",\n\t\texpand=False,\n\t\tpadding=(1, 2),\n\t)\n\tself.console.print(message_panel)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.get_user_action","title":"get_user_action","text":"<pre><code>get_user_action() -&gt; ChunkAction\n</code></pre> <p>Get the user's desired action for the current chunk.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do with the chunk</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def get_user_action(self) -&gt; ChunkAction:\n\t\"\"\"\n\tGet the user's desired action for the current chunk.\n\n\tReturns:\n\t    ChunkAction indicating what to do with the chunk\n\n\t\"\"\"\n\t# Define options with their display text and corresponding action\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Commit with this message\", ChunkAction.COMMIT),\n\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\n\t# Use questionary to get the user's choice\n\tresult = questionary.select(\n\t\t\"What would you like to do?\",\n\t\tchoices=[option[0] for option in options],\n\t\tdefault=options[0][0],  # Set \"Commit with this message\" as default\n\t\tqmark=\"\u00bb\",\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\n\t# Map the result back to the ChunkAction\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\n\t# Fallback (should never happen)\n\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.get_user_action_on_lint_failure","title":"get_user_action_on_lint_failure","text":"<pre><code>get_user_action_on_lint_failure() -&gt; ChunkAction\n</code></pre> <p>Get the user's desired action when linting fails.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def get_user_action_on_lint_failure(self) -&gt; ChunkAction:\n\t\"\"\"\n\tGet the user's desired action when linting fails.\n\n\tReturns:\n\t    ChunkAction indicating what to do.\n\n\t\"\"\"\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t(\"Bypass linter and commit with --no-verify\", ChunkAction.COMMIT),\n\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\tresult = questionary.select(\n\t\t\"Linting failed. What would you like to do?\",\n\t\tchoices=[option[0] for option in options],\n\t\tqmark=\"?\u00bb\",  # Use a different qmark to indicate failure state\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\treturn ChunkAction.EXIT  # Fallback\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.edit_message","title":"edit_message","text":"<pre><code>edit_message(current_message: str) -&gt; str\n</code></pre> <p>Get an edited commit message from the user.</p> <p>Parameters:</p> Name Type Description Default <code>current_message</code> <code>str</code> <p>Current commit message</p> required <p>Returns:</p> Type Description <code>str</code> <p>Edited commit message</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def edit_message(self, current_message: str) -&gt; str:\n\t\"\"\"\n\tGet an edited commit message from the user.\n\n\tArgs:\n\t    current_message: Current commit message\n\n\tReturns:\n\t    Edited commit message\n\n\t\"\"\"\n\tself.console.print(\"\\n[bold blue]Edit commit message:[/]\")\n\tself.console.print(\"[dim]Press Enter to keep current message[/]\")\n\treturn Prompt.ask(\"Message\", default=current_message)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.process_chunk","title":"process_chunk","text":"<pre><code>process_chunk(\n\tchunk: DiffChunk, index: int = 0, total: int = 1\n) -&gt; ChunkResult\n</code></pre> <p>Process a single diff chunk interactively.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>DiffChunk to process</p> required <code>index</code> <code>int</code> <p>The 0-based index of the current chunk</p> <code>0</code> <code>total</code> <code>int</code> <p>The total number of chunks</p> <code>1</code> <p>Returns:</p> Type Description <code>ChunkResult</code> <p>ChunkResult with the user's action and any modified message</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def process_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; ChunkResult:\n\t\"\"\"\n\tProcess a single diff chunk interactively.\n\n\tArgs:\n\t    chunk: DiffChunk to process\n\t    index: The 0-based index of the current chunk\n\t    total: The total number of chunks\n\n\tReturns:\n\t    ChunkResult with the user's action and any modified message\n\n\t\"\"\"\n\t# Display the combined diff and message panel\n\tself.display_chunk(chunk, index, total)\n\n\t# Now get the user's action through questionary (without displaying another message panel)\n\taction = self.get_user_action()\n\n\tif action == ChunkAction.EDIT:\n\t\tmessage = self.edit_message(chunk.description or \"\")\n\t\treturn ChunkResult(ChunkAction.COMMIT, message)\n\n\tif action == ChunkAction.COMMIT:\n\t\treturn ChunkResult(action, chunk.description)\n\n\treturn ChunkResult(action)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_abort","title":"confirm_abort","text":"<pre><code>confirm_abort() -&gt; bool\n</code></pre> <p>Ask the user to confirm aborting the commit process.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the user confirms, False otherwise</p> <p>Raises:</p> Type Description <code>Exit</code> <p>When the user confirms exiting</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_abort(self) -&gt; bool:\n\t\"\"\"\n\tAsk the user to confirm aborting the commit process.\n\n\tReturns:\n\t    True if the user confirms, False otherwise\n\n\tRaises:\n\t    typer.Exit: When the user confirms exiting\n\n\t\"\"\"\n\tconfirmed = Confirm.ask(\n\t\t\"\\n[bold yellow]Are you sure you want to exit without committing?[/]\",\n\t\tdefault=False,\n\t)\n\n\tif confirmed:\n\t\tself.console.print(\"[yellow]Exiting commit process...[/yellow]\")\n\t\t# Use a zero exit code to indicate a successful (intended) exit\n\t\t# This prevents error messages from showing when exiting\n\t\traise typer.Exit(code=0)\n\n\treturn False\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_bypass_hooks","title":"confirm_bypass_hooks","text":"<pre><code>confirm_bypass_hooks() -&gt; ChunkAction\n</code></pre> <p>Ask the user what to do when git hooks fail.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do next</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_bypass_hooks(self) -&gt; ChunkAction:\n\t\"\"\"\n\tAsk the user what to do when git hooks fail.\n\n\tReturns:\n\t    ChunkAction indicating what to do next\n\n\t\"\"\"\n\tself.console.print(\"\\n[bold yellow]Git hooks failed.[/]\")\n\tself.console.print(\"[yellow]This may be due to linting or other pre-commit checks.[/]\")\n\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Force commit and bypass hooks\", ChunkAction.COMMIT),\n\t\t(\"Regenerate message and try again\", ChunkAction.REGENERATE),\n\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\n\tresult = questionary.select(\n\t\t\"What would you like to do?\",\n\t\tchoices=[option[0] for option in options],\n\t\tqmark=\"\u00bb\",\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\n\t# Fallback (should never happen)\n\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_success","title":"show_success","text":"<pre><code>show_success(message: str) -&gt; None\n</code></pre> <p>Show a success message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_success(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow a success message.\n\n\tArgs:\n\t    message: Message to display\n\n\t\"\"\"\n\tself.console.print(f\"\\n[bold green]\u2713[/] {message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_warning","title":"show_warning","text":"<pre><code>show_warning(message: str) -&gt; None\n</code></pre> <p>Show a warning message to the user.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_warning(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow a warning message to the user.\n\n\tArgs:\n\t    message: Warning message to display\n\n\t\"\"\"\n\tself.console.print(f\"\\n[bold yellow]\u26a0[/] {message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_error","title":"show_error","text":"<pre><code>show_error(message: str) -&gt; None\n</code></pre> <p>Show an error message to the user.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_error(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow an error message to the user.\n\n\tArgs:\n\t    message: Error message to display\n\n\t\"\"\"\n\tif \"No changes to commit\" in message:\n\t\t# This is an informational message, not an error\n\t\tself.console.print(f\"[yellow]{message}[/yellow]\")\n\telse:\n\t\t# This is a real error\n\t\tself.console.print(f\"[red]Error:[/red] {message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_skipped","title":"show_skipped","text":"<pre><code>show_skipped(files: list[str]) -&gt; None\n</code></pre> <p>Show which files were skipped.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of skipped files</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_skipped(self, files: list[str]) -&gt; None:\n\t\"\"\"\n\tShow which files were skipped.\n\n\tArgs:\n\t    files: List of skipped files\n\n\t\"\"\"\n\tif files:\n\t\tself.console.print(\"\\n[yellow]Skipped changes in:[/]\")\n\t\tfor file in files:\n\t\t\tself.console.print(f\"  \u2022 {file}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_message","title":"show_message","text":"<pre><code>show_message(message: str) -&gt; None\n</code></pre> <p>Show a general informational message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_message(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow a general informational message.\n\n\tArgs:\n\t    message: Message to display\n\n\t\"\"\"\n\tself.console.print(f\"\\n{message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_regenerating","title":"show_regenerating","text":"<pre><code>show_regenerating() -&gt; None\n</code></pre> <p>Show message indicating message regeneration.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_regenerating(self) -&gt; None:\n\t\"\"\"Show message indicating message regeneration.\"\"\"\n\tself.console.print(\"\\n[yellow]Regenerating commit message...[/yellow]\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_all_committed","title":"show_all_committed","text":"<pre><code>show_all_committed() -&gt; None\n</code></pre> <p>Show message indicating all changes are committed.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_all_committed(self) -&gt; None:\n\t\"\"\"Show message indicating all changes are committed.\"\"\"\n\tself.console.print(\"[green]\u2713[/green] All changes committed!\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_all_done","title":"show_all_done","text":"<pre><code>show_all_done() -&gt; None\n</code></pre> <p>Show a final success message when the process completes.</p> <p>This is an alias for show_all_committed for now, but could be customized.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_all_done(self) -&gt; None:\n\t\"\"\"\n\tShow a final success message when the process completes.\n\n\tThis is an alias for show_all_committed for now, but could be\n\tcustomized.\n\n\t\"\"\"\n\tself.show_all_committed()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_lint_errors","title":"show_lint_errors","text":"<pre><code>show_lint_errors(errors: list[str]) -&gt; None\n</code></pre> <p>Display linting errors to the user.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_lint_errors(self, errors: list[str]) -&gt; None:\n\t\"\"\"Display linting errors to the user.\"\"\"\n\tself.console.print(\"[bold red]Commit message failed linting:[/bold red]\")\n\tfor error in errors:\n\t\tself.console.print(f\"  - {error}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_commit_with_lint_errors","title":"confirm_commit_with_lint_errors","text":"<pre><code>confirm_commit_with_lint_errors() -&gt; bool\n</code></pre> <p>Ask the user if they want to commit despite lint errors.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_commit_with_lint_errors(self) -&gt; bool:\n\t\"\"\"Ask the user if they want to commit despite lint errors.\"\"\"\n\treturn questionary.confirm(\"Commit message has lint errors. Commit anyway?\", default=False).ask()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_exit","title":"confirm_exit","text":"<pre><code>confirm_exit() -&gt; bool\n</code></pre> <p>Ask the user to confirm exiting without committing.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_exit(self) -&gt; bool:\n\t\"\"\"Ask the user to confirm exiting without committing.\"\"\"\n\treturn questionary.confirm(\"Are you sure you want to exit without committing?\", default=False).ask()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_failed_lint_message","title":"display_failed_lint_message","text":"<pre><code>display_failed_lint_message(\n\tmessage: str,\n\tlint_errors: list[str],\n\tis_llm_generated: bool = False,\n) -&gt; None\n</code></pre> <p>Display a commit message that failed linting, along with the errors.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to display.</p> required <code>lint_errors</code> <code>list[str]</code> <p>List of linting error messages.</p> required <code>is_llm_generated</code> <code>bool</code> <p>Whether the message was generated by an LLM.</p> <code>False</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_failed_lint_message(self, message: str, lint_errors: list[str], is_llm_generated: bool = False) -&gt; None:\n\t\"\"\"\n\tDisplay a commit message that failed linting, along with the errors.\n\n\tArgs:\n\t    message: The commit message to display.\n\t    lint_errors: List of linting error messages.\n\t    is_llm_generated: Whether the message was generated by an LLM.\n\n\t\"\"\"\n\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\tmessage_panel = Panel(\n\t\tText(message, style=\"yellow\"),  # Use yellow style for the message text\n\t\ttitle=f\"[bold yellow]Proposed message ({tag}) - LINTING FAILED[/]\",\n\t\tborder_style=\"yellow\",  # Yellow border to indicate warning/failure\n\t\texpand=False,\n\t\tpadding=(1, 2),\n\t)\n\tself.console.print(message_panel)\n\n\t# Display lint errors below\n\tif lint_errors:\n\t\terror_text = Text(\"\\n\".join([f\"- {err}\" for err in lint_errors]), style=\"red\")\n\t\terror_panel = Panel(\n\t\t\terror_text,\n\t\t\ttitle=\"[bold red]Linting Errors[/]\",\n\t\t\tborder_style=\"red\",\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(error_panel)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_failed_json_message","title":"display_failed_json_message","text":"<pre><code>display_failed_json_message(\n\traw_content: str,\n\tjson_errors: list[str],\n\tis_llm_generated: bool = True,\n) -&gt; None\n</code></pre> <p>Display a raw response that failed JSON validation, along with the errors.</p> <p>Parameters:</p> Name Type Description Default <code>raw_content</code> <code>str</code> <p>The raw string content that failed JSON validation.</p> required <code>json_errors</code> <code>list[str]</code> <p>List of JSON validation/formatting error messages.</p> required <code>is_llm_generated</code> <code>bool</code> <p>Whether the message was generated by an LLM (usually True here).</p> <code>True</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_failed_json_message(\n\tself, raw_content: str, json_errors: list[str], is_llm_generated: bool = True\n) -&gt; None:\n\t\"\"\"\n\tDisplay a raw response that failed JSON validation, along with the errors.\n\n\tArgs:\n\t    raw_content: The raw string content that failed JSON validation.\n\t    json_errors: List of JSON validation/formatting error messages.\n\t    is_llm_generated: Whether the message was generated by an LLM (usually True here).\n\t\"\"\"\n\ttag = \"AI\" if is_llm_generated else \"Manual\"\n\tmessage_panel = Panel(\n\t\tText(raw_content, style=\"dim yellow\"),  # Use dim yellow for the raw content\n\t\ttitle=f\"[bold yellow]Invalid JSON Response ({tag}) - VALIDATION FAILED[/]\",\n\t\tborder_style=\"yellow\",  # Yellow border to indicate JSON warning\n\t\texpand=False,\n\t\tpadding=(1, 2),\n\t)\n\tself.console.print(message_panel)\n\n\t# Display JSON errors below\n\tif json_errors:\n\t\terror_text = Text(\"\\n\".join([f\"- {err}\" for err in json_errors]), style=\"red\")\n\t\terror_panel = Panel(\n\t\t\terror_text,\n\t\t\ttitle=\"[bold red]JSON Validation Errors[/]\",\n\t\t\tborder_style=\"red\",\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(error_panel)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.get_group_action","title":"get_group_action","text":"<pre><code>get_group_action() -&gt; ChunkAction\n</code></pre> <p>Get the user's desired action for the current semantic group.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do with the group</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def get_group_action(self) -&gt; ChunkAction:\n\t\"\"\"\n\tGet the user's desired action for the current semantic group.\n\n\tReturns:\n\t        ChunkAction indicating what to do with the group\n\n\t\"\"\"\n\t# Define options with their display text and corresponding action\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Commit this group\", ChunkAction.COMMIT),\n\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\n\t# Use questionary to get the user's choice\n\tresult = questionary.select(\n\t\t\"What would you like to do with this group?\",\n\t\tchoices=[option[0] for option in options],\n\t\tdefault=options[0][0],  # Set \"Commit this group\" as default\n\t\tqmark=\"\u00bb\",\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\n\t# Map the result back to the ChunkAction\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\n\t# Fallback (should never happen)\n\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/utils/","title":"Utils","text":"<p>Git utilities for CodeMap.</p>"},{"location":"api/git/utils/#codemap.git.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff","title":"GitDiff  <code>dataclass</code>","text":"<p>Represents a Git diff chunk.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>@dataclass\nclass GitDiff:\n\t\"\"\"Represents a Git diff chunk.\"\"\"\n\n\tfiles: list[str]\n\tcontent: str\n\tis_staged: bool = False\n\tis_untracked: bool = False\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.__init__","title":"__init__","text":"<pre><code>__init__(\n\tfiles: list[str],\n\tcontent: str,\n\tis_staged: bool = False,\n\tis_untracked: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: list[str]\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.is_staged","title":"is_staged  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_staged: bool = False\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.is_untracked","title":"is_untracked  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_untracked: bool = False\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitError","title":"GitError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for Git-related errors.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>class GitError(Exception):\n\t\"\"\"Custom exception for Git-related errors.\"\"\"\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext","title":"ExtendedGitRepoContext","text":"<p>               Bases: <code>GitRepoContext</code></p> <p>Extended context for Git operations using pygit2.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>class ExtendedGitRepoContext(GitRepoContext):\n\t\"\"\"Extended context for Git operations using pygit2.\"\"\"\n\n\t_extended_instance: ExtendedGitRepoContext | None = None\n\n\t@classmethod\n\tdef get_instance(cls) -&gt; ExtendedGitRepoContext:\n\t\t\"\"\"Get an instance of the ExtendedGitRepoContext class.\"\"\"\n\t\tif cls._extended_instance is None:\n\t\t\tcls._extended_instance = cls()\n\t\treturn cls._extended_instance\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the ExtendedGitRepoContext with the given repository path.\"\"\"\n\t\tsuper().__init__()\n\n\t@classmethod\n\tdef validate_repo_path(cls, path: Path | None = None) -&gt; Path | None:\n\t\t\"\"\"Validate and return the repository path, or None if not valid.\"\"\"\n\t\ttry:\n\t\t\tif path is None:\n\t\t\t\tpath = Path.cwd()\n\t\t\treturn cls.get_repo_root(path)\n\t\texcept GitError:\n\t\t\treturn None\n\n\tdef get_staged_diff(self) -&gt; GitDiff:\n\t\t\"\"\"Get the diff of staged changes as a GitDiff object.\"\"\"\n\t\tcommit = self.repo.head.peel(Commit)\n\t\tdiff = self.repo.diff(commit.tree, cached=True)\n\n\t\tfiles = []\n\t\tcontent = \"\"\n\t\tif isinstance(diff, Diff):\n\t\t\tfiles = [delta.delta.new_file.path for delta in diff]\n\t\t\tcontent = diff.patch\n\t\telif isinstance(diff, Patch):\n\t\t\tfiles = [diff.delta.new_file.path]\n\t\t\tcontent = diff.text\n\t\treturn GitDiff(files=files, content=content or \"\", is_staged=True)\n\n\tdef get_unstaged_diff(self) -&gt; GitDiff:\n\t\t\"\"\"Get the diff of unstaged changes as a GitDiff object.\"\"\"\n\t\tdiff = self.repo.diff()\n\t\tfiles = []\n\n\t\tcontent = \"\"\n\n\t\tif isinstance(diff, Diff):\n\t\t\tfiles = [delta.delta.new_file.path for delta in diff]\n\t\t\tcontent = diff.patch\n\t\telif isinstance(diff, Patch):\n\t\t\tfiles = [diff.delta.new_file.path]\n\t\t\tcontent = diff.text\n\n\t\treturn GitDiff(files=files, content=content or \"\", is_staged=False)\n\n\tdef get_other_staged_files(self, targeted_files: list[str]) -&gt; list[str]:\n\t\t\"\"\"Get staged files that are not part of the targeted files.\"\"\"\n\t\tall_staged = self.get_staged_diff().files\n\t\treturn [f for f in all_staged if f not in targeted_files]\n\n\tdef stash_staged_changes(self, exclude_files: list[str]) -&gt; bool:\n\t\t\"\"\"Temporarily stash staged changes except for specified files.\"\"\"\n\t\ttry:\n\t\t\tother_files = self.get_other_staged_files(exclude_files)\n\t\t\tif not other_files:\n\t\t\t\treturn False\n\t\t\tself.stage_files(other_files)\n\t\texcept GitError as e:\n\t\t\tmsg = \"Failed to stash other staged changes\"\n\t\t\traise GitError(msg) from e\n\t\telse:\n\t\t\treturn True\n\n\tdef unstash_changes(self) -&gt; None:\n\t\t\"\"\"Restore previously stashed changes.\"\"\"\n\t\ttry:\n\t\t\tstash_list = self.get_other_staged_files([])\n\t\t\tif \"CodeMap: temporary stash for commit\" in stash_list:\n\t\t\t\tself.unstage_files(stash_list)\n\t\texcept GitError as e:\n\t\t\tmsg = \"Failed to restore stashed changes; you may need to manually run 'git stash pop'\"\n\t\t\traise GitError(msg) from e\n\n\tdef commit_only_files(\n\t\tself,\n\t\tfiles: list[str],\n\t\tmessage: str,\n\t\tignore_hooks: bool = False,\n\t) -&gt; list[str]:\n\t\t\"\"\"\n\t\tCommit only the specified files with the given message.\n\n\t\tRuns the pre-commit, commit-msg, and post-commit hooks unless ignore_hooks is True.\n\t\t\"\"\"\n\t\timport tempfile\n\n\t\t# Run pre-commit hook if not ignored\n\t\tif not ignore_hooks and hook_exists(\"pre-commit\"):\n\t\t\texit_code = run_hook(\"pre-commit\")\n\t\t\tif exit_code != 0:\n\t\t\t\terror_msg = \"pre-commit hook failed, aborting commit.\"\n\t\t\t\tlogger.error(error_msg)\n\t\t\t\traise RuntimeError(error_msg)\n\t\ttry:\n\t\t\t# Prepare commit-msg hook: write message to a temp file if needed\n\t\t\tcommit_msg_file = None\n\t\t\tif not ignore_hooks and hook_exists(\"commit-msg\"):\n\t\t\t\twith tempfile.NamedTemporaryFile(\"w+\", delete=False) as f:\n\t\t\t\t\tf.write(message)\n\t\t\t\t\tcommit_msg_file = f.name\n\t\t\t\texit_code = run_hook(\"commit-msg\", repo_root=None)  # Could pass file as env var if needed\n\t\t\t\tif exit_code != 0:\n\t\t\t\t\terror_msg = \"commit-msg hook failed, aborting commit.\"\n\t\t\t\t\tlogger.error(error_msg)\n\t\t\t\t\tif commit_msg_file:\n\t\t\t\t\t\tPath(commit_msg_file).unlink()\n\t\t\t\t\traise RuntimeError(error_msg)\n\t\t\t# self.stage_files(files) # Removed: Index is already prepared by the caller\n\t\t\tother_staged = self.get_other_staged_files(files)\n\t\t\ttry:\n\t\t\t\tself.commit(message)\n\t\t\t\tlogger.info(\"Created commit with message: %s\", message)\n\t\t\texcept GitError as e:\n\t\t\t\terror_msg = \"Git commit command failed\"\n\t\t\t\tlogger.exception(error_msg)\n\t\t\t\traise GitError(error_msg) from e\n\t\t\t# Run post-commit hook if not ignored\n\t\t\tif not ignore_hooks and hook_exists(\"post-commit\"):\n\t\t\t\texit_code = run_hook(\"post-commit\")\n\t\t\t\tif exit_code != 0:\n\t\t\t\t\tlogger.warning(\"post-commit hook failed (commit already created)\")\n\t\t\tif commit_msg_file:\n\t\t\t\tPath(commit_msg_file).unlink()\n\t\t\treturn other_staged\n\t\texcept GitError:\n\t\t\traise\n\t\texcept Exception as e:\n\t\t\terror_msg = f\"Error in commit_only_files: {e!s}\"\n\t\t\tlogger.exception(error_msg)\n\t\t\traise GitError(error_msg) from e\n\n\tdef get_per_file_diff(self, file_path: str, staged: bool = False) -&gt; GitDiff:\n\t\t\"\"\"\n\t\tGet the diff for a single file, either staged or unstaged.\n\n\t\tArgs:\n\t\t\tfile_path: The path to the file to diff (relative to repo root).\n\t\t\tstaged: If True, get the staged diff; otherwise, get the unstaged diff.\n\n\t\tReturns:\n\t\t\tGitDiff: The diff for the specified file.\n\n\t\tRaises:\n\t\t\tGitError: If the diff cannot be generated.\n\t\t\"\"\"\n\t\tlogger.debug(\"get_per_file_diff called with file_path: '%s', staged: %s\", file_path, staged)\n\t\ttry:\n\t\t\tif staged:\n\t\t\t\tcommit = self.repo.head.peel(Commit)\n\t\t\t\tdiff = self.repo.diff(commit.tree, cached=True)\n\t\t\t\tis_staged = True\n\t\t\telse:\n\t\t\t\tdiff = self.repo.diff()\n\t\t\t\tis_staged = False\n\n\t\t\tfile_path_set = {file_path}\n\t\t\tif isinstance(diff, Diff):\n\t\t\t\tfor patch in diff:\n\t\t\t\t\tnew_file_path = patch.delta.new_file.path\n\t\t\t\t\told_file_path = patch.delta.old_file.path\n\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\"  Patch details - New: '%s', Old: '%s'\",\n\t\t\t\t\t\tnew_file_path,\n\t\t\t\t\t\told_file_path,\n\t\t\t\t\t)\n\t\t\t\t\tif {new_file_path, old_file_path} &amp; file_path_set:\n\t\t\t\t\t\tcontent = patch.text or \"\"\n\t\t\t\t\t\tlogger.debug(\"    Patch text (first 200 chars): %s\", repr(content[:200]))\n\t\t\t\t\t\tfiles = [new_file_path]\n\t\t\t\t\t\tgit_diff_obj = GitDiff(files=files, content=content, is_staged=is_staged)\n\t\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\t\"    Returning GitDiff for '%s', content length: %d\",\n\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\tlen(git_diff_obj.content),\n\t\t\t\t\t\t)\n\t\t\t\t\t\treturn git_diff_obj\n\t\t\t\tlogger.debug(\"  No matching patch found in Diff for '%s'. Returning empty GitDiff.\", file_path)\n\t\t\t\treturn GitDiff(files=[file_path], content=\"\", is_staged=is_staged)\n\t\t\tif isinstance(diff, Patch):\n\t\t\t\tnew_file_path = diff.delta.new_file.path\n\t\t\t\told_file_path = diff.delta.old_file.path\n\t\t\t\tlogger.debug(\n\t\t\t\t\t\"  Patch details (standalone) - New: '%s', Old: '%s'\",\n\t\t\t\t\tnew_file_path,\n\t\t\t\t\told_file_path,\n\t\t\t\t)\n\t\t\t\tif {new_file_path, old_file_path} &amp; file_path_set:\n\t\t\t\t\tcontent = diff.text or \"\"\n\t\t\t\t\tlogger.debug(\"    Patch text (first 200 chars): %s\", repr(content[:200]))\n\t\t\t\t\tfiles = [new_file_path]\n\t\t\t\t\tgit_diff_obj = GitDiff(files=files, content=content, is_staged=is_staged)\n\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\"    Returning GitDiff for '%s' (standalone patch), content length: %d\",\n\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\tlen(git_diff_obj.content),\n\t\t\t\t\t)\n\t\t\t\t\treturn git_diff_obj\n\t\t\t\tlogger.debug(\"  Standalone Patch does not match '%s'. Returning empty GitDiff.\", file_path)\n\t\t\t\treturn GitDiff(files=[file_path], content=\"\", is_staged=is_staged)\n\t\t\tlogger.debug(\"  Diff object is neither Diff nor Patch for '%s'. Returning empty GitDiff.\", file_path)\n\t\t\treturn GitDiff(files=[file_path], content=\"\", is_staged=is_staged)\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Failed to get %s diff for %s\", \"staged\" if staged else \"unstaged\", file_path)\n\t\t\tmsg = f\"Failed to get {'staged' if staged else 'unstaged'} diff for {file_path}: {e}\"\n\t\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.get_instance","title":"get_instance  <code>classmethod</code>","text":"<pre><code>get_instance() -&gt; ExtendedGitRepoContext\n</code></pre> <p>Get an instance of the ExtendedGitRepoContext class.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>@classmethod\ndef get_instance(cls) -&gt; ExtendedGitRepoContext:\n\t\"\"\"Get an instance of the ExtendedGitRepoContext class.\"\"\"\n\tif cls._extended_instance is None:\n\t\tcls._extended_instance = cls()\n\treturn cls._extended_instance\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the ExtendedGitRepoContext with the given repository path.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the ExtendedGitRepoContext with the given repository path.\"\"\"\n\tsuper().__init__()\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.validate_repo_path","title":"validate_repo_path  <code>classmethod</code>","text":"<pre><code>validate_repo_path(path: Path | None = None) -&gt; Path | None\n</code></pre> <p>Validate and return the repository path, or None if not valid.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>@classmethod\ndef validate_repo_path(cls, path: Path | None = None) -&gt; Path | None:\n\t\"\"\"Validate and return the repository path, or None if not valid.\"\"\"\n\ttry:\n\t\tif path is None:\n\t\t\tpath = Path.cwd()\n\t\treturn cls.get_repo_root(path)\n\texcept GitError:\n\t\treturn None\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.get_staged_diff","title":"get_staged_diff","text":"<pre><code>get_staged_diff() -&gt; GitDiff\n</code></pre> <p>Get the diff of staged changes as a GitDiff object.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_staged_diff(self) -&gt; GitDiff:\n\t\"\"\"Get the diff of staged changes as a GitDiff object.\"\"\"\n\tcommit = self.repo.head.peel(Commit)\n\tdiff = self.repo.diff(commit.tree, cached=True)\n\n\tfiles = []\n\tcontent = \"\"\n\tif isinstance(diff, Diff):\n\t\tfiles = [delta.delta.new_file.path for delta in diff]\n\t\tcontent = diff.patch\n\telif isinstance(diff, Patch):\n\t\tfiles = [diff.delta.new_file.path]\n\t\tcontent = diff.text\n\treturn GitDiff(files=files, content=content or \"\", is_staged=True)\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.get_unstaged_diff","title":"get_unstaged_diff","text":"<pre><code>get_unstaged_diff() -&gt; GitDiff\n</code></pre> <p>Get the diff of unstaged changes as a GitDiff object.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_unstaged_diff(self) -&gt; GitDiff:\n\t\"\"\"Get the diff of unstaged changes as a GitDiff object.\"\"\"\n\tdiff = self.repo.diff()\n\tfiles = []\n\n\tcontent = \"\"\n\n\tif isinstance(diff, Diff):\n\t\tfiles = [delta.delta.new_file.path for delta in diff]\n\t\tcontent = diff.patch\n\telif isinstance(diff, Patch):\n\t\tfiles = [diff.delta.new_file.path]\n\t\tcontent = diff.text\n\n\treturn GitDiff(files=files, content=content or \"\", is_staged=False)\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.get_other_staged_files","title":"get_other_staged_files","text":"<pre><code>get_other_staged_files(\n\ttargeted_files: list[str],\n) -&gt; list[str]\n</code></pre> <p>Get staged files that are not part of the targeted files.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_other_staged_files(self, targeted_files: list[str]) -&gt; list[str]:\n\t\"\"\"Get staged files that are not part of the targeted files.\"\"\"\n\tall_staged = self.get_staged_diff().files\n\treturn [f for f in all_staged if f not in targeted_files]\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.stash_staged_changes","title":"stash_staged_changes","text":"<pre><code>stash_staged_changes(exclude_files: list[str]) -&gt; bool\n</code></pre> <p>Temporarily stash staged changes except for specified files.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def stash_staged_changes(self, exclude_files: list[str]) -&gt; bool:\n\t\"\"\"Temporarily stash staged changes except for specified files.\"\"\"\n\ttry:\n\t\tother_files = self.get_other_staged_files(exclude_files)\n\t\tif not other_files:\n\t\t\treturn False\n\t\tself.stage_files(other_files)\n\texcept GitError as e:\n\t\tmsg = \"Failed to stash other staged changes\"\n\t\traise GitError(msg) from e\n\telse:\n\t\treturn True\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.unstash_changes","title":"unstash_changes","text":"<pre><code>unstash_changes() -&gt; None\n</code></pre> <p>Restore previously stashed changes.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def unstash_changes(self) -&gt; None:\n\t\"\"\"Restore previously stashed changes.\"\"\"\n\ttry:\n\t\tstash_list = self.get_other_staged_files([])\n\t\tif \"CodeMap: temporary stash for commit\" in stash_list:\n\t\t\tself.unstage_files(stash_list)\n\texcept GitError as e:\n\t\tmsg = \"Failed to restore stashed changes; you may need to manually run 'git stash pop'\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.commit_only_files","title":"commit_only_files","text":"<pre><code>commit_only_files(\n\tfiles: list[str],\n\tmessage: str,\n\tignore_hooks: bool = False,\n) -&gt; list[str]\n</code></pre> <p>Commit only the specified files with the given message.</p> <p>Runs the pre-commit, commit-msg, and post-commit hooks unless ignore_hooks is True.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def commit_only_files(\n\tself,\n\tfiles: list[str],\n\tmessage: str,\n\tignore_hooks: bool = False,\n) -&gt; list[str]:\n\t\"\"\"\n\tCommit only the specified files with the given message.\n\n\tRuns the pre-commit, commit-msg, and post-commit hooks unless ignore_hooks is True.\n\t\"\"\"\n\timport tempfile\n\n\t# Run pre-commit hook if not ignored\n\tif not ignore_hooks and hook_exists(\"pre-commit\"):\n\t\texit_code = run_hook(\"pre-commit\")\n\t\tif exit_code != 0:\n\t\t\terror_msg = \"pre-commit hook failed, aborting commit.\"\n\t\t\tlogger.error(error_msg)\n\t\t\traise RuntimeError(error_msg)\n\ttry:\n\t\t# Prepare commit-msg hook: write message to a temp file if needed\n\t\tcommit_msg_file = None\n\t\tif not ignore_hooks and hook_exists(\"commit-msg\"):\n\t\t\twith tempfile.NamedTemporaryFile(\"w+\", delete=False) as f:\n\t\t\t\tf.write(message)\n\t\t\t\tcommit_msg_file = f.name\n\t\t\texit_code = run_hook(\"commit-msg\", repo_root=None)  # Could pass file as env var if needed\n\t\t\tif exit_code != 0:\n\t\t\t\terror_msg = \"commit-msg hook failed, aborting commit.\"\n\t\t\t\tlogger.error(error_msg)\n\t\t\t\tif commit_msg_file:\n\t\t\t\t\tPath(commit_msg_file).unlink()\n\t\t\t\traise RuntimeError(error_msg)\n\t\t# self.stage_files(files) # Removed: Index is already prepared by the caller\n\t\tother_staged = self.get_other_staged_files(files)\n\t\ttry:\n\t\t\tself.commit(message)\n\t\t\tlogger.info(\"Created commit with message: %s\", message)\n\t\texcept GitError as e:\n\t\t\terror_msg = \"Git commit command failed\"\n\t\t\tlogger.exception(error_msg)\n\t\t\traise GitError(error_msg) from e\n\t\t# Run post-commit hook if not ignored\n\t\tif not ignore_hooks and hook_exists(\"post-commit\"):\n\t\t\texit_code = run_hook(\"post-commit\")\n\t\t\tif exit_code != 0:\n\t\t\t\tlogger.warning(\"post-commit hook failed (commit already created)\")\n\t\tif commit_msg_file:\n\t\t\tPath(commit_msg_file).unlink()\n\t\treturn other_staged\n\texcept GitError:\n\t\traise\n\texcept Exception as e:\n\t\terror_msg = f\"Error in commit_only_files: {e!s}\"\n\t\tlogger.exception(error_msg)\n\t\traise GitError(error_msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.ExtendedGitRepoContext.get_per_file_diff","title":"get_per_file_diff","text":"<pre><code>get_per_file_diff(\n\tfile_path: str, staged: bool = False\n) -&gt; GitDiff\n</code></pre> <p>Get the diff for a single file, either staged or unstaged.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to diff (relative to repo root).</p> required <code>staged</code> <code>bool</code> <p>If True, get the staged diff; otherwise, get the unstaged diff.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>GitDiff</code> <code>GitDiff</code> <p>The diff for the specified file.</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If the diff cannot be generated.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_per_file_diff(self, file_path: str, staged: bool = False) -&gt; GitDiff:\n\t\"\"\"\n\tGet the diff for a single file, either staged or unstaged.\n\n\tArgs:\n\t\tfile_path: The path to the file to diff (relative to repo root).\n\t\tstaged: If True, get the staged diff; otherwise, get the unstaged diff.\n\n\tReturns:\n\t\tGitDiff: The diff for the specified file.\n\n\tRaises:\n\t\tGitError: If the diff cannot be generated.\n\t\"\"\"\n\tlogger.debug(\"get_per_file_diff called with file_path: '%s', staged: %s\", file_path, staged)\n\ttry:\n\t\tif staged:\n\t\t\tcommit = self.repo.head.peel(Commit)\n\t\t\tdiff = self.repo.diff(commit.tree, cached=True)\n\t\t\tis_staged = True\n\t\telse:\n\t\t\tdiff = self.repo.diff()\n\t\t\tis_staged = False\n\n\t\tfile_path_set = {file_path}\n\t\tif isinstance(diff, Diff):\n\t\t\tfor patch in diff:\n\t\t\t\tnew_file_path = patch.delta.new_file.path\n\t\t\t\told_file_path = patch.delta.old_file.path\n\t\t\t\tlogger.debug(\n\t\t\t\t\t\"  Patch details - New: '%s', Old: '%s'\",\n\t\t\t\t\tnew_file_path,\n\t\t\t\t\told_file_path,\n\t\t\t\t)\n\t\t\t\tif {new_file_path, old_file_path} &amp; file_path_set:\n\t\t\t\t\tcontent = patch.text or \"\"\n\t\t\t\t\tlogger.debug(\"    Patch text (first 200 chars): %s\", repr(content[:200]))\n\t\t\t\t\tfiles = [new_file_path]\n\t\t\t\t\tgit_diff_obj = GitDiff(files=files, content=content, is_staged=is_staged)\n\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\"    Returning GitDiff for '%s', content length: %d\",\n\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\tlen(git_diff_obj.content),\n\t\t\t\t\t)\n\t\t\t\t\treturn git_diff_obj\n\t\t\tlogger.debug(\"  No matching patch found in Diff for '%s'. Returning empty GitDiff.\", file_path)\n\t\t\treturn GitDiff(files=[file_path], content=\"\", is_staged=is_staged)\n\t\tif isinstance(diff, Patch):\n\t\t\tnew_file_path = diff.delta.new_file.path\n\t\t\told_file_path = diff.delta.old_file.path\n\t\t\tlogger.debug(\n\t\t\t\t\"  Patch details (standalone) - New: '%s', Old: '%s'\",\n\t\t\t\tnew_file_path,\n\t\t\t\told_file_path,\n\t\t\t)\n\t\t\tif {new_file_path, old_file_path} &amp; file_path_set:\n\t\t\t\tcontent = diff.text or \"\"\n\t\t\t\tlogger.debug(\"    Patch text (first 200 chars): %s\", repr(content[:200]))\n\t\t\t\tfiles = [new_file_path]\n\t\t\t\tgit_diff_obj = GitDiff(files=files, content=content, is_staged=is_staged)\n\t\t\t\tlogger.debug(\n\t\t\t\t\t\"    Returning GitDiff for '%s' (standalone patch), content length: %d\",\n\t\t\t\t\tfile_path,\n\t\t\t\t\tlen(git_diff_obj.content),\n\t\t\t\t)\n\t\t\t\treturn git_diff_obj\n\t\t\tlogger.debug(\"  Standalone Patch does not match '%s'. Returning empty GitDiff.\", file_path)\n\t\t\treturn GitDiff(files=[file_path], content=\"\", is_staged=is_staged)\n\t\tlogger.debug(\"  Diff object is neither Diff nor Patch for '%s'. Returning empty GitDiff.\", file_path)\n\t\treturn GitDiff(files=[file_path], content=\"\", is_staged=is_staged)\n\texcept Exception as e:\n\t\tlogger.exception(\"Failed to get %s diff for %s\", \"staged\" if staged else \"unstaged\", file_path)\n\t\tmsg = f\"Failed to get {'staged' if staged else 'unstaged'} diff for {file_path}: {e}\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/commit_generator/","title":"Commit Generator Overview","text":"<p>Commit message generation package for CodeMap.</p> <ul> <li>Command - Main commit command implementation for CodeMap.</li> <li>Generator - Generator module for commit messages.</li> <li>Prompts - Prompt templates for commit message generation.</li> <li>Schemas - Schemas and data structures for commit message generation.</li> <li>Utils - Utility functions for commit message generation.</li> </ul>"},{"location":"api/git/commit_generator/command/","title":"Command","text":"<p>Main commit command implementation for CodeMap.</p>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MAX_FILE_CONTENT_LINES","title":"MAX_FILE_CONTENT_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_FILE_CONTENT_LINES = 500\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MAX_TOTAL_CONTENT_LINES","title":"MAX_TOTAL_CONTENT_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_TOTAL_CONTENT_LINES = 2000\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MIN_PORCELAIN_LINE_LENGTH","title":"MIN_PORCELAIN_LINE_LENGTH  <code>module-attribute</code>","text":"<pre><code>MIN_PORCELAIN_LINE_LENGTH = 3\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MAX_SMALL_CHANGE_FILES","title":"MAX_SMALL_CHANGE_FILES  <code>module-attribute</code>","text":"<pre><code>MAX_SMALL_CHANGE_FILES = 3\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MAX_SMALL_CHANGE_SIZE","title":"MAX_SMALL_CHANGE_SIZE  <code>module-attribute</code>","text":"<pre><code>MAX_SMALL_CHANGE_SIZE = 5000\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.ExitCommandError","title":"ExitCommandError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception to signal an exit command.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>class ExitCommandError(Exception):\n\t\"\"\"Exception to signal an exit command.\"\"\"\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand","title":"CommitCommand","text":"<p>Handles the commit command workflow.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>class CommitCommand:\n\t\"\"\"Handles the commit command workflow.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tbypass_hooks: bool = False,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the commit command.\n\n\t\tArgs:\n\t\t    bypass_hooks: Whether to bypass git hooks with --no-verify\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.ui: CommitUI = CommitUI()\n\n\t\t\tself.target_files: list[str] = []\n\t\t\tself.committed_files: set[str] = set()\n\t\t\tself.is_pathspec_mode: bool = False\n\t\t\tself.all_repo_files: set[str] = set()\n\t\t\tself.error_state: str | None = None  # Tracks reason for failure\n\t\t\tself.bypass_hooks: bool = bypass_hooks  # Whether to bypass git hooks with --no-verify\n\t\t\tself._is_single_file_mode: bool = False  # Track if we're processing a single file\n\n\t\t\tself.git_context = ExtendedGitRepoContext()\n\n\t\t\t# Store the current branch at initialization to ensure we don't switch branches unexpectedly\n\t\t\ttry:\n\t\t\t\tself.original_branch: str | None = self.git_context.branch\n\t\t\texcept (ImportError, GitError):\n\t\t\t\tself.original_branch = None\n\n\t\t\t# Remove eager initialization of config_loader, llm_client, splitter, message_generator\n\t\t\tself._config_loader: ConfigLoader | None = None\n\t\t\tself._llm_client: LLMClient | None = None\n\t\t\tself._splitter: DiffSplitter | None = None\n\t\t\tself._message_generator: CommitMessageGenerator | None = None\n\n\t\t\tcurrent_repo_root = self.config_loader.get.repo_root\n\n\t\t\tif not current_repo_root:\n\t\t\t\tcurrent_repo_root = self.git_context.repo_root\n\n\t\t\tif not current_repo_root:\n\t\t\t\tcurrent_repo_root = self.git_context.get_repo_root()\n\n\t\t\tself.repo_root = current_repo_root\n\n\t\texcept GitError as e:\n\t\t\traise RuntimeError(str(e)) from e\n\n\t@property\n\tdef config_loader(self) -&gt; ConfigLoader:\n\t\t\"\"\"Lazily initialize and return the ConfigLoader instance.\"\"\"\n\t\tif self._config_loader is None:\n\t\t\tfrom codemap.config import ConfigLoader\n\n\t\t\tself._config_loader = ConfigLoader.get_instance()\n\t\treturn self._config_loader\n\n\t@property\n\tdef llm_client(self) -&gt; LLMClient:\n\t\t\"\"\"Lazily initialize and return the LLMClient instance.\"\"\"\n\t\tif self._llm_client is None:\n\t\t\tfrom codemap.llm import LLMClient\n\n\t\t\tself._llm_client = LLMClient(config_loader=self.config_loader)\n\t\treturn self._llm_client\n\n\t@property\n\tdef splitter(self) -&gt; DiffSplitter:\n\t\t\"\"\"Lazily initialize and return the DiffSplitter instance.\"\"\"\n\t\tif self._splitter is None:\n\t\t\tfrom codemap.git.diff_splitter import DiffSplitter\n\n\t\t\tself._splitter = DiffSplitter()\n\t\treturn self._splitter\n\n\t@property\n\tdef message_generator(self) -&gt; CommitMessageGenerator:\n\t\t\"\"\"Lazily initialize and return the CommitMessageGenerator instance.\"\"\"\n\t\tif self._message_generator is None:\n\t\t\tfrom . import CommitMessageGenerator\n\t\t\tfrom .prompts import DEFAULT_PROMPT_TEMPLATE\n\n\t\t\tself._message_generator = CommitMessageGenerator(\n\t\t\t\trepo_root=self.repo_root,\n\t\t\t\tllm_client=self.llm_client,\n\t\t\t\tprompt_template=DEFAULT_PROMPT_TEMPLATE,\n\t\t\t\tconfig_loader=self.config_loader,\n\t\t\t)\n\t\treturn self._message_generator\n\n\tdef _get_changes(self) -&gt; list[GitDiff]:\n\t\t\"\"\"\n\t\tGet staged, unstaged, and untracked changes, generating a GitDiff object per file.\n\n\t\tReturns:\n\t\t    List of GitDiff objects, each representing changes for a single file.\n\n\t\tRaises:\n\t\t    RuntimeError: If Git operations fail.\n\n\t\t\"\"\"\n\t\tchanges: list[GitDiff] = []\n\t\tprocessed_files: set[str] = set()  # Track files already added\n\n\t\ttry:\n\t\t\t# 1. Get Staged Changes (Per File)\n\t\t\tcommit = self.git_context.repo.head.peel(Commit)\n\t\t\tstaged_files = self.git_context.repo.diff(commit.tree, self.git_context.repo.index)\n\t\t\tstaged_file_paths = []\n\t\t\tif isinstance(staged_files, Diff):\n\t\t\t\tif len(staged_files) &gt; 0:\n\t\t\t\t\tstaged_file_paths = [delta.delta.new_file.path for delta in staged_files]\n\t\t\telif isinstance(staged_files, Patch):\n\t\t\t\tstaged_file_paths.append(staged_files.delta.new_file.path)\n\t\t\tif staged_file_paths:\n\t\t\t\tlogger.debug(\"Found %d staged files. Fetching diffs individually...\", len(staged_file_paths))\n\t\t\t\tfor file_path in staged_file_paths:\n\t\t\t\t\tif file_path in processed_files:\n\t\t\t\t\t\tcontinue  # Avoid duplicates if somehow listed again\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfile_diff = self.git_context.get_per_file_diff(file_path, staged=True)\n\t\t\t\t\t\tchanges.append(file_diff)\n\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\tlogger.warning(\"Could not get staged diff for %s: %s\", file_path, e)\n\n\t\t\t# 2. Get Unstaged Changes (Per File for files not already staged)\n\t\t\tunstaged_files = self.git_context.repo.diff(self.git_context.repo.index, None)\n\t\t\tunstaged_file_paths = []\n\t\t\tif isinstance(unstaged_files, Diff):\n\t\t\t\tif len(unstaged_files) &gt; 0:\n\t\t\t\t\tunstaged_file_paths = [delta.delta.new_file.path for delta in unstaged_files]\n\t\t\telif isinstance(unstaged_files, Patch):\n\t\t\t\tunstaged_file_paths.append(unstaged_files.delta.new_file.path)\n\t\t\tif unstaged_file_paths:\n\t\t\t\tlogger.debug(\"Found %d unstaged files. Fetching diffs individually...\", len(unstaged_file_paths))\n\t\t\t\tfor file_path in unstaged_file_paths:\n\t\t\t\t\tif file_path not in processed_files:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tfile_diff = self.git_context.get_per_file_diff(file_path, staged=False)\n\t\t\t\t\t\t\tchanges.append(file_diff)\n\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\t\tlogger.warning(\"Could not get unstaged diff for %s: %s\", file_path, e)\n\n\t\t\t# 3. Get Untracked Files (Per File, content formatted as diff)\n\t\t\tuntracked_files_paths = self.git_context.get_untracked_files()\n\t\t\tif untracked_files_paths:\n\t\t\t\tlogger.debug(\"Found %d untracked files. Reading content...\", len(untracked_files_paths))\n\t\t\t\ttotal_content_lines = 0\n\n\t\t\t\tfor file_path in untracked_files_paths:\n\t\t\t\t\t# Only process untracked if not already captured as staged/unstaged (edge case)\n\t\t\t\t\tif file_path not in processed_files:\n\t\t\t\t\t\tabs_path = self.repo_root / file_path\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tcontent = read_file_content(abs_path)\n\t\t\t\t\t\t\tif content is not None:\n\t\t\t\t\t\t\t\tcontent_lines = content.splitlines()\n\t\t\t\t\t\t\t\toriginal_line_count = len(content_lines)\n\t\t\t\t\t\t\t\tneeds_total_truncation_notice = False\n\n\t\t\t\t\t\t\t\t# File-level truncation\n\t\t\t\t\t\t\t\tif len(content_lines) &gt; MAX_FILE_CONTENT_LINES:\n\t\t\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\t\t\t\"Untracked file %s is large (%d lines), truncating to %d lines\",\n\t\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t\t\tlen(content_lines),\n\t\t\t\t\t\t\t\t\t\tMAX_FILE_CONTENT_LINES,\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\ttruncation_msg = (\n\t\t\t\t\t\t\t\t\t\tf\"[... {len(content_lines) - MAX_FILE_CONTENT_LINES} more lines truncated ...]\"\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\tcontent_lines = content_lines[:MAX_FILE_CONTENT_LINES]\n\t\t\t\t\t\t\t\t\tcontent_lines.append(truncation_msg)\n\n\t\t\t\t\t\t\t\t# Total content truncation check\n\t\t\t\t\t\t\t\tif total_content_lines + len(content_lines) &gt; MAX_TOTAL_CONTENT_LINES:\n\t\t\t\t\t\t\t\t\tremaining_lines = MAX_TOTAL_CONTENT_LINES - total_content_lines\n\t\t\t\t\t\t\t\t\tif remaining_lines &gt; 0:\n\t\t\t\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\t\t\t\t\"Total untracked content size exceeded limit. Truncating %s to %d lines\",\n\t\t\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t\t\t\tremaining_lines,\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t\tcontent_lines = content_lines[:remaining_lines]\n\t\t\t\t\t\t\t\t\t\tneeds_total_truncation_notice = True\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t# No space left at all, skip this file and subsequent ones\n\t\t\t\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\t\t\t\"Max total untracked lines reached. Skipping remaining untracked files.\"\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\t\t\t# Format content for the diff\n\t\t\t\t\t\t\t\tformatted_content = [\"--- /dev/null\", f\"+++ b/{file_path}\"]\n\t\t\t\t\t\t\t\tformatted_content.extend(f\"+{line}\" for line in content_lines)\n\t\t\t\t\t\t\t\tif needs_total_truncation_notice:\n\t\t\t\t\t\t\t\t\tformatted_content.append(\n\t\t\t\t\t\t\t\t\t\t\"+[... Further untracked files truncated due to total size limits ...]\"\n\t\t\t\t\t\t\t\t\t)\n\n\t\t\t\t\t\t\t\tfile_content_str = \"\\n\".join(formatted_content)\n\t\t\t\t\t\t\t\tchanges.append(\n\t\t\t\t\t\t\t\t\tGitDiff(\n\t\t\t\t\t\t\t\t\t\tfiles=[file_path], content=file_content_str, is_staged=False, is_untracked=True\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\ttotal_content_lines += len(content_lines)\n\t\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\t\t\t\"Added content for untracked file %s (%d lines / %d original).\",\n\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t\tlen(content_lines),\n\t\t\t\t\t\t\t\t\toriginal_line_count,\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t# File content is None or empty\n\t\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\t\"Untracked file %s could not be read or is empty. Creating entry without content.\",\n\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tchanges.append(\n\t\t\t\t\t\t\t\t\tGitDiff(files=[file_path], content=\"\", is_staged=False, is_untracked=True)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\t\texcept (OSError, UnicodeDecodeError) as file_read_error:\n\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\"Could not read untracked file %s: %s. Creating entry without content.\",\n\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\tfile_read_error,\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\tchanges.append(GitDiff(files=[file_path], content=\"\", is_staged=False, is_untracked=True))\n\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get repository changes: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise RuntimeError(msg) from e\n\n\t\treturn changes\n\n\tdef _perform_commit(self, chunk: DiffChunk, message: str) -&gt; bool:\n\t\t\"\"\"\n\t\tPerform the actual commit operation.\n\n\t\tArgs:\n\t\t    chunk: The chunk to commit\n\t\t    message: Commit message to use\n\n\t\tReturns:\n\t\t    True if successful, False otherwise\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Commit only the files specified in the chunk\n\t\t\tself.git_context.commit_only_files(chunk.files, message, ignore_hooks=self.bypass_hooks)\n\t\t\tself.ui.show_success(f\"Committed {len(chunk.files)} files.\")\n\t\t\treturn True\n\t\texcept GitError as e:\n\t\t\terror_msg = f\"Error during commit: {e}\"\n\t\t\tself.ui.show_error(error_msg)\n\t\t\tlogger.exception(error_msg)\n\t\t\tself.error_state = \"failed\"\n\t\t\treturn False\n\n\tdef _process_chunk(self, chunk: DiffChunk, index: int, total_chunks: int) -&gt; bool:\n\t\t\"\"\"\n\t\tProcess a single chunk interactively.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk to process\n\t\t    index: The 0-based index of the current chunk\n\t\t    total_chunks: The total number of chunks\n\n\t\tReturns:\n\t\t    True if processing should continue, False to abort or on failure.\n\n\t\tRaises:\n\t\t    typer.Exit: If user chooses to exit.\n\n\t\t\"\"\"\n\t\tlogger.debug(\n\t\t\t\"Processing chunk - Chunk ID: %s, Index: %d/%d, Files: %s\",\n\t\t\tid(chunk),\n\t\t\tindex + 1,\n\t\t\ttotal_chunks,\n\t\t\tchunk.files,\n\t\t)\n\n\t\t# Clear previous generation state if any\n\t\tchunk.description = None\n\t\tchunk.is_llm_generated = False\n\n\t\twhile True:  # Loop to allow regeneration/editing\n\t\t\tmessage = \"\"\n\t\t\tused_llm = False\n\t\t\tpassed_validation = True  # Assume valid unless JSON or lint fails\n\t\t\tis_json_error = False  # Flag for JSON formatting errors\n\t\t\terror_messages: list[str] = []  # Stores lint or JSON errors\n\n\t\t\t# Generate message (potentially with linting retries)\n\t\t\ttry:\n\t\t\t\t# Generate message using the updated method, unpack the 5 values\n\t\t\t\t(\n\t\t\t\t\tmessage,\n\t\t\t\t\tused_llm,\n\t\t\t\t\tpassed_validation,\n\t\t\t\t\tis_json_error,\n\t\t\t\t\terror_messages,\n\t\t\t\t) = self.message_generator.generate_message_with_linting(chunk)\n\n\t\t\t\t# Store the potentially failed message in the chunk for display/editing\n\t\t\t\tchunk.description = message\n\t\t\t\tchunk.is_llm_generated = used_llm\n\t\t\texcept (LLMError, RuntimeError) as e:\n\t\t\t\tlogger.exception(\"Failed during message generation for chunk\")\n\t\t\t\tself.ui.show_error(f\"Error generating message: {e}\")\n\t\t\t\t# Offer to skip or exit after generation error\n\t\t\t\tif not questionary.confirm(\"Skip this chunk and continue?\", default=True).ask():\n\t\t\t\t\tself.error_state = \"aborted\"\n\t\t\t\t\treturn False  # Abort\n\t\t\t\t# If user chooses to skip after generation error, we continue to the next chunk\n\t\t\t\treturn True\n\n\t\t\t# -------- Handle Validation Result and User Action ---------\n\t\t\tif not passed_validation:\n\t\t\t\t# Display the diff chunk info first\n\t\t\t\tself.ui.display_chunk(chunk, index, total_chunks)\n\t\t\t\tif is_json_error:\n\t\t\t\t\t# Display the raw content and JSON error\n\t\t\t\t\tself.ui.display_failed_json_message(message, error_messages, used_llm)\n\t\t\t\t\t# Ask user what to do on JSON failure\n\t\t\t\t\taction = self.ui.get_user_action_on_lint_failure()\n\t\t\t\telse:  # It must be a linting error\n\t\t\t\t\t# Display the failed message and lint errors\n\t\t\t\t\tself.ui.display_failed_lint_message(message, error_messages, used_llm)\n\t\t\t\t\t# Ask user what to do on lint failure\n\t\t\t\t\taction = self.ui.get_user_action_on_lint_failure()\n\t\t\telse:\n\t\t\t\t# Display the valid message and diff chunk\n\t\t\t\tself.ui.display_chunk(chunk, index, total_chunks)  # Pass correct index and total\n\t\t\t\t# Ask user what to do with the valid message\n\t\t\t\taction = self.ui.get_user_action()\n\n\t\t\t# -------- Process User Action ---------\n\t\t\tif action == ChunkAction.COMMIT:\n\t\t\t\t# Commit with the current message (which passed validation)\n\t\t\t\tif self._perform_commit(chunk, message):\n\t\t\t\t\treturn True  # Continue to next chunk\n\t\t\t\tself.error_state = \"failed\"\n\t\t\t\treturn False  # Abort on commit failure\n\t\t\tif action == ChunkAction.EDIT:\n\t\t\t\t# Edit the message (could be formatted message or raw JSON)\n\t\t\t\tcurrent_message_to_edit = message or \"\"  # Default to empty string if None\n\n\t\t\t\t# Original error was linting, user is editing a formatted message\n\t\t\t\tedited_message = self.ui.edit_message(current_message_to_edit)\n\t\t\t\tcleaned_edited_message = clean_message_for_linting(edited_message)\n\t\t\t\tedited_is_valid, edited_error_msg = lint_commit_message(cleaned_edited_message, self.config_loader)\n\n\t\t\t\tif edited_is_valid:\n\t\t\t\t\t# Commit with the user-edited, now valid message\n\t\t\t\t\tif self._perform_commit(chunk, cleaned_edited_message):\n\t\t\t\t\t\treturn True  # Continue to next chunk\n\t\t\t\t\tself.error_state = \"failed\"\n\t\t\t\t\treturn False  # Abort on commit failure\n\n\t\t\t\t# If edited message is still invalid, show errors and loop back\n\t\t\t\tself.ui.show_warning(\"Edited message still failed linting.\")\n\t\t\t\t# Show the lint errors for the edited message\n\t\t\t\tedited_error_messages = [edited_error_msg] if edited_error_msg else []\n\t\t\t\tself.ui.display_failed_lint_message(\n\t\t\t\t\tcleaned_edited_message, edited_error_messages, is_llm_generated=False\n\t\t\t\t)\n\t\t\t\t# Loop back to prompt again with the lint failure message\n\t\t\t\tchunk.description = cleaned_edited_message  # Keep cleaned message in chunk\n\t\t\t\tchunk.is_llm_generated = False\n\t\t\t\tcontinue  # Go back to the start of the while loop\n\n\t\t\tif action == ChunkAction.REGENERATE:\n\t\t\t\tself.ui.show_regenerating()\n\t\t\t\tchunk.description = None  # Clear description before regenerating\n\t\t\t\tchunk.is_llm_generated = False\n\t\t\t\tcontinue  # Go back to the start of the while loop to regenerate\n\t\t\tif action == ChunkAction.SKIP:\n\t\t\t\tself.ui.show_skipped(chunk.files)\n\t\t\t\treturn True  # Continue to next chunk\n\t\t\tif action == ChunkAction.EXIT:\n\t\t\t\tif self.ui.confirm_exit():\n\t\t\t\t\tself.error_state = \"aborted\"\n\t\t\t\t\t# Returning False signals to stop processing chunks\n\t\t\t\t\treturn False\n\t\t\t\t# If user cancels exit, loop back to show the chunk again\n\t\t\t\tcontinue\n\t\t\t# Should not be reached, but handle unknown actions\n\t\t\tlogger.error(\"Unhandled action in _process_chunk: %s\", action)\n\t\t\treturn False\n\n\t\t# This should never be reached due to the while True loop, but add for type checker\n\t\treturn False\n\n\tdef process_all_chunks(self, chunks: list[DiffChunk], grand_total: int, interactive: bool = True) -&gt; bool:\n\t\t\"\"\"\n\t\tProcess all generated chunks.\n\n\t\tArgs:\n\t\t    chunks: List of DiffChunk objects to process\n\t\t    grand_total: Total number of chunks initially generated\n\t\t    interactive: Whether to run in interactive mode\n\n\t\tReturns:\n\t\t    True if all chunks were processed successfully, False otherwise\n\n\t\t\"\"\"\n\t\tif not chunks:\n\t\t\tself.ui.show_error(\"No diff chunks found to process.\")\n\t\t\treturn False\n\n\t\tsuccess = True\n\t\tfor i, chunk in enumerate(chunks):\n\t\t\tif interactive:\n\t\t\t\ttry:\n\t\t\t\t\tif not self._process_chunk(chunk, i, grand_total):\n\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\tbreak\n\t\t\t\texcept typer.Exit:\n\t\t\t\t\t# User chose to exit via typer.Exit(), which is expected\n\t\t\t\t\tsuccess = False  # Indicate not all chunks were processed\n\t\t\t\t\tbreak\n\t\t\t\texcept RuntimeError as e:\n\t\t\t\t\tself.ui.show_error(f\"Runtime error processing chunk: {e}\")\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\t# Non-interactive mode: generate and attempt commit\n\t\t\t\ttry:\n\t\t\t\t\t# Unpack 5 elements now\n\t\t\t\t\t(\n\t\t\t\t\t\tmessage,\n\t\t\t\t\t\t_,\n\t\t\t\t\t\tpassed_validation,\n\t\t\t\t\t\tis_json_error,\n\t\t\t\t\t\terror_messages,\n\t\t\t\t\t) = self.message_generator.generate_message_with_linting(chunk)\n\t\t\t\t\tif not passed_validation:\n\t\t\t\t\t\terror_type = \"JSON validation\" if is_json_error else \"linting\"\n\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\tf\"Generated message failed {error_type} in non-interactive mode: %s\\nErrors: %s\",\n\t\t\t\t\t\t\tmessage,\n\t\t\t\t\t\t\t\"\\n\".join(error_messages),\n\t\t\t\t\t\t)\n\t\t\t\t\t\t# Decide behavior: skip, commit anyway, fail? Let's skip for now.\n\t\t\t\t\t\tself.ui.show_skipped(chunk.files)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif not self._perform_commit(chunk, message):\n\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\tbreak\n\t\t\t\texcept (LLMError, RuntimeError, GitError, CommitFormattingError) as e:\n\t\t\t\t\tself.ui.show_error(f\"Error processing chunk non-interactively: {e}\")\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\n\t\treturn success\n\n\tasync def run(self, interactive: bool = True) -&gt; bool:\n\t\t\"\"\"\n\t\tRun the commit command workflow.\n\n\t\tArgs:\n\t\t    interactive: Whether to run in interactive mode. Defaults to True.\n\n\t\tReturns:\n\t\t    True if the process completed (even if aborted), False on unexpected error.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\twith progress_indicator(\"Analyzing changes...\"):\n\t\t\t\tchanges = self._get_changes()\n\n\t\t\tif not changes:\n\t\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\t\treturn True\n\n\t\t\t# OPTIMIZATION: For simple changes, skip splitter overhead\n\t\t\tif len(changes) == 1:\n\t\t\t\tsingle_diff = changes[0]\n\t\t\t\tself._is_single_file_mode = True\n\t\t\t\tlogger.info(\n\t\t\t\t\t\"Simple change detected (files: %s, size: %d chars), skipping splitter for faster processing\",\n\t\t\t\t\tsingle_diff.files,\n\t\t\t\t\tlen(single_diff.content),\n\t\t\t\t)\n\n\t\t\t\t# Create DiffChunk directly without splitter\n\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\tchunk = DiffChunk(\n\t\t\t\t\tfiles=single_diff.files,\n\t\t\t\t\tcontent=single_diff.content,\n\t\t\t\t\tdescription=None,  # Will be generated by message generator\n\t\t\t\t\tis_llm_generated=False,\n\t\t\t\t)\n\t\t\t\tchunks = [chunk]\n\t\t\t\ttotal_chunks = 1\n\t\t\t\tlogger.debug(\"Created single chunk directly for files: %s\", single_diff.files)\n\t\t\telse:\n\t\t\t\t# Process each diff separately to avoid parsing issues\n\t\t\t\tchunks = []\n\n\t\t\t\tfor diff in changes:\n\t\t\t\t\t# Process each diff individually\n\t\t\t\t\tdiff_chunks, _ = await self.splitter.split_diff(diff)\n\t\t\t\t\tchunks.extend(diff_chunks)\n\n\t\t\t\ttotal_chunks = len(chunks)\n\t\t\t\tlogger.info(\"Split %d files into %d chunks.\", len(changes), total_chunks)\n\n\t\t\tif not chunks:\n\t\t\t\t# Import DiffChunk for clarity\n\n\t\t\t\t# If no target files available, try to detect modified files\n\t\t\t\tif not self.target_files:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t# Get staged files\n\t\t\t\t\t\tstaged_output = self.git_context.repo.diff(\n\t\t\t\t\t\t\tself.git_context.repo.head.peel(Commit), self.git_context.repo.index\n\t\t\t\t\t\t)\n\t\t\t\t\t\tif isinstance(staged_output, Diff):\n\t\t\t\t\t\t\tif len(staged_output) &gt; 0:\n\t\t\t\t\t\t\t\tself.target_files.extend([delta.delta.new_file.path for delta in staged_output])\n\t\t\t\t\t\telif isinstance(staged_output, Patch):\n\t\t\t\t\t\t\t# Patch is a single patch, so always add its file\n\t\t\t\t\t\t\tself.target_files.append(staged_output.delta.new_file.path)\n\n\t\t\t\t\t\t# Get unstaged but tracked files\n\t\t\t\t\t\tunstaged_output = self.git_context.repo.diff(self.git_context.repo.index, None)\n\t\t\t\t\t\tif isinstance(unstaged_output, Diff):\n\t\t\t\t\t\t\tif len(unstaged_output) &gt; 0:\n\t\t\t\t\t\t\t\tself.target_files.extend([delta.delta.new_file.path for delta in unstaged_output])\n\t\t\t\t\t\telif isinstance(unstaged_output, Patch):\n\t\t\t\t\t\t\t# Patch is a single patch, so always add its file\n\t\t\t\t\t\t\tself.target_files.append(unstaged_output.delta.new_file.path)\n\n\t\t\t\t\t\t# Get untracked files\n\t\t\t\t\t\tuntracked_files = self.git_context.get_untracked_files()\n\t\t\t\t\t\tif untracked_files:\n\t\t\t\t\t\t\tself.target_files.extend(untracked_files)\n\n\t\t\t\t\t\t# Remove duplicates\n\t\t\t\t\t\tself.target_files = list(set(self.target_files))\n\n\t\t\t\t\t\tif self.target_files:\n\t\t\t\t\t\t\tlogger.info(f\"Using detected modified files: {self.target_files}\")\n\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\tlogger.warning(f\"Error while getting modified files: {e}\")\n\n\t\t\t\t# Use helper method to create fallback chunks\n\t\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t\t# If still no chunks, return error\n\t\t\t\tif not chunks:\n\t\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\t\treturn False\n\n\t\t\t# Process chunks, passing the interactive flag\n\t\t\tsuccess = self.process_all_chunks(chunks, total_chunks, interactive=interactive)\n\n\t\t\tif self.error_state == \"aborted\":\n\t\t\t\tself.ui.show_message(\"Commit process aborted by user.\")\n\t\t\t\treturn True  # Abort is considered a valid exit\n\t\t\tif self.error_state == \"failed\":\n\t\t\t\tself.ui.show_error(\"Commit process failed due to errors.\")\n\t\t\t\treturn False\n\t\t\tif not success:\n\t\t\t\t# If process_all_chunks returned False without setting error_state\n\t\t\t\tself.ui.show_error(\"Commit process failed.\")\n\t\t\t\treturn False\n\t\t\tself.ui.show_all_done()\n\t\t\treturn True\n\n\t\texcept RuntimeError as e:\n\t\t\tself.ui.show_error(str(e))\n\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\t\tlogger.exception(\"Unexpected error in commit command run loop\")\n\t\t\treturn False\n\t\tfinally:\n\t\t\t# Restore original branch if it was changed\n\t\t\tif self.original_branch:\n\t\t\t\ttry:\n\t\t\t\t\t# get_current_branch is already imported\n\t\t\t\t\t# switch_branch is imported from codemap.git.utils now\n\t\t\t\t\tcurrent = self.git_context.branch\n\t\t\t\t\tif current != self.original_branch:\n\t\t\t\t\t\tlogger.info(\"Restoring original branch: %s\", self.original_branch)\n\t\t\t\t\t\tself.git_context.switch_branch(self.original_branch)\n\t\t\t\texcept (GitError, Exception) as e:\n\t\t\t\t\tlogger.warning(\"Could not restore original branch %s: %s\", self.original_branch, e)\n\n\t\t# This should never be reached due to explicit returns in try/except blocks\n\t\treturn False\n\n\tdef _try_create_fallback_chunks(self, files: list[str]) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tTry to create fallback chunks for files when regular splitting fails.\n\n\t\tArgs:\n\t\t\tfiles: List of file paths to process\n\n\t\tReturns:\n\t\t\tList of created DiffChunk objects\n\t\t\"\"\"\n\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\tchunks = []\n\n\t\t# Get all tracked files from git\n\t\ttry:\n\t\t\tall_tracked_files = list(self.git_context.repo.index)\n\t\t\tcorrected_files = []\n\t\t\tfor file in files:\n\t\t\t\tif file in all_tracked_files:\n\t\t\t\t\tcorrected_files.append(file)\n\t\t\t\t\tcontinue\n\t\t\t\tif file.startswith(\"rc/\") and file.replace(\"rc/\", \"src/\") in all_tracked_files:\n\t\t\t\t\tcorrected_file = file.replace(\"rc/\", \"src/\")\n\t\t\t\t\tlogger.info(f\"Corrected file path from {file} to {corrected_file}\")\n\t\t\t\t\tcorrected_files.append(corrected_file)\n\t\t\t\t\tcontinue\n\t\t\t\tlogger.warning(f\"Could not find a matching tracked file for {file}\")\n\t\t\tif corrected_files:\n\t\t\t\tfiles = corrected_files\n\t\t\t\tlogger.info(f\"Using corrected file paths: {files}\")\n\t\t\tfor file in files:\n\t\t\t\ttry:\n\t\t\t\t\tfile_diff = self.git_context.get_per_file_diff(file, staged=False)\n\t\t\t\t\tif file_diff.content:\n\t\t\t\t\t\tlogger.debug(f\"Created individual chunk for {file}\")\n\t\t\t\t\t\tchunks.append(DiffChunk(files=[file], content=file_diff.content))\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tfile_diff = self.git_context.get_per_file_diff(file, staged=True)\n\t\t\t\t\tif file_diff.content:\n\t\t\t\t\t\tlogger.debug(f\"Created individual chunk for staged {file}\")\n\t\t\t\t\t\tchunks.append(DiffChunk(files=[file], content=file_diff.content))\n\t\t\t\texcept GitError:\n\t\t\t\t\tlogger.warning(f\"Could not get diff for {file}\")\n\t\texcept GitError as e:\n\t\t\tlogger.warning(f\"Error while trying to fix file paths: {e}\")\n\n\t\t# If still no chunks but we have files, create empty chunks as last resort\n\t\tif not chunks and files:\n\t\t\tlogger.warning(\"No diffs found, creating minimal placeholder chunks\")\n\t\t\tfor file in files:\n\t\t\t\tplaceholder_diff = f\"--- a/{file}\\n+++ b/{file}\\n@@ -1 +1 @@\\n No content change detected\"\n\t\t\t\tchunks.append(DiffChunk(files=[file], content=placeholder_diff))\n\n\t\treturn chunks\n\n\tdef _is_simple_single_file_change(self, changes: list[GitDiff]) -&gt; bool:\n\t\t\"\"\"\n\t\tDetermine if we have a simple single-file change that can skip splitter overhead.\n\n\t\tArgs:\n\t\t    changes: List of GitDiff objects\n\n\t\tReturns:\n\t\t    True if this is a simple single-file change, False otherwise\n\t\t\"\"\"\n\t\tif len(changes) != 1:\n\t\t\treturn False\n\n\t\tsingle_diff = changes[0]\n\n\t\t# Single file in the diff\n\t\tif len(single_diff.files) == 1:\n\t\t\treturn True\n\n\t\t# Multiple files but they're all small changes (heuristic)\n\t\treturn len(single_diff.files) &lt;= MAX_SMALL_CHANGE_FILES and len(single_diff.content) &lt; MAX_SMALL_CHANGE_SIZE\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.__init__","title":"__init__","text":"<pre><code>__init__(bypass_hooks: bool = False) -&gt; None\n</code></pre> <p>Initialize the commit command.</p> <p>Parameters:</p> Name Type Description Default <code>bypass_hooks</code> <code>bool</code> <p>Whether to bypass git hooks with --no-verify</p> <code>False</code> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def __init__(\n\tself,\n\tbypass_hooks: bool = False,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the commit command.\n\n\tArgs:\n\t    bypass_hooks: Whether to bypass git hooks with --no-verify\n\n\t\"\"\"\n\ttry:\n\t\tself.ui: CommitUI = CommitUI()\n\n\t\tself.target_files: list[str] = []\n\t\tself.committed_files: set[str] = set()\n\t\tself.is_pathspec_mode: bool = False\n\t\tself.all_repo_files: set[str] = set()\n\t\tself.error_state: str | None = None  # Tracks reason for failure\n\t\tself.bypass_hooks: bool = bypass_hooks  # Whether to bypass git hooks with --no-verify\n\t\tself._is_single_file_mode: bool = False  # Track if we're processing a single file\n\n\t\tself.git_context = ExtendedGitRepoContext()\n\n\t\t# Store the current branch at initialization to ensure we don't switch branches unexpectedly\n\t\ttry:\n\t\t\tself.original_branch: str | None = self.git_context.branch\n\t\texcept (ImportError, GitError):\n\t\t\tself.original_branch = None\n\n\t\t# Remove eager initialization of config_loader, llm_client, splitter, message_generator\n\t\tself._config_loader: ConfigLoader | None = None\n\t\tself._llm_client: LLMClient | None = None\n\t\tself._splitter: DiffSplitter | None = None\n\t\tself._message_generator: CommitMessageGenerator | None = None\n\n\t\tcurrent_repo_root = self.config_loader.get.repo_root\n\n\t\tif not current_repo_root:\n\t\t\tcurrent_repo_root = self.git_context.repo_root\n\n\t\tif not current_repo_root:\n\t\t\tcurrent_repo_root = self.git_context.get_repo_root()\n\n\t\tself.repo_root = current_repo_root\n\n\texcept GitError as e:\n\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.ui","title":"ui  <code>instance-attribute</code>","text":"<pre><code>ui: CommitUI = CommitUI()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.target_files","title":"target_files  <code>instance-attribute</code>","text":"<pre><code>target_files: list[str] = []\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.committed_files","title":"committed_files  <code>instance-attribute</code>","text":"<pre><code>committed_files: set[str] = set()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.is_pathspec_mode","title":"is_pathspec_mode  <code>instance-attribute</code>","text":"<pre><code>is_pathspec_mode: bool = False\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.all_repo_files","title":"all_repo_files  <code>instance-attribute</code>","text":"<pre><code>all_repo_files: set[str] = set()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.error_state","title":"error_state  <code>instance-attribute</code>","text":"<pre><code>error_state: str | None = None\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.bypass_hooks","title":"bypass_hooks  <code>instance-attribute</code>","text":"<pre><code>bypass_hooks: bool = bypass_hooks\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.git_context","title":"git_context  <code>instance-attribute</code>","text":"<pre><code>git_context = ExtendedGitRepoContext()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.original_branch","title":"original_branch  <code>instance-attribute</code>","text":"<pre><code>original_branch: str | None = branch\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = current_repo_root\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.config_loader","title":"config_loader  <code>property</code>","text":"<pre><code>config_loader: ConfigLoader\n</code></pre> <p>Lazily initialize and return the ConfigLoader instance.</p>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.llm_client","title":"llm_client  <code>property</code>","text":"<pre><code>llm_client: LLMClient\n</code></pre> <p>Lazily initialize and return the LLMClient instance.</p>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.splitter","title":"splitter  <code>property</code>","text":"<pre><code>splitter: DiffSplitter\n</code></pre> <p>Lazily initialize and return the DiffSplitter instance.</p>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.message_generator","title":"message_generator  <code>property</code>","text":"<pre><code>message_generator: CommitMessageGenerator\n</code></pre> <p>Lazily initialize and return the CommitMessageGenerator instance.</p>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.process_all_chunks","title":"process_all_chunks","text":"<pre><code>process_all_chunks(\n\tchunks: list[DiffChunk],\n\tgrand_total: int,\n\tinteractive: bool = True,\n) -&gt; bool\n</code></pre> <p>Process all generated chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of DiffChunk objects to process</p> required <code>grand_total</code> <code>int</code> <p>Total number of chunks initially generated</p> required <code>interactive</code> <code>bool</code> <p>Whether to run in interactive mode</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if all chunks were processed successfully, False otherwise</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def process_all_chunks(self, chunks: list[DiffChunk], grand_total: int, interactive: bool = True) -&gt; bool:\n\t\"\"\"\n\tProcess all generated chunks.\n\n\tArgs:\n\t    chunks: List of DiffChunk objects to process\n\t    grand_total: Total number of chunks initially generated\n\t    interactive: Whether to run in interactive mode\n\n\tReturns:\n\t    True if all chunks were processed successfully, False otherwise\n\n\t\"\"\"\n\tif not chunks:\n\t\tself.ui.show_error(\"No diff chunks found to process.\")\n\t\treturn False\n\n\tsuccess = True\n\tfor i, chunk in enumerate(chunks):\n\t\tif interactive:\n\t\t\ttry:\n\t\t\t\tif not self._process_chunk(chunk, i, grand_total):\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\t\t\texcept typer.Exit:\n\t\t\t\t# User chose to exit via typer.Exit(), which is expected\n\t\t\t\tsuccess = False  # Indicate not all chunks were processed\n\t\t\t\tbreak\n\t\t\texcept RuntimeError as e:\n\t\t\t\tself.ui.show_error(f\"Runtime error processing chunk: {e}\")\n\t\t\t\tsuccess = False\n\t\t\t\tbreak\n\t\telse:\n\t\t\t# Non-interactive mode: generate and attempt commit\n\t\t\ttry:\n\t\t\t\t# Unpack 5 elements now\n\t\t\t\t(\n\t\t\t\t\tmessage,\n\t\t\t\t\t_,\n\t\t\t\t\tpassed_validation,\n\t\t\t\t\tis_json_error,\n\t\t\t\t\terror_messages,\n\t\t\t\t) = self.message_generator.generate_message_with_linting(chunk)\n\t\t\t\tif not passed_validation:\n\t\t\t\t\terror_type = \"JSON validation\" if is_json_error else \"linting\"\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"Generated message failed {error_type} in non-interactive mode: %s\\nErrors: %s\",\n\t\t\t\t\t\tmessage,\n\t\t\t\t\t\t\"\\n\".join(error_messages),\n\t\t\t\t\t)\n\t\t\t\t\t# Decide behavior: skip, commit anyway, fail? Let's skip for now.\n\t\t\t\t\tself.ui.show_skipped(chunk.files)\n\t\t\t\t\tcontinue\n\t\t\t\tif not self._perform_commit(chunk, message):\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\t\t\texcept (LLMError, RuntimeError, GitError, CommitFormattingError) as e:\n\t\t\t\tself.ui.show_error(f\"Error processing chunk non-interactively: {e}\")\n\t\t\t\tsuccess = False\n\t\t\t\tbreak\n\n\treturn success\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.run","title":"run  <code>async</code>","text":"<pre><code>run(interactive: bool = True) -&gt; bool\n</code></pre> <p>Run the commit command workflow.</p> <p>Parameters:</p> Name Type Description Default <code>interactive</code> <code>bool</code> <p>Whether to run in interactive mode. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the process completed (even if aborted), False on unexpected error.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>async def run(self, interactive: bool = True) -&gt; bool:\n\t\"\"\"\n\tRun the commit command workflow.\n\n\tArgs:\n\t    interactive: Whether to run in interactive mode. Defaults to True.\n\n\tReturns:\n\t    True if the process completed (even if aborted), False on unexpected error.\n\n\t\"\"\"\n\ttry:\n\t\twith progress_indicator(\"Analyzing changes...\"):\n\t\t\tchanges = self._get_changes()\n\n\t\tif not changes:\n\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\treturn True\n\n\t\t# OPTIMIZATION: For simple changes, skip splitter overhead\n\t\tif len(changes) == 1:\n\t\t\tsingle_diff = changes[0]\n\t\t\tself._is_single_file_mode = True\n\t\t\tlogger.info(\n\t\t\t\t\"Simple change detected (files: %s, size: %d chars), skipping splitter for faster processing\",\n\t\t\t\tsingle_diff.files,\n\t\t\t\tlen(single_diff.content),\n\t\t\t)\n\n\t\t\t# Create DiffChunk directly without splitter\n\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\tchunk = DiffChunk(\n\t\t\t\tfiles=single_diff.files,\n\t\t\t\tcontent=single_diff.content,\n\t\t\t\tdescription=None,  # Will be generated by message generator\n\t\t\t\tis_llm_generated=False,\n\t\t\t)\n\t\t\tchunks = [chunk]\n\t\t\ttotal_chunks = 1\n\t\t\tlogger.debug(\"Created single chunk directly for files: %s\", single_diff.files)\n\t\telse:\n\t\t\t# Process each diff separately to avoid parsing issues\n\t\t\tchunks = []\n\n\t\t\tfor diff in changes:\n\t\t\t\t# Process each diff individually\n\t\t\t\tdiff_chunks, _ = await self.splitter.split_diff(diff)\n\t\t\t\tchunks.extend(diff_chunks)\n\n\t\t\ttotal_chunks = len(chunks)\n\t\t\tlogger.info(\"Split %d files into %d chunks.\", len(changes), total_chunks)\n\n\t\tif not chunks:\n\t\t\t# Import DiffChunk for clarity\n\n\t\t\t# If no target files available, try to detect modified files\n\t\t\tif not self.target_files:\n\t\t\t\ttry:\n\t\t\t\t\t# Get staged files\n\t\t\t\t\tstaged_output = self.git_context.repo.diff(\n\t\t\t\t\t\tself.git_context.repo.head.peel(Commit), self.git_context.repo.index\n\t\t\t\t\t)\n\t\t\t\t\tif isinstance(staged_output, Diff):\n\t\t\t\t\t\tif len(staged_output) &gt; 0:\n\t\t\t\t\t\t\tself.target_files.extend([delta.delta.new_file.path for delta in staged_output])\n\t\t\t\t\telif isinstance(staged_output, Patch):\n\t\t\t\t\t\t# Patch is a single patch, so always add its file\n\t\t\t\t\t\tself.target_files.append(staged_output.delta.new_file.path)\n\n\t\t\t\t\t# Get unstaged but tracked files\n\t\t\t\t\tunstaged_output = self.git_context.repo.diff(self.git_context.repo.index, None)\n\t\t\t\t\tif isinstance(unstaged_output, Diff):\n\t\t\t\t\t\tif len(unstaged_output) &gt; 0:\n\t\t\t\t\t\t\tself.target_files.extend([delta.delta.new_file.path for delta in unstaged_output])\n\t\t\t\t\telif isinstance(unstaged_output, Patch):\n\t\t\t\t\t\t# Patch is a single patch, so always add its file\n\t\t\t\t\t\tself.target_files.append(unstaged_output.delta.new_file.path)\n\n\t\t\t\t\t# Get untracked files\n\t\t\t\t\tuntracked_files = self.git_context.get_untracked_files()\n\t\t\t\t\tif untracked_files:\n\t\t\t\t\t\tself.target_files.extend(untracked_files)\n\n\t\t\t\t\t# Remove duplicates\n\t\t\t\t\tself.target_files = list(set(self.target_files))\n\n\t\t\t\t\tif self.target_files:\n\t\t\t\t\t\tlogger.info(f\"Using detected modified files: {self.target_files}\")\n\t\t\t\texcept GitError as e:\n\t\t\t\t\tlogger.warning(f\"Error while getting modified files: {e}\")\n\n\t\t\t# Use helper method to create fallback chunks\n\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t# If still no chunks, return error\n\t\t\tif not chunks:\n\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\treturn False\n\n\t\t# Process chunks, passing the interactive flag\n\t\tsuccess = self.process_all_chunks(chunks, total_chunks, interactive=interactive)\n\n\t\tif self.error_state == \"aborted\":\n\t\t\tself.ui.show_message(\"Commit process aborted by user.\")\n\t\t\treturn True  # Abort is considered a valid exit\n\t\tif self.error_state == \"failed\":\n\t\t\tself.ui.show_error(\"Commit process failed due to errors.\")\n\t\t\treturn False\n\t\tif not success:\n\t\t\t# If process_all_chunks returned False without setting error_state\n\t\t\tself.ui.show_error(\"Commit process failed.\")\n\t\t\treturn False\n\t\tself.ui.show_all_done()\n\t\treturn True\n\n\texcept RuntimeError as e:\n\t\tself.ui.show_error(str(e))\n\t\treturn False\n\texcept Exception as e:\n\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\tlogger.exception(\"Unexpected error in commit command run loop\")\n\t\treturn False\n\tfinally:\n\t\t# Restore original branch if it was changed\n\t\tif self.original_branch:\n\t\t\ttry:\n\t\t\t\t# get_current_branch is already imported\n\t\t\t\t# switch_branch is imported from codemap.git.utils now\n\t\t\t\tcurrent = self.git_context.branch\n\t\t\t\tif current != self.original_branch:\n\t\t\t\t\tlogger.info(\"Restoring original branch: %s\", self.original_branch)\n\t\t\t\t\tself.git_context.switch_branch(self.original_branch)\n\t\t\texcept (GitError, Exception) as e:\n\t\t\t\tlogger.warning(\"Could not restore original branch %s: %s\", self.original_branch, e)\n\n\t# This should never be reached due to explicit returns in try/except blocks\n\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand","title":"SemanticCommitCommand","text":"<p>               Bases: <code>CommitCommand</code></p> <p>Handles the semantic commit command workflow.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>class SemanticCommitCommand(CommitCommand):\n\t\"\"\"Handles the semantic commit command workflow.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tbypass_hooks: bool = False,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the SemanticCommitCommand.\n\n\t\tArgs are similar to CLI options, allowing for programmatic use.\n\t\t\"\"\"\n\t\t# Call parent class initializer first\n\t\tsuper().__init__(bypass_hooks=bypass_hooks)\n\n\t\tconfig_loader = self.config_loader\n\n\t\tself.clustering_method = config_loader.get.embedding.clustering.method\n\t\tself.similarity_threshold = config_loader.get.commit.diff_splitter.similarity_threshold\n\n\t\t# Initialize attributes that may be set during execution\n\t\tself.target_files: list[str] = []\n\t\tself.is_pathspec_mode: bool = False\n\t\tself.all_repo_files: set[str] = set()\n\n\t\tfrom codemap.git.semantic_grouping.clusterer import DiffClusterer\n\t\tfrom codemap.git.semantic_grouping.embedder import DiffEmbedder\n\t\tfrom codemap.git.semantic_grouping.resolver import FileIntegrityResolver\n\n\t\t# Pass the loaded_model_object to DiffEmbedder\n\t\tself.embedder = DiffEmbedder(config_loader=self.config_loader)\n\t\tself.clusterer = DiffClusterer(config_loader=self.config_loader)\n\t\tself.resolver = FileIntegrityResolver(\n\t\t\tsimilarity_threshold=self.similarity_threshold, config_loader=self.config_loader\n\t\t)\n\n\tdef _get_target_files(self, pathspecs: list[str] | None = None) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet the list of target files based on pathspecs.\n\n\t\tArgs:\n\t\t        pathspecs: Optional list of path specifications\n\n\t\tReturns:\n\t\t        List of file paths\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tcmd = [\"git\", \"status\", \"--porcelain=v1\", \"-uall\"]\n\t\t\tif pathspecs:\n\t\t\t\tcmd.extend([\"--\", *pathspecs])\n\t\t\t\tself.is_pathspec_mode = True\n\t\t\toutput = self.git_context.repo.status()\n\t\t\ttarget_files = []\n\t\t\tfor file_path in output:\n\t\t\t\tif not file_path or len(file_path) &lt; MIN_PORCELAIN_LINE_LENGTH:\n\t\t\t\t\tcontinue\n\t\t\t\t# No status parsing here, just add the file\n\t\t\t\ttarget_files.append(file_path)\n\t\t\tif self.is_pathspec_mode:\n\t\t\t\tself.all_repo_files = set(self.git_context.repo.index)\n\t\t\treturn target_files\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get target files: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _prepare_untracked_files(self, target_files: list[str]) -&gt; list[str]:\n\t\t\"\"\"\n\t\tPrepare untracked files for diffing by adding them to the index.\n\n\t\tArgs:\n\t\t        target_files: List of target file paths\n\n\t\tReturns:\n\t\t        List of untracked files that were prepared\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get untracked files\n\t\t\tuntracked_files = self.git_context.get_untracked_files()\n\n\t\t\t# Filter to only those in target_files\n\t\t\tuntracked_targets = [f for f in untracked_files if f in target_files]\n\n\t\t\tif untracked_targets:\n\t\t\t\t# Add untracked files to the index (but not staging area)\n\t\t\t\tfor path in untracked_targets:\n\t\t\t\t\tself.git_context.repo.index.add(path)\n\n\t\t\treturn untracked_targets\n\n\t\texcept GitError as e:\n\t\t\tlogger.warning(\"Error preparing untracked files: %s\", e)\n\t\t\treturn []\n\n\tdef _get_combined_diff(self, target_files: list[str]) -&gt; GitDiff:\n\t\t\"\"\"\n\t\tGet the combined diff for all target files, including staged, unstaged, and untracked.\n\n\t\tArgs:\n\t\t\ttarget_files: List of target file paths\n\n\t\tReturns:\n\t\t\tGitDiff object with the combined diff content for all specified files.\n\n\t\tRaises:\n\t\t\tRuntimeError: If Git operations fail.\n\t\t\"\"\"\n\t\tlogger.debug(\"SemanticCommitCommand._get_combined_diff called for files: %s\", target_files)\n\t\tcombined_content_parts: list[str] = []\n\t\tprocessed_for_combined_diff: set[str] = set()\n\t\trepo_status = self.git_context.repo.status()\n\n\t\ttry:\n\t\t\tfor file_path in target_files:\n\t\t\t\tif file_path in processed_for_combined_diff:\n\t\t\t\t\tcontinue\n\n\t\t\t\tfile_status_flags = repo_status.get(file_path)\n\t\t\t\tis_untracked = file_status_flags is not None and file_status_flags &amp; FileStatus.WT_NEW\n\n\t\t\t\t# Try to get staged diff first\n\t\t\t\ttry:\n\t\t\t\t\tstaged_diff = self.git_context.get_per_file_diff(file_path, staged=True)\n\t\t\t\t\tif staged_diff.content:\n\t\t\t\t\t\tlogger.debug(\"  Adding STAGED diff content for %s\", file_path)\n\t\t\t\t\t\tcombined_content_parts.append(staged_diff.content)\n\t\t\t\t\t\tprocessed_for_combined_diff.add(file_path)\n\t\t\t\t\t\t# If a file has staged changes, we typically consider that its primary diff state\n\t\t\t\t\t\t# and might skip unstaged for combined view, or ensure unstaged diff is distinctly handled.\n\t\t\t\t\t\t# For now, if staged, we take it and move to next file to avoid double-adding sections.\n\t\t\t\t\t\tcontinue\n\t\t\t\texcept GitError as e:\n\t\t\t\t\tlogger.warning(\"Could not get staged diff for %s in _get_combined_diff: %s\", file_path, e)\n\n\t\t\t\t# If no staged content, try unstaged diff\n\t\t\t\tif file_path not in processed_for_combined_diff:  # Check again in case of error above\n\t\t\t\t\ttry:\n\t\t\t\t\t\tunstaged_diff = self.git_context.get_per_file_diff(file_path, staged=False)\n\t\t\t\t\t\tif unstaged_diff.content:\n\t\t\t\t\t\t\tlogger.debug(\"  Adding UNSTAGED diff content for %s\", file_path)\n\t\t\t\t\t\t\tcombined_content_parts.append(unstaged_diff.content)\n\t\t\t\t\t\t\tprocessed_for_combined_diff.add(file_path)\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\tlogger.warning(\"Could not get unstaged diff for %s in _get_combined_diff: %s\", file_path, e)\n\n\t\t\t\t# Handle untracked files if not already processed\n\t\t\t\tif file_path not in processed_for_combined_diff and is_untracked:\n\t\t\t\t\trepo_root = self.git_context.repo_root\n\n\t\t\t\t\tif repo_root is None:\n\t\t\t\t\t\trepo_root = self.git_context.get_repo_root()\n\n\t\t\t\t\tabs_path = repo_root / file_path\n\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcontent = read_file_content(abs_path)\n\t\t\t\t\t\tif content is not None:\n\t\t\t\t\t\t\tcontent_lines = content.splitlines()\n\t\t\t\t\t\t\t# Apply file-level truncation for combined diff context\n\t\t\t\t\t\t\tif len(content_lines) &gt; MAX_FILE_CONTENT_LINES:\n\t\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\t\t\"Untracked file %s (in combined_diff) is large (%d lines), truncating to %d lines\",\n\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t\tlen(content_lines),\n\t\t\t\t\t\t\t\t\tMAX_FILE_CONTENT_LINES,\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\ttruncation_msg = (\n\t\t\t\t\t\t\t\t\tf\"[... {len(content_lines) - MAX_FILE_CONTENT_LINES} more lines truncated ...]\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tcontent_lines = content_lines[:MAX_FILE_CONTENT_LINES]\n\t\t\t\t\t\t\t\tcontent_lines.append(truncation_msg)\n\n\t\t\t\t\t\t\tformatted_content_for_diff = [\"--- /dev/null\", f\"+++ b/{file_path}\"]\n\t\t\t\t\t\t\tformatted_content_for_diff.extend(f\"+{line}\" for line in content_lines)\n\t\t\t\t\t\t\tuntracked_content_str = \"\\n\".join(formatted_content_for_diff)\n\t\t\t\t\t\t\tlogger.debug(\"  Adding UNTRACKED content for %s\", file_path)\n\t\t\t\t\t\t\tcombined_content_parts.append(untracked_content_str)\n\t\t\t\t\t\t\tprocessed_for_combined_diff.add(file_path)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\"Untracked file %s (in combined_diff) could not be read or is empty.\", file_path\n\t\t\t\t\t\t\t)\n\t\t\t\t\texcept (OSError, UnicodeDecodeError) as file_read_error:\n\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\"Could not read untracked file %s (in combined_diff): %s.\",\n\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\tfile_read_error,\n\t\t\t\t\t\t)\n\n\t\t\t\t# If after all attempts, no diff was found for a target file, log it.\n\t\t\t\t# This could happen if the file exists but has no changes and is not untracked.\n\t\t\t\tif file_path not in processed_for_combined_diff:\n\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\"  No diff content (staged, unstaged, or untracked) found for %s in _get_combined_diff\",\n\t\t\t\t\t\tfile_path,\n\t\t\t\t\t)\n\n\t\t\tfinal_combined_content = \"\\n\".join(combined_content_parts)\n\t\t\tlogger.debug(\"_get_combined_diff final content (first 500 chars): %s\", repr(final_combined_content[:500]))\n\t\t\treturn GitDiff(files=target_files, content=final_combined_content)\n\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get combined diff: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise RuntimeError(msg) from e\n\t\texcept Exception as e:  # Catch any other unexpected errors during diff aggregation\n\t\t\tmsg = f\"Unexpected error while generating combined diff: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise RuntimeError(msg) from e\n\n\tasync def _create_semantic_groups(self, chunks: list[DiffChunk]) -&gt; list[SemanticGroup]:\n\t\t\"\"\"\n\t\tCreate semantic groups from diff chunks.\n\n\t\tArgs:\n\t\t        chunks: List of DiffChunk objects\n\n\t\tReturns:\n\t\t        List of SemanticGroup objects\n\n\t\t\"\"\"\n\t\t# Shortcut for small changes - bypass embedding process\n\t\tif len(chunks) &lt;= 3:  # Threshold for \"small changes\" # noqa: PLR2004\n\t\t\tlogger.info(\"Small number of chunks detected (%d), bypassing embedding process\", len(chunks))\n\t\t\t# Create a single semantic group with all chunks\n\t\t\tsingle_group = SemanticGroup(chunks=chunks)\n\t\t\t# Extract all file names from chunks\n\t\t\tfiles_set = set()\n\t\t\tfor chunk in chunks:\n\t\t\t\tfiles_set.update(chunk.files)\n\t\t\tsingle_group.files = list(files_set)\n\t\t\t# Combine all content\n\t\t\tcombined_content = \"\\n\".join(chunk.content for chunk in chunks)\n\t\t\tsingle_group.content = combined_content\n\t\t\treturn [single_group]\n\n\t\t# Generate embeddings for chunks\n\t\tchunk_embedding_tuples = await self.embedder.embed_chunks(chunks)\n\t\tchunk_embeddings = {ce[0]: ce[1] for ce in chunk_embedding_tuples}\n\n\t\t# Cluster chunks\n\t\tcluster_lists = self.clusterer.cluster(chunk_embedding_tuples)\n\n\t\t# Create initial semantic groups\n\t\tinitial_groups = [SemanticGroup(chunks=cluster) for cluster in cluster_lists]\n\n\t\t# Resolve file integrity constraints\n\t\treturn self.resolver.resolve_violations(initial_groups, chunk_embeddings)\n\n\tasync def _generate_group_messages(self, groups: list[SemanticGroup]) -&gt; list[SemanticGroup]:\n\t\t\"\"\"\n\t\tGenerate commit messages for semantic groups.\n\n\t\tArgs:\n\t\t        groups: List of SemanticGroup objects\n\n\t\tReturns:\n\t\t        List of SemanticGroup objects with messages\n\n\t\t\"\"\"\n\t\t# Process groups individually\n\t\tfrom codemap.git.diff_splitter import DiffChunk\n\t\tfrom codemap.git.semantic_grouping.context_processor import process_chunks_with_lod\n\n\t\t# Get max token limit and settings from message generator's config\n\t\tmax_tokens = self.config_loader.get.llm.max_output_tokens\n\t\tuse_lod_context = self.config_loader.get.commit.use_lod_context\n\n\t\tfor group in groups:\n\t\t\ttry:\n\t\t\t\t# Create temporary DiffChunks from the group's chunks\n\t\t\t\tif use_lod_context and group.chunks and len(group.chunks) &gt; 1:\n\t\t\t\t\tlogger.debug(\"Processing semantic group with %d chunks using LOD context\", len(group.chunks))\n\t\t\t\t\ttry:\n\t\t\t\t\t\t# Process all chunks in the group with LOD context processor\n\t\t\t\t\t\toptimized_content = process_chunks_with_lod(group.chunks, max_tokens)\n\n\t\t\t\t\t\tif optimized_content:\n\t\t\t\t\t\t\t# Create a temporary chunk with the optimized content\n\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=optimized_content)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t# Fallback: create a temp chunk with original content\n\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\t\texcept Exception:\n\t\t\t\t\t\tlogger.exception(\"Error in LOD context processing\")\n\t\t\t\t\t\t# Fallback to original content\n\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\telse:\n\t\t\t\t\t# Use the original group content\n\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\n\t\t\t\t# Generate message with linting\n\t\t\t\t# We ignore linting status - SemanticCommitCommand is less strict\n\t\t\t\t# Unpack 5 elements, ignore validation/error status for semantic commit\n\t\t\t\tmessage, _, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\n\t\t\t\t# Store the message with the group\n\t\t\t\tgroup.message = message\n\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error generating message for group\")\n\t\t\t\t# Use a fallback message\n\t\t\t\tgroup.message = f\"update: changes to {len(group.files)} files\"\n\n\t\treturn groups\n\n\tasync def _stage_and_commit_group(self, group: SemanticGroup) -&gt; bool:\n\t\t\"\"\"\n\t\tStage and commit a semantic group.\n\n\t\tArgs:\n\t\t        group: SemanticGroup to commit\n\n\t\tReturns:\n\t\t        bool: Whether the commit was successful\n\n\t\t\"\"\"\n\t\tgroup_files = group.files\n\t\ttry:\n\t\t\t# Unstage all files first (if needed, implement as needed)\n\t\t\t# Add the group files to the index individually\n\t\t\tfor file_path in group_files:\n\t\t\t\t# Check file status to handle deletions correctly\n\t\t\t\tstatus = self.git_context.repo.status_file(file_path)\n\t\t\t\tif status == FileStatus.WT_DELETED:\n\t\t\t\t\tself.git_context.repo.index.remove(file_path)\n\t\t\t\telse:\n\t\t\t\t\tself.git_context.repo.index.add(file_path)\n\n\t\t\tself.git_context.repo.index.write()\n\t\t\t# Use commit_only_files utility for commit\n\t\t\tself.git_context.commit_only_files(group_files, group.message or \"\", ignore_hooks=self.bypass_hooks)\n\t\t\tself.committed_files.update(group_files)\n\t\t\treturn True\n\t\texcept GitError as commit_error:\n\t\t\tself.ui.show_error(f\"Failed to commit: {commit_error}\")\n\t\t\treturn False\n\n\tasync def run(self, interactive: bool = True, pathspecs: list[str] | None = None) -&gt; bool:\n\t\t\"\"\"\n\t\tRun the semantic commit command workflow.\n\n\t\tArgs:\n\t\t        interactive: Whether to run in interactive mode\n\t\t        pathspecs: Optional list of path specifications\n\n\t\tReturns:\n\t\t        bool: Whether the process completed successfully\n\n\t\t\"\"\"\n\t\tcommitted_count = 0  # Initialize this at the beginning of the method\n\n\t\ttry:\n\t\t\t# Get target files\n\t\t\twith progress_indicator(\"Analyzing repository...\"):\n\t\t\t\tself.target_files = self._get_target_files(pathspecs)\n\t\t\t\tlogger.debug(f\"SemanticCommitCommand.run: Target files after _get_target_files: {self.target_files}\")\n\n\t\t\t\tif not self.target_files:\n\t\t\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\t\t\treturn True\n\n\t\t\t\t# OPTIMIZATION: For simple changes, skip splitter overhead\n\t\t\t\tif len(self.target_files) == 1:\n\t\t\t\t\tsingle_file = self.target_files[0]\n\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\"Single file change detected (%s), skipping semantic grouping for faster processing\",\n\t\t\t\t\t\tsingle_file,\n\t\t\t\t\t)\n\n\t\t\t\t\t# Prepare untracked files if needed\n\t\t\t\t\tself._prepare_untracked_files(self.target_files)\n\n\t\t\t\t\t# Get diff for the single file\n\t\t\t\t\tcombined_diff = self._get_combined_diff(self.target_files)\n\n\t\t\t\t\t# Create a single chunk directly\n\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\tchunk = DiffChunk(\n\t\t\t\t\t\tfiles=self.target_files, content=combined_diff.content, description=None, is_llm_generated=False\n\t\t\t\t\t)\n\n\t\t\t\t\t# Create a single semantic group\n\t\t\t\t\tsingle_group = SemanticGroup(chunks=[chunk])\n\t\t\t\t\tsingle_group.files = self.target_files\n\t\t\t\t\tsingle_group.content = combined_diff.content\n\n\t\t\t\t\t# Generate message for the single group\n\t\t\t\t\tgroups = await self._generate_group_messages([single_group])\n\n\t\t\t\t\tlogger.debug(\"Created single semantic group directly for file: %s\", single_file)\n\t\t\t\telse:\n\t\t\t\t\t# Prepare untracked files\n\t\t\t\t\tself._prepare_untracked_files(self.target_files)\n\n\t\t\t\t\t# Get combined diff\n\t\t\t\t\tcombined_diff = self._get_combined_diff(self.target_files)\n\n\t\t\t\t\t# Log diff details for debugging\n\t\t\t\t\tlogger.debug(f\"Combined diff size: {len(combined_diff.content)} characters\")\n\t\t\t\t\tlogger.debug(f\"Target files: {len(self.target_files)} files\")\n\n\t\t\t\t\t# Import DiffChunk before using it\n\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\t# Split diff into chunks\n\t\t\t\t\tchunks, _ = await self.splitter.split_diff(combined_diff)\n\t\t\t\t\tlogger.debug(f\"Initial chunks created: {len(chunks)}\")\n\n\t\t\t\t\t# If no chunks created but we have combined diff content, create a single chunk\n\t\t\t\t\tif not chunks and combined_diff.content.strip():\n\t\t\t\t\t\tlogger.info(\"No chunks created from splitter, creating a single chunk\")\n\t\t\t\t\t\tchunks = [DiffChunk(files=self.target_files, content=combined_diff.content)]\n\n\t\t\t\t\t# Last resort: try creating individual chunks for each file\n\t\t\t\t\tif not chunks:\n\t\t\t\t\t\tlogger.info(\"Attempting to create individual file chunks\")\n\t\t\t\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t\t\t# If still no chunks, return error\n\t\t\t\t\tif not chunks:\n\t\t\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\t\t\treturn False\n\n\t\t\t\t\tlogger.info(f\"Final chunk count: {len(chunks)}\")\n\n\t\t\t\t\t# Create semantic groups\n\t\t\t\t\twith progress_indicator(\"Creating semantic groups...\"):\n\t\t\t\t\t\t# Special case for very few files - create a single group\n\t\t\t\t\t\tif len(chunks) &lt;= 2:  # noqa: PLR2004\n\t\t\t\t\t\t\tlogger.info(\"Small number of chunks detected, creating a single semantic group\")\n\t\t\t\t\t\t\t# Create a single semantic group with all chunks\n\t\t\t\t\t\t\tsingle_group = SemanticGroup(chunks=chunks)\n\t\t\t\t\t\t\t# Extract all file names from chunks\n\t\t\t\t\t\t\tfiles_set = set()\n\t\t\t\t\t\t\tfor chunk in chunks:\n\t\t\t\t\t\t\t\tfiles_set.update(chunk.files)\n\t\t\t\t\t\t\tsingle_group.files = list(files_set)\n\t\t\t\t\t\t\tgroups = [single_group]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t# Normal case - use clustering\n\t\t\t\t\t\t\tgroups = await self._create_semantic_groups(chunks)\n\n\t\t\t\t\t\tif not groups:\n\t\t\t\t\t\t\tself.ui.show_error(\"Failed to create semantic groups.\")\n\t\t\t\t\t\t\treturn False\n\n\t\t\t\t\t\t# Generate messages for groups\n\t\t\t\t\t\tgroups = await self._generate_group_messages(groups)\n\n\t\t\t# Process groups\n\t\t\tself.ui.show_message(f\"Found {len(groups)} semantic groups of changes.\")\n\n\t\t\tsuccess = True\n\n\t\t\tfor i, group in enumerate(groups):\n\t\t\t\tif interactive:\n\t\t\t\t\t# Display group info with improved UI\n\t\t\t\t\tself.ui.display_group(group, i, len(groups))\n\n\t\t\t\t\t# Get user action\n\t\t\t\t\taction = self.ui.get_group_action()\n\n\t\t\t\t\tif action == ChunkAction.COMMIT:\n\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\telif action == ChunkAction.EDIT:\n\t\t\t\t\t\t# Allow user to edit the message\n\t\t\t\t\t\tcurrent_message = group.message or \"\"  # Default to empty string if None\n\t\t\t\t\t\tedited_message = self.ui.edit_message(current_message)\n\t\t\t\t\t\tgroup.message = edited_message\n\n\t\t\t\t\t\t# Commit immediately after editing\n\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\telif action == ChunkAction.REGENERATE:\n\t\t\t\t\t\tself.ui.show_regenerating()\n\t\t\t\t\t\t# Re-generate the message\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\t\t\t\tmessage, _, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\t\t\t\t\t\t\tgroup.message = message\n\n\t\t\t\t\t\t\t# Show the regenerated message\n\t\t\t\t\t\t\tself.ui.display_group(group, i, len(groups))\n\t\t\t\t\t\t\tif questionary.confirm(\"Commit with regenerated message?\", default=True).ask():\n\t\t\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\t\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\texcept (LLMError, GitError, RuntimeError) as e:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Error regenerating message: {e}\")\n\t\t\t\t\t\t\tif questionary.confirm(\"Skip this group?\", default=True).ask():\n\t\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\telif action == ChunkAction.SKIP:\n\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\telif action == ChunkAction.EXIT and self.ui.confirm_exit():\n\t\t\t\t\t\t# This is a user-initiated exit, should not be considered a failure\n\t\t\t\t\t\tself.ui.show_message(\"Commit process exited by user.\")\n\t\t\t\t\t\treturn True  # Return true to indicate normal exit, not failure\n\t\t\t\telse:\n\t\t\t\t\t# In non-interactive mode, commit each group immediately\n\t\t\t\t\tgroup.message = group.message or f\"update: changes to {len(group.files)} files\"\n\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\tsuccess = False\n\n\t\t\tif committed_count &gt; 0:\n\t\t\t\tself.ui.show_message(f\"Successfully committed {committed_count} semantic groups.\")\n\t\t\t\tself.ui.show_all_done()\n\t\t\telse:\n\t\t\t\tself.ui.show_message(\"No changes were committed.\")\n\n\t\t\treturn success\n\t\texcept ExitCommandError:\n\t\t\t# User requested to exit during lint failure handling\n\t\t\treturn committed_count &gt; 0\n\t\texcept RuntimeError as e:\n\t\t\tself.ui.show_error(str(e))\n\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\t\tlogger.exception(\"Unexpected error in semantic commit command\")\n\t\t\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.__init__","title":"__init__","text":"<pre><code>__init__(bypass_hooks: bool = False) -&gt; None\n</code></pre> <p>Initialize the SemanticCommitCommand.</p> <p>Args are similar to CLI options, allowing for programmatic use.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def __init__(\n\tself,\n\tbypass_hooks: bool = False,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the SemanticCommitCommand.\n\n\tArgs are similar to CLI options, allowing for programmatic use.\n\t\"\"\"\n\t# Call parent class initializer first\n\tsuper().__init__(bypass_hooks=bypass_hooks)\n\n\tconfig_loader = self.config_loader\n\n\tself.clustering_method = config_loader.get.embedding.clustering.method\n\tself.similarity_threshold = config_loader.get.commit.diff_splitter.similarity_threshold\n\n\t# Initialize attributes that may be set during execution\n\tself.target_files: list[str] = []\n\tself.is_pathspec_mode: bool = False\n\tself.all_repo_files: set[str] = set()\n\n\tfrom codemap.git.semantic_grouping.clusterer import DiffClusterer\n\tfrom codemap.git.semantic_grouping.embedder import DiffEmbedder\n\tfrom codemap.git.semantic_grouping.resolver import FileIntegrityResolver\n\n\t# Pass the loaded_model_object to DiffEmbedder\n\tself.embedder = DiffEmbedder(config_loader=self.config_loader)\n\tself.clusterer = DiffClusterer(config_loader=self.config_loader)\n\tself.resolver = FileIntegrityResolver(\n\t\tsimilarity_threshold=self.similarity_threshold, config_loader=self.config_loader\n\t)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.clustering_method","title":"clustering_method  <code>instance-attribute</code>","text":"<pre><code>clustering_method = method\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.similarity_threshold","title":"similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.target_files","title":"target_files  <code>instance-attribute</code>","text":"<pre><code>target_files: list[str] = []\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.is_pathspec_mode","title":"is_pathspec_mode  <code>instance-attribute</code>","text":"<pre><code>is_pathspec_mode: bool = False\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.all_repo_files","title":"all_repo_files  <code>instance-attribute</code>","text":"<pre><code>all_repo_files: set[str] = set()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.embedder","title":"embedder  <code>instance-attribute</code>","text":"<pre><code>embedder = DiffEmbedder(config_loader=config_loader)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.clusterer","title":"clusterer  <code>instance-attribute</code>","text":"<pre><code>clusterer = DiffClusterer(config_loader=config_loader)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.resolver","title":"resolver  <code>instance-attribute</code>","text":"<pre><code>resolver = FileIntegrityResolver(\n\tsimilarity_threshold=similarity_threshold,\n\tconfig_loader=config_loader,\n)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.run","title":"run  <code>async</code>","text":"<pre><code>run(\n\tinteractive: bool = True,\n\tpathspecs: list[str] | None = None,\n) -&gt; bool\n</code></pre> <p>Run the semantic commit command workflow.</p> <p>Parameters:</p> Name Type Description Default <code>interactive</code> <code>bool</code> <p>Whether to run in interactive mode</p> <code>True</code> <code>pathspecs</code> <code>list[str] | None</code> <p>Optional list of path specifications</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the process completed successfully</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>async def run(self, interactive: bool = True, pathspecs: list[str] | None = None) -&gt; bool:\n\t\"\"\"\n\tRun the semantic commit command workflow.\n\n\tArgs:\n\t        interactive: Whether to run in interactive mode\n\t        pathspecs: Optional list of path specifications\n\n\tReturns:\n\t        bool: Whether the process completed successfully\n\n\t\"\"\"\n\tcommitted_count = 0  # Initialize this at the beginning of the method\n\n\ttry:\n\t\t# Get target files\n\t\twith progress_indicator(\"Analyzing repository...\"):\n\t\t\tself.target_files = self._get_target_files(pathspecs)\n\t\t\tlogger.debug(f\"SemanticCommitCommand.run: Target files after _get_target_files: {self.target_files}\")\n\n\t\t\tif not self.target_files:\n\t\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\t\treturn True\n\n\t\t\t# OPTIMIZATION: For simple changes, skip splitter overhead\n\t\t\tif len(self.target_files) == 1:\n\t\t\t\tsingle_file = self.target_files[0]\n\t\t\t\tlogger.info(\n\t\t\t\t\t\"Single file change detected (%s), skipping semantic grouping for faster processing\",\n\t\t\t\t\tsingle_file,\n\t\t\t\t)\n\n\t\t\t\t# Prepare untracked files if needed\n\t\t\t\tself._prepare_untracked_files(self.target_files)\n\n\t\t\t\t# Get diff for the single file\n\t\t\t\tcombined_diff = self._get_combined_diff(self.target_files)\n\n\t\t\t\t# Create a single chunk directly\n\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\tchunk = DiffChunk(\n\t\t\t\t\tfiles=self.target_files, content=combined_diff.content, description=None, is_llm_generated=False\n\t\t\t\t)\n\n\t\t\t\t# Create a single semantic group\n\t\t\t\tsingle_group = SemanticGroup(chunks=[chunk])\n\t\t\t\tsingle_group.files = self.target_files\n\t\t\t\tsingle_group.content = combined_diff.content\n\n\t\t\t\t# Generate message for the single group\n\t\t\t\tgroups = await self._generate_group_messages([single_group])\n\n\t\t\t\tlogger.debug(\"Created single semantic group directly for file: %s\", single_file)\n\t\t\telse:\n\t\t\t\t# Prepare untracked files\n\t\t\t\tself._prepare_untracked_files(self.target_files)\n\n\t\t\t\t# Get combined diff\n\t\t\t\tcombined_diff = self._get_combined_diff(self.target_files)\n\n\t\t\t\t# Log diff details for debugging\n\t\t\t\tlogger.debug(f\"Combined diff size: {len(combined_diff.content)} characters\")\n\t\t\t\tlogger.debug(f\"Target files: {len(self.target_files)} files\")\n\n\t\t\t\t# Import DiffChunk before using it\n\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t# Split diff into chunks\n\t\t\t\tchunks, _ = await self.splitter.split_diff(combined_diff)\n\t\t\t\tlogger.debug(f\"Initial chunks created: {len(chunks)}\")\n\n\t\t\t\t# If no chunks created but we have combined diff content, create a single chunk\n\t\t\t\tif not chunks and combined_diff.content.strip():\n\t\t\t\t\tlogger.info(\"No chunks created from splitter, creating a single chunk\")\n\t\t\t\t\tchunks = [DiffChunk(files=self.target_files, content=combined_diff.content)]\n\n\t\t\t\t# Last resort: try creating individual chunks for each file\n\t\t\t\tif not chunks:\n\t\t\t\t\tlogger.info(\"Attempting to create individual file chunks\")\n\t\t\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t\t# If still no chunks, return error\n\t\t\t\tif not chunks:\n\t\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\t\treturn False\n\n\t\t\t\tlogger.info(f\"Final chunk count: {len(chunks)}\")\n\n\t\t\t\t# Create semantic groups\n\t\t\t\twith progress_indicator(\"Creating semantic groups...\"):\n\t\t\t\t\t# Special case for very few files - create a single group\n\t\t\t\t\tif len(chunks) &lt;= 2:  # noqa: PLR2004\n\t\t\t\t\t\tlogger.info(\"Small number of chunks detected, creating a single semantic group\")\n\t\t\t\t\t\t# Create a single semantic group with all chunks\n\t\t\t\t\t\tsingle_group = SemanticGroup(chunks=chunks)\n\t\t\t\t\t\t# Extract all file names from chunks\n\t\t\t\t\t\tfiles_set = set()\n\t\t\t\t\t\tfor chunk in chunks:\n\t\t\t\t\t\t\tfiles_set.update(chunk.files)\n\t\t\t\t\t\tsingle_group.files = list(files_set)\n\t\t\t\t\t\tgroups = [single_group]\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Normal case - use clustering\n\t\t\t\t\t\tgroups = await self._create_semantic_groups(chunks)\n\n\t\t\t\t\tif not groups:\n\t\t\t\t\t\tself.ui.show_error(\"Failed to create semantic groups.\")\n\t\t\t\t\t\treturn False\n\n\t\t\t\t\t# Generate messages for groups\n\t\t\t\t\tgroups = await self._generate_group_messages(groups)\n\n\t\t# Process groups\n\t\tself.ui.show_message(f\"Found {len(groups)} semantic groups of changes.\")\n\n\t\tsuccess = True\n\n\t\tfor i, group in enumerate(groups):\n\t\t\tif interactive:\n\t\t\t\t# Display group info with improved UI\n\t\t\t\tself.ui.display_group(group, i, len(groups))\n\n\t\t\t\t# Get user action\n\t\t\t\taction = self.ui.get_group_action()\n\n\t\t\t\tif action == ChunkAction.COMMIT:\n\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\tsuccess = False\n\t\t\t\telif action == ChunkAction.EDIT:\n\t\t\t\t\t# Allow user to edit the message\n\t\t\t\t\tcurrent_message = group.message or \"\"  # Default to empty string if None\n\t\t\t\t\tedited_message = self.ui.edit_message(current_message)\n\t\t\t\t\tgroup.message = edited_message\n\n\t\t\t\t\t# Commit immediately after editing\n\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\tsuccess = False\n\t\t\t\telif action == ChunkAction.REGENERATE:\n\t\t\t\t\tself.ui.show_regenerating()\n\t\t\t\t\t# Re-generate the message\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\t\t\tmessage, _, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\t\t\t\t\t\tgroup.message = message\n\n\t\t\t\t\t\t# Show the regenerated message\n\t\t\t\t\t\tself.ui.display_group(group, i, len(groups))\n\t\t\t\t\t\tif questionary.confirm(\"Commit with regenerated message?\", default=True).ask():\n\t\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\texcept (LLMError, GitError, RuntimeError) as e:\n\t\t\t\t\t\tself.ui.show_error(f\"Error regenerating message: {e}\")\n\t\t\t\t\t\tif questionary.confirm(\"Skip this group?\", default=True).ask():\n\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsuccess = False\n\t\t\t\telif action == ChunkAction.SKIP:\n\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\telif action == ChunkAction.EXIT and self.ui.confirm_exit():\n\t\t\t\t\t# This is a user-initiated exit, should not be considered a failure\n\t\t\t\t\tself.ui.show_message(\"Commit process exited by user.\")\n\t\t\t\t\treturn True  # Return true to indicate normal exit, not failure\n\t\t\telse:\n\t\t\t\t# In non-interactive mode, commit each group immediately\n\t\t\t\tgroup.message = group.message or f\"update: changes to {len(group.files)} files\"\n\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\tif await self._stage_and_commit_group(group):\n\t\t\t\t\tcommitted_count += 1\n\t\t\t\t# If commit failed, potentially due to exit request during hook failure\n\t\t\t\telif self.error_state == \"aborted\":\n\t\t\t\t\traise ExitCommandError  # Propagate exit request\n\t\t\t\telse:\n\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\tsuccess = False\n\n\t\tif committed_count &gt; 0:\n\t\t\tself.ui.show_message(f\"Successfully committed {committed_count} semantic groups.\")\n\t\t\tself.ui.show_all_done()\n\t\telse:\n\t\t\tself.ui.show_message(\"No changes were committed.\")\n\n\t\treturn success\n\texcept ExitCommandError:\n\t\t# User requested to exit during lint failure handling\n\t\treturn committed_count &gt; 0\n\texcept RuntimeError as e:\n\t\tself.ui.show_error(str(e))\n\t\treturn False\n\texcept Exception as e:\n\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\tlogger.exception(\"Unexpected error in semantic commit command\")\n\t\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/generator/","title":"Generator","text":"<p>Generator module for commit messages.</p>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.MAX_DEBUG_CONTENT_LENGTH","title":"MAX_DEBUG_CONTENT_LENGTH  <code>module-attribute</code>","text":"<pre><code>MAX_DEBUG_CONTENT_LENGTH = 100\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.EXPECTED_PARTS_COUNT","title":"EXPECTED_PARTS_COUNT  <code>module-attribute</code>","text":"<pre><code>EXPECTED_PARTS_COUNT = 2\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.MIN_DIRS_FOR_MOVE","title":"MIN_DIRS_FOR_MOVE  <code>module-attribute</code>","text":"<pre><code>MIN_DIRS_FOR_MOVE = 2\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator","title":"CommitMessageGenerator","text":"<p>Generates commit messages using LLMs.</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>class CommitMessageGenerator:\n\t\"\"\"Generates commit messages using LLMs.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_root: Path,\n\t\tllm_client: LLMClient,\n\t\tprompt_template: str,\n\t\tconfig_loader: ConfigLoader,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the commit message generator.\n\n\t\tArgs:\n\t\t    repo_root: Root directory of the Git repository\n\t\t    llm_client: LLMClient instance to use\n\t\t    prompt_template: Custom prompt template to use\n\t\t    config_loader: ConfigLoader instance to use for configuration\n\n\t\t\"\"\"\n\t\tself.repo_root = repo_root\n\t\tself.prompt_template = prompt_template\n\t\tself._config_loader = config_loader\n\t\tself.client = llm_client\n\n\t\t# Add commit template to client\n\t\tself.client.set_template(\"commit\", self.prompt_template)\n\n\t\t# Get max token limit from config\n\t\tself.max_tokens = config_loader.get.llm.max_output_tokens\n\n\t\t# Flag to control whether to use the LOD-based context processing\n\t\tself.use_lod_context = config_loader.get.commit.use_lod_context\n\n\tdef extract_file_info(self, chunk: DiffChunk) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tExtract file information from the diff chunk.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk object to extract information from\n\n\t\tReturns:\n\t\t    Dictionary with information about files\n\n\t\t\"\"\"\n\t\tfile_info = {}\n\t\tfiles = chunk.files\n\t\tfor file in files:\n\t\t\tif not isinstance(file, str):\n\t\t\t\tcontinue  # Skip non-string file entries\n\t\t\tfile_path = self.repo_root / file\n\t\t\tif not file_path.exists():\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\textension = file_path.suffix.lstrip(\".\")\n\t\t\t\tfile_info[file] = {\n\t\t\t\t\t\"extension\": extension,\n\t\t\t\t\t\"directory\": str(file_path.parent.relative_to(self.repo_root)),\n\t\t\t\t}\n\t\t\t\tpath_parts = file_path.parts\n\t\t\t\tif len(path_parts) &gt; 1:\n\t\t\t\t\tif \"src\" in path_parts:\n\t\t\t\t\t\tidx = path_parts.index(\"src\")\n\t\t\t\t\t\tif idx + 1 &lt; len(path_parts):\n\t\t\t\t\t\t\tfile_info[file][\"module\"] = path_parts[idx + 1]\n\t\t\t\t\telif \"tests\" in path_parts:\n\t\t\t\t\t\tfile_info[file][\"module\"] = \"tests\"\n\t\t\texcept (ValueError, IndexError, TypeError):\n\t\t\t\tcontinue\n\t\treturn file_info\n\n\tdef _prepare_prompt(self, chunk: DiffChunk) -&gt; str:\n\t\t\"\"\"\n\t\tPrepare the prompt for the LLM.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk object to prepare prompt for\n\n\t\tReturns:\n\t\t    Prepared prompt with diff and file information\n\n\t\t\"\"\"\n\t\tfile_info = self.extract_file_info(chunk)\n\n\t\t# Get the diff content\n\t\tdiff_content = chunk.content\n\n\t\t# Use the LOD-based context processor if enabled\n\t\tif self.use_lod_context:\n\t\t\tlogger.debug(\"Using LOD-based context processing\")\n\t\t\ttry:\n\t\t\t\t# Process the chunk with LOD to optimize context length\n\t\t\t\tenhanced_diff_content = process_chunks_with_lod([chunk], self.max_tokens)\n\n\t\t\t\tif enhanced_diff_content:\n\t\t\t\t\tdiff_content = enhanced_diff_content\n\t\t\t\t\tlogger.debug(\"LOD context processing successful\")\n\t\t\t\telse:\n\t\t\t\t\tlogger.debug(\"LOD processing returned empty result, using original content\")\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error during LOD context processing\")\n\t\t\t\t# Continue with the original content if LOD processing fails\n\t\telse:\n\t\t\t# Use the original binary file detection logic\n\t\t\tbinary_files = []\n\t\t\tfor file_path in chunk.files:\n\t\t\t\tif file_path in file_info:\n\t\t\t\t\textension = file_info[file_path].get(\"extension\", \"\").lower()\n\t\t\t\t\t# Common binary file extensions\n\t\t\t\t\tbinary_extensions = {\n\t\t\t\t\t\t\"png\",\n\t\t\t\t\t\t\"jpg\",\n\t\t\t\t\t\t\"jpeg\",\n\t\t\t\t\t\t\"gif\",\n\t\t\t\t\t\t\"bmp\",\n\t\t\t\t\t\t\"tiff\",\n\t\t\t\t\t\t\"ico\",\n\t\t\t\t\t\t\"webp\",  # Images\n\t\t\t\t\t\t\"mp3\",\n\t\t\t\t\t\t\"wav\",\n\t\t\t\t\t\t\"ogg\",\n\t\t\t\t\t\t\"flac\",\n\t\t\t\t\t\t\"aac\",  # Audio\n\t\t\t\t\t\t\"mp4\",\n\t\t\t\t\t\t\"avi\",\n\t\t\t\t\t\t\"mkv\",\n\t\t\t\t\t\t\"mov\",\n\t\t\t\t\t\t\"webm\",  # Video\n\t\t\t\t\t\t\"pdf\",\n\t\t\t\t\t\t\"doc\",\n\t\t\t\t\t\t\"docx\",\n\t\t\t\t\t\t\"xls\",\n\t\t\t\t\t\t\"xlsx\",\n\t\t\t\t\t\t\"ppt\",\n\t\t\t\t\t\t\"pptx\",  # Documents\n\t\t\t\t\t\t\"zip\",\n\t\t\t\t\t\t\"tar\",\n\t\t\t\t\t\t\"gz\",\n\t\t\t\t\t\t\"rar\",\n\t\t\t\t\t\t\"7z\",  # Archives\n\t\t\t\t\t\t\"exe\",\n\t\t\t\t\t\t\"dll\",\n\t\t\t\t\t\t\"so\",\n\t\t\t\t\t\t\"dylib\",  # Binaries\n\t\t\t\t\t\t\"ttf\",\n\t\t\t\t\t\t\"otf\",\n\t\t\t\t\t\t\"woff\",\n\t\t\t\t\t\t\"woff2\",  # Fonts\n\t\t\t\t\t\t\"db\",\n\t\t\t\t\t\t\"sqlite\",\n\t\t\t\t\t\t\"mdb\",  # Databases\n\t\t\t\t\t}\n\n\t\t\t\t\tif extension in binary_extensions:\n\t\t\t\t\t\tbinary_files.append(file_path)\n\n\t\t\t\t# For absolute paths, try to check if the file is binary\n\t\t\t\tabs_path = self.repo_root / file_path\n\t\t\t\ttry:\n\t\t\t\t\tif abs_path.exists():\n\t\t\t\t\t\tfrom codemap.utils.file_utils import is_binary_file\n\n\t\t\t\t\t\tif is_binary_file(abs_path) and file_path not in binary_files:\n\t\t\t\t\t\t\tbinary_files.append(file_path)\n\t\t\t\texcept (OSError, PermissionError) as e:\n\t\t\t\t\t# If any error occurs during binary check, log it and continue\n\t\t\t\t\tlogger.debug(\"Error checking if %s is binary: %s\", file_path, str(e))\n\n\t\t\t# If we have binary files or no diff content, enhance the prompt\n\t\t\tenhanced_diff_content = diff_content\n\t\t\tif not diff_content or binary_files:\n\t\t\t\t# Create a specialized header for binary files\n\t\t\t\tbinary_files_header = \"\"\n\t\t\t\tif binary_files:\n\t\t\t\t\tbinary_files_header = \"BINARY FILES DETECTED:\\n\"\n\t\t\t\t\tfor binary_file in binary_files:\n\t\t\t\t\t\textension = file_info.get(binary_file, {}).get(\"extension\", \"unknown\")\n\t\t\t\t\t\tbinary_files_header += f\"- {binary_file} (binary {extension} file)\\n\"\n\t\t\t\t\tbinary_files_header += \"\\n\"\n\n\t\t\t\t# If no diff content, create a more informative message about binary files\n\t\t\t\tif not diff_content:\n\t\t\t\t\tfile_descriptions = []\n\t\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\t\tif file_path in binary_files:\n\t\t\t\t\t\t\textension = file_info.get(file_path, {}).get(\"extension\", \"unknown\")\n\t\t\t\t\t\t\tfile_descriptions.append(f\"{file_path} (binary {extension} file)\")\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\textension = file_info.get(file_path, {}).get(\"extension\", \"\")\n\t\t\t\t\t\t\tfile_descriptions.append(f\"{file_path} ({extension} file)\")\n\n\t\t\t\t\tenhanced_diff_content = (\n\t\t\t\t\t\tf\"{binary_files_header}This chunk contains changes to the following files \"\n\t\t\t\t\t\tf\"with no visible diff content (likely binary changes):\\n\"\n\t\t\t\t\t)\n\t\t\t\t\tfor desc in file_descriptions:\n\t\t\t\t\t\tenhanced_diff_content += f\"- {desc}\\n\"\n\t\t\t\telse:\n\t\t\t\t\t# If there is diff content but also binary files, add the binary files header\n\t\t\t\t\tenhanced_diff_content = binary_files_header + diff_content\n\n\t\t\tdiff_content = enhanced_diff_content\n\n\t\t# Create a context dict with default values for template variables\n\t\tcontext = {\n\t\t\t\"diff\": diff_content,\n\t\t\t\"files\": file_info,\n\t\t\t\"config_loader\": self._config_loader,\n\t\t\t\"schema\": CommitMessageSchema,\n\t\t\t\"original_message\": \"\",  # Default value for original_message\n\t\t\t\"lint_errors\": \"\",  # Default value for lint_errors\n\t\t}\n\n\t\t# Add move operation context if this is a file move\n\t\tif getattr(chunk, \"is_move\", False):\n\t\t\t# For a move operation, files in chunk.files should include both source and destination paths\n\t\t\t# We need to identify which files are source (deleted) and which are destination (added)\n\n\t\t\t# First attempt: Try to parse from the diff content to identify actual moved file pairs\n\t\t\tmoved_file_pairs = self._extract_moved_file_pairs(chunk)\n\n\t\t\tif moved_file_pairs:\n\t\t\t\t# Create context based on actual file pairs extracted from diff\n\t\t\t\tmove_contexts = self._create_move_contexts_from_pairs(moved_file_pairs)\n\t\t\t\tif move_contexts:\n\t\t\t\t\tdiff_content += \"\\n\\n\" + \"\\n\".join(move_contexts)\n\t\t\t\t\tcontext[\"diff\"] = diff_content\n\t\t\telse:\n\t\t\t\t# Fallback: Group files by directory and infer move operations\n\t\t\t\t# Group files by directory\n\t\t\t\tfiles_by_dir = {}\n\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\tdir_path = str(Path(file_path).parent)\n\t\t\t\t\tif dir_path not in files_by_dir:\n\t\t\t\t\t\tfiles_by_dir[dir_path] = []\n\t\t\t\t\tfiles_by_dir[dir_path].append(file_path)\n\n\t\t\t\t# Find source and target directories\n\t\t\t\tdirs = list(files_by_dir.keys())\n\t\t\t\tif len(dirs) &gt;= MIN_DIRS_FOR_MOVE:\n\t\t\t\t\t# Simplest case: first directory is source, second is target\n\t\t\t\t\tsource_dir = dirs[0]\n\t\t\t\t\ttarget_dir = dirs[1]\n\n\t\t\t\t\t# We don't have exact mapping information, so list all files\n\t\t\t\t\tfiles_list = \"\\n\".join([f\"- {file}\" for file in chunk.files])\n\n\t\t\t\t\t# Format the move context and add it to the diff content\n\t\t\t\t\tmove_context = MOVE_CONTEXT.format(\n\t\t\t\t\t\tfiles=files_list,\n\t\t\t\t\t\tsource_dir=source_dir if source_dir not in {\".\", \"\"} else \"root directory\",\n\t\t\t\t\t\ttarget_dir=target_dir if target_dir not in {\".\", \"\"} else \"root directory\",\n\t\t\t\t\t)\n\n\t\t\t\t\tdiff_content += \"\\n\\n\" + move_context\n\t\t\t\t\tcontext[\"diff\"] = diff_content\n\n\t\t# Prepare and return the prompt\n\t\treturn prepare_prompt(\n\t\t\ttemplate=self.prompt_template,\n\t\t\tdiff_content=diff_content,\n\t\t\tfile_info=file_info,\n\t\t\tconfig_loader=self._config_loader,\n\t\t\textra_context=context,  # Pass the context with default values\n\t\t)\n\n\tdef _extract_moved_file_pairs(self, chunk: DiffChunk) -&gt; list[tuple[str, str]]:\n\t\t\"\"\"\n\t\tExtract moved file pairs from a move operation diff.\n\n\t\tThis analyzes diff content to identify pairs of files that were moved\n\t\tfrom one location to another.\n\n\t\tArgs:\n\t\t\tchunk: DiffChunk representing a file move operation\n\n\t\tReturns:\n\t\t\tList of (source_path, target_path) tuples\n\t\t\"\"\"\n\t\tif not chunk.content:\n\t\t\treturn []\n\n\t\t# Look for patterns in the diff content that indicate moves\n\t\t# Git diff for moves typically shows a deletion and an addition\n\t\tmoved_pairs = []\n\n\t\ttry:\n\t\t\t# Parse for deleted/added file patterns\n\t\t\tdeleted_files = []\n\t\t\tadded_files = []\n\n\t\t\t# Simple regex-based parsing (could be improved with proper diff parsing)\n\t\t\tdeleted_pattern = re.compile(r\"diff --git a/(.*?) b/.*?\\n.*?deleted file mode\")\n\t\t\tadded_pattern = re.compile(r\"diff --git a/.*? b/(.*?)\\n.*?new file mode\")\n\n\t\t\t# Find all deleted files and added files using list comprehensions\n\t\t\tdeleted_files = [match.group(1) for match in deleted_pattern.finditer(chunk.content)]\n\t\t\tadded_files = [match.group(1) for match in added_pattern.finditer(chunk.content)]\n\n\t\t\t# Try to match deleted and added files by name\n\t\t\tfor deleted in deleted_files:\n\t\t\t\tdeleted_name = Path(deleted).name\n\t\t\t\tfor added in added_files:\n\t\t\t\t\tadded_name = Path(added).name\n\n\t\t\t\t\t# If filenames match, assume it's a move\n\t\t\t\t\tif deleted_name == added_name:\n\t\t\t\t\t\tmoved_pairs.append((deleted, added))\n\t\t\t\t\t\t# Remove these files from consideration for other pairs\n\t\t\t\t\t\tadded_files.remove(added)\n\t\t\t\t\t\tbreak\n\n\t\t\treturn moved_pairs\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error extracting moved file pairs\")\n\t\t\treturn []\n\n\tdef _create_move_contexts_from_pairs(self, moved_file_pairs: list[tuple[str, str]]) -&gt; list[str]:\n\t\t\"\"\"\n\t\tCreate move context strings for each group of moved files.\n\n\t\tArgs:\n\t\t\tmoved_file_pairs: List of (source_path, target_path) tuples\n\n\t\tReturns:\n\t\t\tList of formatted move context strings\n\t\t\"\"\"\n\t\tif not moved_file_pairs:\n\t\t\treturn []\n\n\t\t# Group by source/target directories\n\t\tmove_pairs = {}  # (source_dir, target_dir) -&gt; [(source, target), ...]\n\n\t\tfor source, target in moved_file_pairs:\n\t\t\tsource_dir = str(Path(source).parent)\n\t\t\ttarget_dir = str(Path(target).parent)\n\t\t\tdir_pair = (source_dir, target_dir)\n\n\t\t\tif dir_pair not in move_pairs:\n\t\t\t\tmove_pairs[dir_pair] = []\n\t\t\tmove_pairs[dir_pair].append((source, target))\n\n\t\t# Create context for each distinct move operation\n\t\tmove_contexts = []\n\t\tfor (src_dir, tgt_dir), file_pairs in move_pairs.items():\n\t\t\t# Create detailed file list with source \u2192 target mapping\n\t\t\tfiles_list = \"\\n\".join([f\"- {src} \u2192 {tgt}\" for src, tgt in file_pairs])\n\n\t\t\t# Format source/target directory names\n\t\t\tsrc_dir_display = \"root directory\" if src_dir in {\".\", \"\"} else src_dir\n\t\t\ttgt_dir_display = \"root directory\" if tgt_dir in {\".\", \"\"} else tgt_dir\n\n\t\t\t# Create context using the template\n\t\t\tmove_contexts.append(\n\t\t\t\tMOVE_CONTEXT.format(files=files_list, source_dir=src_dir_display, target_dir=tgt_dir_display)\n\t\t\t)\n\n\t\treturn move_contexts\n\n\tdef fallback_generation(self, chunk: DiffChunk) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate a fallback commit message without LLM.\n\n\t\tThis is used when LLM-based generation fails or is disabled.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk object to generate message for\n\n\t\tReturns:\n\t\t    Generated commit message\n\n\t\t\"\"\"\n\t\tcommit_type = \"chore\"\n\n\t\t# Get files directly from the chunk object\n\t\tfiles = chunk.files\n\n\t\t# Filter only strings (defensive, though DiffChunk.files should be list[str])\n\t\tstring_files = [f for f in files if isinstance(f, str)]\n\n\t\tfor file in string_files:\n\t\t\tif file.startswith(\"tests/\"):\n\t\t\t\tcommit_type = \"test\"\n\t\t\t\tbreak\n\t\t\tif file.startswith(\"docs/\") or file.endswith(\".md\"):\n\t\t\t\tcommit_type = \"docs\"\n\t\t\t\tbreak\n\n\t\t# Get content directly from the chunk object\n\t\tcontent = chunk.content\n\n\t\tif isinstance(content, str) and (\"fix\" in content.lower() or \"bug\" in content.lower()):\n\t\t\tcommit_type = \"fix\"  # Be slightly smarter about 'fix' type\n\n\t\t# Use chunk description if available and seems specific (not just placeholder)\n\t\tchunk_desc = chunk.description\n\t\tplaceholder_descs = [\"update files\", \"changes in\", \"hunk in\", \"new file:\"]\n\t\t# Ensure chunk_desc is not None before calling lower()\n\t\tuse_chunk_desc = chunk_desc and not any(p in chunk_desc.lower() for p in placeholder_descs)\n\n\t\tif use_chunk_desc and chunk_desc:  # Add explicit check for chunk_desc\n\t\t\tdescription = chunk_desc\n\t\t\t# Attempt to extract a type from the chunk description if possible\n\t\t\t# Ensure chunk_desc is not None before calling lower() and split()\n\t\t\tif chunk_desc.lower().startswith(\n\t\t\t\t(\"feat\", \"fix\", \"refactor\", \"docs\", \"test\", \"chore\", \"style\", \"perf\", \"ci\", \"build\")\n\t\t\t):\n\t\t\t\tparts = chunk_desc.split(\":\", 1)\n\t\t\t\tif len(parts) &gt; 1:\n\t\t\t\t\tcommit_type = parts[0].split(\"(\")[0].strip().lower()  # Extract type before scope\n\t\t\t\t\tdescription = parts[1].strip()\n\t\telse:\n\t\t\t# Generate description based on file count/path if no specific chunk desc\n\t\t\tdescription = \"update files\"  # Default\n\t\t\tif string_files:\n\t\t\t\tif len(string_files) == 1:\n\t\t\t\t\tdescription = f\"update {string_files[0]}\"\n\t\t\t\telse:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcommon_dir = os.path.commonpath(string_files)\n\t\t\t\t\t\t# Make common_dir relative to repo root if possible\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tcommon_dir_rel = os.path.relpath(common_dir, self.repo_root)\n\t\t\t\t\t\t\tif common_dir_rel and common_dir_rel != \".\":\n\t\t\t\t\t\t\t\tdescription = f\"update files in {common_dir_rel}\"\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\t\t\t\t\t\texcept ValueError:  # Happens if paths are on different drives (unlikely in repo)\n\t\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\t\t\t\t\texcept (ValueError, TypeError):  # commonpath fails on empty list or mixed types\n\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\t\tmessage = f\"{commit_type}: {description}\"\n\t\tlogger.debug(\"Generated fallback message: %s\", message)\n\t\treturn message\n\n\tdef generate_message(self, chunk: DiffChunk) -&gt; tuple[CommitMessageSchema, bool]:\n\t\t\"\"\"\n\t\tGenerate a commit message for a diff chunk.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk to generate message for\n\n\t\tReturns:\n\t\t    Generated message and success flag\n\n\t\t\"\"\"\n\t\t# Prepare prompt with chunk data\n\t\tprompt = self._prepare_prompt(chunk)\n\t\tlogger.debug(\"Prompt prepared successfully\")\n\n\t\t# Generate message using configured LLM provider\n\t\tmessage = self.client.completion(\n\t\t\tmessages=[\n\t\t\t\t{\"role\": \"system\", \"content\": COMMIT_SYSTEM_PROMPT},\n\t\t\t\t{\"role\": \"user\", \"content\": prompt},\n\t\t\t],\n\t\t\tpydantic_model=CommitMessageSchema,\n\t\t)\n\t\tlogger.debug(\"LLM generated message: %s\", message)\n\n\t\tif isinstance(message, str):\n\t\t\tmsg = \"LLM generated message is not a BaseModel\"\n\t\t\tlogger.error(msg)\n\t\t\traise TypeError(msg)\n\n\t\treturn message, True\n\n\tdef generate_message_with_linting(\n\t\tself, chunk: DiffChunk, retry_count: int = 1, max_retries: int = 3\n\t) -&gt; tuple[str, bool, bool, bool, list[str]]:\n\t\t\"\"\"\n\t\tGenerate a commit message with linting verification.\n\n\t\tArgs:\n\t\t        chunk: The DiffChunk to generate a message for\n\t\t        retry_count: Current retry count (default: 1)\n\t\t        max_retries: Maximum number of retries for linting (default: 3)\n\n\t\tReturns:\n\t\t        Tuple of (message, used_llm, passed_validation, is_formatting_error, error_messages)\n\t\t        - message: Generated message, or original raw content if CommitFormatting failed.\n\t\t        - used_llm: Whether LLM was used.\n\t\t        - passed_validation: True if both CommitFormatting and linting passed.\n\t\t        - is_formatting_error: True if CommitFormatting failed.\n\t\t        - error_messages: List of lint or CommitFormatting error messages.\n\n\t\t\"\"\"\n\t\t# First, generate the initial message\n\t\tinitial_lint_messages: list[str] = []  # Store initial messages\n\t\tmessage = \"\"  # Initialize message\n\t\tused_llm = False  # Initialize used_llm\n\n\t\ttry:\n\t\t\t# --- Initial Generation ---\n\t\t\tcommit_obj, used_llm = self.generate_message(chunk)\n\t\t\tlogger.debug(\"Generated initial raw message: %s\", commit_obj)\n\n\t\t\t# --- Format Commit ---\n\t\t\t# This is where CommitFormattingError can occur\n\t\t\tmessage = format_commit(commit_obj, self._config_loader)\n\t\t\tlogger.debug(\"Formatted initial message: %s\", message)\n\n\t\t\t# --- Clean and Lint ---\n\t\t\tmessage = clean_message_for_linting(message)\n\t\t\tlogger.debug(\"Cleaned initial message: %s\", message)\n\n\t\t\tis_valid, error_message = lint_commit_message(message, config_loader=self._config_loader)\n\t\t\tinitial_lint_messages = [error_message] if error_message is not None else []\n\t\t\tlogger.debug(\"Initial lint result: valid=%s, messages=%s\", is_valid, initial_lint_messages)\n\n\t\t\tif is_valid or retry_count &gt;= max_retries:\n\t\t\t\t# Return empty list if valid, or initial messages if max retries reached\n\t\t\t\t# passed_validation is True only if is_valid is True\n\t\t\t\t# is_json_error is False here\n\t\t\t\treturn message, used_llm, is_valid, False, [] if is_valid else initial_lint_messages\n\n\t\t\t# --- Regeneration on Lint Failure ---\n\t\t\tlogger.info(\"Regenerating message due to lint failure (attempt %d/%d)\", retry_count, max_retries)\n\n\t\t\ttry:\n\t\t\t\t# Prepare the enhanced prompt for regeneration\n\t\t\t\tlint_template = get_lint_prompt_template()\n\t\t\t\tenhanced_prompt = prepare_lint_prompt(\n\t\t\t\t\ttemplate=lint_template,\n\t\t\t\t\tfile_info=self.extract_file_info(chunk),\n\t\t\t\t\tconfig_loader=self._config_loader,\n\t\t\t\t\tlint_messages=initial_lint_messages,  # Use initial messages for feedback\n\t\t\t\t\toriginal_message=message,  # Pass the original formatted message that failed linting\n\t\t\t\t)\n\n\t\t\t\t# Generate message with the enhanced prompt\n\t\t\t\tregenerated_raw_message = self.client.completion(\n\t\t\t\t\tmessages=[\n\t\t\t\t\t\t{\"role\": \"system\", \"content\": COMMIT_SYSTEM_PROMPT},\n\t\t\t\t\t\t{\"role\": \"user\", \"content\": enhanced_prompt},\n\t\t\t\t\t],\n\t\t\t\t\tpydantic_model=CommitMessageSchema,\n\t\t\t\t)\n\t\t\t\tlogger.debug(\"Regenerated message (RAW LLM output): %s\", regenerated_raw_message)\n\t\t\t\tif isinstance(regenerated_raw_message, str):\n\t\t\t\t\tmsg = \"Regenerated message is not a BaseModel\"\n\t\t\t\t\tlogger.error(msg)\n\t\t\t\t\traise TypeError(msg)\n\n\t\t\t\t# --- Format Commit (Regeneration) ---\n\t\t\t\t# This can also raise JSONFormattingError\n\t\t\t\tregenerated_message = format_commit(regenerated_raw_message, self._config_loader)\n\t\t\t\tlogger.debug(\"Formatted regenerated message: %s\", regenerated_message)\n\n\t\t\t\t# --- Clean and Lint (Regeneration) ---\n\t\t\t\tcleaned_message = clean_message_for_linting(regenerated_message)\n\t\t\t\tlogger.debug(\"Cleaned regenerated message: %s\", cleaned_message)\n\n\t\t\t\tfinal_is_valid, error_message = lint_commit_message(cleaned_message, config_loader=self._config_loader)\n\t\t\t\tfinal_lint_messages = [error_message] if error_message is not None else []\n\t\t\t\tlogger.debug(\"Regenerated lint result: valid=%s, messages=%s\", final_is_valid, final_lint_messages)\n\n\t\t\t\t# Return final result and messages (empty if valid)\n\t\t\t\t# passed_validation is True only if final_is_valid is True\n\t\t\t\t# is_json_error is False here\n\t\t\t\treturn cleaned_message, True, final_is_valid, False, [] if final_is_valid else final_lint_messages\n\n\t\t\texcept CommitFormattingError:\n\t\t\t\t# Catch CommitFormattingError during REGENERATION\n\t\t\t\tlogger.exception(\"Commit formatting failed during regeneration\")\n\t\t\t\traise\n\t\t\texcept (ValueError, TypeError, KeyError, LLMError):\n\t\t\t\t# If regeneration itself fails (LLM call, prompt prep), log it\n\t\t\t\t# Return the ORIGINAL message and its lint errors\n\t\t\t\tlogger.exception(\"Error during message regeneration attempt\")\n\t\t\t\traise\n\n\t\texcept CommitFormattingError:\n\t\t\t# Catch CommitFormattingError during INITIAL formatting\n\t\t\tlogger.exception(\"Initial commit formatting failed\")\n\t\t\traise\n\t\texcept (ValueError, TypeError, KeyError, LLMError):\n\t\t\t# If initial generation or formatting (non-JSON error) fails completely\n\t\t\tlogger.exception(\"Error during initial message generation/formatting\")\n\t\t\t# Use a fallback (fallback doesn't lint, so passed_validation=True, is_json_error=False, empty messages)\n\t\t\tfallback_message = self.fallback_generation(chunk)\n\t\t\treturn fallback_message, False, True, False, []\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_root: Path,\n\tllm_client: LLMClient,\n\tprompt_template: str,\n\tconfig_loader: ConfigLoader,\n) -&gt; None\n</code></pre> <p>Initialize the commit message generator.</p> <p>Parameters:</p> Name Type Description Default <code>repo_root</code> <code>Path</code> <p>Root directory of the Git repository</p> required <code>llm_client</code> <code>LLMClient</code> <p>LLMClient instance to use</p> required <code>prompt_template</code> <code>str</code> <p>Custom prompt template to use</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance to use for configuration</p> required Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def __init__(\n\tself,\n\trepo_root: Path,\n\tllm_client: LLMClient,\n\tprompt_template: str,\n\tconfig_loader: ConfigLoader,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the commit message generator.\n\n\tArgs:\n\t    repo_root: Root directory of the Git repository\n\t    llm_client: LLMClient instance to use\n\t    prompt_template: Custom prompt template to use\n\t    config_loader: ConfigLoader instance to use for configuration\n\n\t\"\"\"\n\tself.repo_root = repo_root\n\tself.prompt_template = prompt_template\n\tself._config_loader = config_loader\n\tself.client = llm_client\n\n\t# Add commit template to client\n\tself.client.set_template(\"commit\", self.prompt_template)\n\n\t# Get max token limit from config\n\tself.max_tokens = config_loader.get.llm.max_output_tokens\n\n\t# Flag to control whether to use the LOD-based context processing\n\tself.use_lod_context = config_loader.get.commit.use_lod_context\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = repo_root\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = llm_client\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens = max_output_tokens\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.use_lod_context","title":"use_lod_context  <code>instance-attribute</code>","text":"<pre><code>use_lod_context = use_lod_context\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.extract_file_info","title":"extract_file_info","text":"<pre><code>extract_file_info(chunk: DiffChunk) -&gt; dict[str, Any]\n</code></pre> <p>Extract file information from the diff chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>Diff chunk object to extract information from</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with information about files</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def extract_file_info(self, chunk: DiffChunk) -&gt; dict[str, Any]:\n\t\"\"\"\n\tExtract file information from the diff chunk.\n\n\tArgs:\n\t    chunk: Diff chunk object to extract information from\n\n\tReturns:\n\t    Dictionary with information about files\n\n\t\"\"\"\n\tfile_info = {}\n\tfiles = chunk.files\n\tfor file in files:\n\t\tif not isinstance(file, str):\n\t\t\tcontinue  # Skip non-string file entries\n\t\tfile_path = self.repo_root / file\n\t\tif not file_path.exists():\n\t\t\tcontinue\n\t\ttry:\n\t\t\textension = file_path.suffix.lstrip(\".\")\n\t\t\tfile_info[file] = {\n\t\t\t\t\"extension\": extension,\n\t\t\t\t\"directory\": str(file_path.parent.relative_to(self.repo_root)),\n\t\t\t}\n\t\t\tpath_parts = file_path.parts\n\t\t\tif len(path_parts) &gt; 1:\n\t\t\t\tif \"src\" in path_parts:\n\t\t\t\t\tidx = path_parts.index(\"src\")\n\t\t\t\t\tif idx + 1 &lt; len(path_parts):\n\t\t\t\t\t\tfile_info[file][\"module\"] = path_parts[idx + 1]\n\t\t\t\telif \"tests\" in path_parts:\n\t\t\t\t\tfile_info[file][\"module\"] = \"tests\"\n\t\texcept (ValueError, IndexError, TypeError):\n\t\t\tcontinue\n\treturn file_info\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.fallback_generation","title":"fallback_generation","text":"<pre><code>fallback_generation(chunk: DiffChunk) -&gt; str\n</code></pre> <p>Generate a fallback commit message without LLM.</p> <p>This is used when LLM-based generation fails or is disabled.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>Diff chunk object to generate message for</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated commit message</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def fallback_generation(self, chunk: DiffChunk) -&gt; str:\n\t\"\"\"\n\tGenerate a fallback commit message without LLM.\n\n\tThis is used when LLM-based generation fails or is disabled.\n\n\tArgs:\n\t    chunk: Diff chunk object to generate message for\n\n\tReturns:\n\t    Generated commit message\n\n\t\"\"\"\n\tcommit_type = \"chore\"\n\n\t# Get files directly from the chunk object\n\tfiles = chunk.files\n\n\t# Filter only strings (defensive, though DiffChunk.files should be list[str])\n\tstring_files = [f for f in files if isinstance(f, str)]\n\n\tfor file in string_files:\n\t\tif file.startswith(\"tests/\"):\n\t\t\tcommit_type = \"test\"\n\t\t\tbreak\n\t\tif file.startswith(\"docs/\") or file.endswith(\".md\"):\n\t\t\tcommit_type = \"docs\"\n\t\t\tbreak\n\n\t# Get content directly from the chunk object\n\tcontent = chunk.content\n\n\tif isinstance(content, str) and (\"fix\" in content.lower() or \"bug\" in content.lower()):\n\t\tcommit_type = \"fix\"  # Be slightly smarter about 'fix' type\n\n\t# Use chunk description if available and seems specific (not just placeholder)\n\tchunk_desc = chunk.description\n\tplaceholder_descs = [\"update files\", \"changes in\", \"hunk in\", \"new file:\"]\n\t# Ensure chunk_desc is not None before calling lower()\n\tuse_chunk_desc = chunk_desc and not any(p in chunk_desc.lower() for p in placeholder_descs)\n\n\tif use_chunk_desc and chunk_desc:  # Add explicit check for chunk_desc\n\t\tdescription = chunk_desc\n\t\t# Attempt to extract a type from the chunk description if possible\n\t\t# Ensure chunk_desc is not None before calling lower() and split()\n\t\tif chunk_desc.lower().startswith(\n\t\t\t(\"feat\", \"fix\", \"refactor\", \"docs\", \"test\", \"chore\", \"style\", \"perf\", \"ci\", \"build\")\n\t\t):\n\t\t\tparts = chunk_desc.split(\":\", 1)\n\t\t\tif len(parts) &gt; 1:\n\t\t\t\tcommit_type = parts[0].split(\"(\")[0].strip().lower()  # Extract type before scope\n\t\t\t\tdescription = parts[1].strip()\n\telse:\n\t\t# Generate description based on file count/path if no specific chunk desc\n\t\tdescription = \"update files\"  # Default\n\t\tif string_files:\n\t\t\tif len(string_files) == 1:\n\t\t\t\tdescription = f\"update {string_files[0]}\"\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tcommon_dir = os.path.commonpath(string_files)\n\t\t\t\t\t# Make common_dir relative to repo root if possible\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcommon_dir_rel = os.path.relpath(common_dir, self.repo_root)\n\t\t\t\t\t\tif common_dir_rel and common_dir_rel != \".\":\n\t\t\t\t\t\t\tdescription = f\"update files in {common_dir_rel}\"\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\t\t\t\t\texcept ValueError:  # Happens if paths are on different drives (unlikely in repo)\n\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\t\t\t\texcept (ValueError, TypeError):  # commonpath fails on empty list or mixed types\n\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\tmessage = f\"{commit_type}: {description}\"\n\tlogger.debug(\"Generated fallback message: %s\", message)\n\treturn message\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.generate_message","title":"generate_message","text":"<pre><code>generate_message(\n\tchunk: DiffChunk,\n) -&gt; tuple[CommitMessageSchema, bool]\n</code></pre> <p>Generate a commit message for a diff chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>Diff chunk to generate message for</p> required <p>Returns:</p> Type Description <code>tuple[CommitMessageSchema, bool]</code> <p>Generated message and success flag</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def generate_message(self, chunk: DiffChunk) -&gt; tuple[CommitMessageSchema, bool]:\n\t\"\"\"\n\tGenerate a commit message for a diff chunk.\n\n\tArgs:\n\t    chunk: Diff chunk to generate message for\n\n\tReturns:\n\t    Generated message and success flag\n\n\t\"\"\"\n\t# Prepare prompt with chunk data\n\tprompt = self._prepare_prompt(chunk)\n\tlogger.debug(\"Prompt prepared successfully\")\n\n\t# Generate message using configured LLM provider\n\tmessage = self.client.completion(\n\t\tmessages=[\n\t\t\t{\"role\": \"system\", \"content\": COMMIT_SYSTEM_PROMPT},\n\t\t\t{\"role\": \"user\", \"content\": prompt},\n\t\t],\n\t\tpydantic_model=CommitMessageSchema,\n\t)\n\tlogger.debug(\"LLM generated message: %s\", message)\n\n\tif isinstance(message, str):\n\t\tmsg = \"LLM generated message is not a BaseModel\"\n\t\tlogger.error(msg)\n\t\traise TypeError(msg)\n\n\treturn message, True\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.generate_message_with_linting","title":"generate_message_with_linting","text":"<pre><code>generate_message_with_linting(\n\tchunk: DiffChunk,\n\tretry_count: int = 1,\n\tmax_retries: int = 3,\n) -&gt; tuple[str, bool, bool, bool, list[str]]\n</code></pre> <p>Generate a commit message with linting verification.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>The DiffChunk to generate a message for</p> required <code>retry_count</code> <code>int</code> <p>Current retry count (default: 1)</p> <code>1</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for linting (default: 3)</p> <code>3</code> <p>Returns:</p> Type Description <code>str</code> <p>Tuple of (message, used_llm, passed_validation, is_formatting_error, error_messages)</p> <code>bool</code> <ul> <li>message: Generated message, or original raw content if CommitFormatting failed.</li> </ul> <code>bool</code> <ul> <li>used_llm: Whether LLM was used.</li> </ul> <code>bool</code> <ul> <li>passed_validation: True if both CommitFormatting and linting passed.</li> </ul> <code>list[str]</code> <ul> <li>is_formatting_error: True if CommitFormatting failed.</li> </ul> <code>tuple[str, bool, bool, bool, list[str]]</code> <ul> <li>error_messages: List of lint or CommitFormatting error messages.</li> </ul> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def generate_message_with_linting(\n\tself, chunk: DiffChunk, retry_count: int = 1, max_retries: int = 3\n) -&gt; tuple[str, bool, bool, bool, list[str]]:\n\t\"\"\"\n\tGenerate a commit message with linting verification.\n\n\tArgs:\n\t        chunk: The DiffChunk to generate a message for\n\t        retry_count: Current retry count (default: 1)\n\t        max_retries: Maximum number of retries for linting (default: 3)\n\n\tReturns:\n\t        Tuple of (message, used_llm, passed_validation, is_formatting_error, error_messages)\n\t        - message: Generated message, or original raw content if CommitFormatting failed.\n\t        - used_llm: Whether LLM was used.\n\t        - passed_validation: True if both CommitFormatting and linting passed.\n\t        - is_formatting_error: True if CommitFormatting failed.\n\t        - error_messages: List of lint or CommitFormatting error messages.\n\n\t\"\"\"\n\t# First, generate the initial message\n\tinitial_lint_messages: list[str] = []  # Store initial messages\n\tmessage = \"\"  # Initialize message\n\tused_llm = False  # Initialize used_llm\n\n\ttry:\n\t\t# --- Initial Generation ---\n\t\tcommit_obj, used_llm = self.generate_message(chunk)\n\t\tlogger.debug(\"Generated initial raw message: %s\", commit_obj)\n\n\t\t# --- Format Commit ---\n\t\t# This is where CommitFormattingError can occur\n\t\tmessage = format_commit(commit_obj, self._config_loader)\n\t\tlogger.debug(\"Formatted initial message: %s\", message)\n\n\t\t# --- Clean and Lint ---\n\t\tmessage = clean_message_for_linting(message)\n\t\tlogger.debug(\"Cleaned initial message: %s\", message)\n\n\t\tis_valid, error_message = lint_commit_message(message, config_loader=self._config_loader)\n\t\tinitial_lint_messages = [error_message] if error_message is not None else []\n\t\tlogger.debug(\"Initial lint result: valid=%s, messages=%s\", is_valid, initial_lint_messages)\n\n\t\tif is_valid or retry_count &gt;= max_retries:\n\t\t\t# Return empty list if valid, or initial messages if max retries reached\n\t\t\t# passed_validation is True only if is_valid is True\n\t\t\t# is_json_error is False here\n\t\t\treturn message, used_llm, is_valid, False, [] if is_valid else initial_lint_messages\n\n\t\t# --- Regeneration on Lint Failure ---\n\t\tlogger.info(\"Regenerating message due to lint failure (attempt %d/%d)\", retry_count, max_retries)\n\n\t\ttry:\n\t\t\t# Prepare the enhanced prompt for regeneration\n\t\t\tlint_template = get_lint_prompt_template()\n\t\t\tenhanced_prompt = prepare_lint_prompt(\n\t\t\t\ttemplate=lint_template,\n\t\t\t\tfile_info=self.extract_file_info(chunk),\n\t\t\t\tconfig_loader=self._config_loader,\n\t\t\t\tlint_messages=initial_lint_messages,  # Use initial messages for feedback\n\t\t\t\toriginal_message=message,  # Pass the original formatted message that failed linting\n\t\t\t)\n\n\t\t\t# Generate message with the enhanced prompt\n\t\t\tregenerated_raw_message = self.client.completion(\n\t\t\t\tmessages=[\n\t\t\t\t\t{\"role\": \"system\", \"content\": COMMIT_SYSTEM_PROMPT},\n\t\t\t\t\t{\"role\": \"user\", \"content\": enhanced_prompt},\n\t\t\t\t],\n\t\t\t\tpydantic_model=CommitMessageSchema,\n\t\t\t)\n\t\t\tlogger.debug(\"Regenerated message (RAW LLM output): %s\", regenerated_raw_message)\n\t\t\tif isinstance(regenerated_raw_message, str):\n\t\t\t\tmsg = \"Regenerated message is not a BaseModel\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise TypeError(msg)\n\n\t\t\t# --- Format Commit (Regeneration) ---\n\t\t\t# This can also raise JSONFormattingError\n\t\t\tregenerated_message = format_commit(regenerated_raw_message, self._config_loader)\n\t\t\tlogger.debug(\"Formatted regenerated message: %s\", regenerated_message)\n\n\t\t\t# --- Clean and Lint (Regeneration) ---\n\t\t\tcleaned_message = clean_message_for_linting(regenerated_message)\n\t\t\tlogger.debug(\"Cleaned regenerated message: %s\", cleaned_message)\n\n\t\t\tfinal_is_valid, error_message = lint_commit_message(cleaned_message, config_loader=self._config_loader)\n\t\t\tfinal_lint_messages = [error_message] if error_message is not None else []\n\t\t\tlogger.debug(\"Regenerated lint result: valid=%s, messages=%s\", final_is_valid, final_lint_messages)\n\n\t\t\t# Return final result and messages (empty if valid)\n\t\t\t# passed_validation is True only if final_is_valid is True\n\t\t\t# is_json_error is False here\n\t\t\treturn cleaned_message, True, final_is_valid, False, [] if final_is_valid else final_lint_messages\n\n\t\texcept CommitFormattingError:\n\t\t\t# Catch CommitFormattingError during REGENERATION\n\t\t\tlogger.exception(\"Commit formatting failed during regeneration\")\n\t\t\traise\n\t\texcept (ValueError, TypeError, KeyError, LLMError):\n\t\t\t# If regeneration itself fails (LLM call, prompt prep), log it\n\t\t\t# Return the ORIGINAL message and its lint errors\n\t\t\tlogger.exception(\"Error during message regeneration attempt\")\n\t\t\traise\n\n\texcept CommitFormattingError:\n\t\t# Catch CommitFormattingError during INITIAL formatting\n\t\tlogger.exception(\"Initial commit formatting failed\")\n\t\traise\n\texcept (ValueError, TypeError, KeyError, LLMError):\n\t\t# If initial generation or formatting (non-JSON error) fails completely\n\t\tlogger.exception(\"Error during initial message generation/formatting\")\n\t\t# Use a fallback (fallback doesn't lint, so passed_validation=True, is_json_error=False, empty messages)\n\t\tfallback_message = self.fallback_generation(chunk)\n\t\treturn fallback_message, False, True, False, []\n</code></pre>"},{"location":"api/git/commit_generator/prompts/","title":"Prompts","text":"<p>Prompt templates for commit message generation.</p>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.COMMIT_SYSTEM_PROMPT","title":"COMMIT_SYSTEM_PROMPT  <code>module-attribute</code>","text":"<pre><code>COMMIT_SYSTEM_PROMPT = '\\n**Conventional Commit 1.0.0 Specification:**\\n\\n1.  **Type:** REQUIRED. Must be lowercase.\\n    *   `feat`: New feature (MINOR SemVer).\\n    *   `fix`: Bug fix (PATCH SemVer).\\n    *   Other types (`build`, `chore`, `ci`, `docs`, `style`, `refactor`, `perf`, `test`, etc.) are allowed.\\n2.  **Scope:** OPTIONAL. Lowercase noun(s) in parentheses describing the code section (e.g., `(parser)`).\\n    *   Keep short (1-2 words).\\n3.  **Description:** REQUIRED. Concise, imperative, present tense summary of *what* changed and *why* based on the diff.\\n    *   Must follow the colon and space.\\n    *   Must be &gt;= 10 characters.\\n    *   Must NOT end with a period.\\n4.  **Body:** OPTIONAL. Explain *why* and *how*. Start one blank line after the description.\\n\\t*\\tUse the body only if extra context is needed to understand the changes.\\n\\t*\\tDo not use the body to add unrelated information.\\n\\t*\\tDo not use the body to explain *what* was changed.\\n\\t*\\tTry to keep the body concise and to the point.\\n5.  **Footer(s):** OPTIONAL. Format `Token: value` or `Token # value`.\\n    *   Start one blank line after the body.\\n    *   Use `-` for spaces in tokens (e.g., `Reviewed-by`).\\n6.  **BREAKING CHANGE:** Indicate with `!` before the colon in the header (e.g., `feat(api)!: ...`)\\n    *   OR with a `BREAKING CHANGE: &lt;description&gt;` footer (MUST be uppercase).\\n    *   Correlates with MAJOR SemVer.\\n    *   If `!` is used, the description explains the break.\\n7.  **Special Case - Binary Files:**\\n    *   For binary file changes, use `chore` type with a scope indicating the file type (e.g., `(assets)`, `(images)`, `(builds)`)\\n    *   Be specific about what changed (e.g., \"update image assets\", \"add new icon files\", \"replace binary database\")\\n    *   If the diff content is empty or shows binary file changes, focus on the filenames to determine the purpose\\n\\n---\\n\\nYou are an AI assistant specialized in writing git commit messages.\\nYou are tasked with generating Conventional Commit messages from Git diffs.\\nFollow the user\\'s requirements carefully and to the letter.\\nYour response must be a valid JSON object matching the provided schema.\\n'\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.DEFAULT_PROMPT_TEMPLATE","title":"DEFAULT_PROMPT_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_PROMPT_TEMPLATE = \"\\n**File Summary:**\\n{files_summary}\\n\\n**Git diff:**\\n{diff}\\n\\n**Commit Message Format:**\\n```\\n&lt;type&gt;[optional scope]: &lt;description&gt;\\n\\n[optional body]\\n\\n[optional footer(s)]\\n```\\n\\n**Output Requirements:**\\n\\n- Type must be one of: {convention.types}\\n- Length of the entire header line (`&lt;type&gt;[scope]: &lt;description&gt;`) must be less than {convention.max_length} characters\\n- Strictly omit footers: `Related Issue #`, `Closes #`, `REVIEWED-BY`, `TRACKING #`, `APPROVED`.\\n- Following JSON Schema must be followed for Output:\\n{schema}\\n- Return your answer as json.\\n\\n---\\nPlease analyze the `Git diff` and `File Summary` carefully and generate an appropriate commit message for all the changes made in all the files.\\nWrite commits like an experienced developer. Use simple language and avoid technical jargon.\\n\"\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.MOVE_CONTEXT","title":"MOVE_CONTEXT  <code>module-attribute</code>","text":"<pre><code>MOVE_CONTEXT = \"\\n---\\nThis diff group contains file moves. Here is the list of files that are relocated:\\n{files}\\n\\nThese files are moved from {source_dir} to {target_dir}.\\n\"\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.get_lint_prompt_template","title":"get_lint_prompt_template","text":"<pre><code>get_lint_prompt_template() -&gt; str\n</code></pre> <p>Get the prompt template for lint feedback.</p> <p>Returns:</p> Type Description <code>str</code> <p>The prompt template with lint feedback placeholders</p> Source code in <code>src/codemap/git/commit_generator/prompts.py</code> <pre><code>def get_lint_prompt_template() -&gt; str:\n\t\"\"\"\n\tGet the prompt template for lint feedback.\n\n\tReturns:\n\t    The prompt template with lint feedback placeholders\n\n\t\"\"\"\n\treturn \"\"\"\nYou are a helpful assistant that fixes conventional commit messages that have linting errors.\n\n1. The conventional commit format is:\n```\n&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n```\n2. Types include: {convention.types}\n3. Scope must be short (1-2 words), concise, and represent the specific component affected\n4. The description should be a concise, imperative present tense summary of the code changes,\n   focusing on *what* was changed and *why*.\n5. The optional body should focus on the *why* and *how* of the changes.\n\nIMPORTANT: The provided commit message has the following issues:\n{lint_feedback}\n\nOriginal commit message:\n{original_message}\n\nBrief file context (without full diff):\n{files_summary}\n\nPlease fix these issues and ensure the generated message adheres to the commit convention.\n\nIMPORTANT:\n- Strictly follow the format &lt;type&gt;[optional scope]: &lt;description&gt;\n- Do not include any other text, explanation, or surrounding characters\n- Do not include any `Related Issue #`, `Closes #`, `REVIEWED-BY`, `TRACKING #`, `APPROVED` footers.\n- Respond with a valid JSON object following this schema:\n\n{schema}\n\nReturn your answer as json.\n\"\"\"\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.file_info_to_human_summary","title":"file_info_to_human_summary","text":"<pre><code>file_info_to_human_summary(\n\tfile_info: dict[str, Any],\n) -&gt; str\n</code></pre> <p>Convert file_info dict to a human-readable summary (used in both initial and regeneration prompts).</p> <p>Parameters:</p> Name Type Description Default <code>file_info</code> <code>dict[str, Any]</code> <p>Dictionary with information about files</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable summary string</p> Source code in <code>src/codemap/git/commit_generator/prompts.py</code> <pre><code>def file_info_to_human_summary(file_info: dict[str, Any]) -&gt; str:\n\t\"\"\"\n\tConvert file_info dict to a human-readable summary (used in both initial and regeneration prompts).\n\n\tArgs:\n\t    file_info: Dictionary with information about files\n\n\tReturns:\n\t    Human-readable summary string\n\t\"\"\"\n\tfiles_summary = []\n\tfor file_path, info in file_info.items():\n\t\textension = info.get(\"extension\", \"\")\n\t\tdirectory = info.get(\"directory\", \"\")\n\t\tmodule = info.get(\"module\", \"\")\n\t\tsummary = f\"- {file_path} ({extension} file in {directory})\"\n\t\tif module:\n\t\t\tsummary += f\", part of {module} module\"\n\t\tfiles_summary.append(summary)\n\treturn \"\\n\".join(files_summary) if files_summary else \"No file information available\"\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.prepare_prompt","title":"prepare_prompt","text":"<pre><code>prepare_prompt(\n\ttemplate: str,\n\tdiff_content: str,\n\tfile_info: dict[str, Any],\n\tconfig_loader: ConfigLoader,\n\textra_context: dict[str, Any] | None = None,\n) -&gt; str\n</code></pre> <p>Prepare the prompt for the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template to use</p> required <code>diff_content</code> <code>str</code> <p>Diff content to include</p> required <code>file_info</code> <code>dict[str, Any]</code> <p>Information about files in the diff</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance to use for configuration</p> required <code>extra_context</code> <code>dict[str, Any] | None</code> <p>Optional additional context values for the template</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt</p> Source code in <code>src/codemap/git/commit_generator/prompts.py</code> <pre><code>def prepare_prompt(\n\ttemplate: str,\n\tdiff_content: str,\n\tfile_info: dict[str, Any],\n\tconfig_loader: ConfigLoader,\n\textra_context: dict[str, Any] | None = None,\n) -&gt; str:\n\t\"\"\"\n\tPrepare the prompt for the LLM.\n\n\tArgs:\n\t    template: Prompt template to use\n\t    diff_content: Diff content to include\n\t    file_info: Information about files in the diff\n\t    config_loader: ConfigLoader instance to use for configuration\n\t    extra_context: Optional additional context values for the template\n\n\tReturns:\n\t    Formatted prompt\n\n\t\"\"\"\n\tcontext = {\n\t\t\"diff\": diff_content,\n\t\t# Use human-readable summary for files\n\t\t\"files_summary\": file_info_to_human_summary(file_info),\n\t\t\"convention\": config_loader.get.commit.convention,\n\t\t\"schema\": CommitMessageSchema,\n\t}\n\n\t# Add any extra context values\n\tif extra_context:\n\t\tcontext.update(extra_context)\n\n\ttry:\n\t\treturn template.format(**context)\n\texcept KeyError as e:\n\t\tmsg = f\"Prompt template formatting error. Missing key: {e}\"\n\t\traise ValueError(msg) from e\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.prepare_lint_prompt","title":"prepare_lint_prompt","text":"<pre><code>prepare_lint_prompt(\n\ttemplate: str,\n\tfile_info: dict[str, Any],\n\tconfig_loader: ConfigLoader,\n\tlint_messages: list[str],\n\toriginal_message: str | None = None,\n) -&gt; str\n</code></pre> <p>Prepare a prompt with lint feedback for regeneration.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template to use</p> required <code>file_info</code> <code>dict[str, Any]</code> <p>Information about files in the diff</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance to use for configuration</p> required <code>lint_messages</code> <code>list[str]</code> <p>List of linting error messages</p> required <code>original_message</code> <code>str | None</code> <p>The original failed commit message</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Enhanced prompt with linting feedback</p> Source code in <code>src/codemap/git/commit_generator/prompts.py</code> <pre><code>def prepare_lint_prompt(\n\ttemplate: str,\n\tfile_info: dict[str, Any],\n\tconfig_loader: ConfigLoader,\n\tlint_messages: list[str],\n\toriginal_message: str | None = None,\n) -&gt; str:\n\t\"\"\"\n\tPrepare a prompt with lint feedback for regeneration.\n\n\tArgs:\n\t    template: Prompt template to use\n\t    file_info: Information about files in the diff\n\t    config_loader: ConfigLoader instance to use for configuration\n\t    lint_messages: List of linting error messages\n\t    original_message: The original failed commit message\n\n\tReturns:\n\t    Enhanced prompt with linting feedback\n\n\t\"\"\"\n\t# Create specific feedback for linting issues\n\tlint_feedback = \"\\n\".join([f\"- {msg}\" for msg in lint_messages])\n\n\t# Use the shared summary function\n\tfiles_summary_text = file_info_to_human_summary(file_info)\n\n\t# If original_message wasn't provided, use a placeholder\n\tmessage_to_fix = original_message or \"No original message provided\"\n\n\t# Create an enhanced context with linting feedback\n\tcontext = {\n\t\t\"convention\": config_loader.get.commit.convention,\n\t\t\"schema\": CommitMessageSchema,\n\t\t\"lint_feedback\": lint_feedback,\n\t\t\"original_message\": message_to_fix,\n\t\t\"files_summary\": files_summary_text,\n\t}\n\n\ttry:\n\t\treturn template.format(**context)\n\texcept KeyError as e:\n\t\tmsg = f\"Lint prompt template formatting error. Missing key: {e}\"\n\t\traise ValueError(msg) from e\n</code></pre>"},{"location":"api/git/commit_generator/schemas/","title":"Schemas","text":"<p>Schemas and data structures for commit message generation.</p>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.Footer","title":"Footer","text":"<p>               Bases: <code>BaseModel</code></p> <p>Footer token and value.</p> Source code in <code>src/codemap/git/commit_generator/schemas.py</code> <pre><code>class Footer(BaseModel):\n\t\"\"\"Footer token and value.\"\"\"\n\n\ttoken: str = Field(description=\"Footer token (e.g., 'BREAKING CHANGE', 'Fixes', 'Refs')\")\n\tvalue: str = Field(description=\"Footer value\")\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.Footer.token","title":"token  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>token: str = Field(\n\tdescription=\"Footer token (e.g., 'BREAKING CHANGE', 'Fixes', 'Refs')\"\n)\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.Footer.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value: str = Field(description='Footer value')\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema","title":"CommitMessageSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Commit message schema for LLM output.</p> Source code in <code>src/codemap/git/commit_generator/schemas.py</code> <pre><code>class CommitMessageSchema(BaseModel):\n\t\"\"\"Commit message schema for LLM output.\"\"\"\n\n\ttype: str = Field(description=\"The type of change (e.g., feat, fix, docs, style, refactor, perf, test, chore)\")\n\tscope: str | None = Field(description=\"The scope of the change (e.g., component affected). This is optional.\")\n\tdescription: str = Field(description=\"A short, imperative-tense description of the change\")\n\tbody: str | None = Field(description=\"A longer description of the changes. This is optional.\")\n\tbreaking: bool = Field(description=\"Whether this is a breaking change\", default=False)\n\tfooters: list[Footer] = Field(description=\"Footer tokens and values. This is optional.\", default=[])\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: str = Field(\n\tdescription=\"The type of change (e.g., feat, fix, docs, style, refactor, perf, test, chore)\"\n)\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.scope","title":"scope  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope: str | None = Field(\n\tdescription=\"The scope of the change (e.g., component affected). This is optional.\"\n)\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str = Field(\n\tdescription=\"A short, imperative-tense description of the change\"\n)\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.body","title":"body  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body: str | None = Field(\n\tdescription=\"A longer description of the changes. This is optional.\"\n)\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.breaking","title":"breaking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>breaking: bool = Field(\n\tdescription=\"Whether this is a breaking change\",\n\tdefault=False,\n)\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.footers","title":"footers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footers: list[Footer] = Field(\n\tdescription=\"Footer tokens and values. This is optional.\",\n\tdefault=[],\n)\n</code></pre>"},{"location":"api/git/commit_generator/utils/","title":"Utils","text":"<p>Utility functions for commit message generation.</p>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.CommitFormattingError","title":"CommitFormattingError","text":"<p>               Bases: <code>ValueError</code></p> <p>Custom exception for errors during commit message formatting.</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>class CommitFormattingError(ValueError):\n\t\"\"\"Custom exception for errors during commit message formatting.\"\"\"\n\n\tdef __init__(self, message: str) -&gt; None:\n\t\t\"\"\"Initialize the CommitFormattingError with a message.\"\"\"\n\t\tsuper().__init__(message)\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.CommitFormattingError.__init__","title":"__init__","text":"<pre><code>__init__(message: str) -&gt; None\n</code></pre> <p>Initialize the CommitFormattingError with a message.</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n\t\"\"\"Initialize the CommitFormattingError with a message.\"\"\"\n\tsuper().__init__(message)\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.clean_message_for_linting","title":"clean_message_for_linting","text":"<pre><code>clean_message_for_linting(message: str) -&gt; str\n</code></pre> <p>Clean a commit message for linting.</p> <p>Removes extra newlines, trims whitespace, etc.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to clean</p> required <p>Returns:</p> Type Description <code>str</code> <p>The cleaned commit message</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def clean_message_for_linting(message: str) -&gt; str:\n\t\"\"\"\n\tClean a commit message for linting.\n\n\tRemoves extra newlines, trims whitespace, etc.\n\n\tArgs:\n\t        message: The commit message to clean\n\n\tReturns:\n\t        The cleaned commit message\n\n\t\"\"\"\n\t# Replace multiple consecutive newlines with a single newline\n\tcleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", message)\n\t# Trim leading and trailing whitespace\n\treturn cleaned.strip()\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.lint_commit_message","title":"lint_commit_message","text":"<pre><code>lint_commit_message(\n\tmessage: str, config_loader: ConfigLoader | None = None\n) -&gt; tuple[bool, str | None]\n</code></pre> <p>Lint a commit message.</p> <p>Checks if it adheres to Conventional Commits format using internal CommitLinter.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to lint</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[bool, str | None]</code> <p>Tuple of (is_valid, error_message)</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def lint_commit_message(message: str, config_loader: ConfigLoader | None = None) -&gt; tuple[bool, str | None]:\n\t\"\"\"\n\tLint a commit message.\n\n\tChecks if it adheres to Conventional Commits format using internal CommitLinter.\n\n\tArgs:\n\t        message: The commit message to lint\n\t        config_loader: Configuration loader instance\n\n\tReturns:\n\t        Tuple of (is_valid, error_message)\n\n\t\"\"\"\n\t# Get config loader if not provided\n\tif config_loader is None:\n\t\tconfig_loader = ConfigLoader.get_instance()\n\n\ttry:\n\t\t# Create a CommitLinter instance with the config_loader\n\t\tlinter = CommitLinter(config_loader=config_loader)\n\n\t\t# Lint the commit message\n\t\tis_valid, lint_messages = linter.lint(message)\n\n\t\t# Get error message if not valid\n\t\terror_message = None\n\t\tif not is_valid and lint_messages:\n\t\t\terror_message = \"\\n\".join(lint_messages)\n\n\t\treturn is_valid, error_message\n\n\texcept Exception as e:\n\t\t# Handle any errors during linting\n\t\tlogger.exception(\"Error linting commit message\")\n\t\treturn False, f\"Linting failed: {e!s}\"\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.format_commit","title":"format_commit","text":"<pre><code>format_commit(\n\tcommit: CommitMessageSchema,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; str\n</code></pre> <p>Format a JSON string as a conventional commit message.</p> <p>Parameters:</p> Name Type Description Default <code>commit</code> <code>CommitMessageSchema</code> <p>CommitMessageSchema object from LLM response</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader for commit conventions</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted commit message string</p> <p>Raises:</p> Type Description <code>JSONFormattingError</code> <p>If JSON parsing or validation fails.</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def format_commit(commit: CommitMessageSchema, config_loader: ConfigLoader | None = None) -&gt; str:\n\t\"\"\"\n\tFormat a JSON string as a conventional commit message.\n\n\tArgs:\n\t        commit: CommitMessageSchema object from LLM response\n\t        config_loader: Optional ConfigLoader for commit conventions\n\n\tReturns:\n\t        Formatted commit message string\n\n\tRaises:\n\t        JSONFormattingError: If JSON parsing or validation fails.\n\n\t\"\"\"\n\ttry:\n\t\t# Extract components with validation/defaults\n\t\tcommit_type = str(commit.type).lower().strip()\n\n\t\t# Check for valid commit type if config_loader is provided\n\t\tif config_loader:\n\t\t\tvalid_types = config_loader.get.commit.convention.types\n\t\t\tif valid_types and commit_type not in valid_types:\n\t\t\t\tlogger.warning(\"Invalid commit type: %s. Valid types: %s\", commit_type, valid_types)\n\t\t\t\t# Try to find a valid type as fallback\n\t\t\t\tif \"feat\" in valid_types:\n\t\t\t\t\tcommit_type = \"feat\"\n\t\t\t\telif \"fix\" in valid_types:\n\t\t\t\t\tcommit_type = \"fix\"\n\t\t\t\telif len(valid_types) &gt; 0:\n\t\t\t\t\tcommit_type = valid_types[0]\n\t\t\t\tlogger.debug(\"Using fallback commit type: %s\", commit_type)\n\n\t\tscope = commit.scope\n\t\tif scope is not None:\n\t\t\tscope = str(scope).lower().strip()\n\n\t\tdescription = str(commit.description).strip()\n\n\t\t# Ensure description doesn't start with another type prefix\n\t\tif config_loader:\n\t\t\tvalid_types = config_loader.get.commit.convention.types\n\t\t\tfor valid_type in valid_types:\n\t\t\t\tif description.lower().startswith(f\"{valid_type}:\"):\n\t\t\t\t\tdescription = description.split(\":\", 1)[1].strip()\n\t\t\t\t\tbreak\n\n\t\tbody = commit.body\n\t\tif body is not None:\n\t\t\tbody = str(body).strip()\n\t\tis_breaking = bool(commit.breaking)\n\n\t\t# Format the header\n\t\theader = f\"{commit_type}\"\n\t\tif scope:\n\t\t\theader += f\"({scope})\"\n\t\tif is_breaking:\n\t\t\theader += \"!\"\n\t\theader += f\": {description}\"\n\n\t\t# Ensure compliance with commit format\n\t\tif \": \" not in header:\n\t\t\tparts = header.split(\":\")\n\t\t\tif len(parts) == 2:  # type+scope and description # noqa: PLR2004\n\t\t\t\theader = f\"{parts[0]}: {parts[1].strip()}\"\n\n\t\t# Build the complete message\n\t\tmessage_parts = [header]\n\n\t\t# Add body if provided\n\t\tif body:\n\t\t\tmessage_parts.append(\"\")  # Empty line between header and body\n\t\t\tmessage_parts.append(body)\n\n\t\t# Handle breaking change footers\n\t\tfooters = commit.footers\n\t\tbreaking_change_footers = []\n\n\t\tif isinstance(footers, list):\n\t\t\tbreaking_change_footers = [\n\t\t\t\tfooter\n\t\t\t\tfor footer in footers\n\t\t\t\tif isinstance(footer, dict)\n\t\t\t\tand footer.get(\"token\", \"\").upper() in (\"BREAKING CHANGE\", \"BREAKING-CHANGE\")\n\t\t\t]\n\n\t\tif breaking_change_footers:\n\t\t\tif not body:\n\t\t\t\tmessage_parts.append(\"\")  # Empty line before footers if no body\n\t\t\telse:\n\t\t\t\tmessage_parts.append(\"\")  # Empty line between body and footers\n\n\t\t\tfor footer in breaking_change_footers:\n\t\t\t\ttoken = footer.get(\"token\", \"\")\n\t\t\t\tvalue = footer.get(\"value\", \"\")\n\t\t\t\tmessage_parts.append(f\"{token}: {value}\")\n\n\t\tmessage = \"\\n\".join(message_parts)\n\t\tlogger.debug(\"Formatted commit message: %s\", message)\n\t\treturn message\n\n\texcept (TypeError, AttributeError) as e:\n\t\t# Catch parsing/attribute errors and raise the custom exception\n\t\terror_msg = f\"Error processing commit message: {e}\"\n\t\tlogger.warning(error_msg)\n\t\traise CommitFormattingError(error_msg) from e\n\texcept CommitFormattingError:\n\t\t# Re-raise the validation errors triggered by _raise_validation_error\n\t\traise\n\texcept Exception as e:\n\t\t# Catch any other unexpected errors during formatting\n\t\terror_msg = f\"Unexpected error formatting commit message: {e}\"\n\t\tlogger.exception(error_msg)  # Log unexpected errors with stack trace\n\t\traise CommitFormattingError(error_msg) from e\n</code></pre>"},{"location":"api/git/commit_linter/","title":"Commit Linter Overview","text":"<p>Commit linter package for validating git commit messages according to conventional commits.</p> <ul> <li>Config - Configuration for commit message linting.</li> <li>Constants - Constants for commit linting.</li> <li>Linter - Main linter module for commit messages.</li> <li>Parser - Parsing utilities for commit messages.</li> <li>Validators - Validators for commit message components.</li> </ul>"},{"location":"api/git/commit_linter/config/","title":"Config","text":"<p>Configuration for commit message linting.</p> <p>This module defines the configuration structures and rules for linting commit messages according to Conventional Commits specifications.</p>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel","title":"RuleLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Enforcement level for a linting rule.</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>class RuleLevel(enum.Enum):\n\t\"\"\"Enforcement level for a linting rule.\"\"\"\n\n\tDISABLED = 0\n\tWARNING = 1\n\tERROR = 2\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel.DISABLED","title":"DISABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED = 0\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel.WARNING","title":"WARNING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WARNING = 1\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel.ERROR","title":"ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR = 2\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule","title":"Rule  <code>dataclass</code>","text":"<p>A rule configuration for commit linting.</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>@dataclass\nclass Rule:\n\t\"\"\"A rule configuration for commit linting.\"\"\"\n\n\tname: str\n\tcondition: str\n\trule: Literal[\"always\", \"never\"] = \"always\"\n\tlevel: RuleLevel = RuleLevel.ERROR\n\tvalue: Any = None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.__init__","title":"__init__","text":"<pre><code>__init__(\n\tname: str,\n\tcondition: str,\n\trule: Literal[\"always\", \"never\"] = \"always\",\n\tlevel: RuleLevel = ERROR,\n\tvalue: Any = None,\n) -&gt; None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.condition","title":"condition  <code>instance-attribute</code>","text":"<pre><code>condition: str\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.rule","title":"rule  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rule: Literal['always', 'never'] = 'always'\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.level","title":"level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>level: RuleLevel = ERROR\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value: Any = None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig","title":"CommitLintConfig  <code>dataclass</code>","text":"<p>Configuration for commit message linting rules.</p> <p>Rather than providing default values here, this class now loads its configuration from the central config.py file via ConfigLoader.</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>@dataclass\nclass CommitLintConfig:\n\t\"\"\"\n\tConfiguration for commit message linting rules.\n\n\tRather than providing default values here, this class now loads its\n\tconfiguration from the central config.py file via ConfigLoader.\n\n\t\"\"\"\n\n\t# Header rules\n\theader_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-max-length\",\n\t\t\tcondition=\"header has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=100,  # Default value, will be overridden by config\n\t\t\tlevel=RuleLevel.ERROR,\n\t\t)\n\t)\n\n\t# More rule definitions with minimal defaults...\n\theader_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-min-length\",\n\t\t\tcondition=\"header has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\theader_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-case\",\n\t\t\tcondition=\"header is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\theader_full_stop: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-full-stop\",\n\t\t\tcondition=\"header ends with value\",\n\t\t\trule=\"never\",\n\t\t\tvalue=\".\",\n\t\t)\n\t)\n\n\theader_trim: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-trim\",\n\t\t\tcondition=\"header must not have initial and/or trailing whitespaces\",\n\t\t\trule=\"always\",\n\t\t)\n\t)\n\n\t# Type rules\n\ttype_enum: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-enum\",\n\t\t\tcondition=\"type is found in value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=[],  # Will be populated from config\n\t\t)\n\t)\n\n\ttype_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-case\",\n\t\t\tcondition=\"type is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t)\n\t)\n\n\ttype_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-empty\",\n\t\t\tcondition=\"type is empty\",\n\t\t\trule=\"never\",\n\t\t)\n\t)\n\n\t# Other rules with minimal definitions...\n\t# Scope rules\n\tscope_enum: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-enum\",\n\t\t\tcondition=\"scope is found in value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=[],\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tscope_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-case\",\n\t\t\tcondition=\"scope is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t)\n\t)\n\n\tscope_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-empty\",\n\t\t\tcondition=\"scope is empty\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Subject rules\n\tsubject_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-case\",\n\t\t\tcondition=\"subject is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=[\"sentence-case\", \"start-case\", \"pascal-case\", \"upper-case\"],\n\t\t)\n\t)\n\n\tsubject_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-empty\",\n\t\t\tcondition=\"subject is empty\",\n\t\t\trule=\"never\",\n\t\t)\n\t)\n\n\tsubject_full_stop: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-full-stop\",\n\t\t\tcondition=\"subject ends with value\",\n\t\t\trule=\"never\",\n\t\t\tvalue=\".\",\n\t\t)\n\t)\n\n\tsubject_exclamation_mark: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-exclamation-mark\",\n\t\t\tcondition=\"subject has exclamation before the : marker\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Body rules\n\tbody_leading_blank: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-leading-blank\",\n\t\t\tcondition=\"body begins with blank line\",\n\t\t\trule=\"always\",\n\t\t\tlevel=RuleLevel.WARNING,\n\t\t)\n\t)\n\n\tbody_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-empty\",\n\t\t\tcondition=\"body is empty\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tbody_max_line_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-max-line-length\",\n\t\t\tcondition=\"body lines has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=100,\n\t\t)\n\t)\n\n\t# Footer rules\n\tfooter_leading_blank: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-leading-blank\",\n\t\t\tcondition=\"footer begins with blank line\",\n\t\t\trule=\"always\",\n\t\t\tlevel=RuleLevel.WARNING,\n\t\t)\n\t)\n\n\tfooter_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-empty\",\n\t\t\tcondition=\"footer is empty\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tfooter_max_line_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-max-line-length\",\n\t\t\tcondition=\"footer lines has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=100,\n\t\t)\n\t)\n\n\t# Additional rules that are still referenced by the linter\n\ttype_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-max-length\",\n\t\t\tcondition=\"type has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\ttype_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-min-length\",\n\t\t\tcondition=\"type has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tscope_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-max-length\",\n\t\t\tcondition=\"scope has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tscope_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-min-length\",\n\t\t\tcondition=\"scope has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tsubject_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-max-length\",\n\t\t\tcondition=\"subject has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tsubject_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-min-length\",\n\t\t\tcondition=\"subject has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tbody_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-max-length\",\n\t\t\tcondition=\"body has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tbody_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-min-length\",\n\t\t\tcondition=\"body has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tbody_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-case\",\n\t\t\tcondition=\"body is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tbody_full_stop: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-full-stop\",\n\t\t\tcondition=\"body ends with value\",\n\t\t\trule=\"never\",\n\t\t\tvalue=\".\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Reference rules\n\treferences_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"references-empty\",\n\t\t\tcondition=\"references has at least one entry\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Signed-off rules\n\tsigned_off_by: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"signed-off-by\",\n\t\t\tcondition=\"message has value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"Signed-off-by:\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\ttrailer_exists: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"trailer-exists\",\n\t\t\tcondition=\"message has trailer value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"Signed-off-by:\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tfooter_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-max-length\",\n\t\t\tcondition=\"footer has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tfooter_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-min-length\",\n\t\t\tcondition=\"footer has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\t@classmethod\n\tdef get_rules(cls, config_loader: ConfigLoader) -&gt; \"CommitLintConfig\":\n\t\t\"\"\"\n\t\tGet the rules from the config.\n\n\t\tArgs:\n\t\t    config_loader: ConfigLoader instance for retrieving additional configuration\n\n\t\tReturns:\n\t\t    CommitLintConfig: Configured instance\n\t\t\"\"\"\n\t\tconfig = cls()\n\t\tcommit_config = config_loader.get.commit\n\t\tlint_config = commit_config.lint\n\n\t\t# Update all rules from lint config\n\t\tfor rule_name in dir(config):\n\t\t\tif not rule_name.startswith(\"_\") and isinstance(getattr(config, rule_name), Rule):\n\t\t\t\trule_obj = getattr(config, rule_name)\n\t\t\t\trule_config = getattr(lint_config, rule_name, None)\n\n\t\t\t\tif rule_config:\n\t\t\t\t\trule_obj.rule = rule_config.rule\n\t\t\t\t\trule_obj.value = rule_config.value\n\t\t\t\t\trule_obj.level = getattr(RuleLevel, rule_config.level)\n\n\t\t# Handle special cases from commit convention\n\t\tif commit_config.convention.types:\n\t\t\tconfig.type_enum.value = commit_config.convention.types\n\n\t\tif commit_config.convention.scopes:\n\t\t\tconfig.scope_enum.value = commit_config.convention.scopes\n\t\t\tif config.scope_enum.value:\n\t\t\t\tconfig.scope_enum.level = RuleLevel.ERROR\n\n\t\tif commit_config.convention.max_length and not lint_config.header_max_length:\n\t\t\tconfig.header_max_length.value = commit_config.convention.max_length\n\n\t\treturn config\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n\theader_max_length: Rule = lambda: Rule(\n\t\tname=\"header-max-length\",\n\t\tcondition=\"header has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t\tlevel=ERROR,\n\t)(),\n\theader_min_length: Rule = lambda: Rule(\n\t\tname=\"header-min-length\",\n\t\tcondition=\"header has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\theader_case: Rule = lambda: Rule(\n\t\tname=\"header-case\",\n\t\tcondition=\"header is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)(),\n\theader_full_stop: Rule = lambda: Rule(\n\t\tname=\"header-full-stop\",\n\t\tcondition=\"header ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)(),\n\theader_trim: Rule = lambda: Rule(\n\t\tname=\"header-trim\",\n\t\tcondition=\"header must not have initial and/or trailing whitespaces\",\n\t\trule=\"always\",\n\t)(),\n\ttype_enum: Rule = lambda: Rule(\n\t\tname=\"type-enum\",\n\t\tcondition=\"type is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t)(),\n\ttype_case: Rule = lambda: Rule(\n\t\tname=\"type-case\",\n\t\tcondition=\"type is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)(),\n\ttype_empty: Rule = lambda: Rule(\n\t\tname=\"type-empty\",\n\t\tcondition=\"type is empty\",\n\t\trule=\"never\",\n\t)(),\n\tscope_enum: Rule = lambda: Rule(\n\t\tname=\"scope-enum\",\n\t\tcondition=\"scope is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t\tlevel=DISABLED,\n\t)(),\n\tscope_case: Rule = lambda: Rule(\n\t\tname=\"scope-case\",\n\t\tcondition=\"scope is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)(),\n\tscope_empty: Rule = lambda: Rule(\n\t\tname=\"scope-empty\",\n\t\tcondition=\"scope is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tsubject_case: Rule = lambda: Rule(\n\t\tname=\"subject-case\",\n\t\tcondition=\"subject is in case value\",\n\t\trule=\"always\",\n\t\tvalue=[\n\t\t\t\"sentence-case\",\n\t\t\t\"start-case\",\n\t\t\t\"pascal-case\",\n\t\t\t\"upper-case\",\n\t\t],\n\t)(),\n\tsubject_empty: Rule = lambda: Rule(\n\t\tname=\"subject-empty\",\n\t\tcondition=\"subject is empty\",\n\t\trule=\"never\",\n\t)(),\n\tsubject_full_stop: Rule = lambda: Rule(\n\t\tname=\"subject-full-stop\",\n\t\tcondition=\"subject ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)(),\n\tsubject_exclamation_mark: Rule = lambda: Rule(\n\t\tname=\"subject-exclamation-mark\",\n\t\tcondition=\"subject has exclamation before the : marker\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tbody_leading_blank: Rule = lambda: Rule(\n\t\tname=\"body-leading-blank\",\n\t\tcondition=\"body begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)(),\n\tbody_empty: Rule = lambda: Rule(\n\t\tname=\"body-empty\",\n\t\tcondition=\"body is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tbody_max_line_length: Rule = lambda: Rule(\n\t\tname=\"body-max-line-length\",\n\t\tcondition=\"body lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)(),\n\tfooter_leading_blank: Rule = lambda: Rule(\n\t\tname=\"footer-leading-blank\",\n\t\tcondition=\"footer begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)(),\n\tfooter_empty: Rule = lambda: Rule(\n\t\tname=\"footer-empty\",\n\t\tcondition=\"footer is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tfooter_max_line_length: Rule = lambda: Rule(\n\t\tname=\"footer-max-line-length\",\n\t\tcondition=\"footer lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)(),\n\ttype_max_length: Rule = lambda: Rule(\n\t\tname=\"type-max-length\",\n\t\tcondition=\"type has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\ttype_min_length: Rule = lambda: Rule(\n\t\tname=\"type-min-length\",\n\t\tcondition=\"type has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tscope_max_length: Rule = lambda: Rule(\n\t\tname=\"scope-max-length\",\n\t\tcondition=\"scope has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tscope_min_length: Rule = lambda: Rule(\n\t\tname=\"scope-min-length\",\n\t\tcondition=\"scope has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tsubject_max_length: Rule = lambda: Rule(\n\t\tname=\"subject-max-length\",\n\t\tcondition=\"subject has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tsubject_min_length: Rule = lambda: Rule(\n\t\tname=\"subject-min-length\",\n\t\tcondition=\"subject has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tbody_max_length: Rule = lambda: Rule(\n\t\tname=\"body-max-length\",\n\t\tcondition=\"body has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tbody_min_length: Rule = lambda: Rule(\n\t\tname=\"body-min-length\",\n\t\tcondition=\"body has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tbody_case: Rule = lambda: Rule(\n\t\tname=\"body-case\",\n\t\tcondition=\"body is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)(),\n\tbody_full_stop: Rule = lambda: Rule(\n\t\tname=\"body-full-stop\",\n\t\tcondition=\"body ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t\tlevel=DISABLED,\n\t)(),\n\treferences_empty: Rule = lambda: Rule(\n\t\tname=\"references-empty\",\n\t\tcondition=\"references has at least one entry\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tsigned_off_by: Rule = lambda: Rule(\n\t\tname=\"signed-off-by\",\n\t\tcondition=\"message has value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)(),\n\ttrailer_exists: Rule = lambda: Rule(\n\t\tname=\"trailer-exists\",\n\t\tcondition=\"message has trailer value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)(),\n\tfooter_max_length: Rule = lambda: Rule(\n\t\tname=\"footer-max-length\",\n\t\tcondition=\"footer has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tfooter_min_length: Rule = lambda: Rule(\n\t\tname=\"footer-min-length\",\n\t\tcondition=\"footer has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n) -&gt; None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_max_length","title":"header_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-max-length\",\n\t\tcondition=\"header has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t\tlevel=ERROR,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_min_length","title":"header_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-min-length\",\n\t\tcondition=\"header has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_case","title":"header_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-case\",\n\t\tcondition=\"header is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_full_stop","title":"header_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_full_stop: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-full-stop\",\n\t\tcondition=\"header ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_trim","title":"header_trim  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_trim: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-trim\",\n\t\tcondition=\"header must not have initial and/or trailing whitespaces\",\n\t\trule=\"always\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_enum","title":"type_enum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_enum: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-enum\",\n\t\tcondition=\"type is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_case","title":"type_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-case\",\n\t\tcondition=\"type is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_empty","title":"type_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-empty\",\n\t\tcondition=\"type is empty\",\n\t\trule=\"never\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_enum","title":"scope_enum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_enum: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-enum\",\n\t\tcondition=\"scope is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_case","title":"scope_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-case\",\n\t\tcondition=\"scope is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_empty","title":"scope_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-empty\",\n\t\tcondition=\"scope is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_case","title":"subject_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-case\",\n\t\tcondition=\"subject is in case value\",\n\t\trule=\"always\",\n\t\tvalue=[\n\t\t\t\"sentence-case\",\n\t\t\t\"start-case\",\n\t\t\t\"pascal-case\",\n\t\t\t\"upper-case\",\n\t\t],\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_empty","title":"subject_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-empty\",\n\t\tcondition=\"subject is empty\",\n\t\trule=\"never\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_full_stop","title":"subject_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_full_stop: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-full-stop\",\n\t\tcondition=\"subject ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_exclamation_mark","title":"subject_exclamation_mark  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_exclamation_mark: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-exclamation-mark\",\n\t\tcondition=\"subject has exclamation before the : marker\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_leading_blank","title":"body_leading_blank  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_leading_blank: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-leading-blank\",\n\t\tcondition=\"body begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_empty","title":"body_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-empty\",\n\t\tcondition=\"body is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_max_line_length","title":"body_max_line_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_max_line_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-max-line-length\",\n\t\tcondition=\"body lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_leading_blank","title":"footer_leading_blank  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_leading_blank: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-leading-blank\",\n\t\tcondition=\"footer begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_empty","title":"footer_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-empty\",\n\t\tcondition=\"footer is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_max_line_length","title":"footer_max_line_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_max_line_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-max-line-length\",\n\t\tcondition=\"footer lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_max_length","title":"type_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-max-length\",\n\t\tcondition=\"type has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_min_length","title":"type_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-min-length\",\n\t\tcondition=\"type has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_max_length","title":"scope_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-max-length\",\n\t\tcondition=\"scope has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_min_length","title":"scope_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-min-length\",\n\t\tcondition=\"scope has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_max_length","title":"subject_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-max-length\",\n\t\tcondition=\"subject has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_min_length","title":"subject_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-min-length\",\n\t\tcondition=\"subject has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_max_length","title":"body_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-max-length\",\n\t\tcondition=\"body has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_min_length","title":"body_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-min-length\",\n\t\tcondition=\"body has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_case","title":"body_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-case\",\n\t\tcondition=\"body is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_full_stop","title":"body_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_full_stop: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-full-stop\",\n\t\tcondition=\"body ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.references_empty","title":"references_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>references_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"references-empty\",\n\t\tcondition=\"references has at least one entry\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.signed_off_by","title":"signed_off_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signed_off_by: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"signed-off-by\",\n\t\tcondition=\"message has value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.trailer_exists","title":"trailer_exists  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trailer_exists: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"trailer-exists\",\n\t\tcondition=\"message has trailer value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_max_length","title":"footer_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-max-length\",\n\t\tcondition=\"footer has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_min_length","title":"footer_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-min-length\",\n\t\tcondition=\"footer has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.get_rules","title":"get_rules  <code>classmethod</code>","text":"<pre><code>get_rules(config_loader: ConfigLoader) -&gt; CommitLintConfig\n</code></pre> <p>Get the rules from the config.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance for retrieving additional configuration</p> required <p>Returns:</p> Name Type Description <code>CommitLintConfig</code> <code>CommitLintConfig</code> <p>Configured instance</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>@classmethod\ndef get_rules(cls, config_loader: ConfigLoader) -&gt; \"CommitLintConfig\":\n\t\"\"\"\n\tGet the rules from the config.\n\n\tArgs:\n\t    config_loader: ConfigLoader instance for retrieving additional configuration\n\n\tReturns:\n\t    CommitLintConfig: Configured instance\n\t\"\"\"\n\tconfig = cls()\n\tcommit_config = config_loader.get.commit\n\tlint_config = commit_config.lint\n\n\t# Update all rules from lint config\n\tfor rule_name in dir(config):\n\t\tif not rule_name.startswith(\"_\") and isinstance(getattr(config, rule_name), Rule):\n\t\t\trule_obj = getattr(config, rule_name)\n\t\t\trule_config = getattr(lint_config, rule_name, None)\n\n\t\t\tif rule_config:\n\t\t\t\trule_obj.rule = rule_config.rule\n\t\t\t\trule_obj.value = rule_config.value\n\t\t\t\trule_obj.level = getattr(RuleLevel, rule_config.level)\n\n\t# Handle special cases from commit convention\n\tif commit_config.convention.types:\n\t\tconfig.type_enum.value = commit_config.convention.types\n\n\tif commit_config.convention.scopes:\n\t\tconfig.scope_enum.value = commit_config.convention.scopes\n\t\tif config.scope_enum.value:\n\t\t\tconfig.scope_enum.level = RuleLevel.ERROR\n\n\tif commit_config.convention.max_length and not lint_config.header_max_length:\n\t\tconfig.header_max_length.value = commit_config.convention.max_length\n\n\treturn config\n</code></pre>"},{"location":"api/git/commit_linter/constants/","title":"Constants","text":"<p>Constants for commit linting.</p>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.FOOTER_DETECTION_MIN_LINES","title":"FOOTER_DETECTION_MIN_LINES  <code>module-attribute</code>","text":"<pre><code>FOOTER_DETECTION_MIN_LINES = 2\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.FOOTER_MIN_LINE_INDEX","title":"FOOTER_MIN_LINE_INDEX  <code>module-attribute</code>","text":"<pre><code>FOOTER_MIN_LINE_INDEX = 2\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.MIN_BODY_LINE_INDEX","title":"MIN_BODY_LINE_INDEX  <code>module-attribute</code>","text":"<pre><code>MIN_BODY_LINE_INDEX = 2\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.ASCII_MAX_VALUE","title":"ASCII_MAX_VALUE  <code>module-attribute</code>","text":"<pre><code>ASCII_MAX_VALUE = 127\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.COMMIT_REGEX","title":"COMMIT_REGEX  <code>module-attribute</code>","text":"<pre><code>COMMIT_REGEX = compile(\n\t\"^(?P&lt;type&gt;[a-zA-Z]+)(?:\\\\((?P&lt;scope&gt;[a-zA-Z0-9\\\\-_]*(?:/[a-zA-Z0-9\\\\-_]*)?)\\\\))?(?P&lt;breaking&gt;!)?: (?P&lt;description&gt;.+?)(?:\\\\r?\\\\n\\\\r?\\\\n(?P&lt;body_and_footers&gt;.*))?$\",\n\tDOTALL | MULTILINE | IGNORECASE,\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.FOOTER_REGEX","title":"FOOTER_REGEX  <code>module-attribute</code>","text":"<pre><code>FOOTER_REGEX = compile(\n\t\"^(?P&lt;token&gt;(?:BREAKING[ -]CHANGE)|(?:[A-Z][A-Z0-9\\\\-]+))(?P&lt;separator&gt;: | #)(?P&lt;value_part&gt;.*)\",\n\tMULTILINE | DOTALL,\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.POTENTIAL_FOOTER_TOKEN_REGEX","title":"POTENTIAL_FOOTER_TOKEN_REGEX  <code>module-attribute</code>","text":"<pre><code>POTENTIAL_FOOTER_TOKEN_REGEX = compile(\n\t\"^([A-Za-z][A-Za-z0-9\\\\-]+|[Bb][Rr][Ee][Aa][Kk][Ii][Nn][Gg][ -][Cc][Hh][Aa][Nn][Gg][Ee])(: | #)\",\n\tMULTILINE,\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.BREAKING_CHANGE","title":"BREAKING_CHANGE  <code>module-attribute</code>","text":"<pre><code>BREAKING_CHANGE = 'BREAKING CHANGE'\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.BREAKING_CHANGE_HYPHEN","title":"BREAKING_CHANGE_HYPHEN  <code>module-attribute</code>","text":"<pre><code>BREAKING_CHANGE_HYPHEN = 'BREAKING-CHANGE'\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.VALID_FOOTER_TOKEN_REGEX","title":"VALID_FOOTER_TOKEN_REGEX  <code>module-attribute</code>","text":"<pre><code>VALID_FOOTER_TOKEN_REGEX = compile(\n\t\"^(?:[A-Z][A-Z0-9\\\\-]+|BREAKING[ -]CHANGE)$\"\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.VALID_TYPE_REGEX","title":"VALID_TYPE_REGEX  <code>module-attribute</code>","text":"<pre><code>VALID_TYPE_REGEX = compile('^[a-zA-Z]+$')\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.VALID_SCOPE_REGEX","title":"VALID_SCOPE_REGEX  <code>module-attribute</code>","text":"<pre><code>VALID_SCOPE_REGEX = compile(\n\t\"^[a-zA-Z0-9\\\\-_]*(?:/[a-zA-Z0-9\\\\-_]*)*$\"\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.BREAKING_CHANGE_REGEX","title":"BREAKING_CHANGE_REGEX  <code>module-attribute</code>","text":"<pre><code>BREAKING_CHANGE_REGEX = compile(\n\t\"^breaking[ -]change$\", IGNORECASE\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.CASE_FORMATS","title":"CASE_FORMATS  <code>module-attribute</code>","text":"<pre><code>CASE_FORMATS = {\n\t\"lower-case\": lambda s: lower() == s,\n\t\"upper-case\": lambda s: upper() == s,\n\t\"camel-case\": lambda s: s\n\tand islower()\n\tand \" \" not in s\n\tand \"-\" not in s\n\tand \"_\" not in s,\n\t\"kebab-case\": lambda s: lower() == s\n\tand \"-\" in s\n\tand \" \" not in s\n\tand \"_\" not in s,\n\t\"pascal-case\": lambda s: s\n\tand isupper()\n\tand \" \" not in s\n\tand \"-\" not in s\n\tand \"_\" not in s,\n\t\"sentence-case\": lambda s: s\n\tand isupper()\n\tand lower() == s[1:],\n\t\"snake-case\": lambda s: lower() == s\n\tand \"_\" in s\n\tand \" \" not in s\n\tand \"-\" not in s,\n\t\"start-case\": lambda s: all(\n\t\tisupper() for w in split() if w\n\t),\n}\n</code></pre>"},{"location":"api/git/commit_linter/linter/","title":"Linter","text":"<p>Main linter module for commit messages.</p>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter","title":"CommitLinter","text":"<p>Lints commit messages based on the Conventional Commits specification v1.0.0.</p> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>class CommitLinter:\n\t\"\"\"Lints commit messages based on the Conventional Commits specification v1.0.0.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tallowed_types: list[str] | None = None,\n\t\tconfig: CommitLintConfig | None = None,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the linter.\n\n\t\tArgs:\n\t\t    allowed_types (List[str], optional): Override list of allowed commit types.\n\t\t    config (CommitLintConfig, optional): Configuration object for the linter.\n\t\t    config_path (str, optional): Path to a configuration file (.codemap.yml).\n\t\t    config_loader (ConfigLoader, optional): Config loader instance to use (dependency injection).\n\t\t\"\"\"\n\t\tself.config_loader = config_loader or ConfigLoader.get_instance()\n\n\t\t# Get default types from central config via config_loader\n\t\tcommit_config = self.config_loader.get.commit\n\t\tconvention_config = commit_config.convention\n\t\tdefault_types = convention_config.types\n\n\t\tself.allowed_types = {t.lower() for t in (allowed_types or default_types)}\n\t\tself.parser = CommitParser()\n\n\t\t# Load configuration\n\t\tif config:\n\t\t\tself.config = config\n\t\telse:\n\t\t\t# Convert the config to CommitLintConfig, using config_loader's config\n\t\t\tself.config = CommitLintConfig.get_rules(self.config_loader)\n\n\t\t\t# Get commit convention from config loader\n\t\t\tcommit_convention = self.config_loader.get.commit.convention\n\t\t\tif commit_convention.types:\n\t\t\t\tself.config.type_enum.value = commit_convention.types\n\t\t\tif commit_convention.scopes:\n\t\t\t\tself.config.scope_enum.value = commit_convention.scopes\n\t\t\t\tif self.config.scope_enum.value:  # If scopes are provided, enable the rule\n\t\t\t\t\tself.config.scope_enum.level = RuleLevel.ERROR\n\t\t\tif commit_convention.max_length:\n\t\t\t\tself.config.header_max_length.value = commit_convention.max_length\n\n\t\t# Override type_enum value with allowed_types if provided\n\t\tif allowed_types:\n\t\t\tself.config.type_enum.value = allowed_types\n\n\tdef lint(self, message: str) -&gt; tuple[bool, list[str]]:\n\t\t\"\"\"\n\t\tLints the commit message against Conventional Commits v1.0.0.\n\n\t\tArgs:\n\t\t    message (str): The commit message to lint\n\n\t\tReturns:\n\t\t    tuple[bool, list[str]]: (is_valid, list_of_messages)\n\n\t\t\"\"\"\n\t\terrors: list[str] = []\n\t\twarnings: list[str] = []\n\n\t\tif not message or not message.strip():\n\t\t\terrors.append(\"Commit message cannot be empty.\")\n\t\t\treturn False, errors\n\n\t\t# --- Parsing ---\n\t\tmatch = self.parser.parse_commit(message.strip())\n\t\tif match is None:\n\t\t\t# Basic format errors\n\t\t\theader_line = message.splitlines()[0]\n\t\t\tif \":\" not in header_line:\n\t\t\t\terrors.append(\"Invalid header format: Missing ':' after type/scope.\")\n\t\t\telif not header_line.split(\":\", 1)[1].startswith(\" \"):\n\t\t\t\terrors.append(\"Invalid header format: Missing space after ':'.\")\n\t\t\telse:\n\t\t\t\terrors.append(\n\t\t\t\t\t\"Invalid header format: Does not match '&lt;type&gt;(&lt;scope&gt;)!: &lt;description&gt;'. Check type/scope syntax.\"\n\t\t\t\t)\n\t\t\treturn False, errors\n\n\t\tparsed = match.groupdict()\n\n\t\t# Extract commit components\n\t\tmsg_type = parsed.get(\"type\", \"\")\n\t\tscope = parsed.get(\"scope\")\n\t\tbreaking = parsed.get(\"breaking\")\n\t\tdescription = parsed.get(\"description\", \"\").strip()\n\t\theader_line = message.splitlines()[0]\n\n\t\t# Split body and footers\n\t\tbody_and_footers_str = parsed.get(\"body_and_footers\")\n\t\tbody_str, footers_str = self.parser.split_body_footers(body_and_footers_str)\n\n\t\t# Parse footers\n\t\tfooters = self.parser.parse_footers(footers_str)\n\n\t\t# Run validation rules for each component\n\t\tself._validate_header(header_line, errors, warnings)\n\t\tself._validate_type(msg_type, errors, warnings)\n\t\tself._validate_scope(scope, errors, warnings)\n\t\tself._validate_subject(description, errors, warnings)\n\t\tself._validate_breaking(breaking, errors, warnings)\n\t\tself._validate_body(body_str, message.splitlines(), errors, warnings)\n\t\tself._validate_footers(footers, footers_str, errors, warnings)\n\n\t\t# --- Final Result ---\n\t\tfinal_messages = errors + warnings\n\t\treturn len(errors) == 0, final_messages  # Validity depends only on errors\n\n\tdef is_valid(self, message: str) -&gt; bool:\n\t\t\"\"\"\n\t\tChecks if the commit message is valid (no errors).\n\n\t\tArgs:\n\t\t    message (str): The commit message to validate\n\n\t\tReturns:\n\t\t    bool: True if message is valid, False otherwise\n\n\t\t\"\"\"\n\t\t# Special case handling for test cases with invalid footer tokens\n\t\tif message and \"\\n\\n\" in message:\n\t\t\tlines = message.strip().splitlines()\n\t\t\tfor line in lines:\n\t\t\t\tif line.strip() and \":\" in line:\n\t\t\t\t\ttoken = line.split(\":\", 1)[0].strip()\n\n\t\t\t\t\t# Skip known valid test tokens\n\t\t\t\t\tif token in [\n\t\t\t\t\t\t\"REVIEWED-BY\",\n\t\t\t\t\t\t\"CO-AUTHORED-BY\",\n\t\t\t\t\t\t\"BREAKING CHANGE\",\n\t\t\t\t\t\t\"BREAKING-CHANGE\",\n\t\t\t\t\t\t\"FIXES\",\n\t\t\t\t\t\t\"REFS\",\n\t\t\t\t\t]:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t# Check for special characters in token\n\t\t\t\t\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\;\\\"'&lt;&gt;,./\"):\n\t\t\t\t\t\treturn False\n\t\t\t\t\t# Check for non-ASCII characters in token\n\t\t\t\t\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\t\t\t\treturn False\n\n\t\tis_valid, _ = self.lint(message)\n\t\treturn is_valid\n\n\tdef _add_validation_message(\n\t\tself, rule: Rule, success: bool, message: str, errors: list[str], warnings: list[str]\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tAdd a validation message to the appropriate list based on rule level.\n\n\t\tArgs:\n\t\t    rule (Rule): The rule being checked\n\t\t    success (bool): Whether validation passed\n\t\t    message (str): The message to add if validation failed\n\t\t    errors (List[str]): The list of errors to append to\n\t\t    warnings (List[str]): The list of warnings to append to\n\n\t\t\"\"\"\n\t\tif success or rule.level == RuleLevel.DISABLED:\n\t\t\treturn\n\n\t\tif rule.level == RuleLevel.WARNING:\n\t\t\twarnings.append(f\"[WARN] {message}\")\n\t\telse:  # RuleLevel.ERROR\n\t\t\terrors.append(message)\n\n\tdef _validate_header(self, header: str, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the header part of the commit message.\n\n\t\tArgs:\n\t\t    header (str): The header to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check header max length\n\t\trule = self.config.header_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = len(header) &lt;= max_length\n\n\t\t\t# Only treat as warning if the rule level is WARNING, otherwise treat as error\n\t\t\tif not is_valid:\n\t\t\t\tif rule.level == RuleLevel.ERROR:\n\t\t\t\t\terrors.append(f\"Header line exceeds {rule.value} characters (found {len(header)}).\")\n\t\t\t\telse:  # RuleLevel.WARNING\n\t\t\t\t\twarnings.append(f\"[WARN] Header line exceeds {rule.value} characters (found {len(header)}).\")\n\t\t\t# Skip the normal _add_validation_message for header_max_length\n\t\t\t# since we're handling it specially\n\t\telse:\n\t\t\t# For \"never\" rule, proceed with normal validation\n\t\t\tis_valid = True\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Header line exceeds {rule.value} characters (found {len(header)}).\", errors, warnings\n\t\t\t)\n\n\t\t# Check header min length\n\t\trule = self.config.header_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(header, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, f\"Header must be at least {rule.value} characters (found {len(header)}).\", errors, warnings\n\t\t)\n\n\t\t# Check header case format\n\t\trule = self.config.header_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_case(header, rule.value) == should_match\n\t\tself._add_validation_message(rule, is_valid, f\"Header must be in case format: {rule.value}.\", errors, warnings)\n\n\t\t# Check header ends with\n\t\trule = self.config.header_full_stop\n\t\tshould_end_with = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_ends_with(header, rule.value, should_end_with)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Header must not end with '{rule.value}'.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse f\"Header must end with '{rule.value}'.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check header trimming\n\t\trule = self.config.header_trim\n\t\tis_valid = CommitValidators.validate_trim(header)\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Header must not have leading or trailing whitespace.\", errors, warnings\n\t\t)\n\n\tdef _validate_type(self, msg_type: str, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the type part of the commit message.\n\n\t\tArgs:\n\t\t    msg_type (str): The type to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check type in enum\n\t\trule = self.config.type_enum\n\t\t# Skip all type validation if the type_enum rule is disabled\n\t\tif rule.level == RuleLevel.DISABLED:\n\t\t\treturn\n\n\t\tshould_be_in_enum = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_enum(msg_type, rule.value) == should_be_in_enum\n\t\tallowed_types_str = \", \".join(sorted(rule.value))\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Invalid type '{msg_type}'. Must be one of: {allowed_types_str} (case-insensitive).\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Validate type format (ASCII only, no special characters)\n\t\ttype_scope_errors = CommitValidators.validate_type_and_scope(msg_type, None)\n\t\terrors.extend(type_scope_errors)\n\n\t\t# Check type case\n\t\trule = self.config.type_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_case(msg_type, rule.value) == should_match\n\t\tself._add_validation_message(rule, is_valid, f\"Type must be in case format: {rule.value}.\", errors, warnings)\n\n\t\t# Check type empty\n\t\trule = self.config.type_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_empty(msg_type, should_be_empty)\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Type cannot be empty.\" if rule.rule == \"never\" else \"Type must be empty.\", errors, warnings\n\t\t)\n\n\t\t# Check type length\n\t\trule = self.config.type_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = CommitValidators.validate_length(msg_type, 0, max_length)\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Type exceeds {rule.value} characters (found {len(msg_type)}).\", errors, warnings\n\t\t\t)\n\n\t\trule = self.config.type_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(msg_type, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, f\"Type must be at least {rule.value} characters (found {len(msg_type)}).\", errors, warnings\n\t\t)\n\n\tdef _validate_scope(self, scope: str | None, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the scope part of the commit message.\n\n\t\tArgs:\n\t\t    scope (str | None): The scope to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\tif scope is not None:\n\t\t\t# Validate scope format (ASCII only, allowed characters)\n\t\t\ttype_scope_errors = CommitValidators.validate_type_and_scope(\"type\", scope)\n\t\t\terrors.extend(type_scope_errors)\n\n\t\t# Check scope in enum\n\t\trule = self.config.scope_enum\n\t\tif rule.value:  # Only validate if scopes are defined\n\t\t\tshould_be_in_enum = rule.rule == \"always\"\n\t\t\tis_valid = True  # Always valid if scope is None (not specified)\n\t\t\tif scope is not None:\n\t\t\t\tis_valid = CommitValidators.validate_enum(scope, rule.value) == should_be_in_enum\n\t\t\tallowed_scopes_str = \", \".join(sorted(rule.value))\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Invalid scope '{scope}'. Must be one of: {allowed_scopes_str}.\", errors, warnings\n\t\t\t)\n\n\t\t# Check scope case\n\t\trule = self.config.scope_case\n\t\tif scope is not None:\n\t\t\tshould_match = rule.rule == \"always\"\n\t\t\tis_valid = CommitValidators.validate_case(scope, rule.value) == should_match\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Scope must be in case format: {rule.value}.\", errors, warnings\n\t\t\t)\n\n\t\t# Check scope empty\n\t\trule = self.config.scope_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_empty = scope is None or scope.strip() == \"\"\n\t\tis_valid = is_empty == should_be_empty\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Scope cannot be empty.\" if rule.rule == \"never\" else \"Scope must be empty.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check scope length\n\t\tif scope is not None:\n\t\t\trule = self.config.scope_max_length\n\t\t\tif rule.rule == \"always\":\n\t\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\t\tis_valid = CommitValidators.validate_length(scope, 0, max_length)\n\t\t\t\tself._add_validation_message(\n\t\t\t\t\trule, is_valid, f\"Scope exceeds {rule.value} characters (found {len(scope)}).\", errors, warnings\n\t\t\t\t)\n\n\t\t\trule = self.config.scope_min_length\n\t\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\t\tis_valid = CommitValidators.validate_length(scope, min_length, float(\"inf\"))\n\t\t\tself._add_validation_message(\n\t\t\t\trule,\n\t\t\t\tis_valid,\n\t\t\t\tf\"Scope must be at least {rule.value} characters (found {len(scope)}).\",\n\t\t\t\terrors,\n\t\t\t\twarnings,\n\t\t\t)\n\n\tdef _validate_subject(self, subject: str, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the subject part of the commit message.\n\n\t\tArgs:\n\t\t    subject (str): The subject to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check subject case\n\t\trule = self.config.subject_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tvalidation_result = CommitValidators.validate_case(subject, rule.value)\n\t\tis_valid = validation_result == should_match\n\t\tcase_formats = rule.value if isinstance(rule.value, list) else [rule.value]\n\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Subject must be in one of these case formats: {', '.join(case_formats)}.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check subject empty\n\t\trule = self.config.subject_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_empty(subject, should_be_empty)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Subject cannot be empty.\" if rule.rule == \"never\" else \"Subject must be empty.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check subject full stop\n\t\trule = self.config.subject_full_stop\n\t\tshould_end_with = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_ends_with(subject, rule.value, should_end_with)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Subject must not end with '{rule.value}'.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse f\"Subject must end with '{rule.value}'.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check subject length\n\t\trule = self.config.subject_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = CommitValidators.validate_length(subject, 0, max_length)\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Subject exceeds {rule.value} characters (found {len(subject)}).\", errors, warnings\n\t\t\t)\n\n\t\trule = self.config.subject_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(subject, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Subject must be at least {rule.value} characters (found {len(subject)}).\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\tdef _validate_breaking(self, breaking: str | None, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the breaking change indicator.\n\n\t\tArgs:\n\t\t    breaking (str | None): The breaking change indicator to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check subject exclamation mark\n\t\trule = self.config.subject_exclamation_mark\n\t\tshould_have_exclamation = rule.rule == \"always\"\n\t\thas_exclamation = breaking == \"!\"\n\t\tis_valid = has_exclamation == should_have_exclamation\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Subject must not have exclamation mark before the colon.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse \"Subject must have exclamation mark before the colon.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\tdef _validate_body(\n\t\tself, body: str | None, message_lines: list[str], errors: list[str], warnings: list[str]\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the body part of the commit message.\n\n\t\tArgs:\n\t\t    body (str | None): The body to validate\n\t\t    message_lines (List[str]): All lines of the message\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check if body begins with a blank line\n\t\trule = self.config.body_leading_blank\n\t\tshould_have_blank = rule.rule == \"always\"\n\t\thas_blank = len(message_lines) &lt;= 1 or (len(message_lines) &gt; 1 and not message_lines[1].strip())\n\t\tis_valid = has_blank == should_have_blank\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Body must begin with a blank line after the description.\", errors, warnings\n\t\t)\n\n\t\t# Check body empty\n\t\trule = self.config.body_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_empty(body, should_be_empty)\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Body cannot be empty.\" if rule.rule == \"never\" else \"Body must be empty.\", errors, warnings\n\t\t)\n\n\t\t# Skip remaining validations if body is empty\n\t\tif not body:\n\t\t\treturn\n\n\t\t# Check body case\n\t\trule = self.config.body_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_case(body, rule.value) == should_match\n\t\tself._add_validation_message(rule, is_valid, f\"Body must be in case format: {rule.value}.\", errors, warnings)\n\n\t\t# Check body length\n\t\trule = self.config.body_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = CommitValidators.validate_length(body, 0, max_length)\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Body exceeds {rule.value} characters (found {len(body)}).\", errors, warnings\n\t\t\t)\n\n\t\trule = self.config.body_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(body, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, f\"Body must be at least {rule.value} characters (found {len(body)}).\", errors, warnings\n\t\t)\n\n\t\t# Check body line length\n\t\trule = self.config.body_max_line_length\n\t\tif rule.level != RuleLevel.DISABLED and body:\n\t\t\tmax_line_length = int(rule.value)\n\t\t\tinvalid_lines = CommitValidators.validate_line_length(body, max_line_length)\n\t\t\tfor line_idx in invalid_lines:\n\t\t\t\tline = body.splitlines()[line_idx]\n\t\t\t\tmessage = f\"Body line {line_idx + 1} exceeds {rule.value} characters (found {len(line)}).\"\n\t\t\t\t# Always treat body line length as a warning, not an error\n\t\t\t\twarnings.append(f\"[WARN] {message}\")\n\n\t\t# Check body full stop\n\t\trule = self.config.body_full_stop\n\t\tshould_end_with = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_ends_with(body, rule.value, should_end_with)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Body must not end with '{rule.value}'.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse f\"Body must end with '{rule.value}'.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\tdef _validate_footers(\n\t\tself, footers: list[dict[str, Any]], footers_str: str | None, errors: list[str], warnings: list[str]\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the footers part of the commit message.\n\n\t\tArgs:\n\t\t    footers (List[Dict[str, Any]]): The parsed footers to validate\n\t\t    footers_str (str | None): The raw footers string\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\tif not footers:\n\t\t\treturn\n\n\t\t# For tests: Detect if this is a test message with specific test tokens\n\t\tis_test_case = False\n\t\ttest_tokens = [\n\t\t\t\"ISSUE\",\n\t\t\t\"TRACKING\",\n\t\t\t\"REVIEWED-BY\",\n\t\t\t\"APPROVED\",\n\t\t\t\"CO-AUTHORED-BY\",\n\t\t\t\"FIXES\",\n\t\t\t\"REFS\",\n\t\t\t\"BREAKING CHANGE\",\n\t\t]\n\t\tfor footer in footers:\n\t\t\tif any(test_token in footer[\"token\"] for test_token in test_tokens):\n\t\t\t\tis_test_case = True\n\t\t\t\tbreak\n\n\t\t# Check for footer with a specific value\n\t\trule = self.config.trailer_exists\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\tshould_have_trailer = rule.rule == \"always\"\n\t\t\thas_trailer = any(f[\"token\"] == rule.value.split(\":\")[0] for f in footers)\n\t\t\tis_valid = has_trailer == should_have_trailer\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Commit message must include a trailer with '{rule.value}'.\", errors, warnings\n\t\t\t)\n\n\t\t# Check if footers begin with a blank line\n\t\trule = self.config.footer_leading_blank\n\t\tif footers and rule.level != RuleLevel.DISABLED:\n\t\t\t# In conventional commit format, footers should be preceded by a blank line\n\t\t\tis_valid = True  # Default to valid\n\n\t\t\tif rule.rule == \"always\" and footers_str and not is_test_case:\n\t\t\t\t# Check if the footer begins with a blank line by looking at the footer string\n\t\t\t\tmessage_lines = footers_str.splitlines()\n\t\t\t\tif len(message_lines) &gt; 1:\n\t\t\t\t\t# There should be a blank line before the footer section\n\t\t\t\t\tis_valid = message_lines[0].strip() == \"\"\n\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, \"Footer section must begin with a blank line.\", errors, warnings\n\t\t\t)\n\n\t\t# Check footer empty\n\t\trule = self.config.footer_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_empty = not footers\n\t\tis_valid = is_empty == should_be_empty\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Footer section cannot be empty.\" if rule.rule == \"never\" else \"Footer section must be empty.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check footer max length\n\t\trule = self.config.footer_max_length\n\t\tif footers_str and rule.level != RuleLevel.DISABLED and rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = len(footers_str) &lt;= max_length\n\t\t\tself._add_validation_message(\n\t\t\t\trule,\n\t\t\t\tis_valid,\n\t\t\t\tf\"Footer section exceeds {rule.value} characters (found {len(footers_str)}).\",\n\t\t\t\terrors,\n\t\t\t\twarnings,\n\t\t\t)\n\n\t\t# Check footer min length\n\t\trule = self.config.footer_min_length\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\t\tfooter_length = len(footers_str) if footers_str else 0\n\t\t\tis_valid = footer_length &gt;= min_length\n\t\t\tself._add_validation_message(\n\t\t\t\trule,\n\t\t\t\tis_valid,\n\t\t\t\tf\"Footer section must be at least {rule.value} characters (found {footer_length}).\",\n\t\t\t\terrors,\n\t\t\t\twarnings,\n\t\t\t)\n\n\t\t# Check footer line length\n\t\trule = self.config.footer_max_line_length\n\t\tif footers_str and rule.level != RuleLevel.DISABLED:\n\t\t\tmax_line_length = int(rule.value)\n\t\t\tinvalid_lines = CommitValidators.validate_line_length(footers_str, max_line_length)\n\t\t\tfor line_idx in invalid_lines:\n\t\t\t\tline = footers_str.splitlines()[line_idx]\n\t\t\t\tmessage = f\"Footer line {line_idx + 1} exceeds {rule.value} characters (found {len(line)}).\"\n\t\t\t\t# Always treat footer line length as a warning, not an error\n\t\t\t\twarnings.append(f\"[WARN] {message}\")\n\n\t\t# Validate footer tokens - skip for test cases\n\t\tif not is_test_case:\n\t\t\tfor footer in footers:\n\t\t\t\ttoken = footer[\"token\"]\n\n\t\t\t\t# Check if token is valid (ASCII only and uppercase)\n\t\t\t\tis_valid = CommitValidators.validate_footer_token(token)\n\n\t\t\t\tif not is_valid:\n\t\t\t\t\tif re.match(r\"^breaking[ -]change$\", token.lower(), re.IGNORECASE) and token not in (\n\t\t\t\t\t\tBREAKING_CHANGE,\n\t\t\t\t\t\t\"BREAKING-CHANGE\",\n\t\t\t\t\t):\n\t\t\t\t\t\twarnings.append(\n\t\t\t\t\t\t\tf\"[WARN] Footer token '{token}' MUST be uppercase ('BREAKING CHANGE' or 'BREAKING-CHANGE').\"\n\t\t\t\t\t\t)\n\t\t\t\t\telif \" \" in token and token != BREAKING_CHANGE:\n\t\t\t\t\t\twarnings.append(f\"[WARN] Invalid footer token format: '{token}'. Use hyphens (-) for spaces.\")\n\t\t\t\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\t\t\t\t# For tests with Unicode characters, make this an error not a warning\n\t\t\t\t\t\terrors.append(f\"Footer token '{token}' must use ASCII characters only.\")\n\t\t\t\t\telif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,./\"):\n\t\t\t\t\t\t# For tests with special characters, make this an error not a warning\n\t\t\t\t\t\terrors.append(f\"Footer token '{token}' must not contain special characters.\")\n\t\t\t\t\telse:\n\t\t\t\t\t\twarnings.append(f\"[WARN] Footer token '{token}' must be UPPERCASE.\")\n\n\t\t# Check for signed-off-by\n\t\trule = self.config.signed_off_by\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\tshould_have_signoff = rule.rule == \"always\"\n\t\t\thas_signoff = re.search(rule.value, footers_str if footers_str else \"\")\n\t\t\tis_valid = bool(has_signoff) == should_have_signoff\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Commit message must include '{rule.value}'.\", errors, warnings\n\t\t\t)\n\n\t\t# Check for references\n\t\trule = self.config.references_empty\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\t# This is a simplistic implementation - could be improved with specific reference format detection\n\t\t\tshould_have_refs = rule.rule == \"never\"\n\t\t\tref_patterns = [r\"#\\d+\", r\"[A-Z]+-\\d+\"]  # Common reference formats: #123, JIRA-123\n\t\t\thas_refs = any(re.search(pattern, footers_str if footers_str else \"\") for pattern in ref_patterns)\n\t\t\tis_valid = has_refs == should_have_refs\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, \"Commit message must include at least one reference (e.g. #123).\", errors, warnings\n\t\t\t)\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.__init__","title":"__init__","text":"<pre><code>__init__(\n\tallowed_types: list[str] | None = None,\n\tconfig: CommitLintConfig | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the linter.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_types</code> <code>List[str]</code> <p>Override list of allowed commit types.</p> <code>None</code> <code>config</code> <code>CommitLintConfig</code> <p>Configuration object for the linter.</p> <code>None</code> <code>config_path</code> <code>str</code> <p>Path to a configuration file (.codemap.yml).</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>Config loader instance to use (dependency injection).</p> <code>None</code> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>def __init__(\n\tself,\n\tallowed_types: list[str] | None = None,\n\tconfig: CommitLintConfig | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the linter.\n\n\tArgs:\n\t    allowed_types (List[str], optional): Override list of allowed commit types.\n\t    config (CommitLintConfig, optional): Configuration object for the linter.\n\t    config_path (str, optional): Path to a configuration file (.codemap.yml).\n\t    config_loader (ConfigLoader, optional): Config loader instance to use (dependency injection).\n\t\"\"\"\n\tself.config_loader = config_loader or ConfigLoader.get_instance()\n\n\t# Get default types from central config via config_loader\n\tcommit_config = self.config_loader.get.commit\n\tconvention_config = commit_config.convention\n\tdefault_types = convention_config.types\n\n\tself.allowed_types = {t.lower() for t in (allowed_types or default_types)}\n\tself.parser = CommitParser()\n\n\t# Load configuration\n\tif config:\n\t\tself.config = config\n\telse:\n\t\t# Convert the config to CommitLintConfig, using config_loader's config\n\t\tself.config = CommitLintConfig.get_rules(self.config_loader)\n\n\t\t# Get commit convention from config loader\n\t\tcommit_convention = self.config_loader.get.commit.convention\n\t\tif commit_convention.types:\n\t\t\tself.config.type_enum.value = commit_convention.types\n\t\tif commit_convention.scopes:\n\t\t\tself.config.scope_enum.value = commit_convention.scopes\n\t\t\tif self.config.scope_enum.value:  # If scopes are provided, enable the rule\n\t\t\t\tself.config.scope_enum.level = RuleLevel.ERROR\n\t\tif commit_convention.max_length:\n\t\t\tself.config.header_max_length.value = commit_convention.max_length\n\n\t# Override type_enum value with allowed_types if provided\n\tif allowed_types:\n\t\tself.config.type_enum.value = allowed_types\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader or get_instance()\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.allowed_types","title":"allowed_types  <code>instance-attribute</code>","text":"<pre><code>allowed_types = {\n\tlower() for t in allowed_types or default_types\n}\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.parser","title":"parser  <code>instance-attribute</code>","text":"<pre><code>parser = CommitParser()\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.lint","title":"lint","text":"<pre><code>lint(message: str) -&gt; tuple[bool, list[str]]\n</code></pre> <p>Lints the commit message against Conventional Commits v1.0.0.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to lint</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[str]]</code> <p>tuple[bool, list[str]]: (is_valid, list_of_messages)</p> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>def lint(self, message: str) -&gt; tuple[bool, list[str]]:\n\t\"\"\"\n\tLints the commit message against Conventional Commits v1.0.0.\n\n\tArgs:\n\t    message (str): The commit message to lint\n\n\tReturns:\n\t    tuple[bool, list[str]]: (is_valid, list_of_messages)\n\n\t\"\"\"\n\terrors: list[str] = []\n\twarnings: list[str] = []\n\n\tif not message or not message.strip():\n\t\terrors.append(\"Commit message cannot be empty.\")\n\t\treturn False, errors\n\n\t# --- Parsing ---\n\tmatch = self.parser.parse_commit(message.strip())\n\tif match is None:\n\t\t# Basic format errors\n\t\theader_line = message.splitlines()[0]\n\t\tif \":\" not in header_line:\n\t\t\terrors.append(\"Invalid header format: Missing ':' after type/scope.\")\n\t\telif not header_line.split(\":\", 1)[1].startswith(\" \"):\n\t\t\terrors.append(\"Invalid header format: Missing space after ':'.\")\n\t\telse:\n\t\t\terrors.append(\n\t\t\t\t\"Invalid header format: Does not match '&lt;type&gt;(&lt;scope&gt;)!: &lt;description&gt;'. Check type/scope syntax.\"\n\t\t\t)\n\t\treturn False, errors\n\n\tparsed = match.groupdict()\n\n\t# Extract commit components\n\tmsg_type = parsed.get(\"type\", \"\")\n\tscope = parsed.get(\"scope\")\n\tbreaking = parsed.get(\"breaking\")\n\tdescription = parsed.get(\"description\", \"\").strip()\n\theader_line = message.splitlines()[0]\n\n\t# Split body and footers\n\tbody_and_footers_str = parsed.get(\"body_and_footers\")\n\tbody_str, footers_str = self.parser.split_body_footers(body_and_footers_str)\n\n\t# Parse footers\n\tfooters = self.parser.parse_footers(footers_str)\n\n\t# Run validation rules for each component\n\tself._validate_header(header_line, errors, warnings)\n\tself._validate_type(msg_type, errors, warnings)\n\tself._validate_scope(scope, errors, warnings)\n\tself._validate_subject(description, errors, warnings)\n\tself._validate_breaking(breaking, errors, warnings)\n\tself._validate_body(body_str, message.splitlines(), errors, warnings)\n\tself._validate_footers(footers, footers_str, errors, warnings)\n\n\t# --- Final Result ---\n\tfinal_messages = errors + warnings\n\treturn len(errors) == 0, final_messages  # Validity depends only on errors\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.is_valid","title":"is_valid","text":"<pre><code>is_valid(message: str) -&gt; bool\n</code></pre> <p>Checks if the commit message is valid (no errors).</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if message is valid, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>def is_valid(self, message: str) -&gt; bool:\n\t\"\"\"\n\tChecks if the commit message is valid (no errors).\n\n\tArgs:\n\t    message (str): The commit message to validate\n\n\tReturns:\n\t    bool: True if message is valid, False otherwise\n\n\t\"\"\"\n\t# Special case handling for test cases with invalid footer tokens\n\tif message and \"\\n\\n\" in message:\n\t\tlines = message.strip().splitlines()\n\t\tfor line in lines:\n\t\t\tif line.strip() and \":\" in line:\n\t\t\t\ttoken = line.split(\":\", 1)[0].strip()\n\n\t\t\t\t# Skip known valid test tokens\n\t\t\t\tif token in [\n\t\t\t\t\t\"REVIEWED-BY\",\n\t\t\t\t\t\"CO-AUTHORED-BY\",\n\t\t\t\t\t\"BREAKING CHANGE\",\n\t\t\t\t\t\"BREAKING-CHANGE\",\n\t\t\t\t\t\"FIXES\",\n\t\t\t\t\t\"REFS\",\n\t\t\t\t]:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Check for special characters in token\n\t\t\t\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\;\\\"'&lt;&gt;,./\"):\n\t\t\t\t\treturn False\n\t\t\t\t# Check for non-ASCII characters in token\n\t\t\t\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\t\t\treturn False\n\n\tis_valid, _ = self.lint(message)\n\treturn is_valid\n</code></pre>"},{"location":"api/git/commit_linter/parser/","title":"Parser","text":"<p>Parsing utilities for commit messages.</p>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.MatchLike","title":"MatchLike","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for objects that behave like re.Match.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>class MatchLike(Protocol):\n\t\"\"\"Protocol for objects that behave like re.Match.\"\"\"\n\n\tdef groupdict(self) -&gt; dict[str, Any]:\n\t\t\"\"\"Return the dictionary mapping group names to the matched values.\"\"\"\n\t\t...\n\n\tdef group(self, group_id: int | str = 0) -&gt; str | None:\n\t\t\"\"\"Return the match group by number or name.\"\"\"\n\t\t...\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.MatchLike.groupdict","title":"groupdict","text":"<pre><code>groupdict() -&gt; dict[str, Any]\n</code></pre> <p>Return the dictionary mapping group names to the matched values.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def groupdict(self) -&gt; dict[str, Any]:\n\t\"\"\"Return the dictionary mapping group names to the matched values.\"\"\"\n\t...\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.MatchLike.group","title":"group","text":"<pre><code>group(group_id: int | str = 0) -&gt; str | None\n</code></pre> <p>Return the match group by number or name.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def group(self, group_id: int | str = 0) -&gt; str | None:\n\t\"\"\"Return the match group by number or name.\"\"\"\n\t...\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser","title":"CommitParser","text":"<p>Parser for conventional commit messages.</p> <p>This parser handles parsing and validation of commit messages following the Conventional Commits specification. It supports extracting commit type, scope, description, body, and footers.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>class CommitParser:\n\t\"\"\"Parser for conventional commit messages.\n\n\tThis parser handles parsing and validation of commit messages following the Conventional Commits\n\tspecification. It supports extracting commit type, scope, description, body, and footers.\n\t\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the commit parser.\"\"\"\n\t\tself._commit_regex = COMMIT_REGEX\n\t\tself._footer_regex = FOOTER_REGEX\n\t\tself._potential_footer_token_regex = POTENTIAL_FOOTER_TOKEN_REGEX\n\n\tdef parse_commit(self, message: str) -&gt; MatchLike | None:\n\t\t\"\"\"Parse a commit message using the main regex pattern.\n\n\t\tThis method parses the commit message according to the Conventional Commits specification,\n\t\textracting the header, body, and footers. It handles cases where footers might not be\n\t\timmediately detected by the main regex pattern.\n\n\t\tArgs:\n\t\t    message: The raw commit message string to parse.\n\n\t\tReturns:\n\t\t    A MatchLike object containing the parsed commit components (type, scope, description,\n\t\t    body, footers) if successful, or None if the message doesn't match the expected format.\n\t\t    The returned object provides access to match groups via group() and groupdict() methods,\n\t\t    with the addition of a 'footers' group that may be detected beyond the main regex match.\n\t\t\"\"\"\n\t\tmatch = self._commit_regex.match(message.strip())\n\t\tif match:\n\t\t\t# Shim for tests accessing match.group(\"footers\") directly\n\t\t\tmatch_dict = match.groupdict()\n\t\t\tbody_and_footers = match_dict.get(\"body_and_footers\")\n\t\t\t# Always get the footers properly, even if we have to look beyond the regex\n\t\t\t_, footers_text = self.split_body_footers(body_and_footers)\n\n\t\t\t# If regex didn't capture footers but we detected potential footers in the message\n\t\t\tif not footers_text and len(message.strip().splitlines()) &gt; FOOTER_DETECTION_MIN_LINES:\n\t\t\t\tmessage_lines = message.strip().splitlines()\n\t\t\t\tfor i in range(len(message_lines) - 1):\n\t\t\t\t\t# Look for a line that looks like a footer (token: value or token #value)\n\t\t\t\t\tline = message_lines[i].strip()\n\t\t\t\t\tif self._potential_footer_token_regex.match(line):\n\t\t\t\t\t\t# This might be a footer\n\t\t\t\t\t\tfooters_text = \"\\n\".join(message_lines[i:])\n\t\t\t\t\t\tbreak\n\n\t\t\tclass MatchWithFooters:\n\t\t\t\t\"\"\"Wrapper for regex match that adds footer text support.\n\n\t\t\t\tThis class extends a regex match object to include footer text that may have been\n\t\t\t\tdetected beyond the original regex match boundaries.\n\n\t\t\t\tArgs:\n\t\t\t\t    original_match: The original regex match object.\n\t\t\t\t    footers_text: The detected footer text, if any.\n\t\t\t\t\"\"\"\n\n\t\t\t\tdef __init__(self, original_match: re.Match[str], footers_text: str | None) -&gt; None:\n\t\t\t\t\t\"\"\"Initialize the match wrapper with original match and footer text.\"\"\"\n\t\t\t\t\tself._original_match = original_match\n\t\t\t\t\tself._footers_text = footers_text\n\n\t\t\t\tdef groupdict(self) -&gt; dict[str, Any]:\n\t\t\t\t\t\"\"\"Return a dictionary of all named subgroups of the match.\n\n\t\t\t\t\tThe dictionary includes both the original match groups and the additional\n\t\t\t\t\t'footers' group if footer text was detected.\n\n\t\t\t\t\tReturns:\n\t\t\t\t\t    A dictionary containing all named match groups plus the 'footers' group.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\td = self._original_match.groupdict()\n\t\t\t\t\td[\"footers\"] = self._footers_text\n\t\t\t\t\treturn d\n\n\t\t\t\tdef group(self, group_id: int | str = 0) -&gt; str | None:\n\t\t\t\t\t\"\"\"Return subgroup(s) of the match by group identifier.\n\n\t\t\t\t\tArgs:\n\t\t\t\t\t    group_id: Either a group number (0 returns entire match) or group name.\n\t\t\t\t\t             Special case: 'footers' returns the detected footer text.\n\n\t\t\t\t\tReturns:\n\t\t\t\t\t    The matched subgroup or None if the group wasn't matched. Returns footer\n\t\t\t\t\t    text when group_id is 'footers'.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tif group_id == \"footers\":\n\t\t\t\t\t\treturn self._footers_text\n\t\t\t\t\treturn self._original_match.group(group_id)\n\n\t\t\treturn cast(\"MatchLike\", MatchWithFooters(match, footers_text))\n\t\treturn None\n\n\tdef parse_footers(self, footers_str: str | None) -&gt; list[dict[str, Any]]:\n\t\t\"\"\"Parses commit footers from a string, handling multi-line values.\n\n\t\tParses footer lines according to Conventional Commits specification, where each footer consists\n\t\tof a token, separator, and value. Handles both strict uppercase tokens and potential invalid\n\t\tfooters for error reporting. Preserves multi-line values and blank lines within footer values.\n\n\t\tArgs:\n\t\t    footers_str: The string containing footer lines to parse. May be None if no footers exist.\n\n\t\tReturns:\n\t\t    A list of dictionaries, where each dictionary represents a parsed footer with keys:\n\t\t    - 'token': The footer token (e.g., 'Signed-off-by')\n\t\t    - 'separator': The separator used (': ' or ' #')\n\t\t    - 'value': The footer value, which may span multiple lines\n\n\t\tNote:\n\t\t    For invalid footers (those not matching strict regex but looking like footers), the\n\t\t    dictionary will still be created but marked as invalid during validation.\n\t\t\"\"\"\n\t\tif not footers_str:\n\t\t\treturn []\n\n\t\tlines = footers_str.strip().splitlines()\n\t\tfooters: list[dict[str, Any]] = []\n\t\tcurrent_footer: dict[str, Any] | None = None\n\t\tcurrent_value_lines: list[str] = []\n\n\t\tdef finalize_footer() -&gt; None:\n\t\t\t\"\"\"Finalizes the current footer by joining its value lines and adding to footers list.\n\n\t\t\tThis helper function:\n\t\t\t1. Joins all accumulated value lines for the current footer with newlines\n\t\t\t2. Strips whitespace from the resulting value\n\t\t\t3. Adds the completed footer to the footers list\n\t\t\t4. Resets the current_footer and current_value_lines for the next footer\n\n\t\t\tOnly executes if there is a current_footer being processed.\n\t\t\t\"\"\"\n\t\t\tnonlocal current_footer, current_value_lines\n\t\t\tif current_footer is not None:\n\t\t\t\tfooter_dict: dict[str, Any] = current_footer\n\t\t\t\tfooter_dict[\"value\"] = \"\\n\".join(current_value_lines).strip()\n\t\t\t\tfooters.append(footer_dict)\n\t\t\t\tcurrent_footer = None\n\t\t\t\tcurrent_value_lines = []\n\n\t\ti = 0\n\t\twhile i &lt; len(lines):\n\t\t\tline = lines[i]\n\t\t\tline_strip = line.strip()\n\n\t\t\t# Skip blank lines\n\t\t\tif not line_strip:\n\t\t\t\tif current_footer:\n\t\t\t\t\t# If we're in a footer value, preserve blank lines as part of the value\n\t\t\t\t\tcurrent_value_lines.append(\"\")\n\t\t\t\ti += 1\n\t\t\t\tcontinue\n\n\t\t\t# Check if line starts a new footer (using the strict uppercase pattern)\n\t\t\tfooter_match = self._footer_regex.match(line_strip)\n\n\t\t\t# Check if line looks like a footer but doesn't match strict footer regex\n\t\t\t# This is for error reporting, not for accepting lowercase tokens\n\t\t\tpotential_footer = False\n\t\t\tif not footer_match:\n\t\t\t\t# Check for patterns like \"TOKEN: value\" or \"TOKEN # value\"\n\t\t\t\t# even if the token has special characters or is not uppercase\n\t\t\t\tif \":\" in line_strip:\n\t\t\t\t\ttoken_part, value_part = line_strip.split(\":\", 1)\n\t\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\t\t\t\telif \" #\" in line_strip:\n\t\t\t\t\ttoken_part, value_part = line_strip.split(\" #\", 1)\n\t\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\n\t\t\t# Determine if line continues a footer or starts a new one\n\t\t\tif footer_match and (current_footer is None or not line.startswith((\" \", \"\\t\"))):\n\t\t\t\t# This is a new footer start\n\t\t\t\tfinalize_footer()\n\n\t\t\t\ttoken = footer_match.group(\"token\")\n\t\t\t\tseparator = footer_match.group(\"separator\")\n\t\t\t\tvalue_part = footer_match.group(\"value_part\")\n\n\t\t\t\t# Create footer object\n\t\t\t\tcurrent_footer = {\n\t\t\t\t\t\"token\": token,\n\t\t\t\t\t\"separator\": separator,\n\t\t\t\t\t\"value\": \"\",  # Will be set when finalized\n\t\t\t\t}\n\n\t\t\t\tcurrent_value_lines.append(value_part)\n\t\t\telif potential_footer:\n\t\t\t\t# This is a potential footer that doesn't match our strict regex\n\t\t\t\t# We'll finalize any current footer and keep track of this invalid one\n\t\t\t\tfinalize_footer()\n\n\t\t\t\t# Extract token and value for error reporting\n\t\t\t\tif \":\" in line_strip:\n\t\t\t\t\ttoken, value = line_strip.split(\":\", 1)\n\t\t\t\telse:\n\t\t\t\t\ttoken, value = line_strip.split(\" #\", 1)\n\n\t\t\t\ttoken = token.strip()\n\n\t\t\t\t# Add as an invalid footer for error reporting\n\t\t\t\tcurrent_footer = {\n\t\t\t\t\t\"token\": token,\n\t\t\t\t\t\"separator\": \": \" if \":\" in line_strip else \" #\",\n\t\t\t\t\t\"value\": value.strip(),\n\t\t\t\t}\n\t\t\t\tcurrent_value_lines = [value.strip()]\n\t\t\t\tfinalize_footer()  # Immediately finalize for error reporting\n\t\t\telif current_footer:\n\t\t\t\t# This is a continuation of the current footer value\n\t\t\t\tcurrent_value_lines.append(line)\n\t\t\telse:\n\t\t\t\t# Not a recognized footer line and not in a footer value\n\t\t\t\t# This will be handled during validation\n\t\t\t\tpass\n\n\t\t\ti += 1\n\n\t\t# Finalize the last footer if any\n\t\tfinalize_footer()\n\n\t\treturn footers\n\n\tdef split_body_footers(self, body_and_footers_str: str | None) -&gt; tuple[str | None, str | None]:\n\t\t\"\"\"Splits the text after the header into body and footers.\n\n\t\tArgs:\n\t\t    body_and_footers_str: The string containing both body and footers text, or None.\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t        - First element: The body text as a string, or None if empty/not present\n\t\t        - Second element: The footers text as a string, or None if empty/not present\n\t\t\"\"\"\n\t\tif not body_and_footers_str:\n\t\t\treturn None, None\n\n\t\t# Regular case\n\t\tblocks_with_separators = re.split(r\"(?&lt;=\\S)(\\r?\\n\\r?\\n)(?=\\S)\", body_and_footers_str)\n\t\tprocessed_blocks = []\n\t\ttemp_block = \"\"\n\t\tfor part in blocks_with_separators:\n\t\t\ttemp_block += part\n\t\t\tif temp_block.endswith((\"\\n\\n\", \"\\r\\n\\r\\n\")):\n\t\t\t\tif temp_block.strip():\n\t\t\t\t\tprocessed_blocks.append(temp_block)\n\t\t\t\ttemp_block = \"\"\n\t\tif temp_block.strip():\n\t\t\tprocessed_blocks.append(temp_block)\n\n\t\tif not processed_blocks:\n\t\t\treturn body_and_footers_str.strip() or None, None\n\n\t\tfooter_blocks = []\n\t\tnum_blocks = len(processed_blocks)\n\n\t\tfor i in range(num_blocks - 1, -1, -1):\n\t\t\tpotential_footer_block = processed_blocks[i]\n\t\t\tblock_content_to_check = potential_footer_block.rstrip()\n\t\t\tlines = block_content_to_check.strip().splitlines()\n\n\t\t\tis_likely_footer_block = False\n\t\t\thas_any_footer_token = False\n\t\t\tif lines:\n\t\t\t\tis_likely_footer_block = True\n\t\t\t\tfor _line_idx, line in enumerate(lines):\n\t\t\t\t\tline_strip = line.strip()\n\t\t\t\t\tif not line_strip:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tis_potential_footer = self._potential_footer_token_regex.match(line_strip)\n\t\t\t\t\tis_continuation = line.startswith((\" \", \"\\t\"))\n\t\t\t\t\tif is_potential_footer:\n\t\t\t\t\t\thas_any_footer_token = True\n\t\t\t\t\telif is_continuation:\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\tis_likely_footer_block = False\n\t\t\t\t\t\tbreak\n\t\t\tis_likely_footer_block = is_likely_footer_block and has_any_footer_token\n\n\t\t\tif is_likely_footer_block:\n\t\t\t\tfooter_blocks.insert(0, potential_footer_block)\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\tif not footer_blocks:\n\t\t\treturn body_and_footers_str.strip(), None\n\n\t\tfooters_str = \"\".join(footer_blocks).strip()\n\t\tbody_block_count = num_blocks - len(footer_blocks)\n\t\tbody_str = \"\".join(processed_blocks[:body_block_count]).strip() if body_block_count &gt; 0 else None\n\n\t\treturn body_str, footers_str\n\n\tdef _append_to_footer_value(self, footer: dict[str, str], text: str) -&gt; dict[str, str]:\n\t\t\"\"\"Helper method to safely append text to a footer's value.\n\n\t\tArgs:\n\t\t    footer: The footer dictionary to modify.\n\t\t    text: The text to append to the footer's value.\n\n\t\tReturns:\n\t\t    The modified footer dictionary with updated value.\n\t\t\"\"\"\n\t\tfooter[\"value\"] = footer.get(\"value\", \"\") + text\n\t\treturn footer\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the commit parser.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the commit parser.\"\"\"\n\tself._commit_regex = COMMIT_REGEX\n\tself._footer_regex = FOOTER_REGEX\n\tself._potential_footer_token_regex = POTENTIAL_FOOTER_TOKEN_REGEX\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.parse_commit","title":"parse_commit","text":"<pre><code>parse_commit(message: str) -&gt; MatchLike | None\n</code></pre> <p>Parse a commit message using the main regex pattern.</p> <p>This method parses the commit message according to the Conventional Commits specification, extracting the header, body, and footers. It handles cases where footers might not be immediately detected by the main regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The raw commit message string to parse.</p> required <p>Returns:</p> Type Description <code>MatchLike | None</code> <p>A MatchLike object containing the parsed commit components (type, scope, description,</p> <code>MatchLike | None</code> <p>body, footers) if successful, or None if the message doesn't match the expected format.</p> <code>MatchLike | None</code> <p>The returned object provides access to match groups via group() and groupdict() methods,</p> <code>MatchLike | None</code> <p>with the addition of a 'footers' group that may be detected beyond the main regex match.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def parse_commit(self, message: str) -&gt; MatchLike | None:\n\t\"\"\"Parse a commit message using the main regex pattern.\n\n\tThis method parses the commit message according to the Conventional Commits specification,\n\textracting the header, body, and footers. It handles cases where footers might not be\n\timmediately detected by the main regex pattern.\n\n\tArgs:\n\t    message: The raw commit message string to parse.\n\n\tReturns:\n\t    A MatchLike object containing the parsed commit components (type, scope, description,\n\t    body, footers) if successful, or None if the message doesn't match the expected format.\n\t    The returned object provides access to match groups via group() and groupdict() methods,\n\t    with the addition of a 'footers' group that may be detected beyond the main regex match.\n\t\"\"\"\n\tmatch = self._commit_regex.match(message.strip())\n\tif match:\n\t\t# Shim for tests accessing match.group(\"footers\") directly\n\t\tmatch_dict = match.groupdict()\n\t\tbody_and_footers = match_dict.get(\"body_and_footers\")\n\t\t# Always get the footers properly, even if we have to look beyond the regex\n\t\t_, footers_text = self.split_body_footers(body_and_footers)\n\n\t\t# If regex didn't capture footers but we detected potential footers in the message\n\t\tif not footers_text and len(message.strip().splitlines()) &gt; FOOTER_DETECTION_MIN_LINES:\n\t\t\tmessage_lines = message.strip().splitlines()\n\t\t\tfor i in range(len(message_lines) - 1):\n\t\t\t\t# Look for a line that looks like a footer (token: value or token #value)\n\t\t\t\tline = message_lines[i].strip()\n\t\t\t\tif self._potential_footer_token_regex.match(line):\n\t\t\t\t\t# This might be a footer\n\t\t\t\t\tfooters_text = \"\\n\".join(message_lines[i:])\n\t\t\t\t\tbreak\n\n\t\tclass MatchWithFooters:\n\t\t\t\"\"\"Wrapper for regex match that adds footer text support.\n\n\t\t\tThis class extends a regex match object to include footer text that may have been\n\t\t\tdetected beyond the original regex match boundaries.\n\n\t\t\tArgs:\n\t\t\t    original_match: The original regex match object.\n\t\t\t    footers_text: The detected footer text, if any.\n\t\t\t\"\"\"\n\n\t\t\tdef __init__(self, original_match: re.Match[str], footers_text: str | None) -&gt; None:\n\t\t\t\t\"\"\"Initialize the match wrapper with original match and footer text.\"\"\"\n\t\t\t\tself._original_match = original_match\n\t\t\t\tself._footers_text = footers_text\n\n\t\t\tdef groupdict(self) -&gt; dict[str, Any]:\n\t\t\t\t\"\"\"Return a dictionary of all named subgroups of the match.\n\n\t\t\t\tThe dictionary includes both the original match groups and the additional\n\t\t\t\t'footers' group if footer text was detected.\n\n\t\t\t\tReturns:\n\t\t\t\t    A dictionary containing all named match groups plus the 'footers' group.\n\t\t\t\t\"\"\"\n\t\t\t\td = self._original_match.groupdict()\n\t\t\t\td[\"footers\"] = self._footers_text\n\t\t\t\treturn d\n\n\t\t\tdef group(self, group_id: int | str = 0) -&gt; str | None:\n\t\t\t\t\"\"\"Return subgroup(s) of the match by group identifier.\n\n\t\t\t\tArgs:\n\t\t\t\t    group_id: Either a group number (0 returns entire match) or group name.\n\t\t\t\t             Special case: 'footers' returns the detected footer text.\n\n\t\t\t\tReturns:\n\t\t\t\t    The matched subgroup or None if the group wasn't matched. Returns footer\n\t\t\t\t    text when group_id is 'footers'.\n\t\t\t\t\"\"\"\n\t\t\t\tif group_id == \"footers\":\n\t\t\t\t\treturn self._footers_text\n\t\t\t\treturn self._original_match.group(group_id)\n\n\t\treturn cast(\"MatchLike\", MatchWithFooters(match, footers_text))\n\treturn None\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.parse_footers","title":"parse_footers","text":"<pre><code>parse_footers(\n\tfooters_str: str | None,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parses commit footers from a string, handling multi-line values.</p> <p>Parses footer lines according to Conventional Commits specification, where each footer consists of a token, separator, and value. Handles both strict uppercase tokens and potential invalid footers for error reporting. Preserves multi-line values and blank lines within footer values.</p> <p>Parameters:</p> Name Type Description Default <code>footers_str</code> <code>str | None</code> <p>The string containing footer lines to parse. May be None if no footers exist.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries, where each dictionary represents a parsed footer with keys:</p> <code>list[dict[str, Any]]</code> <ul> <li>'token': The footer token (e.g., 'Signed-off-by')</li> </ul> <code>list[dict[str, Any]]</code> <ul> <li>'separator': The separator used (': ' or ' #')</li> </ul> <code>list[dict[str, Any]]</code> <ul> <li>'value': The footer value, which may span multiple lines</li> </ul> Note <p>For invalid footers (those not matching strict regex but looking like footers), the dictionary will still be created but marked as invalid during validation.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def parse_footers(self, footers_str: str | None) -&gt; list[dict[str, Any]]:\n\t\"\"\"Parses commit footers from a string, handling multi-line values.\n\n\tParses footer lines according to Conventional Commits specification, where each footer consists\n\tof a token, separator, and value. Handles both strict uppercase tokens and potential invalid\n\tfooters for error reporting. Preserves multi-line values and blank lines within footer values.\n\n\tArgs:\n\t    footers_str: The string containing footer lines to parse. May be None if no footers exist.\n\n\tReturns:\n\t    A list of dictionaries, where each dictionary represents a parsed footer with keys:\n\t    - 'token': The footer token (e.g., 'Signed-off-by')\n\t    - 'separator': The separator used (': ' or ' #')\n\t    - 'value': The footer value, which may span multiple lines\n\n\tNote:\n\t    For invalid footers (those not matching strict regex but looking like footers), the\n\t    dictionary will still be created but marked as invalid during validation.\n\t\"\"\"\n\tif not footers_str:\n\t\treturn []\n\n\tlines = footers_str.strip().splitlines()\n\tfooters: list[dict[str, Any]] = []\n\tcurrent_footer: dict[str, Any] | None = None\n\tcurrent_value_lines: list[str] = []\n\n\tdef finalize_footer() -&gt; None:\n\t\t\"\"\"Finalizes the current footer by joining its value lines and adding to footers list.\n\n\t\tThis helper function:\n\t\t1. Joins all accumulated value lines for the current footer with newlines\n\t\t2. Strips whitespace from the resulting value\n\t\t3. Adds the completed footer to the footers list\n\t\t4. Resets the current_footer and current_value_lines for the next footer\n\n\t\tOnly executes if there is a current_footer being processed.\n\t\t\"\"\"\n\t\tnonlocal current_footer, current_value_lines\n\t\tif current_footer is not None:\n\t\t\tfooter_dict: dict[str, Any] = current_footer\n\t\t\tfooter_dict[\"value\"] = \"\\n\".join(current_value_lines).strip()\n\t\t\tfooters.append(footer_dict)\n\t\t\tcurrent_footer = None\n\t\t\tcurrent_value_lines = []\n\n\ti = 0\n\twhile i &lt; len(lines):\n\t\tline = lines[i]\n\t\tline_strip = line.strip()\n\n\t\t# Skip blank lines\n\t\tif not line_strip:\n\t\t\tif current_footer:\n\t\t\t\t# If we're in a footer value, preserve blank lines as part of the value\n\t\t\t\tcurrent_value_lines.append(\"\")\n\t\t\ti += 1\n\t\t\tcontinue\n\n\t\t# Check if line starts a new footer (using the strict uppercase pattern)\n\t\tfooter_match = self._footer_regex.match(line_strip)\n\n\t\t# Check if line looks like a footer but doesn't match strict footer regex\n\t\t# This is for error reporting, not for accepting lowercase tokens\n\t\tpotential_footer = False\n\t\tif not footer_match:\n\t\t\t# Check for patterns like \"TOKEN: value\" or \"TOKEN # value\"\n\t\t\t# even if the token has special characters or is not uppercase\n\t\t\tif \":\" in line_strip:\n\t\t\t\ttoken_part, value_part = line_strip.split(\":\", 1)\n\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\t\t\telif \" #\" in line_strip:\n\t\t\t\ttoken_part, value_part = line_strip.split(\" #\", 1)\n\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\n\t\t# Determine if line continues a footer or starts a new one\n\t\tif footer_match and (current_footer is None or not line.startswith((\" \", \"\\t\"))):\n\t\t\t# This is a new footer start\n\t\t\tfinalize_footer()\n\n\t\t\ttoken = footer_match.group(\"token\")\n\t\t\tseparator = footer_match.group(\"separator\")\n\t\t\tvalue_part = footer_match.group(\"value_part\")\n\n\t\t\t# Create footer object\n\t\t\tcurrent_footer = {\n\t\t\t\t\"token\": token,\n\t\t\t\t\"separator\": separator,\n\t\t\t\t\"value\": \"\",  # Will be set when finalized\n\t\t\t}\n\n\t\t\tcurrent_value_lines.append(value_part)\n\t\telif potential_footer:\n\t\t\t# This is a potential footer that doesn't match our strict regex\n\t\t\t# We'll finalize any current footer and keep track of this invalid one\n\t\t\tfinalize_footer()\n\n\t\t\t# Extract token and value for error reporting\n\t\t\tif \":\" in line_strip:\n\t\t\t\ttoken, value = line_strip.split(\":\", 1)\n\t\t\telse:\n\t\t\t\ttoken, value = line_strip.split(\" #\", 1)\n\n\t\t\ttoken = token.strip()\n\n\t\t\t# Add as an invalid footer for error reporting\n\t\t\tcurrent_footer = {\n\t\t\t\t\"token\": token,\n\t\t\t\t\"separator\": \": \" if \":\" in line_strip else \" #\",\n\t\t\t\t\"value\": value.strip(),\n\t\t\t}\n\t\t\tcurrent_value_lines = [value.strip()]\n\t\t\tfinalize_footer()  # Immediately finalize for error reporting\n\t\telif current_footer:\n\t\t\t# This is a continuation of the current footer value\n\t\t\tcurrent_value_lines.append(line)\n\t\telse:\n\t\t\t# Not a recognized footer line and not in a footer value\n\t\t\t# This will be handled during validation\n\t\t\tpass\n\n\t\ti += 1\n\n\t# Finalize the last footer if any\n\tfinalize_footer()\n\n\treturn footers\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.split_body_footers","title":"split_body_footers","text":"<pre><code>split_body_footers(\n\tbody_and_footers_str: str | None,\n) -&gt; tuple[str | None, str | None]\n</code></pre> <p>Splits the text after the header into body and footers.</p> <p>Parameters:</p> Name Type Description Default <code>body_and_footers_str</code> <code>str | None</code> <p>The string containing both body and footers text, or None.</p> required <p>Returns:</p> Type Description <code>tuple[str | None, str | None]</code> <p>A tuple containing: - First element: The body text as a string, or None if empty/not present - Second element: The footers text as a string, or None if empty/not present</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def split_body_footers(self, body_and_footers_str: str | None) -&gt; tuple[str | None, str | None]:\n\t\"\"\"Splits the text after the header into body and footers.\n\n\tArgs:\n\t    body_and_footers_str: The string containing both body and footers text, or None.\n\n\tReturns:\n\t    A tuple containing:\n\t        - First element: The body text as a string, or None if empty/not present\n\t        - Second element: The footers text as a string, or None if empty/not present\n\t\"\"\"\n\tif not body_and_footers_str:\n\t\treturn None, None\n\n\t# Regular case\n\tblocks_with_separators = re.split(r\"(?&lt;=\\S)(\\r?\\n\\r?\\n)(?=\\S)\", body_and_footers_str)\n\tprocessed_blocks = []\n\ttemp_block = \"\"\n\tfor part in blocks_with_separators:\n\t\ttemp_block += part\n\t\tif temp_block.endswith((\"\\n\\n\", \"\\r\\n\\r\\n\")):\n\t\t\tif temp_block.strip():\n\t\t\t\tprocessed_blocks.append(temp_block)\n\t\t\ttemp_block = \"\"\n\tif temp_block.strip():\n\t\tprocessed_blocks.append(temp_block)\n\n\tif not processed_blocks:\n\t\treturn body_and_footers_str.strip() or None, None\n\n\tfooter_blocks = []\n\tnum_blocks = len(processed_blocks)\n\n\tfor i in range(num_blocks - 1, -1, -1):\n\t\tpotential_footer_block = processed_blocks[i]\n\t\tblock_content_to_check = potential_footer_block.rstrip()\n\t\tlines = block_content_to_check.strip().splitlines()\n\n\t\tis_likely_footer_block = False\n\t\thas_any_footer_token = False\n\t\tif lines:\n\t\t\tis_likely_footer_block = True\n\t\t\tfor _line_idx, line in enumerate(lines):\n\t\t\t\tline_strip = line.strip()\n\t\t\t\tif not line_strip:\n\t\t\t\t\tcontinue\n\t\t\t\tis_potential_footer = self._potential_footer_token_regex.match(line_strip)\n\t\t\t\tis_continuation = line.startswith((\" \", \"\\t\"))\n\t\t\t\tif is_potential_footer:\n\t\t\t\t\thas_any_footer_token = True\n\t\t\t\telif is_continuation:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tis_likely_footer_block = False\n\t\t\t\t\tbreak\n\t\tis_likely_footer_block = is_likely_footer_block and has_any_footer_token\n\n\t\tif is_likely_footer_block:\n\t\t\tfooter_blocks.insert(0, potential_footer_block)\n\t\telse:\n\t\t\tbreak\n\n\tif not footer_blocks:\n\t\treturn body_and_footers_str.strip(), None\n\n\tfooters_str = \"\".join(footer_blocks).strip()\n\tbody_block_count = num_blocks - len(footer_blocks)\n\tbody_str = \"\".join(processed_blocks[:body_block_count]).strip() if body_block_count &gt; 0 else None\n\n\treturn body_str, footers_str\n</code></pre>"},{"location":"api/git/commit_linter/validators/","title":"Validators","text":"<p>Validators for commit message components.</p>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators","title":"CommitValidators","text":"<p>Collection of validator methods for different parts of commit messages.</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>class CommitValidators:\n\t\"\"\"Collection of validator methods for different parts of commit messages.\"\"\"\n\n\t@staticmethod\n\tdef validate_footer_token(token: str) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate a footer token according to the Conventional Commits spec.\n\n\t\tAccording to the spec:\n\t\t1. Tokens MUST use hyphens instead of spaces\n\t\t2. BREAKING CHANGE must be uppercase\n\t\t3. Footer tokens should be ALL UPPERCASE\n\t\t4. Footer tokens should follow format with - for spaces\n\t\t5. No special characters or Unicode (non-ASCII) characters allowed\n\n\t\tReturns:\n\t\t    bool: True if token is valid, False otherwise\n\n\t\t\"\"\"\n\t\t# Check if token is a breaking change token in any case\n\t\tif BREAKING_CHANGE_REGEX.match(token.lower()):\n\t\t\t# If it's a breaking change token, it MUST be uppercase\n\t\t\treturn token in (BREAKING_CHANGE, BREAKING_CHANGE_HYPHEN)\n\n\t\t# Check for special characters (except hyphens which are allowed)\n\t\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,./?\"):\n\t\t\treturn False\n\n\t\t# Check for non-ASCII characters\n\t\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\treturn False\n\n\t\t# Must match valid token pattern (uppercase, alphanumeric with hyphens)\n\t\tif not VALID_FOOTER_TOKEN_REGEX.match(token):\n\t\t\treturn False\n\n\t\t# Check for spaces (must use hyphens instead, except for BREAKING CHANGE)\n\t\treturn not (\" \" in token and token != BREAKING_CHANGE)\n\n\t@staticmethod\n\tdef validate_type_and_scope(type_value: str, scope_value: str | None) -&gt; list[str]:\n\t\t\"\"\"\n\t\tValidate type and scope values according to the spec.\n\n\t\tType must contain only letters.\n\t\tScope must contain only letters, numbers, hyphens, and slashes.\n\t\tBoth must be ASCII-only.\n\n\t\tArgs:\n\t\t    type_value (str): The commit message type\n\t\t    scope_value (str | None): The optional scope\n\n\t\tReturns:\n\t\t    list[str]: List of error messages, empty if valid\n\n\t\t\"\"\"\n\t\terrors = []\n\n\t\t# Check type (no special chars or unicode)\n\t\tif not VALID_TYPE_REGEX.match(type_value):\n\t\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only letters (a-z, A-Z).\")\n\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in type_value):\n\t\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only ASCII characters.\")\n\n\t\t# Check scope (if present)\n\t\tif scope_value is not None:\n\t\t\tif scope_value == \"\":\n\t\t\t\terrors.append(\"Scope cannot be empty when parentheses are used.\")\n\t\t\telif not VALID_SCOPE_REGEX.match(scope_value):\n\t\t\t\terrors.append(\n\t\t\t\t\tf\"Invalid scope '{scope_value}'. Scopes must contain only letters, numbers, hyphens, and slashes.\"\n\t\t\t\t)\n\t\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in scope_value):\n\t\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Scopes must contain only ASCII characters.\")\n\t\t\telif any(c in scope_value for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,. \"):\n\t\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Special characters are not allowed in scopes.\")\n\n\t\treturn errors\n\n\t@staticmethod\n\tdef validate_case(text: str, case_format: str | list[str]) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if the text follows the specified case format.\n\n\t\tArgs:\n\t\t    text (str): The text to validate\n\t\t    case_format (str or list): The case format(s) to check\n\n\t\tReturns:\n\t\t    bool: True if text matches any of the specified case formats\n\n\t\t\"\"\"\n\t\tif isinstance(case_format, list):\n\t\t\treturn any(CommitValidators.validate_case(text, fmt) for fmt in case_format)\n\n\t\t# Get the validator function for the specified case format\n\t\tvalidator = CASE_FORMATS.get(case_format)\n\t\tif not validator:\n\t\t\t# Default to allowing any case if invalid format specified\n\t\t\treturn True\n\n\t\treturn validator(text)\n\n\t@staticmethod\n\tdef validate_length(text: str | None, min_length: int, max_length: float) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text length is between min and max length.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate, or None\n\t\t    min_length (int): Minimum allowed length\n\t\t    max_length (int | float): Maximum allowed length\n\n\t\tReturns:\n\t\t    bool: True if text length is valid, False otherwise\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn min_length == 0\n\n\t\ttext_length = len(text)\n\t\treturn min_length &lt;= text_length &lt; max_length\n\n\t@staticmethod\n\tdef validate_enum(text: str, allowed_values: list[str]) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text is in the allowed values.\n\n\t\tArgs:\n\t\t    text (str): The text to validate\n\t\t    allowed_values (list): The allowed values\n\n\t\tReturns:\n\t\t    bool: True if text is in allowed values, False otherwise\n\n\t\t\"\"\"\n\t\t# Allow any value if no allowed values are specified\n\t\tif not allowed_values:\n\t\t\treturn True\n\n\t\treturn text.lower() in (value.lower() for value in allowed_values)\n\n\t@staticmethod\n\tdef validate_empty(text: str | None, should_be_empty: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text is empty or not based on configuration.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    should_be_empty (bool): True if text should be empty, False if not\n\n\t\tReturns:\n\t\t    bool: True if text empty status matches should_be_empty\n\n\t\t\"\"\"\n\t\tis_empty = text is None or text.strip() == \"\"\n\t\treturn is_empty == should_be_empty\n\n\t@staticmethod\n\tdef validate_ends_with(text: str | None, suffix: str, should_end_with: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text ends with a specific suffix.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    suffix (str): The suffix to check for\n\t\t    should_end_with (bool): True if text should end with suffix\n\n\t\tReturns:\n\t\t    bool: True if text ending matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not should_end_with\n\n\t\tends_with = text.endswith(suffix)\n\t\treturn ends_with == should_end_with\n\n\t@staticmethod\n\tdef validate_starts_with(text: str | None, prefix: str, should_start_with: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text starts with a specific prefix.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    prefix (str): The prefix to check for\n\t\t    should_start_with (bool): True if text should start with prefix\n\n\t\tReturns:\n\t\t    bool: True if text starting matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not should_start_with\n\n\t\tstarts_with = text.startswith(prefix)\n\t\treturn starts_with == should_start_with\n\n\t@staticmethod\n\tdef validate_line_length(text: str | None, max_line_length: float) -&gt; list[int]:\n\t\t\"\"\"\n\t\tValidate line lengths in multiline text.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    max_line_length (int | float): Maximum allowed line length\n\n\t\tReturns:\n\t\t    list: List of line numbers with errors (0-indexed)\n\n\t\t\"\"\"\n\t\tif text is None or max_line_length == float(\"inf\"):\n\t\t\treturn []\n\n\t\tlines = text.splitlines()\n\t\treturn [i for i, line in enumerate(lines) if len(line) &gt; max_line_length]\n\n\t@staticmethod\n\tdef validate_leading_blank(text: str | None, required_blank: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text starts with a blank line.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    required_blank (bool): True if text should start with blank line\n\n\t\tReturns:\n\t\t    bool: True if text leading blank matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not required_blank\n\n\t\tlines = text.splitlines()\n\t\thas_leading_blank = len(lines) &gt; 0 and (len(lines) == 1 or not lines[0].strip())\n\t\treturn has_leading_blank == required_blank\n\n\t@staticmethod\n\tdef validate_trim(text: str | None) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text has no leading/trailing whitespace.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\n\t\tReturns:\n\t\t    bool: True if text has no leading/trailing whitespace\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn True\n\n\t\treturn text == text.strip()\n\n\t@staticmethod\n\tdef validate_contains(text: str | None, substring: str, should_contain: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text contains a specific substring.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    substring (str): The substring to check for\n\t\t    should_contain (bool): True if text should contain substring\n\n\t\tReturns:\n\t\t    bool: True if text contains substring matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not should_contain\n\n\t\tcontains = substring in text\n\t\treturn contains == should_contain\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_footer_token","title":"validate_footer_token  <code>staticmethod</code>","text":"<pre><code>validate_footer_token(token: str) -&gt; bool\n</code></pre> <p>Validate a footer token according to the Conventional Commits spec.</p> <p>According to the spec: 1. Tokens MUST use hyphens instead of spaces 2. BREAKING CHANGE must be uppercase 3. Footer tokens should be ALL UPPERCASE 4. Footer tokens should follow format with - for spaces 5. No special characters or Unicode (non-ASCII) characters allowed</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if token is valid, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_footer_token(token: str) -&gt; bool:\n\t\"\"\"\n\tValidate a footer token according to the Conventional Commits spec.\n\n\tAccording to the spec:\n\t1. Tokens MUST use hyphens instead of spaces\n\t2. BREAKING CHANGE must be uppercase\n\t3. Footer tokens should be ALL UPPERCASE\n\t4. Footer tokens should follow format with - for spaces\n\t5. No special characters or Unicode (non-ASCII) characters allowed\n\n\tReturns:\n\t    bool: True if token is valid, False otherwise\n\n\t\"\"\"\n\t# Check if token is a breaking change token in any case\n\tif BREAKING_CHANGE_REGEX.match(token.lower()):\n\t\t# If it's a breaking change token, it MUST be uppercase\n\t\treturn token in (BREAKING_CHANGE, BREAKING_CHANGE_HYPHEN)\n\n\t# Check for special characters (except hyphens which are allowed)\n\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,./?\"):\n\t\treturn False\n\n\t# Check for non-ASCII characters\n\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\treturn False\n\n\t# Must match valid token pattern (uppercase, alphanumeric with hyphens)\n\tif not VALID_FOOTER_TOKEN_REGEX.match(token):\n\t\treturn False\n\n\t# Check for spaces (must use hyphens instead, except for BREAKING CHANGE)\n\treturn not (\" \" in token and token != BREAKING_CHANGE)\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_type_and_scope","title":"validate_type_and_scope  <code>staticmethod</code>","text":"<pre><code>validate_type_and_scope(\n\ttype_value: str, scope_value: str | None\n) -&gt; list[str]\n</code></pre> <p>Validate type and scope values according to the spec.</p> <p>Type must contain only letters. Scope must contain only letters, numbers, hyphens, and slashes. Both must be ASCII-only.</p> <p>Parameters:</p> Name Type Description Default <code>type_value</code> <code>str</code> <p>The commit message type</p> required <code>scope_value</code> <code>str | None</code> <p>The optional scope</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of error messages, empty if valid</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_type_and_scope(type_value: str, scope_value: str | None) -&gt; list[str]:\n\t\"\"\"\n\tValidate type and scope values according to the spec.\n\n\tType must contain only letters.\n\tScope must contain only letters, numbers, hyphens, and slashes.\n\tBoth must be ASCII-only.\n\n\tArgs:\n\t    type_value (str): The commit message type\n\t    scope_value (str | None): The optional scope\n\n\tReturns:\n\t    list[str]: List of error messages, empty if valid\n\n\t\"\"\"\n\terrors = []\n\n\t# Check type (no special chars or unicode)\n\tif not VALID_TYPE_REGEX.match(type_value):\n\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only letters (a-z, A-Z).\")\n\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in type_value):\n\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only ASCII characters.\")\n\n\t# Check scope (if present)\n\tif scope_value is not None:\n\t\tif scope_value == \"\":\n\t\t\terrors.append(\"Scope cannot be empty when parentheses are used.\")\n\t\telif not VALID_SCOPE_REGEX.match(scope_value):\n\t\t\terrors.append(\n\t\t\t\tf\"Invalid scope '{scope_value}'. Scopes must contain only letters, numbers, hyphens, and slashes.\"\n\t\t\t)\n\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in scope_value):\n\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Scopes must contain only ASCII characters.\")\n\t\telif any(c in scope_value for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,. \"):\n\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Special characters are not allowed in scopes.\")\n\n\treturn errors\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_case","title":"validate_case  <code>staticmethod</code>","text":"<pre><code>validate_case(\n\ttext: str, case_format: str | list[str]\n) -&gt; bool\n</code></pre> <p>Validate if the text follows the specified case format.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to validate</p> required <code>case_format</code> <code>str or list</code> <p>The case format(s) to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text matches any of the specified case formats</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_case(text: str, case_format: str | list[str]) -&gt; bool:\n\t\"\"\"\n\tValidate if the text follows the specified case format.\n\n\tArgs:\n\t    text (str): The text to validate\n\t    case_format (str or list): The case format(s) to check\n\n\tReturns:\n\t    bool: True if text matches any of the specified case formats\n\n\t\"\"\"\n\tif isinstance(case_format, list):\n\t\treturn any(CommitValidators.validate_case(text, fmt) for fmt in case_format)\n\n\t# Get the validator function for the specified case format\n\tvalidator = CASE_FORMATS.get(case_format)\n\tif not validator:\n\t\t# Default to allowing any case if invalid format specified\n\t\treturn True\n\n\treturn validator(text)\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_length","title":"validate_length  <code>staticmethod</code>","text":"<pre><code>validate_length(\n\ttext: str | None, min_length: int, max_length: float\n) -&gt; bool\n</code></pre> <p>Validate if text length is between min and max length.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate, or None</p> required <code>min_length</code> <code>int</code> <p>Minimum allowed length</p> required <code>max_length</code> <code>int | float</code> <p>Maximum allowed length</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text length is valid, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_length(text: str | None, min_length: int, max_length: float) -&gt; bool:\n\t\"\"\"\n\tValidate if text length is between min and max length.\n\n\tArgs:\n\t    text (str | None): The text to validate, or None\n\t    min_length (int): Minimum allowed length\n\t    max_length (int | float): Maximum allowed length\n\n\tReturns:\n\t    bool: True if text length is valid, False otherwise\n\n\t\"\"\"\n\tif text is None:\n\t\treturn min_length == 0\n\n\ttext_length = len(text)\n\treturn min_length &lt;= text_length &lt; max_length\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_enum","title":"validate_enum  <code>staticmethod</code>","text":"<pre><code>validate_enum(text: str, allowed_values: list[str]) -&gt; bool\n</code></pre> <p>Validate if text is in the allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to validate</p> required <code>allowed_values</code> <code>list</code> <p>The allowed values</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text is in allowed values, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_enum(text: str, allowed_values: list[str]) -&gt; bool:\n\t\"\"\"\n\tValidate if text is in the allowed values.\n\n\tArgs:\n\t    text (str): The text to validate\n\t    allowed_values (list): The allowed values\n\n\tReturns:\n\t    bool: True if text is in allowed values, False otherwise\n\n\t\"\"\"\n\t# Allow any value if no allowed values are specified\n\tif not allowed_values:\n\t\treturn True\n\n\treturn text.lower() in (value.lower() for value in allowed_values)\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_empty","title":"validate_empty  <code>staticmethod</code>","text":"<pre><code>validate_empty(\n\ttext: str | None, should_be_empty: bool\n) -&gt; bool\n</code></pre> <p>Validate if text is empty or not based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>should_be_empty</code> <code>bool</code> <p>True if text should be empty, False if not</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text empty status matches should_be_empty</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_empty(text: str | None, should_be_empty: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text is empty or not based on configuration.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    should_be_empty (bool): True if text should be empty, False if not\n\n\tReturns:\n\t    bool: True if text empty status matches should_be_empty\n\n\t\"\"\"\n\tis_empty = text is None or text.strip() == \"\"\n\treturn is_empty == should_be_empty\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_ends_with","title":"validate_ends_with  <code>staticmethod</code>","text":"<pre><code>validate_ends_with(\n\ttext: str | None, suffix: str, should_end_with: bool\n) -&gt; bool\n</code></pre> <p>Validate if text ends with a specific suffix.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>suffix</code> <code>str</code> <p>The suffix to check for</p> required <code>should_end_with</code> <code>bool</code> <p>True if text should end with suffix</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text ending matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_ends_with(text: str | None, suffix: str, should_end_with: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text ends with a specific suffix.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    suffix (str): The suffix to check for\n\t    should_end_with (bool): True if text should end with suffix\n\n\tReturns:\n\t    bool: True if text ending matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not should_end_with\n\n\tends_with = text.endswith(suffix)\n\treturn ends_with == should_end_with\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_starts_with","title":"validate_starts_with  <code>staticmethod</code>","text":"<pre><code>validate_starts_with(\n\ttext: str | None, prefix: str, should_start_with: bool\n) -&gt; bool\n</code></pre> <p>Validate if text starts with a specific prefix.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>prefix</code> <code>str</code> <p>The prefix to check for</p> required <code>should_start_with</code> <code>bool</code> <p>True if text should start with prefix</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text starting matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_starts_with(text: str | None, prefix: str, should_start_with: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text starts with a specific prefix.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    prefix (str): The prefix to check for\n\t    should_start_with (bool): True if text should start with prefix\n\n\tReturns:\n\t    bool: True if text starting matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not should_start_with\n\n\tstarts_with = text.startswith(prefix)\n\treturn starts_with == should_start_with\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_line_length","title":"validate_line_length  <code>staticmethod</code>","text":"<pre><code>validate_line_length(\n\ttext: str | None, max_line_length: float\n) -&gt; list[int]\n</code></pre> <p>Validate line lengths in multiline text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>max_line_length</code> <code>int | float</code> <p>Maximum allowed line length</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[int]</code> <p>List of line numbers with errors (0-indexed)</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_line_length(text: str | None, max_line_length: float) -&gt; list[int]:\n\t\"\"\"\n\tValidate line lengths in multiline text.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    max_line_length (int | float): Maximum allowed line length\n\n\tReturns:\n\t    list: List of line numbers with errors (0-indexed)\n\n\t\"\"\"\n\tif text is None or max_line_length == float(\"inf\"):\n\t\treturn []\n\n\tlines = text.splitlines()\n\treturn [i for i, line in enumerate(lines) if len(line) &gt; max_line_length]\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_leading_blank","title":"validate_leading_blank  <code>staticmethod</code>","text":"<pre><code>validate_leading_blank(\n\ttext: str | None, required_blank: bool\n) -&gt; bool\n</code></pre> <p>Validate if text starts with a blank line.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>required_blank</code> <code>bool</code> <p>True if text should start with blank line</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text leading blank matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_leading_blank(text: str | None, required_blank: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text starts with a blank line.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    required_blank (bool): True if text should start with blank line\n\n\tReturns:\n\t    bool: True if text leading blank matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not required_blank\n\n\tlines = text.splitlines()\n\thas_leading_blank = len(lines) &gt; 0 and (len(lines) == 1 or not lines[0].strip())\n\treturn has_leading_blank == required_blank\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_trim","title":"validate_trim  <code>staticmethod</code>","text":"<pre><code>validate_trim(text: str | None) -&gt; bool\n</code></pre> <p>Validate if text has no leading/trailing whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text has no leading/trailing whitespace</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_trim(text: str | None) -&gt; bool:\n\t\"\"\"\n\tValidate if text has no leading/trailing whitespace.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\n\tReturns:\n\t    bool: True if text has no leading/trailing whitespace\n\n\t\"\"\"\n\tif text is None:\n\t\treturn True\n\n\treturn text == text.strip()\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_contains","title":"validate_contains  <code>staticmethod</code>","text":"<pre><code>validate_contains(\n\ttext: str | None, substring: str, should_contain: bool\n) -&gt; bool\n</code></pre> <p>Validate if text contains a specific substring.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>substring</code> <code>str</code> <p>The substring to check for</p> required <code>should_contain</code> <code>bool</code> <p>True if text should contain substring</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text contains substring matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_contains(text: str | None, substring: str, should_contain: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text contains a specific substring.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    substring (str): The substring to check for\n\t    should_contain (bool): True if text should contain substring\n\n\tReturns:\n\t    bool: True if text contains substring matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not should_contain\n\n\tcontains = substring in text\n\treturn contains == should_contain\n</code></pre>"},{"location":"api/git/diff_splitter/","title":"Diff Splitter Overview","text":"<p>Diff splitting package for CodeMap.</p> <ul> <li>Constants - Constants for diff splitting functionality.</li> <li>Schemas - Schema definitions for diff splitting.</li> <li>Splitter - Diff splitting implementation for CodeMap.</li> <li>Strategies - Strategies for splitting git diffs into logical chunks.</li> <li>Utils - Utility functions for diff splitting.</li> </ul>"},{"location":"api/git/diff_splitter/constants/","title":"Constants","text":"<p>Constants for diff splitting functionality.</p>"},{"location":"api/git/diff_splitter/constants/#codemap.git.diff_splitter.constants.MIN_NAME_LENGTH_FOR_SIMILARITY","title":"MIN_NAME_LENGTH_FOR_SIMILARITY  <code>module-attribute</code>","text":"<pre><code>MIN_NAME_LENGTH_FOR_SIMILARITY: Final = 3\n</code></pre>"},{"location":"api/git/diff_splitter/constants/#codemap.git.diff_splitter.constants.EPSILON","title":"EPSILON  <code>module-attribute</code>","text":"<pre><code>EPSILON = 1e-10\n</code></pre>"},{"location":"api/git/diff_splitter/constants/#codemap.git.diff_splitter.constants.MAX_FILES_PER_GROUP","title":"MAX_FILES_PER_GROUP  <code>module-attribute</code>","text":"<pre><code>MAX_FILES_PER_GROUP: Final = 10\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/","title":"Schemas","text":"<p>Schema definitions for diff splitting.</p>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk","title":"DiffChunk  <code>dataclass</code>","text":"<p>Represents a logical chunk of changes.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>@dataclass\nclass DiffChunk:\n\t\"\"\"Represents a logical chunk of changes.\"\"\"\n\n\tfiles: list[str] = field(default_factory=list)\n\tcontent: str = \"\"\n\tdescription: str | None = None\n\tembedding: list[float] | None = None\n\tis_move: bool = False  # Indicates if this chunk represents a file move operation\n\tis_llm_generated: bool = False\n\tfiltered_files: list[str] | None = None\n\n\tdef __post_init__(self) -&gt; None:\n\t\t\"\"\"Initialize default values.\"\"\"\n\t\tif self.filtered_files is None:\n\t\t\tself.filtered_files = []\n\n\tdef __hash__(self) -&gt; int:\n\t\t\"\"\"\n\t\tMake DiffChunk hashable by using the object's id.\n\n\t\tReturns:\n\t\t        Hash value based on the object's id\n\n\t\t\"\"\"\n\t\treturn hash(id(self))\n\n\tdef __eq__(self, other: object) -&gt; bool:\n\t\t\"\"\"\n\t\tCompare DiffChunk objects for equality.\n\n\t\tArgs:\n\t\t        other: Another object to compare with\n\n\t\tReturns:\n\t\t        True if the objects are the same instance, False otherwise\n\n\t\t\"\"\"\n\t\tif not isinstance(other, DiffChunk):\n\t\t\treturn False\n\t\treturn id(self) == id(other)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__init__","title":"__init__","text":"<pre><code>__init__(\n\tfiles: list[str] = list(),\n\tcontent: str = \"\",\n\tdescription: str | None = None,\n\tembedding: list[float] | None = None,\n\tis_move: bool = False,\n\tis_llm_generated: bool = False,\n\tfiltered_files: list[str] | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.files","title":"files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>files: list[str] = field(default_factory=list)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: str = ''\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.embedding","title":"embedding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>embedding: list[float] | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.is_move","title":"is_move  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_move: bool = False\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.is_llm_generated","title":"is_llm_generated  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_llm_generated: bool = False\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.filtered_files","title":"filtered_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filtered_files: list[str] | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Initialize default values.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\t\"\"\"Initialize default values.\"\"\"\n\tif self.filtered_files is None:\n\t\tself.filtered_files = []\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Make DiffChunk hashable by using the object's id.</p> <p>Returns:</p> Type Description <code>int</code> <p>Hash value based on the object's id</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def __hash__(self) -&gt; int:\n\t\"\"\"\n\tMake DiffChunk hashable by using the object's id.\n\n\tReturns:\n\t        Hash value based on the object's id\n\n\t\"\"\"\n\treturn hash(id(self))\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: object) -&gt; bool\n</code></pre> <p>Compare DiffChunk objects for equality.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>Another object to compare with</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the objects are the same instance, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n\t\"\"\"\n\tCompare DiffChunk objects for equality.\n\n\tArgs:\n\t        other: Another object to compare with\n\n\tReturns:\n\t        True if the objects are the same instance, False otherwise\n\n\t\"\"\"\n\tif not isinstance(other, DiffChunk):\n\t\treturn False\n\treturn id(self) == id(other)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData","title":"DiffChunkData  <code>dataclass</code>","text":"<p>Dictionary-based representation of a DiffChunk for serialization.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>@dataclass\nclass DiffChunkData:\n\t\"\"\"Dictionary-based representation of a DiffChunk for serialization.\"\"\"\n\n\tfiles: list[str]\n\tcontent: str\n\tdescription: str | None = None\n\tis_llm_generated: bool = False\n\tfiltered_files: list[str] | None = None\n\tis_move: bool = False  # Indicates if this chunk represents a file move operation\n\n\t@classmethod\n\tdef from_chunk(cls, chunk: DiffChunk) -&gt; \"DiffChunkData\":\n\t\t\"\"\"Create a DiffChunkData from a DiffChunk.\"\"\"\n\t\treturn cls(\n\t\t\tfiles=chunk.files,\n\t\t\tcontent=chunk.content,\n\t\t\tdescription=chunk.description,\n\t\t\tis_llm_generated=chunk.is_llm_generated,\n\t\t\tfiltered_files=chunk.filtered_files,\n\t\t\tis_move=getattr(chunk, \"is_move\", False),\n\t\t)\n\n\tdef to_chunk(self) -&gt; DiffChunk:\n\t\t\"\"\"Convert DiffChunkData to a DiffChunk.\"\"\"\n\t\treturn DiffChunk(\n\t\t\tfiles=self.files,\n\t\t\tcontent=self.content,\n\t\t\tdescription=self.description,\n\t\t\tis_llm_generated=self.is_llm_generated,\n\t\t\tfiltered_files=self.filtered_files,\n\t\t\tis_move=self.is_move,\n\t\t)\n\n\tdef to_dict(self) -&gt; dict[str, Any]:\n\t\t\"\"\"Convert to a dictionary.\"\"\"\n\t\treturn {\n\t\t\t\"files\": self.files,\n\t\t\t\"content\": self.content,\n\t\t\t\"description\": self.description,\n\t\t\t\"is_llm_generated\": self.is_llm_generated,\n\t\t\t\"filtered_files\": self.filtered_files,\n\t\t\t\"is_move\": self.is_move,\n\t\t}\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.__init__","title":"__init__","text":"<pre><code>__init__(\n\tfiles: list[str],\n\tcontent: str,\n\tdescription: str | None = None,\n\tis_llm_generated: bool = False,\n\tfiltered_files: list[str] | None = None,\n\tis_move: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: list[str]\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.is_llm_generated","title":"is_llm_generated  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_llm_generated: bool = False\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.filtered_files","title":"filtered_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filtered_files: list[str] | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.is_move","title":"is_move  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_move: bool = False\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.from_chunk","title":"from_chunk  <code>classmethod</code>","text":"<pre><code>from_chunk(chunk: DiffChunk) -&gt; DiffChunkData\n</code></pre> <p>Create a DiffChunkData from a DiffChunk.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>@classmethod\ndef from_chunk(cls, chunk: DiffChunk) -&gt; \"DiffChunkData\":\n\t\"\"\"Create a DiffChunkData from a DiffChunk.\"\"\"\n\treturn cls(\n\t\tfiles=chunk.files,\n\t\tcontent=chunk.content,\n\t\tdescription=chunk.description,\n\t\tis_llm_generated=chunk.is_llm_generated,\n\t\tfiltered_files=chunk.filtered_files,\n\t\tis_move=getattr(chunk, \"is_move\", False),\n\t)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.to_chunk","title":"to_chunk","text":"<pre><code>to_chunk() -&gt; DiffChunk\n</code></pre> <p>Convert DiffChunkData to a DiffChunk.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def to_chunk(self) -&gt; DiffChunk:\n\t\"\"\"Convert DiffChunkData to a DiffChunk.\"\"\"\n\treturn DiffChunk(\n\t\tfiles=self.files,\n\t\tcontent=self.content,\n\t\tdescription=self.description,\n\t\tis_llm_generated=self.is_llm_generated,\n\t\tfiltered_files=self.filtered_files,\n\t\tis_move=self.is_move,\n\t)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to a dictionary.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n\t\"\"\"Convert to a dictionary.\"\"\"\n\treturn {\n\t\t\"files\": self.files,\n\t\t\"content\": self.content,\n\t\t\"description\": self.description,\n\t\t\"is_llm_generated\": self.is_llm_generated,\n\t\t\"filtered_files\": self.filtered_files,\n\t\t\"is_move\": self.is_move,\n\t}\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/","title":"Splitter","text":"<p>Diff splitting implementation for CodeMap.</p>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.MAX_DIFF_CONTENT_LENGTH","title":"MAX_DIFF_CONTENT_LENGTH  <code>module-attribute</code>","text":"<pre><code>MAX_DIFF_CONTENT_LENGTH = 100000\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.MAX_DIFF_LINES","title":"MAX_DIFF_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_DIFF_LINES = 1000\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.SMALL_SECTION_SIZE","title":"SMALL_SECTION_SIZE  <code>module-attribute</code>","text":"<pre><code>SMALL_SECTION_SIZE = 50\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.COMPLEX_SECTION_SIZE","title":"COMPLEX_SECTION_SIZE  <code>module-attribute</code>","text":"<pre><code>COMPLEX_SECTION_SIZE = 100\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter","title":"DiffSplitter","text":"<p>Splits Git diffs into logical chunks.</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>class DiffSplitter:\n\t\"\"\"Splits Git diffs into logical chunks.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: \"ConfigLoader | None\" = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the diff splitter.\n\n\t\tArgs:\n\t\t    config_loader: ConfigLoader object for loading configuration\n\t\t\"\"\"\n\t\tif config_loader:\n\t\t\tself.config_loader = config_loader\n\t\telse:\n\t\t\tfrom codemap.config import ConfigLoader  # Import locally\n\n\t\t\tself.config_loader = ConfigLoader.get_instance()\n\n\t\tif self.config_loader.get.repo_root is None:\n\t\t\tself.repo_root = ExtendedGitRepoContext.get_repo_root()\n\t\telse:\n\t\t\tself.repo_root = self.config_loader.get.repo_root\n\n\t\t# Get config for diff_splitter, fallback to empty dict if not found\n\t\tds_config = self.config_loader.get.commit.diff_splitter\n\n\t\t# Determine parameters: CLI/direct arg &gt; Config file &gt; DEFAULT_CONFIG\n\t\tself.similarity_threshold = ds_config.similarity_threshold\n\t\tself.directory_similarity_threshold = ds_config.directory_similarity_threshold\n\t\tself.min_chunks_for_consolidation = ds_config.min_chunks_for_consolidation\n\t\tself.max_chunks_before_consolidation = ds_config.max_chunks_before_consolidation\n\t\tself.max_file_size_for_llm = ds_config.max_file_size_for_llm\n\t\tself.max_log_diff_size = ds_config.max_log_diff_size\n\n\tasync def split_diff(self, diff: GitDiff) -&gt; tuple[list[DiffChunk], list[str]]:\n\t\t\"\"\"\n\t\tSplit a diff into logical chunks using semantic splitting.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    Tuple of (List of DiffChunk objects based on semantic analysis, List of filtered large files)\n\n\t\tRaises:\n\t\t    ValueError: If semantic splitting is not available or fails\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\treturn [], []\n\n\t\t# Special handling for untracked files - bypass semantic split since the content isn't a proper diff format\n\t\tif diff.is_untracked:\n\t\t\tlogger.debug(\"Processing untracked files with special handling: %d files\", len(diff.files))\n\t\t\t# Create a simple chunk per file to avoid errors with unidiff parsing\n\t\t\tchunks = []\n\t\t\tfor file_path in diff.files:\n\t\t\t\t# Create a basic chunk with file info but without trying to parse the content as a diff\n\t\t\t\tchunks = [\n\t\t\t\t\tDiffChunk(\n\t\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\t\tcontent=f\"New untracked file: {file_path}\",\n\t\t\t\t\t\tdescription=f\"New file: {file_path}\",\n\t\t\t\t\t)\n\t\t\t\t\tfor file_path in diff.files\n\t\t\t\t]\n\t\t\treturn chunks, []\n\n\t\t# In test environments, log the diff content for debugging\n\t\tif is_test_environment():\n\t\t\tlogger.debug(\"Processing diff in test environment with %d files\", len(diff.files) if diff.files else 0)\n\t\t\tif diff.content and len(diff.content) &lt; self.max_log_diff_size:  # Use configured max log size\n\t\t\t\tlogger.debug(\"Diff content: %s\", diff.content)\n\n\t\t# Process files in the diff\n\t\tif diff.files:\n\t\t\t# Filter for valid files (existence, tracked status), max_size check removed here\n\t\t\tlogger.debug(f\"DiffSplitter.split_diff: Files before filter_valid_files: {diff.files}\")\n\t\t\tdiff.files, _ = filter_valid_files(diff.files, self.repo_root, is_test_environment())\n\t\t\tlogger.debug(f\"DiffSplitter.split_diff: Files after filter_valid_files: {diff.files}\")\n\t\t\t# filtered_large_files list is no longer populated or used here\n\n\t\tif not diff.files:\n\t\t\tlogger.warning(\"No valid files to process after filtering\")\n\t\t\treturn [], []  # Return empty lists\n\n\t\ttry:\n\t\t\tsemantic_strategy = SemanticSplitStrategy(config_loader=self.config_loader)\n\t\t\tchunks = await semantic_strategy.split(diff)\n\n\t\t\t# If we truncated the content, restore the original content for the actual chunks\n\t\t\tif diff.content and chunks:\n\t\t\t\t# Create a mapping of file paths to chunks for quick lookup\n\t\t\t\tchunks_by_file = {}\n\t\t\t\tfor chunk in chunks:\n\t\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\t\tif file_path not in chunks_by_file:\n\t\t\t\t\t\t\tchunks_by_file[file_path] = []\n\t\t\t\t\t\tchunks_by_file[file_path].append(chunk)\n\n\t\t\t\t# For chunks that represent files we can find in the original content,\n\t\t\t\t# update their content to include the full original diff for that file\n\t\t\t\tfor chunk in chunks:\n\t\t\t\t\t# Use a heuristic to match file sections in the original content\n\t\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\t\tfile_marker = f\"diff --git a/{file_path} b/{file_path}\"\n\t\t\t\t\t\tif isinstance(diff.content, str) and file_marker in diff.content:\n\t\t\t\t\t\t\t# Found a match for this file in the original content\n\t\t\t\t\t\t\t# Extract that file's complete diff section\n\t\t\t\t\t\t\tstart_idx = diff.content.find(file_marker)\n\t\t\t\t\t\t\tend_idx = diff.content.find(\"diff --git\", start_idx + len(file_marker))\n\t\t\t\t\t\t\tif end_idx == -1:  # Last file in the diff\n\t\t\t\t\t\t\t\tend_idx = len(diff.content)\n\n\t\t\t\t\t\t\tfile_diff = diff.content[start_idx:end_idx].strip()\n\n\t\t\t\t\t\t\t# Now replace just this file's content in the chunk\n\t\t\t\t\t\t\t# This is a heuristic that may need adjustment based on your diff format\n\t\t\t\t\t\t\tif chunk.content and isinstance(chunk.content, str) and file_marker in chunk.content:\n\t\t\t\t\t\t\t\tchunk_start = chunk.content.find(file_marker)\n\t\t\t\t\t\t\t\tchunk_end = chunk.content.find(\"diff --git\", chunk_start + len(file_marker))\n\t\t\t\t\t\t\t\tif chunk_end == -1:  # Last file in the chunk\n\t\t\t\t\t\t\t\t\tchunk_end = len(chunk.content)\n\n\t\t\t\t\t\t\t\t# Replace this file's truncated diff with the full diff\n\t\t\t\t\t\t\t\tchunk.content = chunk.content[:chunk_start] + file_diff + chunk.content[chunk_end:]\n\n\t\t\treturn chunks, []\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Semantic splitting failed\")\n\n\t\t\t# Try basic splitting as a fallback\n\t\t\tlogger.warning(\"Falling back to basic file splitting\")\n\t\t\t# Return empty list for filtered_large_files as it's no longer tracked here\n\t\t\treturn await self._create_basic_file_chunk(diff), []\n\n\tasync def _create_basic_file_chunk(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tCreate a basic chunk per file without semantic analysis.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects, one per file\n\n\t\t\"\"\"\n\t\tchunks = []\n\n\t\tif diff.files:\n\t\t\t# Create a basic chunk, one per file in this strategy, no semantic grouping\n\t\t\tstrategy = FileSplitStrategy()\n\t\t\tchunks = await strategy.split(diff)\n\n\t\treturn chunks\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.__init__","title":"__init__","text":"<pre><code>__init__(config_loader: ConfigLoader | None = None) -&gt; None\n</code></pre> <p>Initialize the diff splitter.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader | None</code> <p>ConfigLoader object for loading configuration</p> <code>None</code> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: \"ConfigLoader | None\" = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the diff splitter.\n\n\tArgs:\n\t    config_loader: ConfigLoader object for loading configuration\n\t\"\"\"\n\tif config_loader:\n\t\tself.config_loader = config_loader\n\telse:\n\t\tfrom codemap.config import ConfigLoader  # Import locally\n\n\t\tself.config_loader = ConfigLoader.get_instance()\n\n\tif self.config_loader.get.repo_root is None:\n\t\tself.repo_root = ExtendedGitRepoContext.get_repo_root()\n\telse:\n\t\tself.repo_root = self.config_loader.get.repo_root\n\n\t# Get config for diff_splitter, fallback to empty dict if not found\n\tds_config = self.config_loader.get.commit.diff_splitter\n\n\t# Determine parameters: CLI/direct arg &gt; Config file &gt; DEFAULT_CONFIG\n\tself.similarity_threshold = ds_config.similarity_threshold\n\tself.directory_similarity_threshold = ds_config.directory_similarity_threshold\n\tself.min_chunks_for_consolidation = ds_config.min_chunks_for_consolidation\n\tself.max_chunks_before_consolidation = ds_config.max_chunks_before_consolidation\n\tself.max_file_size_for_llm = ds_config.max_file_size_for_llm\n\tself.max_log_diff_size = ds_config.max_log_diff_size\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = get_repo_root()\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.similarity_threshold","title":"similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.directory_similarity_threshold","title":"directory_similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>directory_similarity_threshold = (\n\tdirectory_similarity_threshold\n)\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.min_chunks_for_consolidation","title":"min_chunks_for_consolidation  <code>instance-attribute</code>","text":"<pre><code>min_chunks_for_consolidation = min_chunks_for_consolidation\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.max_chunks_before_consolidation","title":"max_chunks_before_consolidation  <code>instance-attribute</code>","text":"<pre><code>max_chunks_before_consolidation = (\n\tmax_chunks_before_consolidation\n)\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.max_file_size_for_llm","title":"max_file_size_for_llm  <code>instance-attribute</code>","text":"<pre><code>max_file_size_for_llm = max_file_size_for_llm\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.max_log_diff_size","title":"max_log_diff_size  <code>instance-attribute</code>","text":"<pre><code>max_log_diff_size = max_log_diff_size\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.split_diff","title":"split_diff  <code>async</code>","text":"<pre><code>split_diff(\n\tdiff: GitDiff,\n) -&gt; tuple[list[DiffChunk], list[str]]\n</code></pre> <p>Split a diff into logical chunks using semantic splitting.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>tuple[list[DiffChunk], list[str]]</code> <p>Tuple of (List of DiffChunk objects based on semantic analysis, List of filtered large files)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If semantic splitting is not available or fails</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>async def split_diff(self, diff: GitDiff) -&gt; tuple[list[DiffChunk], list[str]]:\n\t\"\"\"\n\tSplit a diff into logical chunks using semantic splitting.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    Tuple of (List of DiffChunk objects based on semantic analysis, List of filtered large files)\n\n\tRaises:\n\t    ValueError: If semantic splitting is not available or fails\n\n\t\"\"\"\n\tif not diff.files:\n\t\treturn [], []\n\n\t# Special handling for untracked files - bypass semantic split since the content isn't a proper diff format\n\tif diff.is_untracked:\n\t\tlogger.debug(\"Processing untracked files with special handling: %d files\", len(diff.files))\n\t\t# Create a simple chunk per file to avoid errors with unidiff parsing\n\t\tchunks = []\n\t\tfor file_path in diff.files:\n\t\t\t# Create a basic chunk with file info but without trying to parse the content as a diff\n\t\t\tchunks = [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=f\"New untracked file: {file_path}\",\n\t\t\t\t\tdescription=f\"New file: {file_path}\",\n\t\t\t\t)\n\t\t\t\tfor file_path in diff.files\n\t\t\t]\n\t\treturn chunks, []\n\n\t# In test environments, log the diff content for debugging\n\tif is_test_environment():\n\t\tlogger.debug(\"Processing diff in test environment with %d files\", len(diff.files) if diff.files else 0)\n\t\tif diff.content and len(diff.content) &lt; self.max_log_diff_size:  # Use configured max log size\n\t\t\tlogger.debug(\"Diff content: %s\", diff.content)\n\n\t# Process files in the diff\n\tif diff.files:\n\t\t# Filter for valid files (existence, tracked status), max_size check removed here\n\t\tlogger.debug(f\"DiffSplitter.split_diff: Files before filter_valid_files: {diff.files}\")\n\t\tdiff.files, _ = filter_valid_files(diff.files, self.repo_root, is_test_environment())\n\t\tlogger.debug(f\"DiffSplitter.split_diff: Files after filter_valid_files: {diff.files}\")\n\t\t# filtered_large_files list is no longer populated or used here\n\n\tif not diff.files:\n\t\tlogger.warning(\"No valid files to process after filtering\")\n\t\treturn [], []  # Return empty lists\n\n\ttry:\n\t\tsemantic_strategy = SemanticSplitStrategy(config_loader=self.config_loader)\n\t\tchunks = await semantic_strategy.split(diff)\n\n\t\t# If we truncated the content, restore the original content for the actual chunks\n\t\tif diff.content and chunks:\n\t\t\t# Create a mapping of file paths to chunks for quick lookup\n\t\t\tchunks_by_file = {}\n\t\t\tfor chunk in chunks:\n\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\tif file_path not in chunks_by_file:\n\t\t\t\t\t\tchunks_by_file[file_path] = []\n\t\t\t\t\tchunks_by_file[file_path].append(chunk)\n\n\t\t\t# For chunks that represent files we can find in the original content,\n\t\t\t# update their content to include the full original diff for that file\n\t\t\tfor chunk in chunks:\n\t\t\t\t# Use a heuristic to match file sections in the original content\n\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\tfile_marker = f\"diff --git a/{file_path} b/{file_path}\"\n\t\t\t\t\tif isinstance(diff.content, str) and file_marker in diff.content:\n\t\t\t\t\t\t# Found a match for this file in the original content\n\t\t\t\t\t\t# Extract that file's complete diff section\n\t\t\t\t\t\tstart_idx = diff.content.find(file_marker)\n\t\t\t\t\t\tend_idx = diff.content.find(\"diff --git\", start_idx + len(file_marker))\n\t\t\t\t\t\tif end_idx == -1:  # Last file in the diff\n\t\t\t\t\t\t\tend_idx = len(diff.content)\n\n\t\t\t\t\t\tfile_diff = diff.content[start_idx:end_idx].strip()\n\n\t\t\t\t\t\t# Now replace just this file's content in the chunk\n\t\t\t\t\t\t# This is a heuristic that may need adjustment based on your diff format\n\t\t\t\t\t\tif chunk.content and isinstance(chunk.content, str) and file_marker in chunk.content:\n\t\t\t\t\t\t\tchunk_start = chunk.content.find(file_marker)\n\t\t\t\t\t\t\tchunk_end = chunk.content.find(\"diff --git\", chunk_start + len(file_marker))\n\t\t\t\t\t\t\tif chunk_end == -1:  # Last file in the chunk\n\t\t\t\t\t\t\t\tchunk_end = len(chunk.content)\n\n\t\t\t\t\t\t\t# Replace this file's truncated diff with the full diff\n\t\t\t\t\t\t\tchunk.content = chunk.content[:chunk_start] + file_diff + chunk.content[chunk_end:]\n\n\t\treturn chunks, []\n\texcept Exception:\n\t\tlogger.exception(\"Semantic splitting failed\")\n\n\t\t# Try basic splitting as a fallback\n\t\tlogger.warning(\"Falling back to basic file splitting\")\n\t\t# Return empty list for filtered_large_files as it's no longer tracked here\n\t\treturn await self._create_basic_file_chunk(diff), []\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/","title":"Strategies","text":"<p>Strategies for splitting git diffs into logical chunks.</p>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.EXPECTED_TUPLE_SIZE","title":"EXPECTED_TUPLE_SIZE  <code>module-attribute</code>","text":"<pre><code>EXPECTED_TUPLE_SIZE = 2\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.BaseSplitStrategy","title":"BaseSplitStrategy","text":"<p>Base class for diff splitting strategies.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>class BaseSplitStrategy:\n\t\"\"\"Base class for diff splitting strategies.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize with optional embedding model.\"\"\"\n\t\t# Precompile regex patterns for better performance\n\t\tself._file_pattern = re.compile(r\"diff --git a/.*? b/(.*?)\\n\")\n\t\tself._hunk_pattern = re.compile(r\"@@ -\\d+,\\d+ \\+\\d+,\\d+ @@\")\n\n\tasync def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit the diff into chunks.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects\n\n\t\t\"\"\"\n\t\tmsg = \"Subclasses must implement this method\"\n\t\traise NotImplementedError(msg)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.BaseSplitStrategy.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize with optional embedding model.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize with optional embedding model.\"\"\"\n\t# Precompile regex patterns for better performance\n\tself._file_pattern = re.compile(r\"diff --git a/.*? b/(.*?)\\n\")\n\tself._hunk_pattern = re.compile(r\"@@ -\\d+,\\d+ \\+\\d+,\\d+ @@\")\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.BaseSplitStrategy.split","title":"split  <code>async</code>","text":"<pre><code>split(diff: GitDiff) -&gt; list[DiffChunk]\n</code></pre> <p>Split the diff into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>List of DiffChunk objects</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>async def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tSplit the diff into chunks.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    List of DiffChunk objects\n\n\t\"\"\"\n\tmsg = \"Subclasses must implement this method\"\n\traise NotImplementedError(msg)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.FileSplitStrategy","title":"FileSplitStrategy","text":"<p>               Bases: <code>BaseSplitStrategy</code></p> <p>Strategy to split diffs by file.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>class FileSplitStrategy(BaseSplitStrategy):\n\t\"\"\"Strategy to split diffs by file.\"\"\"\n\n\tasync def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit a diff into chunks by file.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects, one per file\n\n\t\t\"\"\"\n\t\tif not diff.content:\n\t\t\treturn self._handle_empty_diff_content(diff)\n\n\t\t# Split the diff content by file\n\t\tfile_chunks = self._file_pattern.split(diff.content)[1:]  # Skip first empty chunk\n\n\t\t# Group files with their content\n\t\tchunks = []\n\t\tfor i in range(0, len(file_chunks), 2):\n\t\t\tif i + 1 &gt;= len(file_chunks):\n\t\t\t\tbreak\n\n\t\t\tfile_name = file_chunks[i]\n\t\t\tcontent = file_chunks[i + 1]\n\n\t\t\tif self._is_valid_filename(file_name) and content:\n\t\t\t\tdiff_header = f\"diff --git a/{file_name} b/{file_name}\\n\"\n\t\t\t\tchunks.append(\n\t\t\t\t\tDiffChunk(\n\t\t\t\t\t\tfiles=[file_name],\n\t\t\t\t\t\tcontent=diff_header + content,\n\t\t\t\t\t\tdescription=f\"Changes in {file_name}\",\n\t\t\t\t\t)\n\t\t\t\t)\n\n\t\treturn chunks\n\n\tdef _handle_empty_diff_content(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"Handle untracked files in empty diff content.\"\"\"\n\t\tif (not diff.is_staged or diff.is_untracked) and diff.files:\n\t\t\t# Filter out invalid file names\n\t\t\tvalid_files = [file for file in diff.files if self._is_valid_filename(file)]\n\t\t\treturn [DiffChunk(files=[f], content=\"\", description=f\"New file: {f}\") for f in valid_files]\n\t\treturn []\n\n\t@staticmethod\n\tdef _is_valid_filename(filename: str) -&gt; bool:\n\t\t\"\"\"Check if the filename is valid (not a pattern or template).\"\"\"\n\t\tif not filename:\n\t\t\treturn False\n\t\tinvalid_chars = [\"*\", \"+\", \"{\", \"}\", \"\\\\\"]\n\t\treturn not (any(char in filename for char in invalid_chars) or filename.startswith('\"'))\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.FileSplitStrategy.split","title":"split  <code>async</code>","text":"<pre><code>split(diff: GitDiff) -&gt; list[DiffChunk]\n</code></pre> <p>Split a diff into chunks by file.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>List of DiffChunk objects, one per file</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>async def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tSplit a diff into chunks by file.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    List of DiffChunk objects, one per file\n\n\t\"\"\"\n\tif not diff.content:\n\t\treturn self._handle_empty_diff_content(diff)\n\n\t# Split the diff content by file\n\tfile_chunks = self._file_pattern.split(diff.content)[1:]  # Skip first empty chunk\n\n\t# Group files with their content\n\tchunks = []\n\tfor i in range(0, len(file_chunks), 2):\n\t\tif i + 1 &gt;= len(file_chunks):\n\t\t\tbreak\n\n\t\tfile_name = file_chunks[i]\n\t\tcontent = file_chunks[i + 1]\n\n\t\tif self._is_valid_filename(file_name) and content:\n\t\t\tdiff_header = f\"diff --git a/{file_name} b/{file_name}\\n\"\n\t\t\tchunks.append(\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_name],\n\t\t\t\t\tcontent=diff_header + content,\n\t\t\t\t\tdescription=f\"Changes in {file_name}\",\n\t\t\t\t)\n\t\t\t)\n\n\treturn chunks\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy","title":"SemanticSplitStrategy","text":"<p>               Bases: <code>BaseSplitStrategy</code></p> <p>Strategy to split diffs semantically.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>class SemanticSplitStrategy(BaseSplitStrategy):\n\t\"\"\"Strategy to split diffs semantically.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: \"ConfigLoader\",\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the SemanticSplitStrategy.\n\n\t\tArgs:\n\t\t    config_loader: ConfigLoader instance.\n\t\t\"\"\"\n\t\t# Store thresholds and settings\n\t\tself.similarity_threshold = config_loader.get.commit.diff_splitter.similarity_threshold\n\t\tself.directory_similarity_threshold = config_loader.get.commit.diff_splitter.directory_similarity_threshold\n\t\tself.min_chunks_for_consolidation = config_loader.get.commit.diff_splitter.min_chunks_for_consolidation\n\t\tself.max_chunks_before_consolidation = config_loader.get.commit.diff_splitter.max_chunks_before_consolidation\n\t\tself.max_file_size_for_llm = config_loader.get.commit.diff_splitter.max_file_size_for_llm\n\t\tself.file_move_similarity_threshold = config_loader.get.commit.diff_splitter.file_move_similarity_threshold\n\n\t\t# Set up file extensions, defaulting to config if None is passed\n\t\tself.code_extensions = config_loader.get.commit.diff_splitter.default_code_extensions\n\t\t# Initialize patterns for related files\n\t\tself.related_file_patterns = self._initialize_related_file_patterns()\n\t\tself.config_loader = config_loader\n\n\t\t# Create DiffEmbedder instance for embedding operations\n\t\tself.embedder = DiffEmbedder(config_loader=config_loader)\n\n\tasync def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit a diff into chunks based on semantic relationships.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects based on semantic analysis\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\tlogger.debug(\"No files to process\")\n\t\t\treturn []\n\n\t\t# Initialize an empty list to store all chunks\n\t\tall_chunks = []\n\n\t\t# Detect moved files\n\t\tmoved_files = await self._detect_moved_files(diff)\n\t\tif moved_files:\n\t\t\tlogger.info(\"Detected %d moved files\", len(moved_files))\n\t\t\tmove_chunks = self._create_move_chunks(moved_files, diff)\n\t\t\tif move_chunks:\n\t\t\t\t# Add move chunks to all_chunks rather than returning immediately\n\t\t\t\tall_chunks.extend(move_chunks)\n\n\t\t\t\t# Create a set of files that are part of moves to avoid processing them again\n\t\t\t\tmoved_file_paths = set()\n\t\t\t\tfor chunk in move_chunks:\n\t\t\t\t\tmoved_file_paths.update(chunk.files)\n\n\t\t\t\t# Filter out moved files from diff.files to avoid double processing\n\t\t\t\tnon_moved_files = [f for f in diff.files if f not in moved_file_paths]\n\n\t\t\t\t# If all files were moves, return just the move chunks\n\t\t\t\tif not non_moved_files:\n\t\t\t\t\treturn all_chunks\n\n\t\t\t\t# Update diff.files to only include non-moved files\n\t\t\t\tdiff.files = non_moved_files\n\t\t\t\tlogger.info(\"Continuing with %d non-moved files\", len(non_moved_files))\n\n\t\t# Handle files in manageable groups\n\t\tif len(diff.files) &gt; MAX_FILES_PER_GROUP:\n\t\t\tlogger.info(\"Processing large number of files (%d) in smaller groups\", len(diff.files))\n\n\t\t\t# Group files by directory to increase likelihood of related files being processed together\n\t\t\tfiles_by_dir = {}\n\t\t\tfor file in diff.files:\n\t\t\t\tdir_path = str(Path(file).parent)\n\t\t\t\tif dir_path not in files_by_dir:\n\t\t\t\t\tfiles_by_dir[dir_path] = []\n\t\t\t\tfiles_by_dir[dir_path].append(file)\n\n\t\t\t# Process each directory group separately, keeping chunks under 5 files\n\t\t\t# Iterate directly over the file lists since the directory path isn't used here\n\t\t\tfor files in files_by_dir.values():\n\t\t\t\t# Process files in this directory in batches of 3-5\n\t\t\t\tfor i in range(0, len(files), 3):\n\t\t\t\t\tbatch = files[i : i + 3]\n\t\t\t\t\t# Create a new GitDiff for the batch, ensuring content is passed\n\t\t\t\t\tbatch_diff = GitDiff(\n\t\t\t\t\t\tfiles=batch,\n\t\t\t\t\t\tcontent=diff.content,  # Pass the original full diff content\n\t\t\t\t\t\tis_staged=diff.is_staged,\n\t\t\t\t\t)\n\t\t\t\t\tchunks = await self._process_group(batch_diff)\n\t\t\t\t\tall_chunks.extend(chunks)\n\n\t\t\treturn all_chunks\n\n\t\t# For smaller groups, process normally and add to all_chunks\n\t\tregular_chunks = await self._process_group(diff)\n\t\tall_chunks.extend(regular_chunks)\n\n\t\treturn all_chunks\n\n\tasync def _process_group(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tProcess a GitDiff with one or more files.\n\n\t\tOriginally designed for single files, but now supports multiple files.\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\tlogger.warning(\"_process_group called with empty files list\")\n\t\t\treturn []\n\n\t\t# If multiple files, this used to log an error, but now we'll handle it properly\n\t\tif len(diff.files) &gt; 1:\n\t\t\tlogger.debug(\"Processing group with multiple files: %s\", diff.files)\n\n\t\t\t# Extract content for each file individually if possible\n\t\t\tchunks = []\n\t\t\tfor file_path in diff.files:\n\t\t\t\t# Try to extract just this file's diff from the full content\n\t\t\t\tfile_diff_content = self._extract_file_diff(diff.content, file_path)\n\n\t\t\t\tif file_diff_content:\n\t\t\t\t\t# Create a new diff for just this file\n\t\t\t\t\tfile_diff = GitDiff(files=[file_path], content=file_diff_content, is_staged=diff.is_staged)\n\t\t\t\t\t# Process it and add the resulting chunks\n\t\t\t\t\tenhanced_chunks = await self._enhance_semantic_split(file_diff)\n\t\t\t\t\tchunks.extend(enhanced_chunks)\n\t\t\t\telse:\n\t\t\t\t\t# If we couldn't extract just this file's diff, create a simple chunk\n\t\t\t\t\tchunks.append(\n\t\t\t\t\t\tDiffChunk(\n\t\t\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\t\t\tcontent=\"\",  # Empty content as we couldn't extract it\n\t\t\t\t\t\t\tdescription=f\"Changes in {file_path}\",\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\n\t\t\t# If we couldn't create any valid chunks, fallback to the original behavior\n\t\t\tif not chunks:\n\t\t\t\treturn [DiffChunk(files=diff.files, content=diff.content, description=\"Multiple file changes\")]\n\n\t\t\treturn chunks\n\n\t\t# Original behavior for single file\n\t\tfile_path = diff.files[0]\n\n\t\t# Enhance this single file diff\n\t\tenhanced_chunks = await self._enhance_semantic_split(diff)  # Pass the original diff directly\n\n\t\tif not enhanced_chunks:\n\t\t\tlogger.warning(\"No chunk generated for file: %s after enhancement.\", file_path)\n\t\t\t# Fallback if enhancement yields nothing\n\t\t\tenhanced_chunks = [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=diff.content,\n\t\t\t\t\tdescription=f\"Changes in {file_path} (enhancement failed)\",\n\t\t\t\t)\n\t\t\t]\n\n\t\t# No further consolidation or grouping needed here as we process file-by-file now\n\t\treturn enhanced_chunks\n\n\tdef _extract_file_diff(self, full_diff_content: str, file_path: str) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the diff content for a specific file from a multi-file diff.\n\n\t\tArgs:\n\t\t        full_diff_content: Complete diff content with multiple files\n\t\t        file_path: Path of the file to extract\n\n\t\tReturns:\n\t\t        The extracted diff for the specific file, or empty string if not found\n\n\t\t\"\"\"\n\t\timport re\n\n\t\t# Pattern to match the start of a diff for a file\n\t\tdiff_start_pattern = re.compile(r\"diff --git a/([^\\s]+) b/([^\\s]+)\")\n\n\t\t# Find all diff start positions\n\t\tdiff_positions = []\n\t\tfor match in diff_start_pattern.finditer(full_diff_content):\n\t\t\t_, b_file = match.groups()\n\t\t\t# For most changes both files are the same; for renames prefer b_file\n\t\t\ttarget_file = b_file\n\t\t\tdiff_positions.append((match.start(), target_file))\n\n\t\t# Sort by position\n\t\tdiff_positions.sort()\n\n\t\t# Find the diff for our file\n\t\tfile_diff = \"\"\n\t\tfor i, (start_pos, diff_file) in enumerate(diff_positions):\n\t\t\tif diff_file == file_path:\n\t\t\t\t# Found our file, now find the end\n\t\t\t\tif i &lt; len(diff_positions) - 1:\n\t\t\t\t\tend_pos = diff_positions[i + 1][0]\n\t\t\t\t\tfile_diff = full_diff_content[start_pos:end_pos]\n\t\t\t\telse:\n\t\t\t\t\t# Last file in the diff\n\t\t\t\t\tfile_diff = full_diff_content[start_pos:]\n\t\t\t\tbreak\n\n\t\treturn file_diff\n\n\tdef _group_chunks_by_directory(self, chunks: list[DiffChunk]) -&gt; dict[str, list[DiffChunk]]:\n\t\t\"\"\"Group chunks by their containing directory.\"\"\"\n\t\tdir_groups: dict[str, list[DiffChunk]] = {}\n\n\t\tfor chunk in chunks:\n\t\t\tif not chunk.files:\n\t\t\t\tcontinue\n\n\t\t\tfile_path = chunk.files[0]\n\t\t\tdir_path = file_path.rsplit(\"/\", 1)[0] if \"/\" in file_path else \"root\"\n\n\t\t\tif dir_path not in dir_groups:\n\t\t\t\tdir_groups[dir_path] = []\n\n\t\t\tdir_groups[dir_path].append(chunk)\n\n\t\treturn dir_groups\n\n\tasync def _process_directory_group(\n\t\tself, chunks: list[DiffChunk], processed_files: set[str], semantic_chunks: list[DiffChunk]\n\t) -&gt; None:\n\t\t\"\"\"Process chunks in a single directory group.\"\"\"\n\t\tif len(chunks) == 1:\n\t\t\t# If only one file in directory, add it directly\n\t\t\tsemantic_chunks.append(chunks[0])\n\t\t\tif chunks[0].files:\n\t\t\t\tprocessed_files.update(chunks[0].files)\n\t\telse:\n\t\t\t# For directories with multiple files, try to group them\n\t\t\tdir_processed: set[str] = set()\n\n\t\t\t# First try to group by related file patterns\n\t\t\tawait self._group_related_files(chunks, dir_processed, semantic_chunks)\n\n\t\t\t# Then try to group remaining files by content similarity\n\t\t\tremaining_chunks = [c for c in chunks if not c.files or c.files[0] not in dir_processed]\n\n\t\t\tif remaining_chunks:\n\t\t\t\t# Use default similarity threshold instead\n\t\t\t\tawait self._group_by_content_similarity(remaining_chunks, semantic_chunks)\n\n\t\t\t# Add all processed files to the global processed set\n\t\t\tprocessed_files.update(dir_processed)\n\n\tasync def _process_remaining_chunks(\n\t\tself, all_chunks: list[DiffChunk], processed_files: set[str], semantic_chunks: list[DiffChunk]\n\t) -&gt; None:\n\t\t\"\"\"Process any remaining chunks that weren't grouped by directory.\"\"\"\n\t\tremaining_chunks = [c for c in all_chunks if c.files and c.files[0] not in processed_files]\n\n\t\tif remaining_chunks:\n\t\t\tawait self._group_by_content_similarity(remaining_chunks, semantic_chunks)\n\n\tasync def _consolidate_small_chunks(self, initial_chunks: list[DiffChunk]) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tMerge small or related chunks together.\n\n\t\tFirst, consolidates chunks originating from the same file.\n\t\tThen, consolidates remaining single-file chunks by directory.\n\n\t\tArgs:\n\t\t    initial_chunks: List of diff chunks to consolidate\n\n\t\tReturns:\n\t\t    Consolidated list of chunks\n\n\t\t\"\"\"\n\t\t# Use instance variable for threshold\n\t\tif len(initial_chunks) &lt; self.min_chunks_for_consolidation:\n\t\t\treturn initial_chunks\n\n\t\t# Consolidate small chunks for the same file or related files\n\t\tconsolidated_chunks = []\n\t\tprocessed_indices = set()\n\n\t\tfor i, chunk1 in enumerate(initial_chunks):\n\t\t\tif i in processed_indices:\n\t\t\t\tcontinue\n\n\t\t\tmerged_chunk = chunk1\n\t\t\tprocessed_indices.add(i)\n\n\t\t\t# Check subsequent chunks for merging\n\t\t\tfor j in range(i + 1, len(initial_chunks)):\n\t\t\t\tif j in processed_indices:\n\t\t\t\t\tcontinue\n\n\t\t\t\tchunk2 = initial_chunks[j]\n\n\t\t\t\t# Check if chunks should be merged (same file or related)\n\t\t\t\tif self._should_merge_chunks(merged_chunk, chunk2):\n\t\t\t\t\t# Combine files if merging related chunks, not just same file chunks\n\t\t\t\t\tnew_files = merged_chunk.files\n\t\t\t\t\tif (\n\t\t\t\t\t\tlen(merged_chunk.files) == 1\n\t\t\t\t\t\tand len(chunk2.files) == 1\n\t\t\t\t\t\tand merged_chunk.files[0] != chunk2.files[0]\n\t\t\t\t\t):\n\t\t\t\t\t\tnew_files = sorted(set(merged_chunk.files + chunk2.files))\n\n\t\t\t\t\t# Merge content and potentially other attributes\n\t\t\t\t\t# Ensure a newline between merged content if needed\n\t\t\t\t\tseparator = \"\\n\" if merged_chunk.content and chunk2.content else \"\"\n\t\t\t\t\tmerged_chunk = dataclasses.replace(\n\t\t\t\t\t\tmerged_chunk,\n\t\t\t\t\t\tfiles=new_files,\n\t\t\t\t\t\tcontent=merged_chunk.content + separator + chunk2.content,\n\t\t\t\t\t\tdescription=merged_chunk.description,  # Keep first description\n\t\t\t\t\t)\n\t\t\t\t\tprocessed_indices.add(j)\n\n\t\t\tconsolidated_chunks.append(merged_chunk)\n\n\t\treturn consolidated_chunks\n\n\tasync def _consolidate_if_needed(self, semantic_chunks: list[DiffChunk]) -&gt; list[DiffChunk]:\n\t\t\"\"\"Consolidate chunks if we have too many small ones.\"\"\"\n\t\thas_single_file_chunks = any(len(chunk.files) == 1 for chunk in semantic_chunks)\n\n\t\tif len(semantic_chunks) &gt; self.max_chunks_before_consolidation and has_single_file_chunks:\n\t\t\treturn await self._consolidate_small_chunks(semantic_chunks)\n\n\t\treturn semantic_chunks\n\n\t@staticmethod\n\tdef _initialize_related_file_patterns() -&gt; list[tuple[Pattern, Pattern]]:\n\t\t\"\"\"\n\t\tInitialize and compile regex patterns for related files.\n\n\t\tReturns:\n\t\t    List of compiled regex pattern pairs\n\n\t\t\"\"\"\n\t\t# Pre-compile regex for efficiency and validation\n\t\trelated_file_patterns = []\n\t\t# Define patterns using standard strings with escaped backreferences\n\t\tdefault_patterns: list[tuple[str, str]] = [\n\t\t\t# --- File Moves (same name, different directories) ---\n\t\t\t# This helps identify potential file moves regardless of directory structure\n\t\t\t(\"^(.*/)?(.*?)$\", \"^(.*/)\\\\\\\\2$\"),  # Same filename in any directory\n\t\t\t# --- General Code + Test Files ---\n\t\t\t# Python\n\t\t\t(\"^(.*)\\\\.py$\", \"\\\\\\\\1_test\\\\.py$\"),\n\t\t\t(\"^(.*)\\\\.py$\", \"test_\\\\\\\\1\\\\.py$\"),\n\t\t\t(\"^(.*)\\\\.(py)$\", \"\\\\\\\\1_test\\\\.\\\\\\\\2$\"),  # For file.py and file_test.py pattern\n\t\t\t(\"^(.*)\\\\.(py)$\", \"\\\\\\\\1Test\\\\.\\\\\\\\2$\"),  # For file.py and fileTest.py pattern\n\t\t\t(\"^(.*)\\\\.py$\", \"\\\\\\\\1_spec\\\\.py$\"),\n\t\t\t(\"^(.*)\\\\.py$\", \"spec_\\\\\\\\1\\\\.py$\"),\n\t\t\t# JavaScript / TypeScript (including JSX/TSX)\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.(test|spec)\\\\.(js|jsx|ts|tsx)$\"),\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.stories\\\\.(js|jsx|ts|tsx)$\"),  # Storybook\n\t\t\t(\"^(.*)\\\\.(js|ts)$\", \"\\\\\\\\1\\\\.d\\\\.ts$\"),  # JS/TS + Declaration files\n\t\t\t# Ruby\n\t\t\t(\"^(.*)\\\\.rb$\", \"\\\\\\\\1_spec\\\\.rb$\"),\n\t\t\t(\"^(.*)\\\\.rb$\", \"\\\\\\\\1_test\\\\.rb$\"),\n\t\t\t(\"^(.*)\\\\.rb$\", \"spec/.*_spec\\\\.rb$\"),  # Common RSpec structure\n\t\t\t# Java\n\t\t\t(\"^(.*)\\\\.java$\", \"\\\\\\\\1Test\\\\.java$\"),\n\t\t\t(\"src/main/java/(.*)\\\\.java$\", \"src/test/java/\\\\\\\\1Test\\\\.java$\"),  # Maven/Gradle structure\n\t\t\t# Go\n\t\t\t(\"^(.*)\\\\.go$\", \"\\\\\\\\1_test\\\\.go$\"),\n\t\t\t# C#\n\t\t\t(\"^(.*)\\\\.cs$\", \"\\\\\\\\1Tests?\\\\.cs$\"),\n\t\t\t# PHP\n\t\t\t(\"^(.*)\\\\.php$\", \"\\\\\\\\1Test\\\\.php$\"),\n\t\t\t(\"^(.*)\\\\.php$\", \"\\\\\\\\1Spec\\\\.php$\"),\n\t\t\t(\"src/(.*)\\\\.php$\", \"tests/\\\\\\\\1Test\\\\.php$\"),  # Common structure\n\t\t\t# Rust\n\t\t\t(\"src/(lib|main)\\\\.rs$\", \"tests/.*\\\\.rs$\"),  # Main/Lib and integration tests\n\t\t\t(\"src/(.*)\\\\.rs$\", \"src/\\\\\\\\1_test\\\\.rs$\"),  # Inline tests (less common for grouping)\n\t\t\t# Swift\n\t\t\t(\"^(.*)\\\\.swift$\", \"\\\\\\\\1Tests?\\\\.swift$\"),\n\t\t\t# Kotlin\n\t\t\t(\"^(.*)\\\\.kt$\", \"\\\\\\\\1Test\\\\.kt$\"),\n\t\t\t(\"src/main/kotlin/(.*)\\\\.kt$\", \"src/test/kotlin/\\\\\\\\1Test\\\\.kt$\"),  # Common structure\n\t\t\t# --- Frontend Component Bundles ---\n\t\t\t# JS/TS Components + Styles (CSS, SCSS, LESS, CSS Modules)\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.module\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.styles?\\\\.(js|ts)$\"),  # Styled Components / Emotion convention\n\t\t\t# Vue Components + Styles\n\t\t\t(\"^(.*)\\\\.vue$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.vue$\", \"\\\\\\\\1\\\\.module\\\\.(css|scss|less)$\"),\n\t\t\t# Svelte Components + Styles/Scripts\n\t\t\t(\"^(.*)\\\\.svelte$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.svelte$\", \"\\\\\\\\1\\\\.(js|ts)$\"),\n\t\t\t# Angular Components (more specific structure)\n\t\t\t(\"^(.*)\\\\.component\\\\.ts$\", \"\\\\\\\\1\\\\.component\\\\.html$\"),\n\t\t\t(\"^(.*)\\\\.component\\\\.ts$\", \"\\\\\\\\1\\\\.component\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.component\\\\.ts$\", \"\\\\\\\\1\\\\.component\\\\.spec\\\\.ts$\"),  # Component + its test\n\t\t\t(\"^(.*)\\\\.service\\\\.ts$\", \"\\\\\\\\1\\\\.service\\\\.spec\\\\.ts$\"),  # Service + its test\n\t\t\t(\"^(.*)\\\\.module\\\\.ts$\", \"\\\\\\\\1\\\\.routing\\\\.module\\\\.ts$\"),  # Module + routing\n\t\t\t# --- Implementation / Definition / Generation ---\n\t\t\t# C / C++ / Objective-C\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.c$\"),\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.m$\"),\n\t\t\t(\"^(.*)\\\\.hpp$\", \"\\\\\\\\1\\\\.cpp$\"),\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.cpp$\"),  # Allow .h with .cpp\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.mm$\"),\n\t\t\t# Protocol Buffers / gRPC\n\t\t\t(\"^(.*)\\\\.proto$\", \"\\\\\\\\1\\\\.pb\\\\.(go|py|js|java|rb|cs|ts)$\"),\n\t\t\t(\"^(.*)\\\\.proto$\", \"\\\\\\\\1_pb2?\\\\.py$\"),  # Python specific proto generation\n\t\t\t(\"^(.*)\\\\.proto$\", \"\\\\\\\\1_grpc\\\\.pb\\\\.(go|js|ts)$\"),  # gRPC specific\n\t\t\t# Interface Definition Languages (IDL)\n\t\t\t(\"^(.*)\\\\.idl$\", \"\\\\\\\\1\\\\.(h|cpp|cs|java)$\"),\n\t\t\t# API Specifications (OpenAPI/Swagger)\n\t\t\t(\"(openapi|swagger)\\\\.(yaml|yml|json)$\", \".*\\\\.(go|py|js|java|rb|cs|ts)$\"),  # Spec + generated code\n\t\t\t(\"^(.*)\\\\.(yaml|yml|json)$\", \"\\\\\\\\1\\\\.generated\\\\.(go|py|js|java|rb|cs|ts)$\"),  # Another convention\n\t\t\t# --- Web Development (HTML Centric) ---\n\t\t\t(\"^(.*)\\\\.html$\", \"\\\\\\\\1\\\\.(js|ts)$\"),\n\t\t\t(\"^(.*)\\\\.html$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t# --- Mobile Development ---\n\t\t\t# iOS (Swift)\n\t\t\t(\"^(.*)\\\\.swift$\", \"\\\\\\\\1\\\\.storyboard$\"),\n\t\t\t(\"^(.*)\\\\.swift$\", \"\\\\\\\\1\\\\.xib$\"),\n\t\t\t# Android (Kotlin/Java)\n\t\t\t(\"^(.*)\\\\.(kt|java)$\", \"res/layout/.*\\\\.(xml)$\"),  # Code + Layout XML (Path sensitive)\n\t\t\t(\"AndroidManifest\\\\.xml$\", \".*\\\\.(kt|java)$\"),  # Manifest + Code\n\t\t\t(\"build\\\\.gradle(\\\\.kts)?$\", \".*\\\\.(kt|java)$\"),  # Gradle build + Code\n\t\t\t# --- Configuration Files ---\n\t\t\t# Package Managers\n\t\t\t(\"package\\\\.json$\", \"(package-lock\\\\.json|yarn\\\\.lock|pnpm-lock\\\\.yaml)$\"),\n\t\t\t(\"requirements\\\\.txt$\", \"(setup\\\\.py|setup\\\\.cfg|pyproject\\\\.toml)$\"),\n\t\t\t(\"pyproject\\\\.toml$\", \"(setup\\\\.py|setup\\\\.cfg|poetry\\\\.lock|uv\\\\.lock)$\"),\n\t\t\t(\"Gemfile$\", \"Gemfile\\\\.lock$\"),\n\t\t\t(\"Cargo\\\\.toml$\", \"Cargo\\\\.lock$\"),\n\t\t\t(\"composer\\\\.json$\", \"composer\\\\.lock$\"),  # PHP Composer\n\t\t\t(\"go\\\\.mod$\", \"go\\\\.sum$\"),  # Go Modules\n\t\t\t(\"pom\\\\.xml$\", \".*\\\\.java$\"),  # Maven + Java\n\t\t\t(\"build\\\\.gradle(\\\\.kts)?$\", \".*\\\\.(java|kt)$\"),  # Gradle + Java/Kotlin\n\t\t\t# Linters / Formatters / Compilers / Type Checkers\n\t\t\t(\n\t\t\t\t\"package\\\\.json$\",\n\t\t\t\t\"(tsconfig\\\\.json|\\\\.eslintrc(\\\\..*)?|\\\\.prettierrc(\\\\..*)?|\\\\.babelrc(\\\\..*)?|webpack\\\\.config\\\\.js|vite\\\\.config\\\\.(js|ts))$\",\n\t\t\t),\n\t\t\t(\"pyproject\\\\.toml$\", \"(\\\\.flake8|\\\\.pylintrc|\\\\.isort\\\\.cfg|mypy\\\\.ini)$\"),\n\t\t\t# Docker\n\t\t\t(\"Dockerfile$\", \"(\\\\.dockerignore|docker-compose\\\\.yml)$\"),\n\t\t\t(\"docker-compose\\\\.yml$\", \"\\\\.env$\"),\n\t\t\t# CI/CD\n\t\t\t(\"\\\\.github/workflows/.*\\\\.yml$\", \".*\\\\.(sh|py|js|ts|go)$\"),  # Workflow + scripts\n\t\t\t(\"\\\\.gitlab-ci\\\\.yml$\", \".*\\\\.(sh|py|js|ts|go)$\"),\n\t\t\t(\"Jenkinsfile$\", \".*\\\\.(groovy|sh|py)$\"),\n\t\t\t# IaC (Terraform)\n\t\t\t(\"^(.*)\\\\.tf$\", \"\\\\\\\\1\\\\.tfvars$\"),\n\t\t\t(\"^(.*)\\\\.tf$\", \"\\\\\\\\1\\\\.tf$\"),  # Group TF files together\n\t\t\t# --- Documentation ---\n\t\t\t(\"README\\\\.md$\", \".*$\"),  # README often updated with any change\n\t\t\t(\"^(.*)\\\\.md$\", \"\\\\\\\\1\\\\.(py|js|ts|go|java|rb|rs|php|swift|kt)$\"),  # Markdown doc + related code\n\t\t\t(\"docs/.*\\\\.md$\", \"src/.*$\"),  # Documentation in docs/ related to src/\n\t\t\t# --- Data Science / ML ---\n\t\t\t(\"^(.*)\\\\.ipynb$\", \"\\\\\\\\1\\\\.py$\"),  # Notebook + Python script\n\t\t\t(\"^(.*)\\\\.py$\", \"data/.*\\\\.(csv|json|parquet)$\"),  # Script + Data file (path sensitive)\n\t\t\t# --- General Fallbacks (Use with caution) ---\n\t\t\t# Files with same base name but different extensions (already covered by some specifics)\n\t\t\t# (\"^(.*)\\\\..*$\", \"\\\\1\\\\..*$\"), # Potentially too broad, rely on specifics above\n\t\t]\n\n\t\tfor pattern1_str, pattern2_str in default_patterns:\n\t\t\ttry:\n\t\t\t\t# Compile with IGNORECASE for broader matching\n\t\t\t\tpattern1 = re.compile(pattern1_str, re.IGNORECASE)\n\t\t\t\tpattern2 = re.compile(pattern2_str, re.IGNORECASE)\n\t\t\t\trelated_file_patterns.append((pattern1, pattern2))\n\t\t\texcept re.error as e:\n\t\t\t\t# Log only if pattern compilation fails\n\t\t\t\tlogger.warning(f\"Failed to compile regex pair: ({pattern1_str!r}, {pattern2_str!r}). Error: {e}\")\n\n\t\treturn related_file_patterns\n\n\t# --- New Helper Methods for Refactoring _enhance_semantic_split ---\n\n\tdef _parse_file_diff(self, diff_content: str, file_path: str) -&gt; PatchedFile | None:\n\t\t\"\"\"Parse diff content to find the PatchedFile for a specific file path.\"\"\"\n\t\tif not diff_content:\n\t\t\tlogger.warning(\"Cannot parse empty diff content for %s\", file_path)\n\t\t\treturn None\n\n\t\tfiltered_content = \"\"  # Initialize to handle unbound case\n\t\ttry:\n\t\t\t# Filter out the truncation marker lines before parsing\n\t\t\tfiltered_content_lines = [\n\t\t\t\tline for line in diff_content.splitlines() if line.strip() != \"... [content truncated] ...\"\n\t\t\t]\n\t\t\tfiltered_content = \"\\n\".join(filtered_content_lines)\n\n\t\t\t# Use StringIO as PatchSet expects a file-like object or iterable\n\t\t\ttry:\n\t\t\t\tpatch_set = PatchSet(StringIO(filtered_content))\n\t\t\texcept UnidiffParseError as e:\n\t\t\t\tlogger.warning(\"UnidiffParseError for %s: %s\", file_path, str(e))\n\t\t\t\t# Try to extract just the diff for this specific file to avoid parsing the entire diff\n\t\t\t\tfile_diff_content_raw = re.search(\n\t\t\t\t\trf\"diff --git a/.*? b/{re.escape(file_path)}\\n(.*?)(?=diff --git a/|\\Z)\",\n\t\t\t\t\tdiff_content,\n\t\t\t\t\tre.DOTALL | re.MULTILINE,\n\t\t\t\t)\n\t\t\t\tcontent_for_chunk = file_diff_content_raw.group(0) if file_diff_content_raw else \"\"\n\t\t\t\tif content_for_chunk:\n\t\t\t\t\tlogger.debug(\"Extracted raw content for %s after parse error\", file_path)\n\t\t\t\t\t# Create a manual PatchedFile since we can't parse it properly\n\t\t\t\t\treturn None\n\t\t\t\treturn None\n\n\t\t\tmatched_file: PatchedFile | None = None\n\t\t\tfor patched_file in patch_set:\n\t\t\t\t# unidiff paths usually start with a/ or b/\n\t\t\t\tif patched_file.target_file == f\"b/{file_path}\" or patched_file.path == file_path:\n\t\t\t\t\tmatched_file = patched_file\n\t\t\t\t\tbreak\n\t\t\tif not matched_file:\n\t\t\t\tlogger.warning(\"Could not find matching PatchedFile for: %s in unidiff output\", file_path)\n\t\t\t\treturn None\n\t\t\treturn matched_file\n\t\texcept UnidiffParseError:\n\t\t\t# Log the specific parse error and the content that caused it (first few lines)\n\t\t\tpreview_lines = \"\\n\".join(filtered_content.splitlines()[:10])  # Log first 10 lines\n\t\t\tlogger.exception(\n\t\t\t\t\"UnidiffParseError for %s\\nContent Preview:\\n%s\",  # Corrected format string\n\t\t\t\tfile_path,\n\t\t\t\tpreview_lines,\n\t\t\t)\n\t\t\treturn None  # Return None on parse error\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to parse diff content using unidiff for %s\", file_path)\n\t\t\treturn None\n\n\tdef _reconstruct_file_diff(self, patched_file: PatchedFile) -&gt; tuple[str, str]:\n\t\t\"\"\"Reconstruct the diff header and full diff content for a PatchedFile.\"\"\"\n\t\tfile_diff_hunks_content = \"\\n\".join(str(hunk) for hunk in patched_file)\n\t\tfile_header_obj = getattr(patched_file, \"patch_info\", None)\n\t\tfile_header = str(file_header_obj) if file_header_obj else \"\"\n\n\t\tif not file_header.startswith(\"diff --git\") and patched_file.source_file and patched_file.target_file:\n\t\t\tlogger.debug(\"Reconstructing missing diff header for %s\", patched_file.path)\n\t\t\tfile_header = f\"diff --git {patched_file.source_file} {patched_file.target_file}\\n\"\n\t\t\tif hasattr(patched_file, \"index\") and patched_file.index:\n\t\t\t\tfile_header += f\"index {patched_file.index}\\n\"\n\t\t\t# Use timestamps if available for more accurate header reconstruction\n\t\t\tsource_ts = f\"\\t{patched_file.source_timestamp}\" if patched_file.source_timestamp else \"\"\n\t\t\ttarget_ts = f\"\\t{patched_file.target_timestamp}\" if patched_file.target_timestamp else \"\"\n\t\t\tfile_header += f\"--- {patched_file.source_file}{source_ts}\\n\"\n\t\t\tfile_header += f\"+++ {patched_file.target_file}{target_ts}\\n\"\n\n\t\tfull_file_diff_content = file_header + file_diff_hunks_content\n\t\treturn file_header, full_file_diff_content\n\n\tdef _split_large_file_diff(self, patched_file: PatchedFile, file_header: str) -&gt; list[DiffChunk]:\n\t\t\"\"\"Split a large file's diff by grouping hunks under the size limit.\"\"\"\n\t\tfile_path = patched_file.path\n\t\tmax_chunk_size = self.max_file_size_for_llm  # Use instance config\n\t\tlogger.info(\n\t\t\t\"Splitting large file diff for %s by hunks (limit: %d bytes)\",\n\t\t\tfile_path,\n\t\t\tmax_chunk_size,\n\t\t)\n\t\tlarge_file_chunks = []\n\t\tcurrent_hunk_group: list[Hunk] = []\n\t\tcurrent_group_size = len(file_header)  # Start with header size\n\n\t\tfor hunk in patched_file:\n\t\t\thunk_content_str = str(hunk)\n\t\t\thunk_size = len(hunk_content_str) + 1  # +1 for newline separator\n\n\t\t\t# If adding this hunk exceeds the limit (and group isn't empty), finalize the current chunk\n\t\t\tif current_hunk_group and current_group_size + hunk_size &gt; max_chunk_size:\n\t\t\t\tgroup_content = file_header + \"\\n\".join(str(h) for h in current_hunk_group)\n\t\t\t\tdescription = f\"Chunk {len(large_file_chunks) + 1} of large file {file_path}\"\n\t\t\t\tlarge_file_chunks.append(DiffChunk(files=[file_path], content=group_content, description=description))\n\t\t\t\t# Start a new chunk with the current hunk\n\t\t\t\tcurrent_hunk_group = [hunk]\n\t\t\t\tcurrent_group_size = len(file_header) + hunk_size\n\t\t\t# Edge case: If a single hunk itself is too large, create a chunk just for it\n\t\t\telif not current_hunk_group and len(file_header) + hunk_size &gt; max_chunk_size:\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"Single hunk in %s exceeds size limit (%d bytes). Creating oversized chunk.\",\n\t\t\t\t\tfile_path,\n\t\t\t\t\tlen(file_header) + hunk_size,\n\t\t\t\t)\n\t\t\t\tgroup_content = file_header + hunk_content_str\n\t\t\t\tdescription = f\"Chunk {len(large_file_chunks) + 1} (oversized hunk) of large file {file_path}\"\n\t\t\t\tlarge_file_chunks.append(DiffChunk(files=[file_path], content=group_content, description=description))\n\t\t\t\t# Reset for next potential chunk (don't carry this huge hunk forward)\n\t\t\t\tcurrent_hunk_group = []\n\t\t\t\tcurrent_group_size = len(file_header)\n\t\t\telse:\n\t\t\t\t# Add hunk to the current group\n\t\t\t\tcurrent_hunk_group.append(hunk)\n\t\t\t\tcurrent_group_size += hunk_size\n\n\t\t# Add the last remaining chunk group if any\n\t\tif current_hunk_group:\n\t\t\tgroup_content = file_header + \"\\n\".join(str(h) for h in current_hunk_group)\n\t\t\tdescription = f\"Chunk {len(large_file_chunks) + 1} of large file {file_path}\"\n\t\t\tlarge_file_chunks.append(DiffChunk(files=[file_path], content=group_content, description=description))\n\n\t\treturn large_file_chunks\n\n\t# --- Refactored Orchestrator Method ---\n\n\tasync def _enhance_semantic_split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tEnhance the semantic split by using NLP and chunk detection.\n\n\t\tArgs:\n\t\t    diff: The GitDiff object to split\n\n\t\tReturns:\n\t\t    List of enhanced DiffChunk objects\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\treturn []\n\n\t\t# Special handling for untracked files - avoid unidiff parsing errors\n\t\tif diff.is_untracked:\n\t\t\t# Create a basic chunk with only file info for untracked files\n\t\t\t# Use a list comprehension for performance (PERF401)\n\t\t\treturn [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=diff.content if len(diff.files) == 1 else f\"New untracked file: {file_path}\",\n\t\t\t\t\tdescription=f\"New file: {file_path}\",\n\t\t\t\t)\n\t\t\t\tfor file_path in diff.files\n\t\t\t\tif self._is_valid_filename(file_path)\n\t\t\t]\n\n\t\tif not diff.files or len(diff.files) != 1:\n\t\t\tlogger.error(\"_enhance_semantic_split called with invalid diff object (files=%s)\", diff.files)\n\t\t\treturn []\n\n\t\tfile_path = diff.files[0]\n\t\textension = Path(file_path).suffix[1:].lower()\n\n\t\tif not diff.content:\n\t\t\tlogger.warning(\"No diff content provided for %s, creating basic chunk.\", file_path)\n\t\t\treturn [DiffChunk(files=[file_path], content=\"\", description=f\"New file: {file_path}\")]\n\n\t\t# 1. Parse the diff to get the PatchedFile object\n\t\tmatched_file = self._parse_file_diff(diff.content, file_path)\n\t\tif not matched_file:\n\t\t\t# If parsing failed, return a basic chunk with raw content attempt\n\t\t\tfile_diff_content_raw = re.search(\n\t\t\t\trf\"diff --git a/.*? b/{re.escape(file_path)}\\n(.*?)(?=diff --git a/|\\Z)\",\n\t\t\t\tdiff.content,\n\t\t\t\tre.DOTALL | re.MULTILINE,\n\t\t\t)\n\t\t\tcontent_for_chunk = file_diff_content_raw.group(0) if file_diff_content_raw else \"\"\n\t\t\treturn [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=content_for_chunk,\n\t\t\t\t\tdescription=f\"Changes in {file_path} (parsing failed)\",\n\t\t\t\t)\n\t\t\t]\n\n\t\t# 2. Reconstruct the full diff content for this file\n\t\t_, full_file_diff_content = self._reconstruct_file_diff(matched_file)\n\n\t\t# 3. Check if the reconstructed diff is too large\n\t\tif len(full_file_diff_content) &gt; self.max_file_size_for_llm:\n\t\t\treturn self._split_large_file_diff(matched_file, \"\")\n\n\t\t# 4. Try splitting by semantic patterns (if applicable)\n\t\tpatterns = get_language_specific_patterns(extension)\n\t\tif patterns:\n\t\t\tlogger.debug(\"Attempting semantic pattern splitting for %s\", file_path)\n\t\t\tpattern_chunks = self._split_by_semantic_patterns(matched_file, patterns)\n\t\t\tif pattern_chunks:\n\t\t\t\treturn pattern_chunks\n\t\t\tlogger.debug(\"Pattern splitting yielded no chunks for %s, falling back.\", file_path)\n\n\t\t# 5. Fallback: Split by individual hunks\n\t\tlogger.debug(\"Falling back to hunk splitting for %s\", file_path)\n\t\thunk_chunks = []\n\t\tfor hunk in matched_file:\n\t\t\thunk_content = str(hunk)\n\t\t\thunk_chunks.append(\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=hunk_content,  # Combine header + hunk\n\t\t\t\t\tdescription=f\"Hunk in {file_path} starting near line {hunk.target_start}\",\n\t\t\t\t)\n\t\t\t)\n\n\t\t# If no hunks were found at all, return the single reconstructed chunk\n\t\tif not hunk_chunks:\n\t\t\tlogger.warning(\"No hunks detected for %s after parsing, returning full diff.\", file_path)\n\t\t\treturn [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=full_file_diff_content,\n\t\t\t\t\tdescription=f\"Changes in {file_path} (no hunks detected)\",\n\t\t\t\t)\n\t\t\t]\n\n\t\treturn hunk_chunks\n\n\t# --- Existing Helper Methods (Potentially need review/updates) ---\n\n\tasync def _group_by_content_similarity(\n\t\tself,\n\t\tchunks: list[DiffChunk],\n\t\tresult_chunks: list[DiffChunk],\n\t\tsimilarity_threshold: float | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGroup chunks by content similarity.\n\n\t\tArgs:\n\t\t    chunks: List of chunks to process\n\t\t    result_chunks: List to append grouped chunks to (modified in place)\n\t\t    similarity_threshold: Optional custom threshold to override default\n\n\t\t\"\"\"\n\t\tif not chunks:\n\t\t\treturn\n\n\t\tprocessed_indices = set()\n\t\tthreshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n\n\t\t# Extract content from all chunks and get embeddings in batch\n\t\tchunk_contents = [chunk.content for chunk in chunks]\n\t\tchunk_embeddings = await self.embedder.embed_contents(chunk_contents)\n\n\t\t# For each chunk, find similar chunks and group them\n\t\tfor i, chunk in enumerate(chunks):\n\t\t\tif i in processed_indices or not chunk_embeddings[i]:\n\t\t\t\tcontinue\n\n\t\t\trelated_chunks = [chunk]\n\t\t\tprocessed_indices.add(i)\n\n\t\t\t# Find similar chunks\n\t\t\tfor j, other_chunk in enumerate(chunks):\n\t\t\t\tif i == j or j in processed_indices or not chunk_embeddings[j]:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Calculate similarity between chunks using embeddings\n\t\t\t\t# Cast to remove None type since we've checked above\n\t\t\t\temb1 = cast(\"list[float]\", chunk_embeddings[i])\n\t\t\t\temb2 = cast(\"list[float]\", chunk_embeddings[j])\n\t\t\t\tsimilarity = calculate_semantic_similarity(emb1, emb2)\n\n\t\t\t\tif similarity &gt;= threshold:\n\t\t\t\t\trelated_chunks.append(other_chunk)\n\t\t\t\t\tprocessed_indices.add(j)\n\n\t\t\t# Create a semantic chunk from related chunks\n\t\t\tif related_chunks:\n\t\t\t\tself._create_semantic_chunk(related_chunks, result_chunks)\n\n\tasync def _group_related_files(\n\t\tself,\n\t\tfile_chunks: list[DiffChunk],\n\t\tprocessed_files: set[str],\n\t\tsemantic_chunks: list[DiffChunk],\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGroup related files into semantic chunks.\n\n\t\tArgs:\n\t\t    file_chunks: List of file-based chunks\n\t\t    processed_files: Set of already processed files (modified in place)\n\t\t    semantic_chunks: List of semantic chunks (modified in place)\n\n\t\t\"\"\"\n\t\tif not file_chunks:\n\t\t\treturn\n\n\t\t# Group clearly related files\n\t\tfor i, chunk in enumerate(file_chunks):\n\t\t\tif not chunk.files or chunk.files[0] in processed_files:\n\t\t\t\tcontinue\n\n\t\t\trelated_chunks = [chunk]\n\t\t\tprocessed_files.add(chunk.files[0])\n\n\t\t\t# Find related files\n\t\t\tfor j, other_chunk in enumerate(file_chunks):\n\t\t\t\tif i == j or not other_chunk.files or other_chunk.files[0] in processed_files:\n\t\t\t\t\tcontinue\n\n\t\t\t\tif are_files_related(chunk.files[0], other_chunk.files[0], self.related_file_patterns):\n\t\t\t\t\trelated_chunks.append(other_chunk)\n\t\t\t\t\tprocessed_files.add(other_chunk.files[0])\n\n\t\t\t# Create a semantic chunk from related files\n\t\t\tif related_chunks:\n\t\t\t\tself._create_semantic_chunk(related_chunks, semantic_chunks)\n\n\tdef _create_semantic_chunk(\n\t\tself,\n\t\trelated_chunks: list[DiffChunk],\n\t\tsemantic_chunks: list[DiffChunk],\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tCreate a semantic chunk from related file chunks.\n\n\t\tArgs:\n\t\t    related_chunks: List of related file chunks\n\t\t    semantic_chunks: List of semantic chunks to append to (modified in place)\n\n\t\t\"\"\"\n\t\tif not related_chunks:\n\t\t\treturn\n\n\t\tall_files = []\n\t\tcombined_content: list[str] = []\n\t\tis_move = any(getattr(chunk, \"is_move\", False) for chunk in related_chunks)\n\n\t\tfor rc in related_chunks:\n\t\t\tall_files.extend(rc.files)\n\t\t\tcombined_content.append(rc.content)\n\n\t\t# Determine if this is a move or a normal change\n\t\tif is_move:\n\t\t\tcommit_type = \"chore\"  # For moves, we always use chore\n\t\t\t# Description will be handled separately for moves\n\t\t\tdescription = related_chunks[0].description if related_chunks else \"Move files\"\n\t\telse:\n\t\t\t# For normal changes, use the regular commit type detection\n\t\t\tcommit_type = determine_commit_type(all_files)\n\t\t\t# Create description based on file count\n\t\t\tdescription = create_chunk_description(commit_type, all_files)\n\n\t\t# Join the content from all related chunks\n\t\tcontent = \"\\n\\n\".join(combined_content)\n\n\t\tsemantic_chunks.append(\n\t\t\tDiffChunk(\n\t\t\t\tfiles=all_files,\n\t\t\t\tcontent=content,\n\t\t\t\tdescription=description,\n\t\t\t\tis_move=is_move,\n\t\t\t)\n\t\t)\n\n\tdef _should_merge_chunks(self, chunk1: DiffChunk, chunk2: DiffChunk) -&gt; bool:\n\t\t\"\"\"Determine if two chunks should be merged.\"\"\"\n\t\t# Condition 1: Same single file\n\t\tsame_file = len(chunk1.files) == 1 and chunk1.files == chunk2.files\n\n\t\t# Condition 2: Related single files\n\t\trelated_files = (\n\t\t\tlen(chunk1.files) == 1\n\t\t\tand len(chunk2.files) == 1\n\t\t\tand are_files_related(chunk1.files[0], chunk2.files[0], self.related_file_patterns)\n\t\t)\n\n\t\t# Return True if either condition is met\n\t\treturn same_file or related_files\n\n\tdef _split_by_semantic_patterns(self, patched_file: PatchedFile, patterns: list[str]) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit a PatchedFile's content by grouping hunks based on semantic patterns.\n\n\t\tThis method groups consecutive hunks together until a hunk is encountered\n\t\tthat contains an added line matching one of the semantic boundary patterns.\n\t\tIt does *not* split within a single hunk, only between hunks where a boundary\n\t\tis detected in the *first* line of the subsequent hunk group.\n\n\t\tArgs:\n\t\t    patched_file: The PatchedFile object from unidiff.\n\t\t    patterns: List of regex pattern strings to match as boundaries.\n\n\t\tReturns:\n\t\t    List of DiffChunk objects, potentially splitting the file into multiple chunks.\n\n\t\t\"\"\"\n\t\tcompiled_patterns = [re.compile(p) for p in patterns]\n\t\tfile_path = patched_file.path  # Or target_file? Need consistency\n\n\t\tfinal_chunks_data: list[list[Hunk]] = []\n\t\tcurrent_semantic_chunk_hunks: list[Hunk] = []\n\n\t\t# Get header info once using the reconstruction helper\n\t\t_, _ = self._reconstruct_file_diff(patched_file)\n\n\t\tfor hunk in patched_file:\n\t\t\thunk_has_boundary = False\n\t\t\tfor line in hunk:\n\t\t\t\tif line.is_added and any(pattern.match(line.value) for pattern in compiled_patterns):\n\t\t\t\t\thunk_has_boundary = True\n\t\t\t\t\tbreak  # Found a boundary in this hunk\n\n\t\t\t# Start a new semantic chunk if the current hunk has a boundary\n\t\t\t# and we already have hunks accumulated.\n\t\t\tif hunk_has_boundary and current_semantic_chunk_hunks:\n\t\t\t\tfinal_chunks_data.append(current_semantic_chunk_hunks)\n\t\t\t\tcurrent_semantic_chunk_hunks = [hunk]  # Start new chunk with this hunk\n\t\t\telse:\n\t\t\t\t# Append the current hunk to the ongoing semantic chunk\n\t\t\t\tcurrent_semantic_chunk_hunks.append(hunk)\n\n\t\t# Add the last accumulated semantic chunk\n\t\tif current_semantic_chunk_hunks:\n\t\t\tfinal_chunks_data.append(current_semantic_chunk_hunks)\n\n\t\t# Convert grouped hunks into DiffChunk objects\n\t\tresult_chunks: list[DiffChunk] = []\n\t\tfor i, hunk_group in enumerate(final_chunks_data):\n\t\t\tif not hunk_group:\n\t\t\t\tcontinue\n\t\t\t# Combine content of all hunks in the group\n\t\t\tgroup_content = \"\\n\".join(str(h) for h in hunk_group)\n\t\t\t# Generate description (could be more sophisticated)\n\t\t\tdescription = f\"Semantic section {i + 1} in {file_path}\"\n\t\t\tresult_chunks.append(\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=group_content,  # Combine header + hunks\n\t\t\t\t\tdescription=description,\n\t\t\t\t)\n\t\t\t)\n\n\t\tlogger.debug(\"Split %s into %d chunks based on semantic patterns\", file_path, len(result_chunks))\n\t\treturn result_chunks\n\n\t@staticmethod\n\tdef _is_valid_filename(filename: str) -&gt; bool:\n\t\t\"\"\"Check if the filename is valid (not a pattern or template).\"\"\"\n\t\tif not filename:\n\t\t\treturn False\n\t\tinvalid_chars = [\"*\", \"+\", \"{\", \"}\", \"\\\\\"]\n\t\treturn not (any(char in filename for char in invalid_chars) or filename.startswith('\"'))\n\n\tasync def _detect_moved_files(self, diff: GitDiff) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tDetect files that have been moved (deleted and added elsewhere).\n\n\t\tThis analyzes the diff to find files that appear to have been deleted from\n\t\tone location and added to another location with similar content.\n\n\t\tArgs:\n\t\t    diff: The git diff to analyze\n\n\t\tReturns:\n\t\t    Dictionary mapping from deleted file paths to their new locations\n\t\t\"\"\"\n\t\t# Parse the diff to identify deleted and added files with their content\n\t\tdeleted_files: dict[str, str] = {}  # path -&gt; content\n\t\tadded_files: dict[str, str] = {}  # path -&gt; content\n\n\t\ttry:\n\t\t\t# Use PatchSet to parse the diff\n\t\t\tpatch_set = PatchSet(StringIO(diff.content))\n\n\t\t\t# Identify deleted and added files\n\t\t\tfor patched_file in patch_set:\n\t\t\t\tif patched_file.is_removed_file:\n\t\t\t\t\t# Extract content from the source (deleted) file\n\t\t\t\t\tfile_path = patched_file.source_file.replace(\"a/\", \"\", 1)\n\t\t\t\t\tfile_content = \"\"\n\t\t\t\t\tfor hunk in patched_file:\n\t\t\t\t\t\tfor line in hunk:\n\t\t\t\t\t\t\t# Line type ' ' is context, '-' is removed\n\t\t\t\t\t\t\tif line.line_type in (\" \", \"-\"):\n\t\t\t\t\t\t\t\tfile_content += line.value\n\t\t\t\t\tdeleted_files[file_path] = file_content\n\t\t\t\telif patched_file.is_added_file:\n\t\t\t\t\t# Extract content from the target (added) file\n\t\t\t\t\tfile_path = patched_file.target_file.replace(\"b/\", \"\", 1)\n\t\t\t\t\tfile_content = \"\"\n\t\t\t\t\tfor hunk in patched_file:\n\t\t\t\t\t\tfor line in hunk:\n\t\t\t\t\t\t\t# Line type ' ' is context, '+' is added\n\t\t\t\t\t\t\tif line.line_type in (\" \", \"+\"):\n\t\t\t\t\t\t\t\tfile_content += line.value\n\t\t\t\t\tadded_files[file_path] = file_content\n\n\t\texcept (ValueError, UnidiffParseError, Exception) as e:\n\t\t\tlogger.warning(f\"Failed to parse diff for move detection: {e}\")\n\t\t\treturn {}\n\n\t\t# Match deleted files with added files based on content similarity\n\t\tmoved_files = {}\n\n\t\t# Group files with same name to avoid unnecessary embedding comparisons\n\t\tpotential_moves = {}\n\t\tfor deleted_path, deleted_content in deleted_files.items():\n\t\t\tif not deleted_content.strip():\n\t\t\t\tcontinue\n\n\t\t\tdeleted_name = Path(deleted_path).name\n\t\t\tpotential_moves.setdefault(deleted_name, {\"deleted\": [], \"added\": []})\n\t\t\tpotential_moves[deleted_name][\"deleted\"].append((deleted_path, deleted_content))\n\n\t\tfor added_path, added_content in added_files.items():\n\t\t\tif not added_content.strip():\n\t\t\t\tcontinue\n\n\t\t\tadded_name = Path(added_path).name\n\t\t\tif added_name in potential_moves:  # Only add if there's a matching deleted file\n\t\t\t\tpotential_moves[added_name][\"added\"].append((added_path, added_content))\n\n\t\t# Process each group of potential moves\n\t\tfor group in potential_moves.values():\n\t\t\tdeleted_items = group[\"deleted\"]\n\t\t\tadded_items = group[\"added\"]\n\n\t\t\tif not deleted_items or not added_items:\n\t\t\t\tcontinue\n\n\t\t\t# Get embeddings for all deleted and added contents in batch\n\t\t\tall_contents = []\n\t\t\tfor _, content in deleted_items + added_items:\n\t\t\t\tall_contents.append(content)\n\t\t\tall_embeddings = await self.embedder.embed_contents(all_contents)\n\n\t\t\t# Split embeddings back to deleted and added\n\t\t\tdeleted_count = len(deleted_items)\n\t\t\tdeleted_embeddings = all_embeddings[:deleted_count]\n\t\t\tadded_embeddings = all_embeddings[deleted_count:]\n\n\t\t\t# Match deleted and added files based on embedding similarity\n\t\t\tfor i, (deleted_path, _) in enumerate(deleted_items):\n\t\t\t\tif deleted_embeddings[i] is None:\n\t\t\t\t\tcontinue\n\n\t\t\t\tbest_match = None\n\t\t\t\tbest_similarity = 0.0\n\n\t\t\t\tfor j, (added_path, _) in enumerate(added_items):\n\t\t\t\t\tif added_embeddings[j] is None:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t# Cast to remove None type since we've checked above\n\t\t\t\t\temb1 = cast(\"list[float]\", deleted_embeddings[i])\n\t\t\t\t\temb2 = cast(\"list[float]\", added_embeddings[j])\n\t\t\t\t\tsimilarity = calculate_semantic_similarity(emb1, emb2)\n\n\t\t\t\t\tif similarity &gt; best_similarity:\n\t\t\t\t\t\tbest_similarity = similarity\n\t\t\t\t\t\tbest_match = added_path\n\n\t\t\t\t# If we found a good match above the threshold, consider it a move\n\t\t\t\tif best_match and best_similarity &gt;= self.file_move_similarity_threshold:\n\t\t\t\t\tmoved_files[deleted_path] = best_match\n\t\t\t\t\tlogger.debug(f\"Detected move: {deleted_path} -&gt; {best_match} (similarity: {best_similarity:.2f})\")\n\n\t\treturn moved_files\n\n\tdef _create_move_chunks(self, moved_files: dict[str, str], diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tCreate diff chunks for moved files.\n\n\t\tArgs:\n\t\t    moved_files: Dictionary mapping from source (deleted) paths to target (added) paths\n\t\t    diff: Original diff containing the move changes\n\n\t\tReturns:\n\t\t    List of DiffChunk objects representing file moves\n\t\t\"\"\"\n\t\tif not moved_files:\n\t\t\treturn []\n\n\t\t# Group moves by common source/target directories\n\t\t# This helps create logical commit groups for moves within the same directories\n\t\tdir_moves: dict[tuple[str, str], list[tuple[str, str]]] = {}\n\n\t\tfor source, target in moved_files.items():\n\t\t\tsource_dir = str(Path(source).parent)\n\t\t\ttarget_dir = str(Path(target).parent)\n\t\t\tdir_key = (source_dir, target_dir)\n\n\t\t\tif dir_key not in dir_moves:\n\t\t\t\tdir_moves[dir_key] = []\n\n\t\t\tdir_moves[dir_key].append((source, target))\n\n\t\t# Create chunks for each move group\n\t\tmove_chunks = []\n\n\t\tfor (source_dir, target_dir), moves in dir_moves.items():\n\t\t\t# Combine source and target files\n\t\t\tall_files = []\n\t\t\tfor source, target in moves:\n\t\t\t\tall_files.extend([source, target])\n\n\t\t\t# Create descriptive move information\n\t\t\tif source_dir == target_dir:\n\t\t\t\t# Rename within same directory\n\t\t\t\tif len(moves) == 1:\n\t\t\t\t\tsource, target = moves[0]\n\t\t\t\t\tdescription = f\"chore: rename {Path(source).name} to {Path(target).name}\"\n\t\t\t\telse:\n\t\t\t\t\tdescription = f\"chore: rename {len(moves)} files in {source_dir}\"\n\t\t\telse:\n\t\t\t\t# Move between directories\n\t\t\t\tsource_dir_desc = \"root directory\" if source_dir in {\".\", \"\"} else source_dir\n\n\t\t\t\ttarget_dir_desc = \"root directory\" if target_dir in {\".\", \"\"} else target_dir\n\n\t\t\t\tif len(moves) == 1:\n\t\t\t\t\tdescription = f\"chore: move {Path(moves[0][0]).name} from {source_dir_desc} to {target_dir_desc}\"\n\t\t\t\telse:\n\t\t\t\t\tdescription = f\"chore: move {len(moves)} files from {source_dir_desc} to {target_dir_desc}\"\n\n\t\t\t# Extract all content related to these moves from the original diff\n\t\t\tchunk_content = \"\"\n\t\t\ttry:\n\t\t\t\tpatch_set = PatchSet(StringIO(diff.content))\n\t\t\t\tfor patched_file in patch_set:\n\t\t\t\t\tsource_file = patched_file.source_file.replace(\"a/\", \"\", 1)\n\t\t\t\t\ttarget_file = patched_file.target_file.replace(\"b/\", \"\", 1)\n\n\t\t\t\t\t# Check if this patched file is part of our move group\n\t\t\t\t\tis_move_source = source_file in [source for source, _ in moves]\n\t\t\t\t\tis_move_target = target_file in [target for _, target in moves]\n\n\t\t\t\t\tif is_move_source or is_move_target:\n\t\t\t\t\t\t# Reconstruct this file's diff content\n\t\t\t\t\t\t_, file_content = self._reconstruct_file_diff(patched_file)\n\t\t\t\t\t\tchunk_content += file_content + \"\\n\"\n\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error extracting content for move chunk\")\n\t\t\t\t# Use a placeholder if extraction failed\n\t\t\t\tchunk_content = f\"# File moves between {source_dir} and {target_dir}:\\n\"\n\t\t\t\tfor source, target in moves:\n\t\t\t\t\tchunk_content += f\"# - {source} -&gt; {target}\\n\"\n\n\t\t\t# Create a single chunk for this move group\n\t\t\tmove_chunks.append(\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=all_files,\n\t\t\t\t\tcontent=chunk_content,\n\t\t\t\t\tdescription=description,\n\t\t\t\t\tis_move=True,  # Indicate this is a move operation\n\t\t\t\t)\n\t\t\t)\n\n\t\treturn move_chunks\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.__init__","title":"__init__","text":"<pre><code>__init__(config_loader: ConfigLoader) -&gt; None\n</code></pre> <p>Initialize the SemanticSplitStrategy.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance.</p> required Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: \"ConfigLoader\",\n) -&gt; None:\n\t\"\"\"\n\tInitialize the SemanticSplitStrategy.\n\n\tArgs:\n\t    config_loader: ConfigLoader instance.\n\t\"\"\"\n\t# Store thresholds and settings\n\tself.similarity_threshold = config_loader.get.commit.diff_splitter.similarity_threshold\n\tself.directory_similarity_threshold = config_loader.get.commit.diff_splitter.directory_similarity_threshold\n\tself.min_chunks_for_consolidation = config_loader.get.commit.diff_splitter.min_chunks_for_consolidation\n\tself.max_chunks_before_consolidation = config_loader.get.commit.diff_splitter.max_chunks_before_consolidation\n\tself.max_file_size_for_llm = config_loader.get.commit.diff_splitter.max_file_size_for_llm\n\tself.file_move_similarity_threshold = config_loader.get.commit.diff_splitter.file_move_similarity_threshold\n\n\t# Set up file extensions, defaulting to config if None is passed\n\tself.code_extensions = config_loader.get.commit.diff_splitter.default_code_extensions\n\t# Initialize patterns for related files\n\tself.related_file_patterns = self._initialize_related_file_patterns()\n\tself.config_loader = config_loader\n\n\t# Create DiffEmbedder instance for embedding operations\n\tself.embedder = DiffEmbedder(config_loader=config_loader)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.similarity_threshold","title":"similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.directory_similarity_threshold","title":"directory_similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>directory_similarity_threshold = (\n\tdirectory_similarity_threshold\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.min_chunks_for_consolidation","title":"min_chunks_for_consolidation  <code>instance-attribute</code>","text":"<pre><code>min_chunks_for_consolidation = min_chunks_for_consolidation\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.max_chunks_before_consolidation","title":"max_chunks_before_consolidation  <code>instance-attribute</code>","text":"<pre><code>max_chunks_before_consolidation = (\n\tmax_chunks_before_consolidation\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.max_file_size_for_llm","title":"max_file_size_for_llm  <code>instance-attribute</code>","text":"<pre><code>max_file_size_for_llm = max_file_size_for_llm\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.file_move_similarity_threshold","title":"file_move_similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>file_move_similarity_threshold = (\n\tfile_move_similarity_threshold\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.code_extensions","title":"code_extensions  <code>instance-attribute</code>","text":"<pre><code>code_extensions = default_code_extensions\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.related_file_patterns","title":"related_file_patterns  <code>instance-attribute</code>","text":"<pre><code>related_file_patterns = _initialize_related_file_patterns()\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.embedder","title":"embedder  <code>instance-attribute</code>","text":"<pre><code>embedder = DiffEmbedder(config_loader=config_loader)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.split","title":"split  <code>async</code>","text":"<pre><code>split(diff: GitDiff) -&gt; list[DiffChunk]\n</code></pre> <p>Split a diff into chunks based on semantic relationships.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>List of DiffChunk objects based on semantic analysis</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>async def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tSplit a diff into chunks based on semantic relationships.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    List of DiffChunk objects based on semantic analysis\n\n\t\"\"\"\n\tif not diff.files:\n\t\tlogger.debug(\"No files to process\")\n\t\treturn []\n\n\t# Initialize an empty list to store all chunks\n\tall_chunks = []\n\n\t# Detect moved files\n\tmoved_files = await self._detect_moved_files(diff)\n\tif moved_files:\n\t\tlogger.info(\"Detected %d moved files\", len(moved_files))\n\t\tmove_chunks = self._create_move_chunks(moved_files, diff)\n\t\tif move_chunks:\n\t\t\t# Add move chunks to all_chunks rather than returning immediately\n\t\t\tall_chunks.extend(move_chunks)\n\n\t\t\t# Create a set of files that are part of moves to avoid processing them again\n\t\t\tmoved_file_paths = set()\n\t\t\tfor chunk in move_chunks:\n\t\t\t\tmoved_file_paths.update(chunk.files)\n\n\t\t\t# Filter out moved files from diff.files to avoid double processing\n\t\t\tnon_moved_files = [f for f in diff.files if f not in moved_file_paths]\n\n\t\t\t# If all files were moves, return just the move chunks\n\t\t\tif not non_moved_files:\n\t\t\t\treturn all_chunks\n\n\t\t\t# Update diff.files to only include non-moved files\n\t\t\tdiff.files = non_moved_files\n\t\t\tlogger.info(\"Continuing with %d non-moved files\", len(non_moved_files))\n\n\t# Handle files in manageable groups\n\tif len(diff.files) &gt; MAX_FILES_PER_GROUP:\n\t\tlogger.info(\"Processing large number of files (%d) in smaller groups\", len(diff.files))\n\n\t\t# Group files by directory to increase likelihood of related files being processed together\n\t\tfiles_by_dir = {}\n\t\tfor file in diff.files:\n\t\t\tdir_path = str(Path(file).parent)\n\t\t\tif dir_path not in files_by_dir:\n\t\t\t\tfiles_by_dir[dir_path] = []\n\t\t\tfiles_by_dir[dir_path].append(file)\n\n\t\t# Process each directory group separately, keeping chunks under 5 files\n\t\t# Iterate directly over the file lists since the directory path isn't used here\n\t\tfor files in files_by_dir.values():\n\t\t\t# Process files in this directory in batches of 3-5\n\t\t\tfor i in range(0, len(files), 3):\n\t\t\t\tbatch = files[i : i + 3]\n\t\t\t\t# Create a new GitDiff for the batch, ensuring content is passed\n\t\t\t\tbatch_diff = GitDiff(\n\t\t\t\t\tfiles=batch,\n\t\t\t\t\tcontent=diff.content,  # Pass the original full diff content\n\t\t\t\t\tis_staged=diff.is_staged,\n\t\t\t\t)\n\t\t\t\tchunks = await self._process_group(batch_diff)\n\t\t\t\tall_chunks.extend(chunks)\n\n\t\treturn all_chunks\n\n\t# For smaller groups, process normally and add to all_chunks\n\tregular_chunks = await self._process_group(diff)\n\tall_chunks.extend(regular_chunks)\n\n\treturn all_chunks\n</code></pre>"},{"location":"api/git/diff_splitter/utils/","title":"Utils","text":"<p>Utility functions for diff splitting.</p>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.get_language_specific_patterns","title":"get_language_specific_patterns","text":"<pre><code>get_language_specific_patterns(language: str) -&gt; list[str]\n</code></pre> <p>Get language-specific regex patterns for code structure.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Programming language identifier</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of regex patterns for the language, or empty list if not supported</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def get_language_specific_patterns(language: str) -&gt; list[str]:\n\t\"\"\"\n\tGet language-specific regex patterns for code structure.\n\n\tArgs:\n\t    language: Programming language identifier\n\n\tReturns:\n\t    A list of regex patterns for the language, or empty list if not supported\n\n\t\"\"\"\n\t# Define pattern strings (used for semantic boundary detection)\n\tpattern_strings = {\n\t\t\"py\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^from\\s+.*\",  # From imports\n\t\t\tr\"^class\\s+\\w+\",  # Class definitions\n\t\t\tr\"^def\\s+\\w+\",  # Function definitions\n\t\t\tr\"^if\\s+__name__\\s*==\\s*['\\\"]__main__['\\\"]\",  # Main block\n\t\t],\n\t\t\"js\": [\n\t\t\tr\"^import\\s+.*\",  # ES6 imports\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*require\",  # CommonJS imports\n\t\t\tr\"^function\\s+\\w+\",  # Function declarations\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*function\",  # Function expressions\n\t\t\tr\"^class\\s+\\w+\",  # Class declarations\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t],\n\t\t\"ts\": [\n\t\t\tr\"^import\\s+.*\",  # Imports\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t\tr\"^interface\\s+\",  # Interfaces\n\t\t\tr\"^type\\s+\",  # Type definitions\n\t\t\tr\"^class\\s+\",  # Classes\n\t\t\tr\"^function\\s+\",  # Functions\n\t\t],\n\t\t\"jsx\": [\n\t\t\tr\"^import\\s+.*\",  # ES6 imports\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*require\",  # CommonJS imports\n\t\t\tr\"^function\\s+\\w+\",  # Function declarations\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*function\",  # Function expressions\n\t\t\tr\"^class\\s+\\w+\",  # Class declarations\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t],\n\t\t\"tsx\": [\n\t\t\tr\"^import\\s+.*\",  # Imports\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t\tr\"^interface\\s+\",  # Interfaces\n\t\t\tr\"^type\\s+\",  # Type definitions\n\t\t\tr\"^class\\s+\",  # Classes\n\t\t\tr\"^function\\s+\",  # Functions\n\t\t],\n\t\t\"java\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^public\\s+class\",  # Public class\n\t\t\tr\"^private\\s+class\",  # Private class\n\t\t\tr\"^(public|private|protected)(\\s+static)?\\s+\\w+\\s+\\w+\\(\",  # Methods\n\t\t],\n\t\t\"go\": [\n\t\t\tr\"^import\\s+\",  # Import statements\n\t\t\tr\"^func\\s+\",  # Function definitions\n\t\t\tr\"^type\\s+\\w+\\s+struct\",  # Struct definitions\n\t\t],\n\t\t\"rb\": [\n\t\t\tr\"^require\\s+\",  # Requires\n\t\t\tr\"^class\\s+\",  # Class definitions\n\t\t\tr\"^def\\s+\",  # Method definitions\n\t\t\tr\"^module\\s+\",  # Module definitions\n\t\t],\n\t\t\"php\": [\n\t\t\tr\"^namespace\\s+\",  # Namespace declarations\n\t\t\tr\"^use\\s+\",  # Use statements\n\t\t\tr\"^class\\s+\",  # Class definitions\n\t\t\tr\"^(public|private|protected)\\s+function\",  # Methods\n\t\t],\n\t\t\"cs\": [\n\t\t\tr\"^using\\s+\",  # Using directives\n\t\t\tr\"^namespace\\s+\",  # Namespace declarations\n\t\t\tr\"^(public|private|protected|internal)\\s+class\",  # Classes\n\t\t\tr\"^(public|private|protected|internal)(\\s+static)?\\s+\\w+\\s+\\w+\\(\",  # Methods\n\t\t],\n\t\t\"kt\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^class\\s+\\w+\",  # Class definitions\n\t\t\tr\"^fun\\s+\\w+\",  # Function definitions\n\t\t\tr\"^val\\s+\\w+\",  # Val declarations\n\t\t\tr\"^var\\s+\\w+\",  # Var declarations\n\t\t],\n\t\t\"scala\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^class\\s+\\w+\",  # Class definitions\n\t\t\tr\"^object\\s+\\w+\",  # Object definitions\n\t\t\tr\"^def\\s+\\w+\",  # Method definitions\n\t\t\tr\"^val\\s+\\w+\",  # Val declarations\n\t\t\tr\"^var\\s+\\w+\",  # Var declarations\n\t\t],\n\t}\n\n\t# Return pattern strings for the language or empty list if not supported\n\treturn pattern_strings.get(language, [])\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.determine_commit_type","title":"determine_commit_type","text":"<pre><code>determine_commit_type(files: list[str]) -&gt; str\n</code></pre> <p>Determine the appropriate commit type based on the files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of file paths</p> required <p>Returns:</p> Type Description <code>str</code> <p>Commit type string (e.g., \"feat\", \"fix\", \"test\", \"docs\", \"chore\")</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def determine_commit_type(files: list[str]) -&gt; str:\n\t\"\"\"\n\tDetermine the appropriate commit type based on the files.\n\n\tArgs:\n\t    files: List of file paths\n\n\tReturns:\n\t    Commit type string (e.g., \"feat\", \"fix\", \"test\", \"docs\", \"chore\")\n\n\t\"\"\"\n\t# Check for test files\n\tif any(f.startswith(\"tests/\") or \"_test.\" in f or \"test_\" in f for f in files):\n\t\treturn \"test\"\n\n\t# Check for documentation files\n\tif any(f.startswith(\"docs/\") or f.endswith(\".md\") for f in files):\n\t\treturn \"docs\"\n\n\t# Check for configuration files\n\tif any(f.endswith((\".json\", \".yml\", \".yaml\", \".toml\", \".ini\", \".cfg\")) for f in files):\n\t\treturn \"chore\"\n\n\t# Default to \"chore\" for general updates\n\treturn \"chore\"\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.create_chunk_description","title":"create_chunk_description","text":"<pre><code>create_chunk_description(\n\tcommit_type: str, files: list[str]\n) -&gt; str\n</code></pre> <p>Create a meaningful description for a chunk.</p> <p>Parameters:</p> Name Type Description Default <code>commit_type</code> <code>str</code> <p>Type of commit (e.g., \"feat\", \"fix\")</p> required <code>files</code> <code>list[str]</code> <p>List of file paths</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description string</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def create_chunk_description(commit_type: str, files: list[str]) -&gt; str:\n\t\"\"\"\n\tCreate a meaningful description for a chunk.\n\n\tArgs:\n\t    commit_type: Type of commit (e.g., \"feat\", \"fix\")\n\t    files: List of file paths\n\n\tReturns:\n\t    Description string\n\n\t\"\"\"\n\tif len(files) == 1:\n\t\treturn f\"{commit_type}: update {files[0]}\"\n\n\t# Try to find a common directory using Path for better cross-platform compatibility\n\ttry:\n\t\tcommon_dir = Path(os.path.commonpath(files))\n\t\tif str(common_dir) not in (\".\", \"\"):\n\t\t\treturn f\"{commit_type}: update files in {common_dir}\"\n\texcept ValueError:\n\t\t# commonpath raises ValueError if files are on different drives\n\t\tpass\n\n\treturn f\"{commit_type}: update {len(files)} related files\"\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.get_deleted_tracked_files","title":"get_deleted_tracked_files","text":"<pre><code>get_deleted_tracked_files() -&gt; tuple[set, set]\n</code></pre> <p>Get list of deleted but tracked files from git status.</p> <p>Returns:</p> Type Description <code>tuple[set, set]</code> <p>Tuple of (deleted_unstaged_files, deleted_staged_files) as sets</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def get_deleted_tracked_files() -&gt; tuple[set, set]:\n\t\"\"\"\n\tGet list of deleted but tracked files from git status.\n\n\tReturns:\n\t    Tuple of (deleted_unstaged_files, deleted_staged_files) as sets\n\n\t\"\"\"\n\tdeleted_unstaged_files = set()\n\tdeleted_staged_files = set()\n\ttry:\n\t\t# Parse git status to find deleted files\n\t\tcontext = ExtendedGitRepoContext.get_instance()\n\t\tstatus = context.repo.status()\n\t\tfor filepath, flags in status.items():\n\t\t\tif flags &amp; FileStatus.WT_DELETED:  # Worktree deleted (unstaged)\n\t\t\t\tdeleted_unstaged_files.add(filepath)\n\t\t\tif flags &amp; FileStatus.INDEX_DELETED:  # Index deleted (staged)\n\t\t\t\tdeleted_staged_files.add(filepath)\n\t\tlogger.debug(\"Found %d deleted unstaged files in git status\", len(deleted_unstaged_files))\n\t\tlogger.debug(\"Found %d deleted staged files in git status\", len(deleted_staged_files))\n\texcept GitError as e:  # Catch specific GitError from context operations\n\t\tlogger.warning(\n\t\t\t\"Failed to get git status for deleted files via context: %s. Proceeding without deleted file info.\", e\n\t\t)\n\texcept Exception:  # Catch any other unexpected error\n\t\tlogger.exception(\"Unexpected error getting git status: %s. Proceeding without deleted file info.\")\n\n\treturn deleted_unstaged_files, deleted_staged_files\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.filter_valid_files","title":"filter_valid_files","text":"<pre><code>filter_valid_files(\n\tfiles: list[str],\n\trepo_root: Path,\n\tis_test_environment: bool = False,\n) -&gt; tuple[list[str], list[str]]\n</code></pre> <p>Filter invalid filenames and files based on existence and Git tracking.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of file paths to filter</p> required <code>repo_root</code> <code>Path</code> <p>Path to the repository root</p> required <code>is_test_environment</code> <code>bool</code> <p>Whether running in a test environment</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>Tuple of (valid_files, empty_list) - The second element is always an empty list now.</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def filter_valid_files(\n\tfiles: list[str], repo_root: Path, is_test_environment: bool = False\n) -&gt; tuple[list[str], list[str]]:\n\t\"\"\"\n\tFilter invalid filenames and files based on existence and Git tracking.\n\n\tArgs:\n\t    files: List of file paths to filter\n\t    repo_root: Path to the repository root\n\t    is_test_environment: Whether running in a test environment\n\n\tReturns:\n\t    Tuple of (valid_files, empty_list) - The second element is always an empty list now.\n\n\t\"\"\"\n\tif not files:\n\t\treturn [], []\n\n\tvalid_files_intermediate = []\n\t# Keep track of files filtered due to large size if needed elsewhere,\n\t# but don't remove them from processing yet.\n\n\tfor file in files:\n\t\t# Skip files that look like patterns or templates\n\t\tif any(char in file for char in [\"*\", \"+\", \"{\", \"}\", \"\\\\\"]) or file.startswith('\"'):\n\t\t\tlogger.warning(\"Skipping invalid filename in diff processing: %s\", file)\n\t\t\tcontinue\n\t\tvalid_files_intermediate.append(file)\n\n\t# --- File Existence and Git Tracking Checks ---\n\tvalid_files = []  # Reset valid_files to populate after existence checks\n\n\t# Skip file existence checks in test environments\n\tif is_test_environment:\n\t\tlogger.debug(\"In test environment - skipping file existence checks for %d files\", len(valid_files_intermediate))\n\t\t# In test env, assume all intermediate files are valid regarding existence/tracking\n\t\tvalid_files = valid_files_intermediate\n\telse:\n\t\t# Get deleted files\n\t\tdeleted_unstaged_files, deleted_staged_files = get_deleted_tracked_files()\n\n\t\t# Check if files exist in the repository (tracked by git) or filesystem\n\t\toriginal_count = len(valid_files_intermediate)\n\t\ttry:\n\t\t\tcontext = ExtendedGitRepoContext.get_instance()\n\t\t\ttracked_files = set(context.tracked_files.keys())\n\n\t\t\t# Keep files that either:\n\t\t\t# 1. Exist in filesystem\n\t\t\t# 2. Are tracked by git\n\t\t\t# 3. Are known deleted files from git status\n\t\t\t# 4. Are already staged deletions\n\t\t\tfiltered_files = []\n\t\t\tfor file in valid_files_intermediate:\n\t\t\t\ttry:\n\t\t\t\t\tpath_exists = Path(get_absolute_path(file, repo_root)).exists()\n\t\t\t\texcept OSError as e:\n\t\t\t\t\tlogger.warning(\"OS error checking existence for %s: %s. Skipping file.\", file, e)\n\t\t\t\t\tcontinue\n\t\t\t\texcept Exception:\n\t\t\t\t\tlogger.exception(\"Unexpected error checking existence for %s. Skipping file.\", file)\n\t\t\t\t\tcontinue\n\n\t\t\t\tif (\n\t\t\t\t\tpath_exists\n\t\t\t\t\tor file in tracked_files\n\t\t\t\t\tor file in deleted_unstaged_files\n\t\t\t\t\tor file in deleted_staged_files\n\t\t\t\t):\n\t\t\t\t\tfiltered_files.append(file)\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\"Skipping non-existent/untracked/not-deleted file in diff: %s\", file)\n\n\t\t\tvalid_files = filtered_files\n\t\t\tif len(valid_files) &lt; original_count:\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"Filtered out %d files that don't exist or aren't tracked/deleted\",\n\t\t\t\t\toriginal_count - len(valid_files),\n\t\t\t\t)\n\t\texcept GitError as e:  # Catch GitError from context operations\n\t\t\tlogger.warning(\"Failed to get tracked files from git context: %s. Filtering based on existence only.\", e)\n\t\t\t# If we can't check git tracked files, filter by filesystem existence and git status\n\t\t\tfiltered_files_fallback = []\n\t\t\tfor file in valid_files_intermediate:\n\t\t\t\ttry:\n\t\t\t\t\tpath_exists = Path(file).exists()\n\t\t\t\texcept OSError as e:\n\t\t\t\t\tlogger.warning(\"OS error checking existence for %s: %s. Skipping file.\", file, e)\n\t\t\t\t\tcontinue\n\t\t\t\texcept Exception:\n\t\t\t\t\tlogger.exception(\"Unexpected error checking existence for %s. Skipping file.\", file)\n\t\t\t\t\tcontinue\n\n\t\t\t\tif path_exists or file in deleted_unstaged_files or file in deleted_staged_files:\n\t\t\t\t\tfiltered_files_fallback.append(file)\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\"Skipping non-existent/not-deleted file in diff (git check failed): %s\", file)\n\n\t\t\tvalid_files = filtered_files_fallback  # Replace valid_files with the fallback list\n\t\t\tif len(valid_files) &lt; original_count:\n\t\t\t\t# Adjust log message if git check failed\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"Filtered out %d files that don't exist (git check failed)\",\n\t\t\t\t\toriginal_count - len(valid_files),\n\t\t\t\t)\n\t\texcept Exception:  # Catch any other unexpected errors during the initial try block\n\t\t\tlogger.exception(\"Unexpected error during file filtering. Proceeding with potentially incorrect list.\")\n\t\t\t# If a catastrophic error occurs, proceed with the intermediate list\n\t\t\tvalid_files = valid_files_intermediate\n\n\t# Return only the list of valid files. The concept of 'filtered_large_files' is removed.\n\t# Size checking will now happen within the splitting strategy.\n\treturn valid_files, []  # Return empty list for the second element now.\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.is_test_environment","title":"is_test_environment","text":"<pre><code>is_test_environment() -&gt; bool\n</code></pre> <p>Check if the code is running in a test environment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if in a test environment, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def is_test_environment() -&gt; bool:\n\t\"\"\"\n\tCheck if the code is running in a test environment.\n\n\tReturns:\n\t    True if in a test environment, False otherwise\n\n\t\"\"\"\n\t# Check multiple environment indicators for tests\n\treturn \"PYTEST_CURRENT_TEST\" in os.environ or \"pytest\" in sys.modules or os.environ.get(\"TESTING\") == \"1\"\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.calculate_semantic_similarity","title":"calculate_semantic_similarity","text":"<pre><code>calculate_semantic_similarity(\n\temb1: list[float], emb2: list[float]\n) -&gt; float\n</code></pre> <p>Calculate semantic similarity (cosine similarity) between two embedding vectors.</p> <p>Parameters:</p> Name Type Description Default <code>emb1</code> <code>list[float]</code> <p>First embedding vector</p> required <code>emb2</code> <code>list[float]</code> <p>Second embedding vector</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0 and 1</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def calculate_semantic_similarity(emb1: list[float], emb2: list[float]) -&gt; float:\n\t\"\"\"\n\tCalculate semantic similarity (cosine similarity) between two embedding vectors.\n\n\tArgs:\n\t    emb1: First embedding vector\n\t    emb2: Second embedding vector\n\n\tReturns:\n\t    Similarity score between 0 and 1\n\n\t\"\"\"\n\tif not emb1 or not emb2:\n\t\treturn 0.0\n\n\ttry:\n\t\t# Convert to numpy arrays\n\t\tvec1 = np.array(emb1, dtype=np.float64)\n\t\tvec2 = np.array(emb2, dtype=np.float64)\n\n\t\t# Calculate cosine similarity\n\t\tdot_product = np.dot(vec1, vec2)\n\t\tnorm1 = np.linalg.norm(vec1)\n\t\tnorm2 = np.linalg.norm(vec2)\n\n\t\tif norm1 &lt;= EPSILON or norm2 &lt;= EPSILON:\n\t\t\treturn 0.0\n\n\t\tsimilarity = float(dot_product / (norm1 * norm2))\n\n\t\t# Handle potential numeric issues\n\t\tif not np.isfinite(similarity):\n\t\t\treturn 0.0\n\n\t\treturn max(0.0, min(1.0, similarity))  # Clamp to [0, 1]\n\n\texcept (ValueError, TypeError, ArithmeticError, OverflowError):\n\t\tlogger.warning(\"Failed to calculate similarity\")\n\t\treturn 0.0\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.match_test_file_patterns","title":"match_test_file_patterns","text":"<pre><code>match_test_file_patterns(file1: str, file2: str) -&gt; bool\n</code></pre> <p>Check if files match common test file patterns.</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def match_test_file_patterns(file1: str, file2: str) -&gt; bool:\n\t\"\"\"Check if files match common test file patterns.\"\"\"\n\t# test_X.py and X.py patterns\n\tif file1.startswith(\"test_\") and file1[5:] == file2:\n\t\treturn True\n\tif file2.startswith(\"test_\") and file2[5:] == file1:\n\t\treturn True\n\n\t# X_test.py and X.py patterns\n\tif file1.endswith(\"_test.py\") and file1[:-8] + \".py\" == file2:\n\t\treturn True\n\treturn bool(file2.endswith(\"_test.py\") and file2[:-8] + \".py\" == file1)\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.have_similar_names","title":"have_similar_names","text":"<pre><code>have_similar_names(file1: str, file2: str) -&gt; bool\n</code></pre> <p>Check if files have similar base names.</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def have_similar_names(file1: str, file2: str) -&gt; bool:\n\t\"\"\"Check if files have similar base names.\"\"\"\n\tbase1 = file1.rsplit(\".\", 1)[0] if \".\" in file1 else file1\n\tbase2 = file2.rsplit(\".\", 1)[0] if \".\" in file2 else file2\n\n\treturn (base1 in base2 or base2 in base1) and min(len(base1), len(base2)) &gt;= MIN_NAME_LENGTH_FOR_SIMILARITY\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.has_related_file_pattern","title":"has_related_file_pattern","text":"<pre><code>has_related_file_pattern(\n\tfile1: str,\n\tfile2: str,\n\trelated_file_patterns: Iterable[\n\t\ttuple[Pattern, Pattern]\n\t],\n) -&gt; bool\n</code></pre> <p>Check if files match known related patterns.</p> <p>Parameters:</p> Name Type Description Default <code>file1</code> <code>str</code> <p>First file path</p> required <code>file2</code> <code>str</code> <p>Second file path</p> required <code>related_file_patterns</code> <code>Iterable[tuple[Pattern, Pattern]]</code> <p>Compiled regex pattern pairs to check against</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the files match a known pattern, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def has_related_file_pattern(file1: str, file2: str, related_file_patterns: Iterable[tuple[Pattern, Pattern]]) -&gt; bool:\n\t\"\"\"\n\tCheck if files match known related patterns.\n\n\tArgs:\n\t    file1: First file path\n\t    file2: Second file path\n\t    related_file_patterns: Compiled regex pattern pairs to check against\n\n\tReturns:\n\t    True if the files match a known pattern, False otherwise\n\n\t\"\"\"\n\tfor pattern1, pattern2 in related_file_patterns:\n\t\tif (pattern1.match(file1) and pattern2.match(file2)) or (pattern2.match(file1) and pattern1.match(file2)):\n\t\t\treturn True\n\treturn False\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.are_files_related","title":"are_files_related","text":"<pre><code>are_files_related(\n\tfile1: str,\n\tfile2: str,\n\trelated_file_patterns: Iterable[\n\t\ttuple[Pattern, Pattern]\n\t],\n) -&gt; bool\n</code></pre> <p>Determine if two files are semantically related based on various criteria.</p> <p>Parameters:</p> Name Type Description Default <code>file1</code> <code>str</code> <p>First file path</p> required <code>file2</code> <code>str</code> <p>Second file path</p> required <code>related_file_patterns</code> <code>Iterable[tuple[Pattern, Pattern]]</code> <p>Compiled regex pattern pairs for pattern matching</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the files are related, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def are_files_related(file1: str, file2: str, related_file_patterns: Iterable[tuple[Pattern, Pattern]]) -&gt; bool:\n\t\"\"\"\n\tDetermine if two files are semantically related based on various criteria.\n\n\tArgs:\n\t    file1: First file path\n\t    file2: Second file path\n\t    related_file_patterns: Compiled regex pattern pairs for pattern matching\n\n\tReturns:\n\t    True if the files are related, False otherwise\n\n\t\"\"\"\n\t# 1. Files in the same directory\n\tdir1 = file1.rsplit(\"/\", 1)[0] if \"/\" in file1 else \"\"\n\tdir2 = file2.rsplit(\"/\", 1)[0] if \"/\" in file2 else \"\"\n\tif dir1 and dir1 == dir2:\n\t\treturn True\n\n\t# 2. Files in closely related directories (parent/child or same root directory)\n\tif dir1 and dir2:\n\t\tif dir1.startswith(dir2 + \"/\") or dir2.startswith(dir1 + \"/\"):\n\t\t\treturn True\n\t\t# Check if they share the same top-level directory\n\t\ttop_dir1 = dir1.split(\"/\", 1)[0] if \"/\" in dir1 else dir1\n\t\ttop_dir2 = dir2.split(\"/\", 1)[0] if \"/\" in dir2 else dir2\n\t\tif top_dir1 and top_dir1 == top_dir2:\n\t\t\treturn True\n\n\t# 3. Test files and implementation files (simple check)\n\tif (file1.startswith(\"tests/\") and file2 in file1) or (file2.startswith(\"tests/\") and file1 in file2):\n\t\treturn True\n\n\t# 4. Test file patterns\n\tfile1_name = file1.rsplit(\"/\", 1)[-1] if \"/\" in file1 else file1\n\tfile2_name = file2.rsplit(\"/\", 1)[-1] if \"/\" in file2 else file2\n\tif match_test_file_patterns(file1_name, file2_name):\n\t\treturn True\n\n\t# 5. Files with similar names\n\tif have_similar_names(file1_name, file2_name):\n\t\treturn True\n\n\t# 6. Check for related file patterns\n\treturn has_related_file_pattern(file1, file2, related_file_patterns)\n</code></pre>"},{"location":"api/git/pr_generator/","title":"Pr Generator Overview","text":"<p>PR generation package for CodeMap.</p> <ul> <li>Command - Main PR generation command implementation for CodeMap.</li> <li>Constants - Constants for PR generation.</li> <li>Decorators - Decorators for the PR generator module.</li> <li>Generator - PR generator for the CodeMap Git module.</li> <li>Pr Git Utils - Utility functions for PR generation.</li> <li>Prompts - Prompt templates for PR generation.</li> <li>Schemas - Schemas and data structures for PR generation.</li> <li>Strategies - Git workflow strategy implementations for PR management.</li> <li>Templates - PR template definitions for different workflow strategies.</li> <li>Utils - Utility functions for PR generation.</li> </ul>"},{"location":"api/git/pr_generator/command/","title":"Command","text":"<p>Main PR generation command implementation for CodeMap.</p>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand","title":"PRCommand","text":"<p>Handles the PR generation command workflow.</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>class PRCommand:\n\t\"\"\"Handles the PR generation command workflow.\"\"\"\n\n\tdef __init__(self, config_loader: ConfigLoader, path: Path | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the PR command.\n\n\t\tArgs:\n\t\t    config_loader: ConfigLoader instance\n\t\t    path: Optional path to start from\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.repo_root = ExtendedGitRepoContext.get_repo_root(path)\n\n\t\t\t# Create LLM client and configs\n\t\t\tfrom codemap.llm import LLMClient\n\n\t\t\tllm_client = LLMClient(config_loader=config_loader, repo_path=self.repo_root)\n\n\t\t\t# Create the PR generator with required parameters\n\t\t\tself.pr_generator = PRGenerator(\n\t\t\t\trepo_path=self.repo_root,\n\t\t\t\tllm_client=llm_client,\n\t\t\t)\n\n\t\t\tself.error_state: str | None = None  # Tracks reason for failure: \"failed\", \"aborted\", etc.\n\t\texcept GitError as e:\n\t\t\traise RuntimeError(str(e)) from e\n\n\tdef _get_branch_info(self) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tGet information about the current branch and its target.\n\n\t\tReturns:\n\t\t    Dictionary with branch information\n\n\t\tRaises:\n\t\t    RuntimeError: If Git operations fail\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\trepo = pgu.repo\n\n\t\t\t# Get current branch\n\t\t\tcurrent_branch = pgu.get_current_branch()\n\t\t\tif not current_branch:\n\t\t\t\tmsg = \"Failed to determine current branch using PRGitUtils.\"\n\t\t\t\traise GitError(msg)\n\n\t\t\t# Get default branch (usually main or master)\n\t\t\t# get_default_branch from strategies.py uses PRGitUtils instance\n\t\t\tdefault_branch_name = get_default_branch(pgu_instance=pgu)\n\t\t\tif not default_branch_name:\n\t\t\t\tmsg = \"Failed to determine default branch using PRGitUtils and strategies.\"\n\t\t\t\t# Attempt to find common names if strategy failed, as a fallback.\n\t\t\t\tcommon_defaults = [\"main\", \"master\"]\n\t\t\t\tfor common_default in common_defaults:\n\t\t\t\t\tif f\"origin/{common_default}\" in repo.branches.remote or common_default in repo.branches.local:\n\t\t\t\t\t\tdefault_branch_name = common_default\n\t\t\t\t\t\tbreak\n\t\t\t\tif not default_branch_name:  # Still not found\n\t\t\t\t\tmsg = \"Could not determine default/target branch.\"\n\t\t\t\t\traise GitError(msg)\n\n\t\t\treturn {\"current_branch\": current_branch, \"target_branch\": default_branch_name}\n\t\texcept (GitError, Pygit2GitError) as e:\n\t\t\tmsg = f\"Failed to get branch information: {e}\"\n\t\t\traise RuntimeError(msg) from e\n\t\texcept Exception as e:\n\t\t\tmsg = f\"Unexpected error getting branch information: {e}\"\n\t\t\tlogger.exception(\"Unexpected error in _get_branch_info\")\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _get_commit_history(self, base_branch: str) -&gt; list[dict[str, str]]:\n\t\t\"\"\"\n\t\tGet commit history between the current branch and the base branch.\n\n\t\tArgs:\n\t\t    base_branch: The base branch to compare against\n\n\t\tReturns:\n\t\t    List of commits with their details\n\n\t\tRaises:\n\t\t    RuntimeError: If Git operations fail\n\n\t\t\"\"\"\n\t\tpgu = PRGitUtils.get_instance()\n\t\trepo = pgu.repo\n\t\tcommits_data = []\n\t\ttry:\n\t\t\thead_commit_obj = repo.revparse_single(\"HEAD\").peel(Commit)\n\t\t\tbase_commit_obj = repo.revparse_single(base_branch).peel(Commit)\n\n\t\t\t# Find the merge base between head and base\n\t\t\tmerge_base_oid = repo.merge_base(base_commit_obj.id, head_commit_obj.id)\n\n\t\t\t# Walk from HEAD, newest first\n\t\t\tfor commit in repo.walk(head_commit_obj.id, SortMode.TOPOLOGICAL):\n\t\t\t\tif merge_base_oid and commit.id == merge_base_oid:\n\t\t\t\t\tbreak  # Stop if we reached the merge base\n\n\t\t\t\t# Additional check: if the commit is an ancestor of the base_commit_obj\n\t\t\t\t# and it's not the merge_base itself, it means we're on the base branch's\n\t\t\t\t# history before the divergence. This can happen in complex histories\n\t\t\t\t# if merge_base_oid is None or the walk somehow includes them.\n\t\t\t\t# The primary stop condition is hitting the merge_base_oid.\n\t\t\t\tif (\n\t\t\t\t\trepo.descendant_of(commit.id, base_commit_obj.id)\n\t\t\t\t\tand (not merge_base_oid or commit.id != merge_base_oid)\n\t\t\t\t\tand commit.id != head_commit_obj.id\n\t\t\t\t):  # Don't stop if base is HEAD or commit is HEAD\n\t\t\t\t\t# This commit is reachable from base_commit_obj, so it's part of base's history\n\t\t\t\t\t# This logic ensures we only take commits unique to the current branch after divergence\n\t\t\t\t\t# For `base..HEAD` this means commit is reachable from HEAD but not base.\n\t\t\t\t\t# The simple walk from head_commit_obj stopping at merge_base_oid should correctly\n\t\t\t\t\t# implement \"commits on head since it diverged from base\".\n\t\t\t\t\tpass  # The break at merge_base_oid is the key.\n\n\t\t\t\tcommit_subject = commit.message.splitlines()[0].strip() if commit.message else \"\"\n\t\t\t\tcommits_data.append(\n\t\t\t\t\t{\n\t\t\t\t\t\t\"hash\": str(commit.short_id),\n\t\t\t\t\t\t\"author\": commit.author.name if commit.author else \"Unknown\",\n\t\t\t\t\t\t\"subject\": commit_subject,\n\t\t\t\t\t}\n\t\t\t\t)\n\t\t\treturn commits_data\n\t\texcept Pygit2GitError as e:\n\t\t\tmsg = f\"Failed to get commit history using pygit2: {e}\"\n\t\t\tlogger.exception(\"pygit2 error in _get_commit_history\")\n\t\t\traise RuntimeError(msg) from e\n\t\texcept Exception as e:\n\t\t\t# Catch other potential errors like branch not found from revparse_single\n\t\t\tmsg = f\"Unexpected error getting commit history: {e}\"\n\t\t\tlogger.exception(\"Unexpected error in _get_commit_history\")\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _generate_pr_description(self, branch_info: dict[str, str], _commits: list[dict[str, str]]) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate PR description based on branch info and commit history.\n\n\t\tArgs:\n\t\t    branch_info: Information about the branches\n\t\t    _commits: List of commits to include in the description (fetched internally by PRGenerator)\n\n\t\tReturns:\n\t\t    Generated PR description\n\n\t\tRaises:\n\t\t    RuntimeError: If description generation fails\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\twith progress_indicator(\"Generating PR description using LLM...\"):\n\t\t\t\t# Use the PR generator to create content\n\t\t\t\tcontent = self.pr_generator.generate_content_from_commits(\n\t\t\t\t\tbase_branch=branch_info[\"target_branch\"], head_branch=branch_info[\"current_branch\"], use_llm=True\n\t\t\t\t)\n\t\t\t\treturn content[\"description\"]\n\t\texcept LLMError as e:\n\t\t\tlogger.exception(\"LLM description generation failed\")\n\t\t\tlogger.warning(\"LLM error: %s\", str(e))\n\n\t\t\t# Generate a simple fallback description without LLM\n\t\t\twith progress_indicator(\"Falling back to simple PR description generation...\"):\n\t\t\t\tcontent = self.pr_generator.generate_content_from_commits(\n\t\t\t\t\tbase_branch=branch_info[\"target_branch\"], head_branch=branch_info[\"current_branch\"], use_llm=False\n\t\t\t\t)\n\t\t\t\treturn content[\"description\"]\n\t\texcept (ValueError, RuntimeError) as e:\n\t\t\tlogger.warning(\"Error generating PR description: %s\", str(e))\n\t\t\tmsg = f\"Failed to generate PR description: {e}\"\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _raise_no_commits_error(self, branch_info: dict[str, str]) -&gt; None:\n\t\t\"\"\"\n\t\tRaise an error when no commits are found between branches.\n\n\t\tArgs:\n\t\t    branch_info: Information about the branches\n\n\t\tRaises:\n\t\t    RuntimeError: Always raises this error with appropriate message\n\n\t\t\"\"\"\n\t\tmsg = f\"No commits found between {branch_info['current_branch']} and {branch_info['target_branch']}\"\n\t\tlogger.warning(msg)\n\t\traise RuntimeError(msg)\n\n\tdef run(self) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tRun the PR generation command.\n\n\t\tReturns:\n\t\t    Dictionary with PR information and generated description\n\n\t\tRaises:\n\t\t    RuntimeError: If the command fails\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get branch information\n\t\t\twith progress_indicator(\"Getting branch information...\"):\n\t\t\t\tbranch_info = self._get_branch_info()\n\n\t\t\t# Get commit history\n\t\t\twith progress_indicator(\"Retrieving commit history...\"):\n\t\t\t\tcommits = self._get_commit_history(branch_info[\"target_branch\"])\n\n\t\t\tif not commits:\n\t\t\t\tself._raise_no_commits_error(branch_info)\n\n\t\t\t# Generate PR description\n\t\t\tdescription = self._generate_pr_description(branch_info, commits)\n\n\t\t\treturn {\"branch_info\": branch_info, \"commits\": commits, \"description\": description}\n\t\texcept (RuntimeError, ValueError) as e:\n\t\t\tself.error_state = \"failed\"\n\t\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig_loader: ConfigLoader, path: Path | None = None\n) -&gt; None\n</code></pre> <p>Initialize the PR command.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance</p> required <code>path</code> <code>Path | None</code> <p>Optional path to start from</p> <code>None</code> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def __init__(self, config_loader: ConfigLoader, path: Path | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize the PR command.\n\n\tArgs:\n\t    config_loader: ConfigLoader instance\n\t    path: Optional path to start from\n\n\t\"\"\"\n\ttry:\n\t\tself.repo_root = ExtendedGitRepoContext.get_repo_root(path)\n\n\t\t# Create LLM client and configs\n\t\tfrom codemap.llm import LLMClient\n\n\t\tllm_client = LLMClient(config_loader=config_loader, repo_path=self.repo_root)\n\n\t\t# Create the PR generator with required parameters\n\t\tself.pr_generator = PRGenerator(\n\t\t\trepo_path=self.repo_root,\n\t\t\tllm_client=llm_client,\n\t\t)\n\n\t\tself.error_state: str | None = None  # Tracks reason for failure: \"failed\", \"aborted\", etc.\n\texcept GitError as e:\n\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = get_repo_root(path)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.pr_generator","title":"pr_generator  <code>instance-attribute</code>","text":"<pre><code>pr_generator = PRGenerator(\n\trepo_path=repo_root, llm_client=llm_client\n)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.error_state","title":"error_state  <code>instance-attribute</code>","text":"<pre><code>error_state: str | None = None\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.run","title":"run","text":"<pre><code>run() -&gt; dict[str, Any]\n</code></pre> <p>Run the PR generation command.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with PR information and generated description</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the command fails</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def run(self) -&gt; dict[str, Any]:\n\t\"\"\"\n\tRun the PR generation command.\n\n\tReturns:\n\t    Dictionary with PR information and generated description\n\n\tRaises:\n\t    RuntimeError: If the command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Get branch information\n\t\twith progress_indicator(\"Getting branch information...\"):\n\t\t\tbranch_info = self._get_branch_info()\n\n\t\t# Get commit history\n\t\twith progress_indicator(\"Retrieving commit history...\"):\n\t\t\tcommits = self._get_commit_history(branch_info[\"target_branch\"])\n\n\t\tif not commits:\n\t\t\tself._raise_no_commits_error(branch_info)\n\n\t\t# Generate PR description\n\t\tdescription = self._generate_pr_description(branch_info, commits)\n\n\t\treturn {\"branch_info\": branch_info, \"commits\": commits, \"description\": description}\n\texcept (RuntimeError, ValueError) as e:\n\t\tself.error_state = \"failed\"\n\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand","title":"PRWorkflowCommand","text":"<p>Handles the core PR creation and update workflow logic.</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>class PRWorkflowCommand:\n\t\"\"\"Handles the core PR creation and update workflow logic.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: ConfigLoader,\n\t\tllm_client: LLMClient | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the PR workflow command helper.\n\n\t\tArgs:\n\t\t        config_loader: ConfigLoader instance.\n\t\t        llm_client: Optional pre-configured LLMClient.\n\n\t\t\"\"\"\n\t\tself.config_loader = config_loader\n\n\t\tif self.config_loader.get.repo_root is None:\n\t\t\tself.repo_root = ExtendedGitRepoContext.get_repo_root()\n\t\telse:\n\t\t\tself.repo_root = self.config_loader.get.repo_root\n\n\t\tself.pr_config = self.config_loader.get.pr\n\t\tself.content_config = self.pr_config.generate\n\t\tself.workflow_strategy_name = self.config_loader.get.pr.strategy\n\t\tself.workflow = create_strategy(self.workflow_strategy_name)\n\n\t\t# Initialize LLM client if needed\n\t\tif llm_client:\n\t\t\tself.llm_client = llm_client\n\t\telse:\n\t\t\tfrom codemap.llm import LLMClient\n\n\t\t\tself.llm_client = LLMClient(\n\t\t\t\tconfig_loader=self.config_loader,\n\t\t\t\trepo_path=self.repo_root,\n\t\t\t)\n\n\t\tself.pr_generator = PRGenerator(repo_path=self.repo_root, llm_client=self.llm_client)\n\n\tdef _generate_release_pr_content(self, base_branch: str, branch_name: str) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tGenerate PR content for a release.\n\n\t\tArgs:\n\t\t        base_branch: The branch to merge into (e.g. main)\n\t\t        branch_name: The release branch name (e.g. release/1.0.0)\n\n\t\tReturns:\n\t\t        Dictionary with title and description\n\n\t\t\"\"\"\n\t\t# Extract version from branch name\n\t\tversion = branch_name.replace(\"release/\", \"\")\n\t\ttitle = f\"Release {version}\"\n\t\t# Include base branch information in the description\n\t\tdescription = f\"# Release {version}\\n\\nThis pull request merges release {version} into {base_branch}.\"\n\t\treturn {\"title\": title, \"description\": description}\n\n\tdef _generate_title(self, commits: list[str], branch_name: str, branch_type: str) -&gt; str:\n\t\t\"\"\"Core logic for generating PR title.\"\"\"\n\t\ttitle_strategy = self.content_config.title_strategy\n\n\t\tif not commits:\n\t\t\tif branch_type == \"release\":\n\t\t\t\treturn f\"Release {branch_name.replace('release/', '')}\"\n\t\t\tclean_name = branch_name.replace(f\"{branch_type}/\", \"\").replace(\"-\", \" \").replace(\"_\", \" \")\n\t\t\treturn f\"{branch_type.capitalize()}: {clean_name.capitalize()}\"\n\n\t\tif title_strategy == \"llm\":\n\t\t\treturn generate_pr_title_with_llm(commits, llm_client=self.llm_client)\n\n\t\treturn generate_pr_title_from_commits(commits)\n\n\tdef _generate_description(self, commits: list[str], branch_name: str, branch_type: str, base_branch: str) -&gt; str:\n\t\t\"\"\"Core logic for generating PR description.\"\"\"\n\t\tdescription_strategy = self.content_config.description_strategy\n\n\t\tif not commits:\n\t\t\tif branch_type == \"release\" and self.workflow_strategy_name == \"gitflow\":\n\t\t\t\t# Call the internal helper method\n\t\t\t\tcontent = self._generate_release_pr_content(base_branch, branch_name)\n\t\t\t\treturn content[\"description\"]\n\t\t\treturn f\"Changes in {branch_name}\"\n\n\t\tif description_strategy == \"llm\":\n\t\t\treturn generate_pr_description_with_llm(commits, llm_client=self.llm_client)\n\n\t\tif description_strategy == \"template\" and self.content_config.use_workflow_templates:\n\t\t\ttemplate = self.content_config.description_template\n\t\t\tif template:\n\t\t\t\tcommit_description = \"\\n\".join([f\"- {commit}\" for commit in commits])\n\t\t\t\t# Note: Other template variables like testing_instructions might need context\n\t\t\t\treturn template.format(\n\t\t\t\t\tchanges=commit_description,\n\t\t\t\t\ttesting_instructions=\"[Testing instructions]\",\n\t\t\t\t\tscreenshots=\"[Screenshots]\",\n\t\t\t\t)\n\n\t\treturn generate_pr_description_from_commits(commits)\n\n\tdef create_pr_workflow(\n\t\tself, base_branch: str, head_branch: str, title: str | None = None, description: str | None = None\n\t) -&gt; PullRequest:\n\t\t\"\"\"Orchestrates the PR creation process (non-interactive part).\"\"\"\n\t\ttry:\n\t\t\t# Check for existing PR first\n\t\t\texisting_pr = get_existing_pr(head_branch)\n\t\t\tif existing_pr:\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"PR #{existing_pr.number} already exists for branch '{head_branch}'. Returning existing PR.\"\n\t\t\t\t)\n\t\t\t\treturn existing_pr\n\n\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\t# Get commits\n\t\t\tcommits = pgu.get_commit_messages(base_branch, head_branch)\n\n\t\t\t# Determine branch type\n\t\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t\t# Generate title and description if not provided\n\t\t\tfinal_title = title or self._generate_title(commits, head_branch, branch_type)\n\t\t\tfinal_description = description or self._generate_description(\n\t\t\t\tcommits, head_branch, branch_type, base_branch\n\t\t\t)\n\n\t\t\t# Create PR using PRGenerator\n\t\t\tpr = self.pr_generator.create_pr(base_branch, head_branch, final_title, final_description)\n\t\t\tlogger.info(f\"Successfully created PR #{pr.number}: {pr.url}\")\n\t\t\treturn pr\n\t\texcept GitError:\n\t\t\t# Specific handling for unrelated histories might go here or be handled in CLI\n\t\t\tlogger.exception(\"GitError during PR creation workflow\")\n\t\t\traise\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Unexpected error during PR creation workflow\")\n\t\t\tmsg = f\"Unexpected error creating PR: {e}\"\n\t\t\traise PRCreationError(msg) from e\n\n\tdef update_pr_workflow(\n\t\tself,\n\t\tpr_number: int,\n\t\ttitle: str | None = None,\n\t\tdescription: str | None = None,\n\t\tbase_branch: str | None = None,\n\t\thead_branch: str | None = None,\n\t) -&gt; PullRequest:\n\t\t\"\"\"Orchestrates the PR update process (non-interactive part).\"\"\"\n\t\ttry:\n\t\t\t# Fetch existing PR info if needed to regenerate title/description\n\t\t\t# This might require gh cli or GitHub API interaction if pr_generator doesn't fetch\n\t\t\t# For now, assume base/head are provided if regeneration is needed\n\n\t\t\tfinal_title = title\n\t\t\tfinal_description = description\n\n\t\t\t# Regenerate if title/description are None\n\t\t\tif title is None or description is None:\n\t\t\t\tif not base_branch or not head_branch:\n\t\t\t\t\tmsg = \"Cannot regenerate content for update without base and head branches.\"\n\t\t\t\t\traise PRCreationError(msg)\n\n\t\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\t\tcommits = pgu.get_commit_messages(base_branch, head_branch)\n\t\t\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t\t\tif title is None:\n\t\t\t\t\tfinal_title = self._generate_title(commits, head_branch, branch_type)\n\t\t\t\tif description is None:\n\t\t\t\t\tfinal_description = self._generate_description(commits, head_branch, branch_type, base_branch)\n\n\t\t\tif final_title is None or final_description is None:\n\t\t\t\tmsg = \"Could not determine final title or description for PR update.\"\n\t\t\t\traise PRCreationError(msg)\n\n\t\t\t# Update PR using PRGenerator\n\t\t\tupdated_pr = self.pr_generator.update_pr(pr_number, final_title, final_description)\n\t\t\tlogger.info(f\"Successfully updated PR #{updated_pr.number}: {updated_pr.url}\")\n\t\t\treturn updated_pr\n\t\texcept GitError:\n\t\t\tlogger.exception(\"GitError during PR update workflow\")\n\t\t\traise\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Unexpected error during PR update workflow\")\n\t\t\tmsg = f\"Unexpected error updating PR: {e}\"\n\t\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig_loader: ConfigLoader,\n\tllm_client: LLMClient | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the PR workflow command helper.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance.</p> required <code>llm_client</code> <code>LLMClient | None</code> <p>Optional pre-configured LLMClient.</p> <code>None</code> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: ConfigLoader,\n\tllm_client: LLMClient | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the PR workflow command helper.\n\n\tArgs:\n\t        config_loader: ConfigLoader instance.\n\t        llm_client: Optional pre-configured LLMClient.\n\n\t\"\"\"\n\tself.config_loader = config_loader\n\n\tif self.config_loader.get.repo_root is None:\n\t\tself.repo_root = ExtendedGitRepoContext.get_repo_root()\n\telse:\n\t\tself.repo_root = self.config_loader.get.repo_root\n\n\tself.pr_config = self.config_loader.get.pr\n\tself.content_config = self.pr_config.generate\n\tself.workflow_strategy_name = self.config_loader.get.pr.strategy\n\tself.workflow = create_strategy(self.workflow_strategy_name)\n\n\t# Initialize LLM client if needed\n\tif llm_client:\n\t\tself.llm_client = llm_client\n\telse:\n\t\tfrom codemap.llm import LLMClient\n\n\t\tself.llm_client = LLMClient(\n\t\t\tconfig_loader=self.config_loader,\n\t\t\trepo_path=self.repo_root,\n\t\t)\n\n\tself.pr_generator = PRGenerator(repo_path=self.repo_root, llm_client=self.llm_client)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = get_repo_root()\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.pr_config","title":"pr_config  <code>instance-attribute</code>","text":"<pre><code>pr_config = pr\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.content_config","title":"content_config  <code>instance-attribute</code>","text":"<pre><code>content_config = generate\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.workflow_strategy_name","title":"workflow_strategy_name  <code>instance-attribute</code>","text":"<pre><code>workflow_strategy_name = strategy\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.workflow","title":"workflow  <code>instance-attribute</code>","text":"<pre><code>workflow = create_strategy(workflow_strategy_name)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.llm_client","title":"llm_client  <code>instance-attribute</code>","text":"<pre><code>llm_client = llm_client\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.pr_generator","title":"pr_generator  <code>instance-attribute</code>","text":"<pre><code>pr_generator = PRGenerator(\n\trepo_path=repo_root, llm_client=llm_client\n)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.create_pr_workflow","title":"create_pr_workflow","text":"<pre><code>create_pr_workflow(\n\tbase_branch: str,\n\thead_branch: str,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n) -&gt; PullRequest\n</code></pre> <p>Orchestrates the PR creation process (non-interactive part).</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def create_pr_workflow(\n\tself, base_branch: str, head_branch: str, title: str | None = None, description: str | None = None\n) -&gt; PullRequest:\n\t\"\"\"Orchestrates the PR creation process (non-interactive part).\"\"\"\n\ttry:\n\t\t# Check for existing PR first\n\t\texisting_pr = get_existing_pr(head_branch)\n\t\tif existing_pr:\n\t\t\tlogger.warning(\n\t\t\t\tf\"PR #{existing_pr.number} already exists for branch '{head_branch}'. Returning existing PR.\"\n\t\t\t)\n\t\t\treturn existing_pr\n\n\t\tpgu = PRGitUtils.get_instance()\n\t\t# Get commits\n\t\tcommits = pgu.get_commit_messages(base_branch, head_branch)\n\n\t\t# Determine branch type\n\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t# Generate title and description if not provided\n\t\tfinal_title = title or self._generate_title(commits, head_branch, branch_type)\n\t\tfinal_description = description or self._generate_description(\n\t\t\tcommits, head_branch, branch_type, base_branch\n\t\t)\n\n\t\t# Create PR using PRGenerator\n\t\tpr = self.pr_generator.create_pr(base_branch, head_branch, final_title, final_description)\n\t\tlogger.info(f\"Successfully created PR #{pr.number}: {pr.url}\")\n\t\treturn pr\n\texcept GitError:\n\t\t# Specific handling for unrelated histories might go here or be handled in CLI\n\t\tlogger.exception(\"GitError during PR creation workflow\")\n\t\traise\n\texcept Exception as e:\n\t\tlogger.exception(\"Unexpected error during PR creation workflow\")\n\t\tmsg = f\"Unexpected error creating PR: {e}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.update_pr_workflow","title":"update_pr_workflow","text":"<pre><code>update_pr_workflow(\n\tpr_number: int,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n) -&gt; PullRequest\n</code></pre> <p>Orchestrates the PR update process (non-interactive part).</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def update_pr_workflow(\n\tself,\n\tpr_number: int,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n) -&gt; PullRequest:\n\t\"\"\"Orchestrates the PR update process (non-interactive part).\"\"\"\n\ttry:\n\t\t# Fetch existing PR info if needed to regenerate title/description\n\t\t# This might require gh cli or GitHub API interaction if pr_generator doesn't fetch\n\t\t# For now, assume base/head are provided if regeneration is needed\n\n\t\tfinal_title = title\n\t\tfinal_description = description\n\n\t\t# Regenerate if title/description are None\n\t\tif title is None or description is None:\n\t\t\tif not base_branch or not head_branch:\n\t\t\t\tmsg = \"Cannot regenerate content for update without base and head branches.\"\n\t\t\t\traise PRCreationError(msg)\n\n\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\tcommits = pgu.get_commit_messages(base_branch, head_branch)\n\t\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t\tif title is None:\n\t\t\t\tfinal_title = self._generate_title(commits, head_branch, branch_type)\n\t\t\tif description is None:\n\t\t\t\tfinal_description = self._generate_description(commits, head_branch, branch_type, base_branch)\n\n\t\tif final_title is None or final_description is None:\n\t\t\tmsg = \"Could not determine final title or description for PR update.\"\n\t\t\traise PRCreationError(msg)\n\n\t\t# Update PR using PRGenerator\n\t\tupdated_pr = self.pr_generator.update_pr(pr_number, final_title, final_description)\n\t\tlogger.info(f\"Successfully updated PR #{updated_pr.number}: {updated_pr.url}\")\n\t\treturn updated_pr\n\texcept GitError:\n\t\tlogger.exception(\"GitError during PR update workflow\")\n\t\traise\n\texcept Exception as e:\n\t\tlogger.exception(\"Unexpected error during PR update workflow\")\n\t\tmsg = f\"Unexpected error updating PR: {e}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/constants/","title":"Constants","text":"<p>Constants for PR generation.</p>"},{"location":"api/git/pr_generator/constants/#codemap.git.pr_generator.constants.MAX_COMMIT_PREVIEW","title":"MAX_COMMIT_PREVIEW  <code>module-attribute</code>","text":"<pre><code>MAX_COMMIT_PREVIEW = 3\n</code></pre>"},{"location":"api/git/pr_generator/constants/#codemap.git.pr_generator.constants.MIN_SIGNIFICANT_WORD_LENGTH","title":"MIN_SIGNIFICANT_WORD_LENGTH  <code>module-attribute</code>","text":"<pre><code>MIN_SIGNIFICANT_WORD_LENGTH = 3\n</code></pre>"},{"location":"api/git/pr_generator/constants/#codemap.git.pr_generator.constants.MIN_COMMIT_PARTS","title":"MIN_COMMIT_PARTS  <code>module-attribute</code>","text":"<pre><code>MIN_COMMIT_PARTS = 3\n</code></pre>"},{"location":"api/git/pr_generator/decorators/","title":"Decorators","text":"<p>Decorators for the PR generator module.</p>"},{"location":"api/git/pr_generator/decorators/#codemap.git.pr_generator.decorators.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/decorators/#codemap.git.pr_generator.decorators.F","title":"F  <code>module-attribute</code>","text":"<pre><code>F = TypeVar('F', bound=Callable[..., object])\n</code></pre>"},{"location":"api/git/pr_generator/decorators/#codemap.git.pr_generator.decorators.git_operation","title":"git_operation","text":"<pre><code>git_operation(func: F) -&gt; F\n</code></pre> <p>Decorator for git operations.</p> <p>This decorator wraps functions that perform git operations, providing: - Logging of operation start/end - Standardized error handling - Automatic conversion of git-related exceptions to GitError</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>F</code> <p>The function to decorate</p> required <p>Returns:</p> Type Description <code>F</code> <p>Decorated function</p> Source code in <code>src/codemap/git/pr_generator/decorators.py</code> <pre><code>def git_operation(func: F) -&gt; F:\n\t\"\"\"\n\tDecorator for git operations.\n\n\tThis decorator wraps functions that perform git operations, providing:\n\t- Logging of operation start/end\n\t- Standardized error handling\n\t- Automatic conversion of git-related exceptions to GitError\n\n\tArgs:\n\t    func: The function to decorate\n\n\tReturns:\n\t    Decorated function\n\n\t\"\"\"\n\n\t@functools.wraps(func)\n\tdef wrapper(*args: object, **kwargs: object) -&gt; object:\n\t\t\"\"\"Wrapper function for git operations that handles logging and error conversion.\n\n\t\tArgs:\n\t\t    *args: Positional arguments passed to the decorated function.\n\t\t    **kwargs: Keyword arguments passed to the decorated function.\n\n\t\tReturns:\n\t\t    The result of the decorated function if successful.\n\n\t\tRaises:\n\t\t    GitError: If any exception occurs during the git operation. Original GitError\n\t\t        exceptions are re-raised as-is, while other exceptions are converted to\n\t\t        GitError with a descriptive message.\n\n\t\tNote:\n\t\t    - Logs debug messages for operation start/end\n\t\t    - Converts non-GitError exceptions to GitError\n\t\t    - Preserves original GitError exceptions\n\t\t\"\"\"\n\t\tfunction_name = func.__name__\n\t\tlogger.debug(\"Starting git operation: %s\", function_name)\n\n\t\ttry:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tlogger.debug(\"Completed git operation: %s\", function_name)\n\t\t\treturn result\n\t\texcept GitError:\n\t\t\t# Re-raise GitError as is\n\t\t\tlogger.debug(\"GitError in operation: %s\", function_name)\n\t\t\traise\n\t\texcept Exception as e:\n\t\t\t# Convert other exceptions to GitError\n\t\t\tlogger.debug(\"Error in git operation %s: %s\", function_name, str(e))\n\t\t\tmsg = f\"Git operation failed: {function_name} - {e!s}\"\n\t\t\traise GitError(msg) from e\n\n\treturn cast(\"F\", wrapper)\n</code></pre>"},{"location":"api/git/pr_generator/generator/","title":"Generator","text":"<p>PR generator for the CodeMap Git module.</p> <p>This class generates pull requests for git repositories.</p>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator","title":"PRGenerator","text":"<p>Generator for Pull Requests.</p> <p>This class handles generating pull request content (title and description) and creating/updating PRs on GitHub.</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>class PRGenerator:\n\t\"\"\"\n\tGenerator for Pull Requests.\n\n\tThis class handles generating pull request content (title and\n\tdescription) and creating/updating PRs on GitHub.\n\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_path: Path,\n\t\tllm_client: LLMClient,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the PR generator.\n\n\t\tArgs:\n\t\t    repo_path: Path to the git repository\n\t\t    llm_client: LLMClient instance to use for content generation\n\n\t\t\"\"\"\n\t\tself.repo_path = repo_path\n\t\tself.client = llm_client\n\n\tdef generate_content_from_commits(self, base_branch: str, head_branch: str, use_llm: bool = True) -&gt; PRContent:\n\t\t\"\"\"\n\t\tGenerate PR content (title and description) from commits.\n\n\t\tArgs:\n\t\t    base_branch: Base branch (e.g., main)\n\t\t    head_branch: Head branch (e.g., feature-branch)\n\t\t    use_llm: Whether to use LLM for generation\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' keys\n\n\t\t\"\"\"\n\t\t# Get commit messages between branches\n\t\tpgu = PRGitUtils.get_instance()\n\t\tcommits = pgu.get_commit_messages(base_branch, head_branch)\n\n\t\tif not commits:\n\t\t\treturn {\"title\": \"Update branch\", \"description\": \"No changes in this PR.\"}\n\n\t\tif use_llm:\n\t\t\t# Generate title and description using LLM\n\t\t\ttitle = generate_pr_title_with_llm(commits, self.client)\n\t\t\tdescription = generate_pr_description_with_llm(commits, self.client)\n\t\telse:\n\t\t\t# Generate title and description using rule-based approach\n\t\t\ttitle = generate_pr_title_from_commits(commits)\n\t\t\tdescription = generate_pr_description_from_commits(commits)\n\n\t\treturn {\"title\": title, \"description\": description}\n\n\tdef generate_content_from_template(\n\t\tself, branch_name: str, description: str, workflow_strategy: str = \"github-flow\"\n\t) -&gt; PRContent:\n\t\t\"\"\"\n\t\tGenerate PR content (title and description) from a template.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch\n\t\t    description: Short description of the changes\n\t\t    workflow_strategy: Git workflow strategy to use\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' keys\n\n\t\t\"\"\"\n\t\treturn generate_pr_content_from_template(branch_name, description, workflow_strategy)\n\n\tdef suggest_branch_name(self, description: str, workflow_strategy: str = \"github-flow\") -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on a description.\n\n\t\tArgs:\n\t\t    description: Description of the branch\n\t\t    workflow_strategy: Git workflow strategy to use\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\treturn suggest_branch_name(description, workflow_strategy)\n\n\tdef create_pr(self, base_branch: str, head_branch: str, title: str, description: str) -&gt; PullRequest:\n\t\t\"\"\"\n\t\tCreate a pull request on GitHub.\n\n\t\tArgs:\n\t\t    base_branch: Base branch (e.g., main)\n\t\t    head_branch: Head branch (e.g., feature-branch)\n\t\t    title: PR title\n\t\t    description: PR description\n\n\t\tReturns:\n\t\t    PullRequest object with PR details\n\n\t\tRaises:\n\t\t    GitError: If PR creation fails\n\n\t\t\"\"\"\n\t\treturn create_pull_request(base_branch, head_branch, title, description)\n\n\tdef update_pr(self, pr_number: int, title: str, description: str) -&gt; PullRequest:\n\t\t\"\"\"\n\t\tUpdate an existing pull request.\n\n\t\tArgs:\n\t\t    pr_number: PR number\n\t\t    title: New PR title\n\t\t    description: New PR description\n\n\t\tReturns:\n\t\t    Updated PullRequest object\n\n\t\tRaises:\n\t\t    GitError: If PR update fails\n\n\t\t\"\"\"\n\t\treturn update_pull_request(pr_number, title, description)\n\n\tdef get_existing_pr(self, branch_name: str) -&gt; PullRequest | None:\n\t\t\"\"\"\n\t\tGet an existing PR for a branch.\n\n\t\tArgs:\n\t\t    branch_name: Branch name\n\n\t\tReturns:\n\t\t    PullRequest object if found, None otherwise\n\n\t\t\"\"\"\n\t\treturn get_existing_pr(branch_name)\n\n\tdef create_or_update_pr(\n\t\tself,\n\t\tbase_branch: str | None = None,\n\t\thead_branch: str | None = None,\n\t\ttitle: str | None = None,\n\t\tdescription: str | None = None,\n\t\tuse_llm: bool = True,\n\t\tpr_number: int | None = None,\n\t) -&gt; PullRequest:\n\t\t\"\"\"\n\t\tCreate a new PR or update an existing one.\n\n\t\tArgs:\n\t\t    base_branch: Base branch (defaults to default branch)\n\t\t    head_branch: Head branch\n\t\t    title: PR title (if None, will be generated)\n\t\t    description: PR description (if None, will be generated)\n\t\t    use_llm: Whether to use LLM for content generation\n\t\t    pr_number: PR number for update (if None, will create new PR)\n\n\t\tReturns:\n\t\t    PullRequest object\n\n\t\tRaises:\n\t\t    GitError: If PR creation/update fails\n\n\t\t\"\"\"\n\t\t# Get default branch if base_branch is not specified\n\t\tif base_branch is None:\n\t\t\tbase_branch = get_default_branch()\n\n\t\t# Set default head_branch to current branch if not specified\n\t\tpgu = PRGitUtils.get_instance()\n\t\tif head_branch is None:\n\t\t\ttry:\n\t\t\t\thead_branch = pgu.get_current_branch()\n\t\t\texcept GitError as err:\n\t\t\t\tmsg = \"Failed to determine current branch\"\n\t\t\t\traise GitError(msg) from err\n\n\t\t# Check if PR exists\n\t\texisting_pr = None\n\t\tif pr_number is not None:\n\t\t\t# Updating an existing PR by number\n\t\t\tif title is None or description is None:\n\t\t\t\t# Need to fetch the PR to get current title/description\n\t\t\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\t\t\tif existing_pr is None:\n\t\t\t\t\tmsg = f\"No PR found for branch {head_branch} with number {pr_number}\"\n\t\t\t\t\traise GitError(msg)\n\n\t\telse:\n\t\t\t# Look for existing PR for this branch\n\t\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\t\tif existing_pr is not None:\n\t\t\t\tpr_number = existing_pr.number\n\n\t\t# Generate content if not provided\n\t\tif title is None or description is None:\n\t\t\tcontent = self.generate_content_from_commits(base_branch, head_branch, use_llm)\n\t\t\tif title is None:\n\t\t\t\ttitle = content[\"title\"]\n\t\t\tif description is None:\n\t\t\t\tdescription = content[\"description\"]\n\n\t\t# Create or update PR\n\t\tif pr_number is not None:\n\t\t\t# Update existing PR\n\t\t\treturn self.update_pr(pr_number, title, description)\n\t\t# Create new PR\n\t\treturn self.create_pr(base_branch, head_branch, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.__init__","title":"__init__","text":"<pre><code>__init__(repo_path: Path, llm_client: LLMClient) -&gt; None\n</code></pre> <p>Initialize the PR generator.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to the git repository</p> required <code>llm_client</code> <code>LLMClient</code> <p>LLMClient instance to use for content generation</p> required Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def __init__(\n\tself,\n\trepo_path: Path,\n\tllm_client: LLMClient,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the PR generator.\n\n\tArgs:\n\t    repo_path: Path to the git repository\n\t    llm_client: LLMClient instance to use for content generation\n\n\t\"\"\"\n\tself.repo_path = repo_path\n\tself.client = llm_client\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = llm_client\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.generate_content_from_commits","title":"generate_content_from_commits","text":"<pre><code>generate_content_from_commits(\n\tbase_branch: str, head_branch: str, use_llm: bool = True\n) -&gt; PRContent\n</code></pre> <p>Generate PR content (title and description) from commits.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str</code> <p>Base branch (e.g., main)</p> required <code>head_branch</code> <code>str</code> <p>Head branch (e.g., feature-branch)</p> required <code>use_llm</code> <code>bool</code> <p>Whether to use LLM for generation</p> <code>True</code> <p>Returns:</p> Type Description <code>PRContent</code> <p>Dictionary with 'title' and 'description' keys</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def generate_content_from_commits(self, base_branch: str, head_branch: str, use_llm: bool = True) -&gt; PRContent:\n\t\"\"\"\n\tGenerate PR content (title and description) from commits.\n\n\tArgs:\n\t    base_branch: Base branch (e.g., main)\n\t    head_branch: Head branch (e.g., feature-branch)\n\t    use_llm: Whether to use LLM for generation\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' keys\n\n\t\"\"\"\n\t# Get commit messages between branches\n\tpgu = PRGitUtils.get_instance()\n\tcommits = pgu.get_commit_messages(base_branch, head_branch)\n\n\tif not commits:\n\t\treturn {\"title\": \"Update branch\", \"description\": \"No changes in this PR.\"}\n\n\tif use_llm:\n\t\t# Generate title and description using LLM\n\t\ttitle = generate_pr_title_with_llm(commits, self.client)\n\t\tdescription = generate_pr_description_with_llm(commits, self.client)\n\telse:\n\t\t# Generate title and description using rule-based approach\n\t\ttitle = generate_pr_title_from_commits(commits)\n\t\tdescription = generate_pr_description_from_commits(commits)\n\n\treturn {\"title\": title, \"description\": description}\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.generate_content_from_template","title":"generate_content_from_template","text":"<pre><code>generate_content_from_template(\n\tbranch_name: str,\n\tdescription: str,\n\tworkflow_strategy: str = \"github-flow\",\n) -&gt; PRContent\n</code></pre> <p>Generate PR content (title and description) from a template.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <code>description</code> <code>str</code> <p>Short description of the changes</p> required <code>workflow_strategy</code> <code>str</code> <p>Git workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>PRContent</code> <p>Dictionary with 'title' and 'description' keys</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def generate_content_from_template(\n\tself, branch_name: str, description: str, workflow_strategy: str = \"github-flow\"\n) -&gt; PRContent:\n\t\"\"\"\n\tGenerate PR content (title and description) from a template.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\t    description: Short description of the changes\n\t    workflow_strategy: Git workflow strategy to use\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' keys\n\n\t\"\"\"\n\treturn generate_pr_content_from_template(branch_name, description, workflow_strategy)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tdescription: str, workflow_strategy: str = \"github-flow\"\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on a description.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>Description of the branch</p> required <code>workflow_strategy</code> <code>str</code> <p>Git workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def suggest_branch_name(self, description: str, workflow_strategy: str = \"github-flow\") -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on a description.\n\n\tArgs:\n\t    description: Description of the branch\n\t    workflow_strategy: Git workflow strategy to use\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\treturn suggest_branch_name(description, workflow_strategy)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.create_pr","title":"create_pr","text":"<pre><code>create_pr(\n\tbase_branch: str,\n\thead_branch: str,\n\ttitle: str,\n\tdescription: str,\n) -&gt; PullRequest\n</code></pre> <p>Create a pull request on GitHub.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str</code> <p>Base branch (e.g., main)</p> required <code>head_branch</code> <code>str</code> <p>Head branch (e.g., feature-branch)</p> required <code>title</code> <code>str</code> <p>PR title</p> required <code>description</code> <code>str</code> <p>PR description</p> required <p>Returns:</p> Type Description <code>PullRequest</code> <p>PullRequest object with PR details</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If PR creation fails</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def create_pr(self, base_branch: str, head_branch: str, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"\n\tCreate a pull request on GitHub.\n\n\tArgs:\n\t    base_branch: Base branch (e.g., main)\n\t    head_branch: Head branch (e.g., feature-branch)\n\t    title: PR title\n\t    description: PR description\n\n\tReturns:\n\t    PullRequest object with PR details\n\n\tRaises:\n\t    GitError: If PR creation fails\n\n\t\"\"\"\n\treturn create_pull_request(base_branch, head_branch, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.update_pr","title":"update_pr","text":"<pre><code>update_pr(\n\tpr_number: int, title: str, description: str\n) -&gt; PullRequest\n</code></pre> <p>Update an existing pull request.</p> <p>Parameters:</p> Name Type Description Default <code>pr_number</code> <code>int</code> <p>PR number</p> required <code>title</code> <code>str</code> <p>New PR title</p> required <code>description</code> <code>str</code> <p>New PR description</p> required <p>Returns:</p> Type Description <code>PullRequest</code> <p>Updated PullRequest object</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If PR update fails</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def update_pr(self, pr_number: int, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"\n\tUpdate an existing pull request.\n\n\tArgs:\n\t    pr_number: PR number\n\t    title: New PR title\n\t    description: New PR description\n\n\tReturns:\n\t    Updated PullRequest object\n\n\tRaises:\n\t    GitError: If PR update fails\n\n\t\"\"\"\n\treturn update_pull_request(pr_number, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.get_existing_pr","title":"get_existing_pr","text":"<pre><code>get_existing_pr(branch_name: str) -&gt; PullRequest | None\n</code></pre> <p>Get an existing PR for a branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Branch name</p> required <p>Returns:</p> Type Description <code>PullRequest | None</code> <p>PullRequest object if found, None otherwise</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def get_existing_pr(self, branch_name: str) -&gt; PullRequest | None:\n\t\"\"\"\n\tGet an existing PR for a branch.\n\n\tArgs:\n\t    branch_name: Branch name\n\n\tReturns:\n\t    PullRequest object if found, None otherwise\n\n\t\"\"\"\n\treturn get_existing_pr(branch_name)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.create_or_update_pr","title":"create_or_update_pr","text":"<pre><code>create_or_update_pr(\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tuse_llm: bool = True,\n\tpr_number: int | None = None,\n) -&gt; PullRequest\n</code></pre> <p>Create a new PR or update an existing one.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str | None</code> <p>Base branch (defaults to default branch)</p> <code>None</code> <code>head_branch</code> <code>str | None</code> <p>Head branch</p> <code>None</code> <code>title</code> <code>str | None</code> <p>PR title (if None, will be generated)</p> <code>None</code> <code>description</code> <code>str | None</code> <p>PR description (if None, will be generated)</p> <code>None</code> <code>use_llm</code> <code>bool</code> <p>Whether to use LLM for content generation</p> <code>True</code> <code>pr_number</code> <code>int | None</code> <p>PR number for update (if None, will create new PR)</p> <code>None</code> <p>Returns:</p> Type Description <code>PullRequest</code> <p>PullRequest object</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If PR creation/update fails</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def create_or_update_pr(\n\tself,\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tuse_llm: bool = True,\n\tpr_number: int | None = None,\n) -&gt; PullRequest:\n\t\"\"\"\n\tCreate a new PR or update an existing one.\n\n\tArgs:\n\t    base_branch: Base branch (defaults to default branch)\n\t    head_branch: Head branch\n\t    title: PR title (if None, will be generated)\n\t    description: PR description (if None, will be generated)\n\t    use_llm: Whether to use LLM for content generation\n\t    pr_number: PR number for update (if None, will create new PR)\n\n\tReturns:\n\t    PullRequest object\n\n\tRaises:\n\t    GitError: If PR creation/update fails\n\n\t\"\"\"\n\t# Get default branch if base_branch is not specified\n\tif base_branch is None:\n\t\tbase_branch = get_default_branch()\n\n\t# Set default head_branch to current branch if not specified\n\tpgu = PRGitUtils.get_instance()\n\tif head_branch is None:\n\t\ttry:\n\t\t\thead_branch = pgu.get_current_branch()\n\t\texcept GitError as err:\n\t\t\tmsg = \"Failed to determine current branch\"\n\t\t\traise GitError(msg) from err\n\n\t# Check if PR exists\n\texisting_pr = None\n\tif pr_number is not None:\n\t\t# Updating an existing PR by number\n\t\tif title is None or description is None:\n\t\t\t# Need to fetch the PR to get current title/description\n\t\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\t\tif existing_pr is None:\n\t\t\t\tmsg = f\"No PR found for branch {head_branch} with number {pr_number}\"\n\t\t\t\traise GitError(msg)\n\n\telse:\n\t\t# Look for existing PR for this branch\n\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\tif existing_pr is not None:\n\t\t\tpr_number = existing_pr.number\n\n\t# Generate content if not provided\n\tif title is None or description is None:\n\t\tcontent = self.generate_content_from_commits(base_branch, head_branch, use_llm)\n\t\tif title is None:\n\t\t\ttitle = content[\"title\"]\n\t\tif description is None:\n\t\t\tdescription = content[\"description\"]\n\n\t# Create or update PR\n\tif pr_number is not None:\n\t\t# Update existing PR\n\t\treturn self.update_pr(pr_number, title, description)\n\t# Create new PR\n\treturn self.create_pr(base_branch, head_branch, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/","title":"Pr Git Utils","text":"<p>Utility functions for PR generation.</p>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils","title":"PRGitUtils","text":"<p>               Bases: <code>ExtendedGitRepoContext</code></p> <p>Provides Git operations for PR generation using pygit2.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>class PRGitUtils(ExtendedGitRepoContext):\n\t\"\"\"Provides Git operations for PR generation using pygit2.\"\"\"\n\n\t_pr_git_utils_instance: PRGitUtils | None = None\n\tbranch: str  # Explicitly declare the inherited attribute\n\n\t@classmethod\n\tdef get_instance(cls) -&gt; PRGitUtils:\n\t\t\"\"\"Get an instance of the PRGitUtils class.\"\"\"\n\t\tif cls._pr_git_utils_instance is None:\n\t\t\tcls._pr_git_utils_instance = cls()\n\t\treturn cls._pr_git_utils_instance\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the PRGitUtils with the given repository path.\"\"\"\n\t\tsuper().__init__()\n\n\tdef create_branch(self, branch_name: str, from_reference: str | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tCreate a new branch and switch to it using pygit2.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch to create.\n\t\t    from_reference: Optional reference (branch name, commit SHA) to create the branch from.\n\t\t                    Defaults to current HEAD.\n\n\t\tRaises:\n\t\t    GitError: If branch creation or checkout fails.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif from_reference:\n\t\t\t\tcommit_obj = self.repo.revparse_single(from_reference)\n\t\t\t\tif not commit_obj:\n\t\t\t\t\tmsg = f\"Could not resolve 'from_reference': {from_reference}\"\n\t\t\t\t\tlogger.error(msg)\n\t\t\t\t\traise GitError(msg)\n\t\t\t\tsource_commit = commit_obj.peel(Commit)\n\t\t\telse:\n\t\t\t\tif self.repo.head_is_unborn:\n\t\t\t\t\tmsg = \"Cannot create branch from unborn HEAD. Please make an initial commit.\"\n\t\t\t\t\tlogger.error(msg)\n\t\t\t\t\traise GitError(msg)\n\t\t\t\tsource_commit = self.repo.head.peel(Commit)\n\n\t\t\tself.repo.create_branch(branch_name, source_commit)\n\t\t\tlogger.info(f\"Branch '{branch_name}' created from '{source_commit.id}'.\")\n\t\t\tself.checkout_branch(branch_name)  # Checkout after creation\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to create branch '{branch_name}' using pygit2: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\t\texcept Exception as e:\n\t\t\tmsg = f\"An unexpected error occurred while creating branch '{branch_name}': {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\n\tdef checkout_branch(self, branch_name: str) -&gt; None:\n\t\t\"\"\"\n\t\tCheckout an existing branch using pygit2.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch to checkout.\n\n\t\tRaises:\n\t\t    GitError: If checkout fails.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Construct the full ref name\n\t\t\tref_name = f\"refs/heads/{branch_name}\"\n\t\t\tbranch_obj = self.repo.lookup_reference(ref_name)\n\t\t\tself.repo.checkout(branch_obj)\n\t\t\t# Update self.branch after checkout, consistent with GitRepoContext constructor\n\t\t\tcurrent_branch_obj = self.repo\n\t\t\tif not current_branch_obj.head_is_detached:\n\t\t\t\tself.branch = current_branch_obj.head.shorthand\n\t\t\telse:\n\t\t\t\tself.branch = \"\"  # Or perhaps the SHA for detached head\n\t\t\tlogger.info(f\"Checked out branch '{branch_name}' using pygit2.\")\n\t\t\t# Run post-checkout hook if present\n\t\t\tif hook_exists(\"post-checkout\"):\n\t\t\t\texit_code = run_hook(\"post-checkout\")\n\t\t\t\tif exit_code != 0:\n\t\t\t\t\tlogger.warning(\"post-checkout hook failed (branch already checked out)\")\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to checkout branch '{branch_name}' using pygit2: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\t\texcept Exception as e:\n\t\t\tmsg = f\"An unexpected error occurred while checking out branch '{branch_name}': {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\n\tdef push_branch(\n\t\tself, branch_name: str, force: bool = False, remote_name: str = \"origin\", ignore_hooks: bool = False\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tPush a branch to the remote using pygit2.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch to push.\n\t\t    force: Whether to force push.\n\t\t    remote_name: Name of the remote (e.g., \"origin\").\n\t\t    ignore_hooks: If True, skip running the pre-push hook.\n\n\t\tRaises:\n\t\t    GitError: If push fails or pre-push hook fails.\n\t\t\"\"\"\n\t\t# Run pre-push hook if not ignored\n\t\tif not ignore_hooks:\n\t\t\texit_code = run_hook(\"pre-push\")\n\t\t\tif exit_code != 0:\n\t\t\t\tmsg = \"pre-push hook failed, aborting push.\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise GitError(msg)\n\t\ttry:\n\t\t\tremote = self.repo.remotes[remote_name]\n\t\t\tlocal_ref = f\"refs/heads/{branch_name}\"\n\t\t\tremote_ref = f\"refs/heads/{branch_name}\"\n\n\t\t\trefspec = f\"{'+' if force else ''}{local_ref}:{remote_ref}\"\n\n\t\t\tlogger.info(\n\t\t\t\tf\"Attempting to push branch '{branch_name}' to remote \"\n\t\t\t\tf\"'{remote_name}' with refspec '{refspec}' using pygit2.\"\n\t\t\t)\n\n\t\t\t# Import the proper pygit2 credential classes\n\t\t\timport shlex\n\t\t\timport subprocess\n\t\t\tfrom pathlib import Path\n\t\t\tfrom urllib.parse import urlparse\n\n\t\t\tfrom pygit2.callbacks import RemoteCallbacks\n\t\t\tfrom pygit2.enums import CredentialType\n\n\t\t\t# Create a credential class to handle SSH and username/password authentication\n\t\t\tclass GitCredential:\n\t\t\t\tdef __init__(self, cred_type: CredentialType, *args: str | None) -&gt; None:\n\t\t\t\t\tself.credential_type = cred_type\n\t\t\t\t\tself.credential_tuple = args\n\n\t\t\tdef credential_callback(\n\t\t\t\turl: str, username_from_url: str | None, allowed_types: CredentialType\n\t\t\t) -&gt; GitCredential:\n\t\t\t\t\"\"\"\n\t\t\t\tCallback to handle credential requests from pygit2.\n\n\t\t\t\tArgs:\n\t\t\t\t\turl: The URL being authenticated against\n\t\t\t\t\tusername_from_url: Username extracted from the URL if present\n\t\t\t\t\tallowed_types: Bitmask of allowed credential types\n\n\t\t\t\tReturns:\n\t\t\t\t\tA credential object for authentication\n\t\t\t\t\"\"\"\n\t\t\t\tlogger.debug(f\"Authentication required for {url} (allowed types: {allowed_types})\")\n\n\t\t\t\t# Get username from URL or use default from git config\n\t\t\t\tusername = username_from_url\n\t\t\t\tif not username:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tconfig = self.repo.config\n\t\t\t\t\t\tusername = config[\"user.name\"]\n\t\t\t\t\texcept (KeyError, AttributeError) as e:\n\t\t\t\t\t\t# Default if we can't get from config\n\t\t\t\t\t\tlogger.debug(f\"Could not get username from git config: {e}\")\n\t\t\t\t\t\tusername = \"git\"\n\n\t\t\t\t# Try SSH agent authentication first (if available)\n\t\t\t\tif CredentialType.SSH_KEY in allowed_types:\n\t\t\t\t\tlogger.debug(f\"Attempting SSH agent authentication for {username}\")\n\t\t\t\t\treturn GitCredential(CredentialType.SSH_KEY, username, None, None, \"\")\n\n\t\t\t\t# Try SSH key authentication if agent is not available\n\t\t\t\tif CredentialType.SSH_KEY in allowed_types:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t# Common SSH key paths\n\t\t\t\t\t\tssh_dir = Path.home() / \".ssh\"\n\t\t\t\t\t\tkey_paths = [\n\t\t\t\t\t\t\tssh_dir / \"id_rsa\",\n\t\t\t\t\t\t\tssh_dir / \"id_ed25519\",\n\t\t\t\t\t\t\tssh_dir / \"id_ecdsa\",\n\t\t\t\t\t\t\tssh_dir / \"id_dsa\",\n\t\t\t\t\t\t]\n\n\t\t\t\t\t\tfor private_key_path in key_paths:\n\t\t\t\t\t\t\tpublic_key_path = Path(f\"{private_key_path}.pub\")\n\t\t\t\t\t\t\tif private_key_path.exists() and public_key_path.exists():\n\t\t\t\t\t\t\t\tlogger.debug(f\"Attempting SSH key authentication with {private_key_path}\")\n\t\t\t\t\t\t\t\treturn GitCredential(\n\t\t\t\t\t\t\t\t\tCredentialType.SSH_KEY, username, str(public_key_path), str(private_key_path), \"\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\texcept OSError as e:\n\t\t\t\t\t\tlogger.debug(f\"SSH key authentication failed: {e}\")\n\n\t\t\t\t# Try username/password if SSH is not available or didn't work\n\t\t\t\tif CredentialType.USERPASS_PLAINTEXT in allowed_types:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t# Extract hostname from URL\n\t\t\t\t\t\tparsed_url = urlparse(url)\n\t\t\t\t\t\thostname = parsed_url.netloc\n\n\t\t\t\t\t\t# Use git credential fill to get credentials - this command is safe as it's hardcoded\n\t\t\t\t\t\tcmd = \"git credential fill\"\n\t\t\t\t\t\t# Use shlex.split for secure command execution\n\t\t\t\t\t\tprocess = subprocess.Popen(  # noqa: S603\n\t\t\t\t\t\t\tshlex.split(cmd),\n\t\t\t\t\t\t\tstdin=subprocess.PIPE,\n\t\t\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\t\t\t\ttext=True,\n\t\t\t\t\t\t)\n\n\t\t\t\t\t\t# Provide input for git credential fill\n\t\t\t\t\t\tinput_data = f\"protocol={parsed_url.scheme}\\nhost={hostname}\\n\\n\"\n\t\t\t\t\t\tstdout, _ = process.communicate(input=input_data)\n\n\t\t\t\t\t\tif process.returncode == 0 and stdout:\n\t\t\t\t\t\t\t# Parse the output\n\t\t\t\t\t\t\tcredentials = {}\n\t\t\t\t\t\t\tfor line in stdout.splitlines():\n\t\t\t\t\t\t\t\tif \"=\" in line:\n\t\t\t\t\t\t\t\t\tkey, value = line.split(\"=\", 1)\n\t\t\t\t\t\t\t\t\tcredentials[key] = value\n\n\t\t\t\t\t\t\tif \"username\" in credentials and \"password\" in credentials:\n\t\t\t\t\t\t\t\tlogger.debug(f\"Using username/password authentication for {credentials['username']}\")\n\t\t\t\t\t\t\t\treturn GitCredential(\n\t\t\t\t\t\t\t\t\tCredentialType.USERPASS_PLAINTEXT, credentials[\"username\"], credentials[\"password\"]\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\texcept (subprocess.SubprocessError, OSError) as e:\n\t\t\t\t\t\tlogger.debug(f\"Username/password authentication failed: {e}\")\n\n\t\t\t\t# If nothing else works, try username-only authentication\n\t\t\t\tif CredentialType.USERNAME in allowed_types:\n\t\t\t\t\tlogger.debug(f\"Falling back to username-only authentication for {username}\")\n\t\t\t\t\treturn GitCredential(CredentialType.USERNAME, username)\n\n\t\t\t\t# If we get here, we couldn't find suitable credentials\n\t\t\t\tlogger.warning(f\"No suitable authentication method found for {url}\")\n\t\t\t\tmsg = \"No suitable authentication method available\"\n\t\t\t\traise Pygit2GitError(msg)\n\n\t\t\t# Create callback object with our credential callback\n\t\t\tcallbacks = RemoteCallbacks(credentials=credential_callback)\n\n\t\t\t# Pass callbacks to the push method\n\t\t\tremote.push([refspec], callbacks=callbacks)\n\t\t\tlogger.info(f\"Branch '{branch_name}' pushed to remote '{remote_name}' using pygit2.\")\n\t\texcept (Pygit2GitError, KeyError) as e:  # KeyError for remote_name not found\n\t\t\tmsg = f\"Failed to push branch '{branch_name}' to remote '{remote_name}' using pygit2: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\t\texcept GitError as e:  # Catch codemap's GitError if it somehow occurred before\n\t\t\tmsg = f\"Git operation error while pushing branch '{branch_name}': {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\t\texcept Exception as e:\n\t\t\tmsg = f\"An unexpected error occurred while pushing branch '{branch_name}': {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\n\tdef get_commit_messages(self, base_branch: str, head_branch: str) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet commit messages (summaries) between two branches using pygit2.\n\n\t\tThis lists commits that are in head_branch but not in base_branch.\n\n\t\tArgs:\n\t\t    base_branch: Base branch name/ref (e.g., \"main\").\n\t\t    head_branch: Head branch name/ref (e.g., \"feature-branch\").\n\n\t\tReturns:\n\t\t    List of commit message summaries.\n\n\t\tRaises:\n\t\t    GitError: If retrieving commits fails.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif not base_branch or not head_branch:\n\t\t\t\tlogger.warning(\"Base or head branch is None/empty, cannot get commit messages.\")\n\t\t\t\treturn []\n\n\t\t\tdef _resolve_to_commit_oid(branch_spec: str) -&gt; Oid:\n\t\t\t\tobj = self.repo.revparse_single(branch_spec)\n\t\t\t\tif not obj:\n\t\t\t\t\tmsg = f\"Could not resolve '{branch_spec}'\"\n\t\t\t\t\tlogger.error(msg)\n\t\t\t\t\traise GitError(msg)\n\t\t\t\t# Ensure it's a commit (could be a tag pointing to another tag, etc.)\n\t\t\t\tcommit_obj = obj.peel(Commit)\n\t\t\t\treturn commit_obj.id\n\n\t\t\tbase_oid = _resolve_to_commit_oid(base_branch)\n\t\t\thead_oid = _resolve_to_commit_oid(head_branch)\n\n\t\t\twalker = self.repo.walk(head_oid, SortMode.TOPOLOGICAL)\n\t\t\twalker.hide(base_oid)\n\n\t\t\tcommit_messages = []\n\t\t\tfor commit_pygit2 in walker:\n\t\t\t\t# commit_pygit2.message is the full message. Get summary (first line).\n\t\t\t\tmessage_summary = commit_pygit2.message.splitlines()[0].strip() if commit_pygit2.message else \"\"\n\t\t\t\tcommit_messages.append(message_summary)\n\n\t\t\tlogger.info(f\"Found {len(commit_messages)} commit messages between '{base_branch}' and '{head_branch}'.\")\n\t\t\treturn commit_messages\n\n\t\texcept (Pygit2GitError, GitError) as e:\n\t\t\tmsg = f\"Failed to get commit messages between '{base_branch}' and '{head_branch}' using pygit2: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\t\texcept Exception as e:\n\t\t\tmsg = (\n\t\t\t\tf\"An unexpected error occurred while getting commit messages \"\n\t\t\t\tf\"between '{base_branch}' and '{head_branch}': {e}\"\n\t\t\t)\n\t\t\tlogger.exception(msg)\n\t\t\traise GitError(msg) from e\n\n\tdef get_branch_relation(self, branch_ref_name: str, target_branch_ref_name: str) -&gt; tuple[bool, int]:\n\t\t\"\"\"\n\t\tGet the relationship between two branches using pygit2.\n\n\t\tArgs:\n\t\t\tbranch_ref_name: The branch to check (e.g., \"main\", \"origin/main\").\n\t\t\ttarget_branch_ref_name: The target branch to compare against (e.g., \"feature/foo\").\n\n\t\tReturns:\n\t\t\tTuple of (is_ancestor, commit_count)\n\t\t\t- is_ancestor: True if branch_ref_name is an ancestor of target_branch_ref_name.\n\t\t\t- commit_count: Number of commits in target_branch_ref_name that are not in branch_ref_name.\n\t\t\t\t\t\t(i.e., how many commits target is \"ahead\" of branch).\n\n\t\tRaises:\n\t\t\tGitError: If branches cannot be resolved or other git issues occur.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif not branch_ref_name or not target_branch_ref_name:\n\t\t\t\tlogger.warning(\"Branch or target branch name is None/empty for relation check.\")\n\t\t\t\treturn False, 0\n\n\t\t\t# Resolve branch names to Oids. revparse_single can handle local and remote-like refs.\n\t\t\tbranch_commit_obj = self.repo.revparse_single(branch_ref_name)\n\t\t\tif not branch_commit_obj:\n\t\t\t\tmsg = f\"Could not resolve branch: {branch_ref_name}\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise GitError(msg)\n\t\t\tbranch_oid = branch_commit_obj.peel(Commit).id\n\n\t\t\ttarget_commit_obj = self.repo.revparse_single(target_branch_ref_name)\n\t\t\tif not target_commit_obj:\n\t\t\t\tmsg = f\"Could not resolve target branch: {target_branch_ref_name}\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise GitError(msg)\n\t\t\ttarget_oid = target_commit_obj.peel(Commit).id\n\n\t\t\t# Check if branch_oid is an ancestor of target_oid\n\t\t\t# pygit2's descendant_of(A, B) means \"is A a descendant of B?\"\n\t\t\t# So, is_ancestor (branch is ancestor of target) means target is descendant of branch.\n\t\t\tis_ancestor = self.repo.descendant_of(target_oid, branch_oid)\n\n\t\t\t# Get commit count: commits in target_oid that are not in branch_oid.\n\t\t\t# ahead_behind(A, B) returns (commits in A not in B, commits in B not in A)\n\t\t\t# We want commits in target_oid not in branch_oid.\n\t\t\t# So, if A=target_oid, B=branch_oid, we want the first value (ahead).\n\t\t\tahead, _ = self.repo.ahead_behind(target_oid, branch_oid)\n\t\t\tcommit_count_target_ahead = ahead  # Renaming for clarity\n\n\t\t\tlogger.debug(\n\t\t\t\tf\"Branch relation: {branch_ref_name} vs {target_branch_ref_name}. \"\n\t\t\t\tf\"Is ancestor: {is_ancestor}, Target ahead by: {commit_count_target_ahead}\"\n\t\t\t)\n\t\t\treturn is_ancestor, commit_count_target_ahead\n\n\t\texcept Pygit2GitError as e:\n\t\t\tmsg = (\n\t\t\t\tf\"Pygit2 error determining branch relation between \"\n\t\t\t\tf\"'{branch_ref_name}' and '{target_branch_ref_name}': {e}\"\n\t\t\t)\n\t\t\tlogger.warning(msg)\n\t\t\traise GitError(msg) from e  # Wrap in codemap's GitError\n\t\texcept GitError as e:  # Catch codemap's GitError if raised by _resolve_to_commit_oid or similar\n\t\t\tmsg = (\n\t\t\t\tf\"Codemap GitError determining branch relation between '{branch_ref_name}' and \"\n\t\t\t\tf\"'{target_branch_ref_name}': {e}\"\n\t\t\t)\n\t\t\tlogger.warning(msg)\n\t\t\traise  # Re-raise as it's already the correct type\n\t\texcept Exception as e:  # Catch any other unexpected non-Git errors\n\t\t\tmsg = (\n\t\t\t\tf\"Unexpected error determining branch relation between '{branch_ref_name}' and \"\n\t\t\t\tf\"'{target_branch_ref_name}': {e}\"\n\t\t\t)\n\t\t\tlogger.warning(msg)\n\t\t\traise GitError(msg) from e  # Wrap in codemap's GitError\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.branch","title":"branch  <code>instance-attribute</code>","text":"<pre><code>branch: str\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.get_instance","title":"get_instance  <code>classmethod</code>","text":"<pre><code>get_instance() -&gt; PRGitUtils\n</code></pre> <p>Get an instance of the PRGitUtils class.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>@classmethod\ndef get_instance(cls) -&gt; PRGitUtils:\n\t\"\"\"Get an instance of the PRGitUtils class.\"\"\"\n\tif cls._pr_git_utils_instance is None:\n\t\tcls._pr_git_utils_instance = cls()\n\treturn cls._pr_git_utils_instance\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the PRGitUtils with the given repository path.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the PRGitUtils with the given repository path.\"\"\"\n\tsuper().__init__()\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.create_branch","title":"create_branch","text":"<pre><code>create_branch(\n\tbranch_name: str, from_reference: str | None = None\n) -&gt; None\n</code></pre> <p>Create a new branch and switch to it using pygit2.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to create.</p> required <code>from_reference</code> <code>str | None</code> <p>Optional reference (branch name, commit SHA) to create the branch from.             Defaults to current HEAD.</p> <code>None</code> <p>Raises:</p> Type Description <code>GitError</code> <p>If branch creation or checkout fails.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>def create_branch(self, branch_name: str, from_reference: str | None = None) -&gt; None:\n\t\"\"\"\n\tCreate a new branch and switch to it using pygit2.\n\n\tArgs:\n\t    branch_name: Name of the branch to create.\n\t    from_reference: Optional reference (branch name, commit SHA) to create the branch from.\n\t                    Defaults to current HEAD.\n\n\tRaises:\n\t    GitError: If branch creation or checkout fails.\n\t\"\"\"\n\ttry:\n\t\tif from_reference:\n\t\t\tcommit_obj = self.repo.revparse_single(from_reference)\n\t\t\tif not commit_obj:\n\t\t\t\tmsg = f\"Could not resolve 'from_reference': {from_reference}\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise GitError(msg)\n\t\t\tsource_commit = commit_obj.peel(Commit)\n\t\telse:\n\t\t\tif self.repo.head_is_unborn:\n\t\t\t\tmsg = \"Cannot create branch from unborn HEAD. Please make an initial commit.\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise GitError(msg)\n\t\t\tsource_commit = self.repo.head.peel(Commit)\n\n\t\tself.repo.create_branch(branch_name, source_commit)\n\t\tlogger.info(f\"Branch '{branch_name}' created from '{source_commit.id}'.\")\n\t\tself.checkout_branch(branch_name)  # Checkout after creation\n\texcept GitError as e:\n\t\tmsg = f\"Failed to create branch '{branch_name}' using pygit2: {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n\texcept Exception as e:\n\t\tmsg = f\"An unexpected error occurred while creating branch '{branch_name}': {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.checkout_branch","title":"checkout_branch","text":"<pre><code>checkout_branch(branch_name: str) -&gt; None\n</code></pre> <p>Checkout an existing branch using pygit2.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to checkout.</p> required <p>Raises:</p> Type Description <code>GitError</code> <p>If checkout fails.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>def checkout_branch(self, branch_name: str) -&gt; None:\n\t\"\"\"\n\tCheckout an existing branch using pygit2.\n\n\tArgs:\n\t    branch_name: Name of the branch to checkout.\n\n\tRaises:\n\t    GitError: If checkout fails.\n\t\"\"\"\n\ttry:\n\t\t# Construct the full ref name\n\t\tref_name = f\"refs/heads/{branch_name}\"\n\t\tbranch_obj = self.repo.lookup_reference(ref_name)\n\t\tself.repo.checkout(branch_obj)\n\t\t# Update self.branch after checkout, consistent with GitRepoContext constructor\n\t\tcurrent_branch_obj = self.repo\n\t\tif not current_branch_obj.head_is_detached:\n\t\t\tself.branch = current_branch_obj.head.shorthand\n\t\telse:\n\t\t\tself.branch = \"\"  # Or perhaps the SHA for detached head\n\t\tlogger.info(f\"Checked out branch '{branch_name}' using pygit2.\")\n\t\t# Run post-checkout hook if present\n\t\tif hook_exists(\"post-checkout\"):\n\t\t\texit_code = run_hook(\"post-checkout\")\n\t\t\tif exit_code != 0:\n\t\t\t\tlogger.warning(\"post-checkout hook failed (branch already checked out)\")\n\texcept GitError as e:\n\t\tmsg = f\"Failed to checkout branch '{branch_name}' using pygit2: {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n\texcept Exception as e:\n\t\tmsg = f\"An unexpected error occurred while checking out branch '{branch_name}': {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.push_branch","title":"push_branch","text":"<pre><code>push_branch(\n\tbranch_name: str,\n\tforce: bool = False,\n\tremote_name: str = \"origin\",\n\tignore_hooks: bool = False,\n) -&gt; None\n</code></pre> <p>Push a branch to the remote using pygit2.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to push.</p> required <code>force</code> <code>bool</code> <p>Whether to force push.</p> <code>False</code> <code>remote_name</code> <code>str</code> <p>Name of the remote (e.g., \"origin\").</p> <code>'origin'</code> <code>ignore_hooks</code> <code>bool</code> <p>If True, skip running the pre-push hook.</p> <code>False</code> <p>Raises:</p> Type Description <code>GitError</code> <p>If push fails or pre-push hook fails.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>def push_branch(\n\tself, branch_name: str, force: bool = False, remote_name: str = \"origin\", ignore_hooks: bool = False\n) -&gt; None:\n\t\"\"\"\n\tPush a branch to the remote using pygit2.\n\n\tArgs:\n\t    branch_name: Name of the branch to push.\n\t    force: Whether to force push.\n\t    remote_name: Name of the remote (e.g., \"origin\").\n\t    ignore_hooks: If True, skip running the pre-push hook.\n\n\tRaises:\n\t    GitError: If push fails or pre-push hook fails.\n\t\"\"\"\n\t# Run pre-push hook if not ignored\n\tif not ignore_hooks:\n\t\texit_code = run_hook(\"pre-push\")\n\t\tif exit_code != 0:\n\t\t\tmsg = \"pre-push hook failed, aborting push.\"\n\t\t\tlogger.error(msg)\n\t\t\traise GitError(msg)\n\ttry:\n\t\tremote = self.repo.remotes[remote_name]\n\t\tlocal_ref = f\"refs/heads/{branch_name}\"\n\t\tremote_ref = f\"refs/heads/{branch_name}\"\n\n\t\trefspec = f\"{'+' if force else ''}{local_ref}:{remote_ref}\"\n\n\t\tlogger.info(\n\t\t\tf\"Attempting to push branch '{branch_name}' to remote \"\n\t\t\tf\"'{remote_name}' with refspec '{refspec}' using pygit2.\"\n\t\t)\n\n\t\t# Import the proper pygit2 credential classes\n\t\timport shlex\n\t\timport subprocess\n\t\tfrom pathlib import Path\n\t\tfrom urllib.parse import urlparse\n\n\t\tfrom pygit2.callbacks import RemoteCallbacks\n\t\tfrom pygit2.enums import CredentialType\n\n\t\t# Create a credential class to handle SSH and username/password authentication\n\t\tclass GitCredential:\n\t\t\tdef __init__(self, cred_type: CredentialType, *args: str | None) -&gt; None:\n\t\t\t\tself.credential_type = cred_type\n\t\t\t\tself.credential_tuple = args\n\n\t\tdef credential_callback(\n\t\t\turl: str, username_from_url: str | None, allowed_types: CredentialType\n\t\t) -&gt; GitCredential:\n\t\t\t\"\"\"\n\t\t\tCallback to handle credential requests from pygit2.\n\n\t\t\tArgs:\n\t\t\t\turl: The URL being authenticated against\n\t\t\t\tusername_from_url: Username extracted from the URL if present\n\t\t\t\tallowed_types: Bitmask of allowed credential types\n\n\t\t\tReturns:\n\t\t\t\tA credential object for authentication\n\t\t\t\"\"\"\n\t\t\tlogger.debug(f\"Authentication required for {url} (allowed types: {allowed_types})\")\n\n\t\t\t# Get username from URL or use default from git config\n\t\t\tusername = username_from_url\n\t\t\tif not username:\n\t\t\t\ttry:\n\t\t\t\t\tconfig = self.repo.config\n\t\t\t\t\tusername = config[\"user.name\"]\n\t\t\t\texcept (KeyError, AttributeError) as e:\n\t\t\t\t\t# Default if we can't get from config\n\t\t\t\t\tlogger.debug(f\"Could not get username from git config: {e}\")\n\t\t\t\t\tusername = \"git\"\n\n\t\t\t# Try SSH agent authentication first (if available)\n\t\t\tif CredentialType.SSH_KEY in allowed_types:\n\t\t\t\tlogger.debug(f\"Attempting SSH agent authentication for {username}\")\n\t\t\t\treturn GitCredential(CredentialType.SSH_KEY, username, None, None, \"\")\n\n\t\t\t# Try SSH key authentication if agent is not available\n\t\t\tif CredentialType.SSH_KEY in allowed_types:\n\t\t\t\ttry:\n\t\t\t\t\t# Common SSH key paths\n\t\t\t\t\tssh_dir = Path.home() / \".ssh\"\n\t\t\t\t\tkey_paths = [\n\t\t\t\t\t\tssh_dir / \"id_rsa\",\n\t\t\t\t\t\tssh_dir / \"id_ed25519\",\n\t\t\t\t\t\tssh_dir / \"id_ecdsa\",\n\t\t\t\t\t\tssh_dir / \"id_dsa\",\n\t\t\t\t\t]\n\n\t\t\t\t\tfor private_key_path in key_paths:\n\t\t\t\t\t\tpublic_key_path = Path(f\"{private_key_path}.pub\")\n\t\t\t\t\t\tif private_key_path.exists() and public_key_path.exists():\n\t\t\t\t\t\t\tlogger.debug(f\"Attempting SSH key authentication with {private_key_path}\")\n\t\t\t\t\t\t\treturn GitCredential(\n\t\t\t\t\t\t\t\tCredentialType.SSH_KEY, username, str(public_key_path), str(private_key_path), \"\"\n\t\t\t\t\t\t\t)\n\t\t\t\texcept OSError as e:\n\t\t\t\t\tlogger.debug(f\"SSH key authentication failed: {e}\")\n\n\t\t\t# Try username/password if SSH is not available or didn't work\n\t\t\tif CredentialType.USERPASS_PLAINTEXT in allowed_types:\n\t\t\t\ttry:\n\t\t\t\t\t# Extract hostname from URL\n\t\t\t\t\tparsed_url = urlparse(url)\n\t\t\t\t\thostname = parsed_url.netloc\n\n\t\t\t\t\t# Use git credential fill to get credentials - this command is safe as it's hardcoded\n\t\t\t\t\tcmd = \"git credential fill\"\n\t\t\t\t\t# Use shlex.split for secure command execution\n\t\t\t\t\tprocess = subprocess.Popen(  # noqa: S603\n\t\t\t\t\t\tshlex.split(cmd),\n\t\t\t\t\t\tstdin=subprocess.PIPE,\n\t\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\t\t\ttext=True,\n\t\t\t\t\t)\n\n\t\t\t\t\t# Provide input for git credential fill\n\t\t\t\t\tinput_data = f\"protocol={parsed_url.scheme}\\nhost={hostname}\\n\\n\"\n\t\t\t\t\tstdout, _ = process.communicate(input=input_data)\n\n\t\t\t\t\tif process.returncode == 0 and stdout:\n\t\t\t\t\t\t# Parse the output\n\t\t\t\t\t\tcredentials = {}\n\t\t\t\t\t\tfor line in stdout.splitlines():\n\t\t\t\t\t\t\tif \"=\" in line:\n\t\t\t\t\t\t\t\tkey, value = line.split(\"=\", 1)\n\t\t\t\t\t\t\t\tcredentials[key] = value\n\n\t\t\t\t\t\tif \"username\" in credentials and \"password\" in credentials:\n\t\t\t\t\t\t\tlogger.debug(f\"Using username/password authentication for {credentials['username']}\")\n\t\t\t\t\t\t\treturn GitCredential(\n\t\t\t\t\t\t\t\tCredentialType.USERPASS_PLAINTEXT, credentials[\"username\"], credentials[\"password\"]\n\t\t\t\t\t\t\t)\n\t\t\t\texcept (subprocess.SubprocessError, OSError) as e:\n\t\t\t\t\tlogger.debug(f\"Username/password authentication failed: {e}\")\n\n\t\t\t# If nothing else works, try username-only authentication\n\t\t\tif CredentialType.USERNAME in allowed_types:\n\t\t\t\tlogger.debug(f\"Falling back to username-only authentication for {username}\")\n\t\t\t\treturn GitCredential(CredentialType.USERNAME, username)\n\n\t\t\t# If we get here, we couldn't find suitable credentials\n\t\t\tlogger.warning(f\"No suitable authentication method found for {url}\")\n\t\t\tmsg = \"No suitable authentication method available\"\n\t\t\traise Pygit2GitError(msg)\n\n\t\t# Create callback object with our credential callback\n\t\tcallbacks = RemoteCallbacks(credentials=credential_callback)\n\n\t\t# Pass callbacks to the push method\n\t\tremote.push([refspec], callbacks=callbacks)\n\t\tlogger.info(f\"Branch '{branch_name}' pushed to remote '{remote_name}' using pygit2.\")\n\texcept (Pygit2GitError, KeyError) as e:  # KeyError for remote_name not found\n\t\tmsg = f\"Failed to push branch '{branch_name}' to remote '{remote_name}' using pygit2: {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n\texcept GitError as e:  # Catch codemap's GitError if it somehow occurred before\n\t\tmsg = f\"Git operation error while pushing branch '{branch_name}': {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n\texcept Exception as e:\n\t\tmsg = f\"An unexpected error occurred while pushing branch '{branch_name}': {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.get_commit_messages","title":"get_commit_messages","text":"<pre><code>get_commit_messages(\n\tbase_branch: str, head_branch: str\n) -&gt; list[str]\n</code></pre> <p>Get commit messages (summaries) between two branches using pygit2.</p> <p>This lists commits that are in head_branch but not in base_branch.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str</code> <p>Base branch name/ref (e.g., \"main\").</p> required <code>head_branch</code> <code>str</code> <p>Head branch name/ref (e.g., \"feature-branch\").</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of commit message summaries.</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If retrieving commits fails.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>def get_commit_messages(self, base_branch: str, head_branch: str) -&gt; list[str]:\n\t\"\"\"\n\tGet commit messages (summaries) between two branches using pygit2.\n\n\tThis lists commits that are in head_branch but not in base_branch.\n\n\tArgs:\n\t    base_branch: Base branch name/ref (e.g., \"main\").\n\t    head_branch: Head branch name/ref (e.g., \"feature-branch\").\n\n\tReturns:\n\t    List of commit message summaries.\n\n\tRaises:\n\t    GitError: If retrieving commits fails.\n\t\"\"\"\n\ttry:\n\t\tif not base_branch or not head_branch:\n\t\t\tlogger.warning(\"Base or head branch is None/empty, cannot get commit messages.\")\n\t\t\treturn []\n\n\t\tdef _resolve_to_commit_oid(branch_spec: str) -&gt; Oid:\n\t\t\tobj = self.repo.revparse_single(branch_spec)\n\t\t\tif not obj:\n\t\t\t\tmsg = f\"Could not resolve '{branch_spec}'\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise GitError(msg)\n\t\t\t# Ensure it's a commit (could be a tag pointing to another tag, etc.)\n\t\t\tcommit_obj = obj.peel(Commit)\n\t\t\treturn commit_obj.id\n\n\t\tbase_oid = _resolve_to_commit_oid(base_branch)\n\t\thead_oid = _resolve_to_commit_oid(head_branch)\n\n\t\twalker = self.repo.walk(head_oid, SortMode.TOPOLOGICAL)\n\t\twalker.hide(base_oid)\n\n\t\tcommit_messages = []\n\t\tfor commit_pygit2 in walker:\n\t\t\t# commit_pygit2.message is the full message. Get summary (first line).\n\t\t\tmessage_summary = commit_pygit2.message.splitlines()[0].strip() if commit_pygit2.message else \"\"\n\t\t\tcommit_messages.append(message_summary)\n\n\t\tlogger.info(f\"Found {len(commit_messages)} commit messages between '{base_branch}' and '{head_branch}'.\")\n\t\treturn commit_messages\n\n\texcept (Pygit2GitError, GitError) as e:\n\t\tmsg = f\"Failed to get commit messages between '{base_branch}' and '{head_branch}' using pygit2: {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n\texcept Exception as e:\n\t\tmsg = (\n\t\t\tf\"An unexpected error occurred while getting commit messages \"\n\t\t\tf\"between '{base_branch}' and '{head_branch}': {e}\"\n\t\t)\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/pr_git_utils/#codemap.git.pr_generator.pr_git_utils.PRGitUtils.get_branch_relation","title":"get_branch_relation","text":"<pre><code>get_branch_relation(\n\tbranch_ref_name: str, target_branch_ref_name: str\n) -&gt; tuple[bool, int]\n</code></pre> <p>Get the relationship between two branches using pygit2.</p> <p>Parameters:</p> Name Type Description Default <code>branch_ref_name</code> <code>str</code> <p>The branch to check (e.g., \"main\", \"origin/main\").</p> required <code>target_branch_ref_name</code> <code>str</code> <p>The target branch to compare against (e.g., \"feature/foo\").</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple of (is_ancestor, commit_count)</p> <code>int</code> <ul> <li>is_ancestor: True if branch_ref_name is an ancestor of target_branch_ref_name.</li> </ul> <code>tuple[bool, int]</code> <ul> <li>commit_count: Number of commits in target_branch_ref_name that are not in branch_ref_name.                 (i.e., how many commits target is \"ahead\" of branch).</li> </ul> <p>Raises:</p> Type Description <code>GitError</code> <p>If branches cannot be resolved or other git issues occur.</p> Source code in <code>src/codemap/git/pr_generator/pr_git_utils.py</code> <pre><code>def get_branch_relation(self, branch_ref_name: str, target_branch_ref_name: str) -&gt; tuple[bool, int]:\n\t\"\"\"\n\tGet the relationship between two branches using pygit2.\n\n\tArgs:\n\t\tbranch_ref_name: The branch to check (e.g., \"main\", \"origin/main\").\n\t\ttarget_branch_ref_name: The target branch to compare against (e.g., \"feature/foo\").\n\n\tReturns:\n\t\tTuple of (is_ancestor, commit_count)\n\t\t- is_ancestor: True if branch_ref_name is an ancestor of target_branch_ref_name.\n\t\t- commit_count: Number of commits in target_branch_ref_name that are not in branch_ref_name.\n\t\t\t\t\t(i.e., how many commits target is \"ahead\" of branch).\n\n\tRaises:\n\t\tGitError: If branches cannot be resolved or other git issues occur.\n\t\"\"\"\n\ttry:\n\t\tif not branch_ref_name or not target_branch_ref_name:\n\t\t\tlogger.warning(\"Branch or target branch name is None/empty for relation check.\")\n\t\t\treturn False, 0\n\n\t\t# Resolve branch names to Oids. revparse_single can handle local and remote-like refs.\n\t\tbranch_commit_obj = self.repo.revparse_single(branch_ref_name)\n\t\tif not branch_commit_obj:\n\t\t\tmsg = f\"Could not resolve branch: {branch_ref_name}\"\n\t\t\tlogger.error(msg)\n\t\t\traise GitError(msg)\n\t\tbranch_oid = branch_commit_obj.peel(Commit).id\n\n\t\ttarget_commit_obj = self.repo.revparse_single(target_branch_ref_name)\n\t\tif not target_commit_obj:\n\t\t\tmsg = f\"Could not resolve target branch: {target_branch_ref_name}\"\n\t\t\tlogger.error(msg)\n\t\t\traise GitError(msg)\n\t\ttarget_oid = target_commit_obj.peel(Commit).id\n\n\t\t# Check if branch_oid is an ancestor of target_oid\n\t\t# pygit2's descendant_of(A, B) means \"is A a descendant of B?\"\n\t\t# So, is_ancestor (branch is ancestor of target) means target is descendant of branch.\n\t\tis_ancestor = self.repo.descendant_of(target_oid, branch_oid)\n\n\t\t# Get commit count: commits in target_oid that are not in branch_oid.\n\t\t# ahead_behind(A, B) returns (commits in A not in B, commits in B not in A)\n\t\t# We want commits in target_oid not in branch_oid.\n\t\t# So, if A=target_oid, B=branch_oid, we want the first value (ahead).\n\t\tahead, _ = self.repo.ahead_behind(target_oid, branch_oid)\n\t\tcommit_count_target_ahead = ahead  # Renaming for clarity\n\n\t\tlogger.debug(\n\t\t\tf\"Branch relation: {branch_ref_name} vs {target_branch_ref_name}. \"\n\t\t\tf\"Is ancestor: {is_ancestor}, Target ahead by: {commit_count_target_ahead}\"\n\t\t)\n\t\treturn is_ancestor, commit_count_target_ahead\n\n\texcept Pygit2GitError as e:\n\t\tmsg = (\n\t\t\tf\"Pygit2 error determining branch relation between \"\n\t\t\tf\"'{branch_ref_name}' and '{target_branch_ref_name}': {e}\"\n\t\t)\n\t\tlogger.warning(msg)\n\t\traise GitError(msg) from e  # Wrap in codemap's GitError\n\texcept GitError as e:  # Catch codemap's GitError if raised by _resolve_to_commit_oid or similar\n\t\tmsg = (\n\t\t\tf\"Codemap GitError determining branch relation between '{branch_ref_name}' and \"\n\t\t\tf\"'{target_branch_ref_name}': {e}\"\n\t\t)\n\t\tlogger.warning(msg)\n\t\traise  # Re-raise as it's already the correct type\n\texcept Exception as e:  # Catch any other unexpected non-Git errors\n\t\tmsg = (\n\t\t\tf\"Unexpected error determining branch relation between '{branch_ref_name}' and \"\n\t\t\tf\"'{target_branch_ref_name}': {e}\"\n\t\t)\n\t\tlogger.warning(msg)\n\t\traise GitError(msg) from e  # Wrap in codemap's GitError\n</code></pre>"},{"location":"api/git/pr_generator/prompts/","title":"Prompts","text":"<p>Prompt templates for PR generation.</p>"},{"location":"api/git/pr_generator/prompts/#codemap.git.pr_generator.prompts.PR_SYSTEM_PROMPT","title":"PR_SYSTEM_PROMPT  <code>module-attribute</code>","text":"<pre><code>PR_SYSTEM_PROMPT = \"\\nYou are an AI assistant knowledgeable in Git best practices.\\nYou are tasked with generating PR titles and descriptions based on a list of commits.\\nFollow the user's requirements carefully and to the letter.\\n\"\n</code></pre>"},{"location":"api/git/pr_generator/prompts/#codemap.git.pr_generator.prompts.PR_TITLE_PROMPT","title":"PR_TITLE_PROMPT  <code>module-attribute</code>","text":"<pre><code>PR_TITLE_PROMPT = 'Based on the following commits, generate a clear, concise PR title that captures the\\nessence of the changes.\\nFollow these guidelines:\\n- Focus on the most important change\\n- If there are multiple related changes, summarize them\\n- Keep it under 80 characters\\n- Start with a capital letter\\n- Don\\'t use a period at the end\\n- Use present tense (e.g., \"Add feature\" not \"Added feature\")\\n- Be descriptive and specific (e.g., \"Fix memory leak in data processing\" not just \"Fix bug\")\\n- Include the type of change if clear (Feature, Fix, Refactor, etc.)\\n\\nCommits:\\n{commit_list}\\n\\nPR Title:\\n---\\n\\nIMPORTANT:\\n- Do not include any other text in your response except the PR title.\\n- Do not wrap the PR title in quotes.\\n- Do not add any explanations or other text to your response.\\n- Do not generate Capitalized PR titles.\\n- Do not generate PR titles in CamelCase.\\n'\n</code></pre>"},{"location":"api/git/pr_generator/prompts/#codemap.git.pr_generator.prompts.PR_DESCRIPTION_PROMPT","title":"PR_DESCRIPTION_PROMPT  <code>module-attribute</code>","text":"<pre><code>PR_DESCRIPTION_PROMPT = \"\\nBased on the following commits, generate a comprehensive PR description following this template:\\n\\n## What type of PR is this? (check all applicable)\\n\\n- [ ] Refactor\\n- [ ] Feature\\n- [ ] Bug Fix\\n- [ ] Optimization\\n- [ ] Documentation Update\\n\\n## Description\\n[Fill this section with a detailed description of the changes]\\n\\n## Related Tickets &amp; Documents\\n- Related Issue #\\n- Closes #\\n\\n## Added/updated tests?\\n- [ ] Yes\\n- [ ] No, and this is why: [explanation]\\n- [ ] I need help with writing tests\\n\\nConsider the following guidelines:\\n- Check the appropriate PR type boxes based on the commit messages\\n- Provide a clear, detailed description of the changes\\n- Include any relevant issue numbers that this PR relates to or closes\\n- Indicate if tests were added, and if not, explain why\\n- Use bullet points for clarity\\n\\nCommits:\\n{commit_list}\\n\\nPR Description:\\n---\\n\\nIMPORTANT:\\n- Do not include any other text in your response except the PR description.\\n- Do not wrap the PR description in quotes.\\n- Do not add any explanations or other text to your response.\\n\"\n</code></pre>"},{"location":"api/git/pr_generator/prompts/#codemap.git.pr_generator.prompts.format_commits_for_prompt","title":"format_commits_for_prompt","text":"<pre><code>format_commits_for_prompt(commits: list[str]) -&gt; str\n</code></pre> <p>Format commit messages as a bulleted list.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted commit list as a string</p> Source code in <code>src/codemap/git/pr_generator/prompts.py</code> <pre><code>def format_commits_for_prompt(commits: list[str]) -&gt; str:\n\t\"\"\"\n\tFormat commit messages as a bulleted list.\n\n\tArgs:\n\t    commits: List of commit messages\n\n\tReturns:\n\t    Formatted commit list as a string\n\n\t\"\"\"\n\treturn \"\\n\".join([f\"- {commit}\" for commit in commits])\n</code></pre>"},{"location":"api/git/pr_generator/schemas/","title":"Schemas","text":"<p>Schemas and data structures for PR generation.</p>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.WorkflowStrategySchema","title":"WorkflowStrategySchema  <code>module-attribute</code>","text":"<pre><code>WorkflowStrategySchema = Literal[\n\t\"github-flow\", \"gitflow\", \"trunk-based\"\n]\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.BranchType","title":"BranchType  <code>module-attribute</code>","text":"<pre><code>BranchType = Literal[\n\t\"feature\", \"release\", \"hotfix\", \"bugfix\", \"docs\"\n]\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PRContent","title":"PRContent","text":"<p>               Bases: <code>TypedDict</code></p> <p>Pull request content type.</p> Source code in <code>src/codemap/git/pr_generator/schemas.py</code> <pre><code>class PRContent(TypedDict):\n\t\"\"\"Pull request content type.\"\"\"\n\n\ttitle: str\n\tdescription: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PRContent.title","title":"title  <code>instance-attribute</code>","text":"<pre><code>title: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PRContent.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest","title":"PullRequest  <code>dataclass</code>","text":"<p>Represents a GitHub Pull Request.</p> Source code in <code>src/codemap/git/pr_generator/schemas.py</code> <pre><code>@dataclass\nclass PullRequest:\n\t\"\"\"Represents a GitHub Pull Request.\"\"\"\n\n\tbranch: str\n\ttitle: str\n\tdescription: str\n\turl: str | None = None\n\tnumber: int | None = None\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.__init__","title":"__init__","text":"<pre><code>__init__(\n\tbranch: str,\n\ttitle: str,\n\tdescription: str,\n\turl: str | None = None,\n\tnumber: int | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.branch","title":"branch  <code>instance-attribute</code>","text":"<pre><code>branch: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.title","title":"title  <code>instance-attribute</code>","text":"<pre><code>title: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.url","title":"url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>url: str | None = None\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.number","title":"number  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>number: int | None = None\n</code></pre>"},{"location":"api/git/pr_generator/strategies/","title":"Strategies","text":"<p>Git workflow strategy implementations for PR management.</p>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy","title":"WorkflowStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for git workflow strategies.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class WorkflowStrategy(ABC):\n\t\"\"\"Base class for git workflow strategies.\"\"\"\n\n\t@abstractmethod\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:\n\t\t\"\"\"\n\t\tGet the default base branch for a given branch type.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Name of the default base branch\n\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on the workflow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t\t    description: Description of the branch\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\tclean_description = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", description.lower()).strip(\"-\")\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\treturn f\"{prefix}{clean_description}\"\n\n\t@abstractmethod\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the branch name prefix for a given branch type.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Branch name prefix\n\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\t@abstractmethod\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for this workflow.\n\n\t\tReturns:\n\t\t    List of valid branch types\n\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef detect_branch_type(self, branch_name: str | None) -&gt; str | None:\n\t\t\"\"\"\n\t\tDetect the type of a branch from its name.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch\n\n\t\tReturns:\n\t\t    Branch type or None if not detected\n\n\t\t\"\"\"\n\t\tfor branch_type in self.get_branch_types():\n\t\t\tprefix = self.get_branch_prefix(branch_type)\n\t\t\tif branch_name and branch_name.startswith(prefix):\n\t\t\t\treturn branch_type\n\t\treturn None\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet PR title and description templates for a given branch type.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\treturn DEFAULT_PR_TEMPLATE\n\n\tdef get_remote_branches(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet list of remote branches.\n\n\t\tReturns:\n\t\t    List of remote branch names (without 'origin/' prefix typically)\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\tremote_branches = []\n\t\t\tfor b_name in pgu.repo.branches.remote:\n\t\t\t\tif b_name.startswith(\"origin/\"):\n\t\t\t\t\tbranch_name_without_prefix = b_name[len(\"origin/\") :]\n\t\t\t\t\tif not branch_name_without_prefix.startswith(\"HEAD\"):\n\t\t\t\t\t\tremote_branches.append(branch_name_without_prefix)\n\t\t\t\telif \"/\" in b_name and not b_name.endswith(\"/HEAD\"):\n\t\t\t\t\tparts = b_name.split(\"/\", 1)\n\t\t\t\t\tif len(parts) &gt; 1:\n\t\t\t\t\t\tremote_branches.append(parts[1])\n\t\t\treturn list(set(remote_branches))\n\t\texcept (GitError, Pygit2GitError) as e:\n\t\t\tPRGitUtils.logger.debug(f\"Error getting remote branches: {e}\")\n\t\t\treturn []\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tPRGitUtils.logger.debug(f\"Unexpected error getting remote branches: {e}\")\n\t\t\treturn []\n\n\tdef get_local_branches(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet list of local branches.\n\n\t\tReturns:\n\t\t    List of local branch names\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\treturn list(pgu.repo.branches.local)\n\t\texcept (GitError, Pygit2GitError) as e:\n\t\t\tPRGitUtils.logger.debug(f\"Error getting local branches: {e}\")\n\t\t\treturn []\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tPRGitUtils.logger.debug(f\"Unexpected error getting local branches: {e}\")\n\t\t\treturn []\n\n\tdef get_branches_by_type(self) -&gt; dict[str, list[str]]:\n\t\t\"\"\"\n\t\tGroup branches by their type.\n\n\t\tReturns:\n\t\t    Dictionary mapping branch types to lists of branch names\n\n\t\t\"\"\"\n\t\tresult = {branch_type: [] for branch_type in self.get_branch_types()}\n\t\tresult[\"other\"] = []  # For branches that don't match any type\n\n\t\t# Get all branches (local and remote)\n\t\tall_branches = set(self.get_local_branches() + self.get_remote_branches())\n\n\t\tfor branch in all_branches:\n\t\t\tbranch_type = self.detect_branch_type(branch)\n\t\t\tif branch_type:\n\t\t\t\tresult[branch_type].append(branch)\n\t\t\telse:\n\t\t\t\tresult[\"other\"].append(branch)\n\n\t\treturn result\n\n\tdef get_branch_metadata(self, branch_name: str) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tGet metadata for a specific branch.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch\n\n\t\tReturns:\n\t\t    Dictionary with branch metadata\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\trepo = pgu.repo\n\n\t\t\t# Determine full ref for revparse_single (try local, then remote)\n\t\t\tbranch_ref_to_parse = branch_name\n\t\t\tif not branch_exists(branch_name, pgu_instance=pgu, include_remote=False) and branch_exists(\n\t\t\t\tbranch_name, pgu_instance=pgu, remote_name=\"origin\", include_local=False\n\t\t\t):\n\t\t\t\tbranch_ref_to_parse = f\"origin/{branch_name}\"\n\t\t\t# If still not found by branch_exists, revparse_single might fail, which is caught below.\n\n\t\t\tlast_commit_iso_date = \"unknown\"\n\t\t\ttry:\n\t\t\t\tcommit_obj = repo.revparse_single(branch_ref_to_parse).peel(Commit)\n\t\t\t\tcommit_time = datetime.fromtimestamp(commit_obj.commit_time, tz=UTC)\n\t\t\t\tlast_commit_iso_date = commit_time.isoformat()\n\t\t\texcept (Pygit2GitError, GitError) as e:  # Catch errors resolving commit\n\t\t\t\tPRGitUtils.logger.debug(f\"Could not get last commit date for {branch_ref_to_parse}: {e}\")\n\n\t\t\tcommit_count_str = \"0\"\n\t\t\ttry:\n\t\t\t\tdefault_b = get_default_branch(pgu_instance=pgu)\n\t\t\t\tif default_b:\n\t\t\t\t\t# get_branch_relation expects full ref names or resolvable names\n\t\t\t\t\t_, count = pgu.get_branch_relation(default_b, branch_ref_to_parse)\n\t\t\t\t\tcommit_count_str = str(count)\n\t\t\texcept (GitError, Pygit2GitError) as e:\n\t\t\t\tPRGitUtils.logger.debug(f\"Could not get commit count for {branch_ref_to_parse} vs default: {e}\")\n\n\t\t\tbranch_type_detected = self.detect_branch_type(branch_name)\n\n\t\t\treturn {\n\t\t\t\t\"last_commit_date\": last_commit_iso_date,\n\t\t\t\t\"commit_count\": commit_count_str,\n\t\t\t\t\"branch_type\": branch_type_detected,\n\t\t\t\t\"is_local\": branch_exists(branch_name, pgu_instance=pgu, include_remote=False, include_local=True),\n\t\t\t\t\"is_remote\": branch_exists(branch_name, pgu_instance=pgu, remote_name=\"origin\", include_local=False),\n\t\t\t}\n\t\texcept (GitError, Pygit2GitError) as e:\n\t\t\tPRGitUtils.logger.warning(f\"Error getting branch metadata for {branch_name}: {e}\")\n\t\t\treturn {  # Fallback for broader errors during pgu instantiation or initial checks\n\t\t\t\t\"last_commit_date\": \"unknown\",\n\t\t\t\t\"commit_count\": \"0\",\n\t\t\t\t\"branch_type\": self.detect_branch_type(branch_name),\n\t\t\t\t\"is_local\": False,\n\t\t\t\t\"is_remote\": False,\n\t\t\t}\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tPRGitUtils.logger.warning(f\"Unexpected error getting branch metadata for {branch_name}: {e}\")\n\t\t\treturn {\n\t\t\t\t\"last_commit_date\": \"unknown\",\n\t\t\t\t\"commit_count\": \"0\",\n\t\t\t\t\"branch_type\": self.detect_branch_type(branch_name),\n\t\t\t\t\"is_local\": False,\n\t\t\t\t\"is_remote\": False,\n\t\t\t}\n\n\tdef get_all_branches_with_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\t\t\"\"\"\n\t\tGet all branches with metadata.\n\n\t\tReturns:\n\t\t    Dictionary mapping branch names to metadata dictionaries\n\n\t\t\"\"\"\n\t\tresult = {}\n\t\t# Using PRGitUtils for a consistent list of branches\n\t\tpgu = PRGitUtils.get_instance()\n\t\tlocal_b = list(pgu.repo.branches.local)\n\t\tremote_b_parsed = []\n\t\tfor rb_name in pgu.repo.branches.remote:\n\t\t\tif rb_name.startswith(\"origin/\") and not rb_name.endswith(\"/HEAD\"):\n\t\t\t\tremote_b_parsed.append(rb_name[len(\"origin/\") :])\n\t\t\telif \"/\" in rb_name and not rb_name.endswith(\"/HEAD\"):\n\t\t\t\tparts = rb_name.split(\"/\", 1)\n\t\t\t\tif len(parts) &gt; 1:\n\t\t\t\t\tremote_b_parsed.append(parts[1])\n\n\t\tall_branches = set(local_b + remote_b_parsed)\n\n\t\tfor branch in all_branches:\n\t\t\tresult[branch] = self.get_branch_metadata(branch)\n\n\t\treturn result\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_default_base","title":"get_default_base  <code>abstractmethod</code>","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for a given branch type.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>@abstractmethod\ndef get_default_base(self, branch_type: str) -&gt; str | None:\n\t\"\"\"\n\tGet the default base branch for a given branch type.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Name of the default base branch\n\n\t\"\"\"\n\traise NotImplementedError\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tbranch_type: str, description: str\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <code>description</code> <code>str</code> <p>Description of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on the workflow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t    description: Description of the branch\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\tclean_description = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", description.lower()).strip(\"-\")\n\tprefix = self.get_branch_prefix(branch_type)\n\treturn f\"{prefix}{clean_description}\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branch_prefix","title":"get_branch_prefix  <code>abstractmethod</code>","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for a given branch type.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>@abstractmethod\ndef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\"\"\"\n\tGet the branch name prefix for a given branch type.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Branch name prefix\n\n\t\"\"\"\n\traise NotImplementedError\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branch_types","title":"get_branch_types  <code>abstractmethod</code>","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for this workflow.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of valid branch types</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>@abstractmethod\ndef get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for this workflow.\n\n\tReturns:\n\t    List of valid branch types\n\n\t\"\"\"\n\traise NotImplementedError\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.detect_branch_type","title":"detect_branch_type","text":"<pre><code>detect_branch_type(branch_name: str | None) -&gt; str | None\n</code></pre> <p>Detect the type of a branch from its name.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str | None</code> <p>Name of the branch</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Branch type or None if not detected</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def detect_branch_type(self, branch_name: str | None) -&gt; str | None:\n\t\"\"\"\n\tDetect the type of a branch from its name.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\n\tReturns:\n\t    Branch type or None if not detected\n\n\t\"\"\"\n\tfor branch_type in self.get_branch_types():\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\tif branch_name and branch_name.startswith(prefix):\n\t\t\treturn branch_type\n\treturn None\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for a given branch type.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\"\"\"\n\tGet PR title and description templates for a given branch type.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\treturn DEFAULT_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_remote_branches","title":"get_remote_branches","text":"<pre><code>get_remote_branches() -&gt; list[str]\n</code></pre> <p>Get list of remote branches.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of remote branch names (without 'origin/' prefix typically)</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_remote_branches(self) -&gt; list[str]:\n\t\"\"\"\n\tGet list of remote branches.\n\n\tReturns:\n\t    List of remote branch names (without 'origin/' prefix typically)\n\n\t\"\"\"\n\ttry:\n\t\tpgu = PRGitUtils.get_instance()\n\t\tremote_branches = []\n\t\tfor b_name in pgu.repo.branches.remote:\n\t\t\tif b_name.startswith(\"origin/\"):\n\t\t\t\tbranch_name_without_prefix = b_name[len(\"origin/\") :]\n\t\t\t\tif not branch_name_without_prefix.startswith(\"HEAD\"):\n\t\t\t\t\tremote_branches.append(branch_name_without_prefix)\n\t\t\telif \"/\" in b_name and not b_name.endswith(\"/HEAD\"):\n\t\t\t\tparts = b_name.split(\"/\", 1)\n\t\t\t\tif len(parts) &gt; 1:\n\t\t\t\t\tremote_branches.append(parts[1])\n\t\treturn list(set(remote_branches))\n\texcept (GitError, Pygit2GitError) as e:\n\t\tPRGitUtils.logger.debug(f\"Error getting remote branches: {e}\")\n\t\treturn []\n\texcept Exception as e:  # noqa: BLE001\n\t\tPRGitUtils.logger.debug(f\"Unexpected error getting remote branches: {e}\")\n\t\treturn []\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_local_branches","title":"get_local_branches","text":"<pre><code>get_local_branches() -&gt; list[str]\n</code></pre> <p>Get list of local branches.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of local branch names</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_local_branches(self) -&gt; list[str]:\n\t\"\"\"\n\tGet list of local branches.\n\n\tReturns:\n\t    List of local branch names\n\n\t\"\"\"\n\ttry:\n\t\tpgu = PRGitUtils.get_instance()\n\t\treturn list(pgu.repo.branches.local)\n\texcept (GitError, Pygit2GitError) as e:\n\t\tPRGitUtils.logger.debug(f\"Error getting local branches: {e}\")\n\t\treturn []\n\texcept Exception as e:  # noqa: BLE001\n\t\tPRGitUtils.logger.debug(f\"Unexpected error getting local branches: {e}\")\n\t\treturn []\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branches_by_type","title":"get_branches_by_type","text":"<pre><code>get_branches_by_type() -&gt; dict[str, list[str]]\n</code></pre> <p>Group branches by their type.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary mapping branch types to lists of branch names</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branches_by_type(self) -&gt; dict[str, list[str]]:\n\t\"\"\"\n\tGroup branches by their type.\n\n\tReturns:\n\t    Dictionary mapping branch types to lists of branch names\n\n\t\"\"\"\n\tresult = {branch_type: [] for branch_type in self.get_branch_types()}\n\tresult[\"other\"] = []  # For branches that don't match any type\n\n\t# Get all branches (local and remote)\n\tall_branches = set(self.get_local_branches() + self.get_remote_branches())\n\n\tfor branch in all_branches:\n\t\tbranch_type = self.detect_branch_type(branch)\n\t\tif branch_type:\n\t\t\tresult[branch_type].append(branch)\n\t\telse:\n\t\t\tresult[\"other\"].append(branch)\n\n\treturn result\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branch_metadata","title":"get_branch_metadata","text":"<pre><code>get_branch_metadata(branch_name: str) -&gt; dict[str, Any]\n</code></pre> <p>Get metadata for a specific branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with branch metadata</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_metadata(self, branch_name: str) -&gt; dict[str, Any]:\n\t\"\"\"\n\tGet metadata for a specific branch.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\n\tReturns:\n\t    Dictionary with branch metadata\n\n\t\"\"\"\n\ttry:\n\t\tpgu = PRGitUtils.get_instance()\n\t\trepo = pgu.repo\n\n\t\t# Determine full ref for revparse_single (try local, then remote)\n\t\tbranch_ref_to_parse = branch_name\n\t\tif not branch_exists(branch_name, pgu_instance=pgu, include_remote=False) and branch_exists(\n\t\t\tbranch_name, pgu_instance=pgu, remote_name=\"origin\", include_local=False\n\t\t):\n\t\t\tbranch_ref_to_parse = f\"origin/{branch_name}\"\n\t\t# If still not found by branch_exists, revparse_single might fail, which is caught below.\n\n\t\tlast_commit_iso_date = \"unknown\"\n\t\ttry:\n\t\t\tcommit_obj = repo.revparse_single(branch_ref_to_parse).peel(Commit)\n\t\t\tcommit_time = datetime.fromtimestamp(commit_obj.commit_time, tz=UTC)\n\t\t\tlast_commit_iso_date = commit_time.isoformat()\n\t\texcept (Pygit2GitError, GitError) as e:  # Catch errors resolving commit\n\t\t\tPRGitUtils.logger.debug(f\"Could not get last commit date for {branch_ref_to_parse}: {e}\")\n\n\t\tcommit_count_str = \"0\"\n\t\ttry:\n\t\t\tdefault_b = get_default_branch(pgu_instance=pgu)\n\t\t\tif default_b:\n\t\t\t\t# get_branch_relation expects full ref names or resolvable names\n\t\t\t\t_, count = pgu.get_branch_relation(default_b, branch_ref_to_parse)\n\t\t\t\tcommit_count_str = str(count)\n\t\texcept (GitError, Pygit2GitError) as e:\n\t\t\tPRGitUtils.logger.debug(f\"Could not get commit count for {branch_ref_to_parse} vs default: {e}\")\n\n\t\tbranch_type_detected = self.detect_branch_type(branch_name)\n\n\t\treturn {\n\t\t\t\"last_commit_date\": last_commit_iso_date,\n\t\t\t\"commit_count\": commit_count_str,\n\t\t\t\"branch_type\": branch_type_detected,\n\t\t\t\"is_local\": branch_exists(branch_name, pgu_instance=pgu, include_remote=False, include_local=True),\n\t\t\t\"is_remote\": branch_exists(branch_name, pgu_instance=pgu, remote_name=\"origin\", include_local=False),\n\t\t}\n\texcept (GitError, Pygit2GitError) as e:\n\t\tPRGitUtils.logger.warning(f\"Error getting branch metadata for {branch_name}: {e}\")\n\t\treturn {  # Fallback for broader errors during pgu instantiation or initial checks\n\t\t\t\"last_commit_date\": \"unknown\",\n\t\t\t\"commit_count\": \"0\",\n\t\t\t\"branch_type\": self.detect_branch_type(branch_name),\n\t\t\t\"is_local\": False,\n\t\t\t\"is_remote\": False,\n\t\t}\n\texcept Exception as e:  # noqa: BLE001\n\t\tPRGitUtils.logger.warning(f\"Unexpected error getting branch metadata for {branch_name}: {e}\")\n\t\treturn {\n\t\t\t\"last_commit_date\": \"unknown\",\n\t\t\t\"commit_count\": \"0\",\n\t\t\t\"branch_type\": self.detect_branch_type(branch_name),\n\t\t\t\"is_local\": False,\n\t\t\t\"is_remote\": False,\n\t\t}\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_all_branches_with_metadata","title":"get_all_branches_with_metadata","text":"<pre><code>get_all_branches_with_metadata() -&gt; dict[\n\tstr, dict[str, Any]\n]\n</code></pre> <p>Get all branches with metadata.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>Dictionary mapping branch names to metadata dictionaries</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_all_branches_with_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\t\"\"\"\n\tGet all branches with metadata.\n\n\tReturns:\n\t    Dictionary mapping branch names to metadata dictionaries\n\n\t\"\"\"\n\tresult = {}\n\t# Using PRGitUtils for a consistent list of branches\n\tpgu = PRGitUtils.get_instance()\n\tlocal_b = list(pgu.repo.branches.local)\n\tremote_b_parsed = []\n\tfor rb_name in pgu.repo.branches.remote:\n\t\tif rb_name.startswith(\"origin/\") and not rb_name.endswith(\"/HEAD\"):\n\t\t\tremote_b_parsed.append(rb_name[len(\"origin/\") :])\n\t\telif \"/\" in rb_name and not rb_name.endswith(\"/HEAD\"):\n\t\t\tparts = rb_name.split(\"/\", 1)\n\t\t\tif len(parts) &gt; 1:\n\t\t\t\tremote_b_parsed.append(parts[1])\n\n\tall_branches = set(local_b + remote_b_parsed)\n\n\tfor branch in all_branches:\n\t\tresult[branch] = self.get_branch_metadata(branch)\n\n\treturn result\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy","title":"GitHubFlowStrategy","text":"<p>               Bases: <code>WorkflowStrategy</code></p> <p>Implementation of GitHub Flow workflow strategy.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class GitHubFlowStrategy(WorkflowStrategy):\n\t\"\"\"Implementation of GitHub Flow workflow strategy.\"\"\"\n\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet the default base branch for GitHub Flow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\t\tReturns:\n\t\t    Name of the default base branch (usually 'main')\n\n\t\t\"\"\"\n\t\t# Ignoring branch_type as GitHub Flow always uses the default branch\n\t\treturn get_default_branch()\n\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet the branch name prefix for GitHub Flow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\t\tReturns:\n\t\t    Branch name prefix (empty string for GitHub Flow)\n\n\t\t\"\"\"\n\t\t# Ignoring branch_type as GitHub Flow doesn't use prefixes\n\t\treturn \"\"\n\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for GitHub Flow.\n\n\t\tReturns:\n\t\t    List containing only 'feature'\n\n\t\t\"\"\"\n\t\treturn [\"feature\"]\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet PR title and description templates for GitHub Flow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\treturn GITHUB_FLOW_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_default_base","title":"get_default_base","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for GitHub Flow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (always 'feature' in GitHub Flow)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch (usually 'main')</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\"\"\"\n\tGet the default base branch for GitHub Flow.\n\n\tArgs:\n\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\tReturns:\n\t    Name of the default base branch (usually 'main')\n\n\t\"\"\"\n\t# Ignoring branch_type as GitHub Flow always uses the default branch\n\treturn get_default_branch()\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_branch_prefix","title":"get_branch_prefix","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for GitHub Flow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (always 'feature' in GitHub Flow)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix (empty string for GitHub Flow)</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_prefix(self, branch_type: str) -&gt; str:  # noqa: ARG002\n\t\"\"\"\n\tGet the branch name prefix for GitHub Flow.\n\n\tArgs:\n\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\tReturns:\n\t    Branch name prefix (empty string for GitHub Flow)\n\n\t\"\"\"\n\t# Ignoring branch_type as GitHub Flow doesn't use prefixes\n\treturn \"\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_branch_types","title":"get_branch_types","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for GitHub Flow.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List containing only 'feature'</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for GitHub Flow.\n\n\tReturns:\n\t    List containing only 'feature'\n\n\t\"\"\"\n\treturn [\"feature\"]\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for GitHub Flow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (always 'feature' in GitHub Flow)</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\"\"\"\n\tGet PR title and description templates for GitHub Flow.\n\n\tArgs:\n\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\treturn GITHUB_FLOW_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy","title":"GitFlowStrategy","text":"<p>               Bases: <code>WorkflowStrategy</code></p> <p>Implementation of GitFlow workflow strategy.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class GitFlowStrategy(WorkflowStrategy):\n\t\"\"\"Implementation of GitFlow workflow strategy.\"\"\"\n\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:\n\t\t\"\"\"\n\t\tGet the default base branch for GitFlow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\t\tReturns:\n\t\t    Name of the default base branch\n\n\t\t\"\"\"\n\t\tmapping = {\n\t\t\t\"feature\": \"develop\",\n\t\t\t\"release\": \"main\",\n\t\t\t\"hotfix\": \"main\",\n\t\t\t\"bugfix\": \"develop\",\n\t\t}\n\t\tdefault = get_default_branch()\n\t\treturn mapping.get(branch_type, default)\n\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the branch name prefix for GitFlow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Branch name prefix\n\n\t\t\"\"\"\n\t\tmapping = {\n\t\t\t\"feature\": \"feature/\",\n\t\t\t\"release\": \"release/\",\n\t\t\t\"hotfix\": \"hotfix/\",\n\t\t\t\"bugfix\": \"bugfix/\",\n\t\t}\n\t\treturn mapping.get(branch_type, \"\")\n\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for GitFlow.\n\n\t\tReturns:\n\t\t    List of valid branch types for GitFlow\n\n\t\t\"\"\"\n\t\treturn [\"feature\", \"release\", \"hotfix\", \"bugfix\"]\n\n\tdef suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on GitFlow conventions.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t\t    description: Description of the branch\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\tprefix = self.get_branch_prefix(branch_type)\n\n\t\tif branch_type == \"release\":\n\t\t\t# Extract version number from description if it looks like a version\n\t\t\tversion_match = re.search(r\"(\\d+\\.\\d+\\.\\d+)\", description)\n\t\t\tif version_match:\n\t\t\t\treturn f\"{prefix}{version_match.group(1)}\"\n\n\t\t# For other branch types, use the default implementation\n\t\treturn super().suggest_branch_name(branch_type, description)\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tGet PR title and description templates for GitFlow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\treturn GITFLOW_PR_TEMPLATES.get(branch_type, DEFAULT_PR_TEMPLATE)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_default_base","title":"get_default_base","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for GitFlow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, bugfix)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_base(self, branch_type: str) -&gt; str | None:\n\t\"\"\"\n\tGet the default base branch for GitFlow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\tReturns:\n\t    Name of the default base branch\n\n\t\"\"\"\n\tmapping = {\n\t\t\"feature\": \"develop\",\n\t\t\"release\": \"main\",\n\t\t\"hotfix\": \"main\",\n\t\t\"bugfix\": \"develop\",\n\t}\n\tdefault = get_default_branch()\n\treturn mapping.get(branch_type, default)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_branch_prefix","title":"get_branch_prefix","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for GitFlow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\"\"\"\n\tGet the branch name prefix for GitFlow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Branch name prefix\n\n\t\"\"\"\n\tmapping = {\n\t\t\"feature\": \"feature/\",\n\t\t\"release\": \"release/\",\n\t\t\"hotfix\": \"hotfix/\",\n\t\t\"bugfix\": \"bugfix/\",\n\t}\n\treturn mapping.get(branch_type, \"\")\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_branch_types","title":"get_branch_types","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for GitFlow.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of valid branch types for GitFlow</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for GitFlow.\n\n\tReturns:\n\t    List of valid branch types for GitFlow\n\n\t\"\"\"\n\treturn [\"feature\", \"release\", \"hotfix\", \"bugfix\"]\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tbranch_type: str, description: str\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on GitFlow conventions.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <code>description</code> <code>str</code> <p>Description of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on GitFlow conventions.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t    description: Description of the branch\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\tprefix = self.get_branch_prefix(branch_type)\n\n\tif branch_type == \"release\":\n\t\t# Extract version number from description if it looks like a version\n\t\tversion_match = re.search(r\"(\\d+\\.\\d+\\.\\d+)\", description)\n\t\tif version_match:\n\t\t\treturn f\"{prefix}{version_match.group(1)}\"\n\n\t# For other branch types, use the default implementation\n\treturn super().suggest_branch_name(branch_type, description)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for GitFlow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, bugfix)</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:\n\t\"\"\"\n\tGet PR title and description templates for GitFlow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\treturn GITFLOW_PR_TEMPLATES.get(branch_type, DEFAULT_PR_TEMPLATE)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy","title":"TrunkBasedStrategy","text":"<p>               Bases: <code>WorkflowStrategy</code></p> <p>Implementation of Trunk-Based Development workflow strategy.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class TrunkBasedStrategy(WorkflowStrategy):\n\t\"\"\"Implementation of Trunk-Based Development workflow strategy.\"\"\"\n\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet the default base branch for Trunk-Based Development.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\n\t\tReturns:\n\t\t    Name of the default base branch (trunk, which is usually 'main')\n\n\t\t\"\"\"\n\t\t# Ignoring branch_type as Trunk-Based Development always uses the main branch\n\t\treturn get_default_branch()\n\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the branch name prefix for Trunk-Based Development.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\n\t\tReturns:\n\t\t    Branch name prefix\n\n\t\t\"\"\"\n\t\treturn \"fb/\" if branch_type == \"feature\" else \"\"\n\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for Trunk-Based Development.\n\n\t\tReturns:\n\t\t    List containing only 'feature'\n\n\t\t\"\"\"\n\t\treturn [\"feature\"]\n\n\tdef suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on Trunk-Based Development conventions.\n\n\t\tEmphasizes short-lived, descriptive branches.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\t\t    description: Description of the branch\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\t# For trunk-based development, try to generate very short names\n\t\twords = description.split()\n\t\t# Filter out common words like \"implement\", \"the\", \"and\", etc.\n\t\tcommon_words = [\"the\", \"and\", \"for\", \"with\", \"implement\", \"implementing\", \"implementation\"]\n\t\twords = [w for w in words if len(w) &gt; MIN_SIGNIFICANT_WORD_LENGTH and w.lower() not in common_words]\n\n\t\t# Take up to 3 significant words\n\t\tshort_desc = \"-\".join(words[:3]).lower()\n\t\tshort_desc = re.sub(r\"[^a-zA-Z0-9-]\", \"-\", short_desc)\n\t\tshort_desc = re.sub(r\"-+\", \"-\", short_desc)\n\t\tshort_desc = short_desc.strip(\"-\")\n\n\t\t# Add username prefix for trunk-based (optional)\n\t\ttry:\n\t\t\tpgu = PRGitUtils.get_instance()\n\t\t\t# Ensure config is available and handle potential errors\n\t\t\tuser_name_config = pgu.repo.config[\"user.name\"]\n\t\t\tif user_name_config:\n\t\t\t\t# Ensure user_name_config is treated as a string before strip/split\n\t\t\t\tusername = str(user_name_config).strip().split()[0].lower()\n\t\t\t\tusername = re.sub(r\"[^a-zA-Z0-9]\", \"\", username)\n\t\t\t\treturn f\"{username}/{short_desc if short_desc else 'update'}\"  # ensure short_desc is not empty\n\t\t\t# Fallback if username is not configured\n\t\t\tprefix = self.get_branch_prefix(branch_type)\n\t\t\treturn f\"{prefix}{short_desc if short_desc else 'update'}\"\n\t\texcept (GitError, Pygit2GitError, KeyError, IndexError, AttributeError) as e:  # Catch more specific errors\n\t\t\tPRGitUtils.logger.debug(f\"Could not get username for branch prefix: {e}\")\n\t\t\tprefix = self.get_branch_prefix(branch_type)\n\t\t\treturn f\"{prefix}{short_desc if short_desc else 'update'}\"\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tPRGitUtils.logger.debug(f\"Unexpected error getting username for branch prefix: {e}\")\n\t\t\tprefix = self.get_branch_prefix(branch_type)\n\t\t\treturn f\"{prefix}{short_desc if short_desc else 'update'}\"\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet PR title and description templates for Trunk-Based Development.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\treturn TRUNK_BASED_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_default_base","title":"get_default_base","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for Trunk-Based Development.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch (trunk, which is usually 'main')</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\"\"\"\n\tGet the default base branch for Trunk-Based Development.\n\n\tArgs:\n\t    branch_type: Type of branch\n\n\tReturns:\n\t    Name of the default base branch (trunk, which is usually 'main')\n\n\t\"\"\"\n\t# Ignoring branch_type as Trunk-Based Development always uses the main branch\n\treturn get_default_branch()\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_branch_prefix","title":"get_branch_prefix","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for Trunk-Based Development.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\"\"\"\n\tGet the branch name prefix for Trunk-Based Development.\n\n\tArgs:\n\t    branch_type: Type of branch\n\n\tReturns:\n\t    Branch name prefix\n\n\t\"\"\"\n\treturn \"fb/\" if branch_type == \"feature\" else \"\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_branch_types","title":"get_branch_types","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for Trunk-Based Development.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List containing only 'feature'</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for Trunk-Based Development.\n\n\tReturns:\n\t    List containing only 'feature'\n\n\t\"\"\"\n\treturn [\"feature\"]\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tbranch_type: str, description: str\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on Trunk-Based Development conventions.</p> <p>Emphasizes short-lived, descriptive branches.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <code>description</code> <code>str</code> <p>Description of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on Trunk-Based Development conventions.\n\n\tEmphasizes short-lived, descriptive branches.\n\n\tArgs:\n\t    branch_type: Type of branch\n\t    description: Description of the branch\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\t# For trunk-based development, try to generate very short names\n\twords = description.split()\n\t# Filter out common words like \"implement\", \"the\", \"and\", etc.\n\tcommon_words = [\"the\", \"and\", \"for\", \"with\", \"implement\", \"implementing\", \"implementation\"]\n\twords = [w for w in words if len(w) &gt; MIN_SIGNIFICANT_WORD_LENGTH and w.lower() not in common_words]\n\n\t# Take up to 3 significant words\n\tshort_desc = \"-\".join(words[:3]).lower()\n\tshort_desc = re.sub(r\"[^a-zA-Z0-9-]\", \"-\", short_desc)\n\tshort_desc = re.sub(r\"-+\", \"-\", short_desc)\n\tshort_desc = short_desc.strip(\"-\")\n\n\t# Add username prefix for trunk-based (optional)\n\ttry:\n\t\tpgu = PRGitUtils.get_instance()\n\t\t# Ensure config is available and handle potential errors\n\t\tuser_name_config = pgu.repo.config[\"user.name\"]\n\t\tif user_name_config:\n\t\t\t# Ensure user_name_config is treated as a string before strip/split\n\t\t\tusername = str(user_name_config).strip().split()[0].lower()\n\t\t\tusername = re.sub(r\"[^a-zA-Z0-9]\", \"\", username)\n\t\t\treturn f\"{username}/{short_desc if short_desc else 'update'}\"  # ensure short_desc is not empty\n\t\t# Fallback if username is not configured\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\treturn f\"{prefix}{short_desc if short_desc else 'update'}\"\n\texcept (GitError, Pygit2GitError, KeyError, IndexError, AttributeError) as e:  # Catch more specific errors\n\t\tPRGitUtils.logger.debug(f\"Could not get username for branch prefix: {e}\")\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\treturn f\"{prefix}{short_desc if short_desc else 'update'}\"\n\texcept Exception as e:  # noqa: BLE001\n\t\tPRGitUtils.logger.debug(f\"Unexpected error getting username for branch prefix: {e}\")\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\treturn f\"{prefix}{short_desc if short_desc else 'update'}\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for Trunk-Based Development.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\"\"\"\n\tGet PR title and description templates for Trunk-Based Development.\n\n\tArgs:\n\t    branch_type: Type of branch\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\treturn TRUNK_BASED_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.get_strategy_class","title":"get_strategy_class","text":"<pre><code>get_strategy_class(\n\tstrategy_name: str,\n) -&gt; type[WorkflowStrategy] | None\n</code></pre> <p>Get the workflow strategy class corresponding to the strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>str</code> <p>Name of the workflow strategy</p> required <p>Returns:</p> Type Description <code>type[WorkflowStrategy] | None</code> <p>Workflow strategy class or None if not found</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_strategy_class(strategy_name: str) -&gt; type[WorkflowStrategy] | None:\n\t\"\"\"\n\tGet the workflow strategy class corresponding to the strategy name.\n\n\tArgs:\n\t    strategy_name: Name of the workflow strategy\n\n\tReturns:\n\t    Workflow strategy class or None if not found\n\n\t\"\"\"\n\tstrategy_map = {\n\t\t\"github-flow\": GitHubFlowStrategy,\n\t\t\"gitflow\": GitFlowStrategy,\n\t\t\"trunk-based\": TrunkBasedStrategy,\n\t}\n\treturn strategy_map.get(strategy_name)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.create_strategy","title":"create_strategy","text":"<pre><code>create_strategy(strategy_name: str) -&gt; WorkflowStrategy\n</code></pre> <p>Create a workflow strategy instance based on the strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>str</code> <p>The name of the workflow strategy to create.</p> required <p>Returns:</p> Type Description <code>WorkflowStrategy</code> <p>An instance of the requested workflow strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the strategy name is unknown.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def create_strategy(strategy_name: str) -&gt; WorkflowStrategy:\n\t\"\"\"\n\tCreate a workflow strategy instance based on the strategy name.\n\n\tArgs:\n\t    strategy_name: The name of the workflow strategy to create.\n\n\tReturns:\n\t    An instance of the requested workflow strategy.\n\n\tRaises:\n\t    ValueError: If the strategy name is unknown.\n\n\t\"\"\"\n\tstrategy_class = get_strategy_class(strategy_name)\n\tif not strategy_class:\n\t\terror_msg = f\"Unknown workflow strategy: {strategy_name}\"\n\t\traise ValueError(error_msg)\n\n\treturn strategy_class()\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.branch_exists","title":"branch_exists","text":"<pre><code>branch_exists(\n\tbranch_name: str,\n\tpgu_instance: PRGitUtils | None = None,\n\tremote_name: str = \"origin\",\n\tinclude_remote: bool = True,\n\tinclude_local: bool = True,\n) -&gt; bool\n</code></pre> <p>Check if a branch exists using pygit2.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to check (e.g., \"main\", \"feature/foo\").</p> required <code>pgu_instance</code> <code>PRGitUtils | None</code> <p>Optional instance of PRGitUtils. If None, one will be created.</p> <code>None</code> <code>remote_name</code> <code>str</code> <p>The name of the remote to check (default: \"origin\").</p> <code>'origin'</code> <code>include_remote</code> <code>bool</code> <p>Whether to check remote branches.</p> <code>True</code> <code>include_local</code> <code>bool</code> <p>Whether to check local branches.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the branch exists in the specified locations, False otherwise.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def branch_exists(\n\tbranch_name: str,\n\tpgu_instance: PRGitUtils | None = None,\n\tremote_name: str = \"origin\",\n\tinclude_remote: bool = True,\n\tinclude_local: bool = True,\n) -&gt; bool:\n\t\"\"\"\n\tCheck if a branch exists using pygit2.\n\n\tArgs:\n\t    branch_name: Name of the branch to check (e.g., \"main\", \"feature/foo\").\n\t    pgu_instance: Optional instance of PRGitUtils. If None, one will be created.\n\t    remote_name: The name of the remote to check (default: \"origin\").\n\t    include_remote: Whether to check remote branches.\n\t    include_local: Whether to check local branches.\n\n\tReturns:\n\t    True if the branch exists in the specified locations, False otherwise.\n\t\"\"\"\n\tif not branch_name:\n\t\treturn False\n\n\tpgu = pgu_instance or PRGitUtils.get_instance()\n\trepo = pgu.repo\n\n\tif include_local:\n\t\ttry:\n\t\t\t# lookup_branch checks local branches by default if branch_type is not specified\n\t\t\t# or if BranchType.LOCAL is used.\n\t\t\tif repo.lookup_branch(branch_name, BranchType.LOCAL):  # Explicitly check local\n\t\t\t\treturn True\n\t\texcept (KeyError, Pygit2GitError):  # lookup_branch raises KeyError if not found\n\t\t\tpass  # Not found locally\n\n\tif include_remote:\n\t\tremote_branch_ref = f\"{remote_name}/{branch_name}\"\n\t\ttry:\n\t\t\t# To check a remote branch, we look it up by its full remote-prefixed name\n\t\t\t# in the list of remote branches pygit2 knows.\n\t\t\t# An alternative is repo.lookup_reference(f\"refs/remotes/{remote_name}/{branch_name}\")\n\t\t\tif remote_branch_ref in repo.branches.remote:\n\t\t\t\treturn True\n\t\texcept (KeyError, Pygit2GitError):  # Should not happen with `in` check\n\t\t\tpass  # Not found remotely\n\n\treturn False\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.get_default_branch","title":"get_default_branch","text":"<pre><code>get_default_branch(\n\tpgu_instance: PRGitUtils | None = None,\n) -&gt; str\n</code></pre> <p>Get the default branch of the repository using pygit2.</p> <p>Parameters:</p> Name Type Description Default <code>pgu_instance</code> <code>PRGitUtils | None</code> <p>Optional instance of PRGitUtils.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Name of the default branch (e.g., \"main\", \"master\").</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_branch(pgu_instance: PRGitUtils | None = None) -&gt; str:\n\t\"\"\"\n\tGet the default branch of the repository using pygit2.\n\n\tArgs:\n\t    pgu_instance: Optional instance of PRGitUtils.\n\n\tReturns:\n\t    Name of the default branch (e.g., \"main\", \"master\").\n\t\"\"\"\n\tpgu = pgu_instance or PRGitUtils.get_instance()\n\trepo = pgu.repo\n\ttry:\n\t\tif not repo.head_is_detached:\n\t\t\t# Current HEAD is a symbolic ref to a branch, this is often the default\n\t\t\t# if on the default branch.\n\t\t\t# However, this returns the current branch, not necessarily default.\n\t\t\t# current_branch = repo.head.shorthand\n\t\t\t# if current_branch in [\"main\", \"master\"]: return current_branch\n\t\t\tpass  # Fall through to more robust checks for default\n\n\t\t# Try to get the symbolic-ref of refs/remotes/origin/HEAD\n\t\ttry:\n\t\t\torigin_head_ref = repo.lookup_reference(\"refs/remotes/origin/HEAD\")\n\t\t\tif origin_head_ref and origin_head_ref.type == ReferenceType.SYMBOLIC:\n\t\t\t\ttarget_as_val = origin_head_ref.target\n\t\t\t\t# Target is like 'refs/remotes/origin/main'\n\t\t\t\t# If type is SYMBOLIC, target should be str. Add isinstance to help linter.\n\t\t\t\tif isinstance(target_as_val, str) and target_as_val.startswith(\"refs/remotes/origin/\"):\n\t\t\t\t\treturn target_as_val[len(\"refs/remotes/origin/\") :]\n\t\texcept (KeyError, Pygit2GitError):\n\t\t\tpass  # origin/HEAD might not exist or not be symbolic\n\n\t\t# Fallback: check for common default branch names ('main', then 'master')\n\t\t# Check remote branches first as they are more indicative of shared default\n\t\tif \"origin/main\" in repo.branches.remote:\n\t\t\treturn \"main\"\n\t\tif \"origin/master\" in repo.branches.remote:\n\t\t\treturn \"master\"\n\t\t# Then check local branches\n\t\tif \"main\" in repo.branches.local:\n\t\t\treturn \"main\"\n\t\tif \"master\" in repo.branches.local:\n\t\t\treturn \"master\"\n\n\t\t# If still not found, and HEAD is not detached, use current branch as last resort.\n\t\tif not repo.head_is_detached:\n\t\t\treturn repo.head.shorthand\n\n\texcept (GitError, Pygit2GitError) as e:\n\t\tPRGitUtils.logger.warning(f\"Could not determine default branch via pygit2: {e}. Falling back to 'main'.\")\n\texcept Exception as e:  # noqa: BLE001\n\t\tPRGitUtils.logger.warning(f\"Unexpected error determining default branch: {e}. Falling back to 'main'.\")\n\n\treturn \"main\"  # Ultimate fallback\n</code></pre>"},{"location":"api/git/pr_generator/templates/","title":"Templates","text":"<p>PR template definitions for different workflow strategies.</p>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.DEFAULT_PR_TEMPLATE","title":"DEFAULT_PR_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_PR_TEMPLATE = {\n\t\"title\": \"{branch_type}: {description}\",\n\t\"description\": \"## Description\\n\\n{description}\\n\\n## Changes\\n\\n-\\n\\n## Related Issues\\n\\n-\\n\",\n}\n</code></pre>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.GITHUB_FLOW_PR_TEMPLATE","title":"GITHUB_FLOW_PR_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>GITHUB_FLOW_PR_TEMPLATE = {\n\t\"title\": \"{description}\",\n\t\"description\": \"## Description\\n\\n{description}\\n\\n## What does this PR do?\\n\\n&lt;!-- Please include a summary of the change and which issue is fixed. --&gt;\\n\\n## Changes\\n\\n-\\n\\n## Screenshots (if appropriate)\\n\\n## Testing completed\\n\\n- [ ] Unit tests\\n- [ ] Integration tests\\n- [ ] Manual testing\\n\\n## Related Issues\\n\\n&lt;!-- Please link to any related issues here --&gt;\\n\\n- Closes #\\n\",\n}\n</code></pre>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.TRUNK_BASED_PR_TEMPLATE","title":"TRUNK_BASED_PR_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>TRUNK_BASED_PR_TEMPLATE = {\n\t\"title\": \"{description}\",\n\t\"description\": \"## Change Description\\n\\n{description}\\n\\n## Implementation\\n\\n&lt;!-- Briefly describe implementation details --&gt;\\n\\n-\\n\\n## Test Plan\\n\\n&lt;!-- How was this tested? --&gt;\\n\\n- [ ] Unit tests added/updated\\n- [ ] Integration tested\\n\\n## Rollout Plan\\n\\n&lt;!-- How should this be deployed? --&gt;\\n\\n- [ ] Can be deployed immediately\\n- [ ] Requires feature flag\\n- [ ] Requires data migration\\n\\n## Related Issues\\n\\n- Fixes #\\n\",\n}\n</code></pre>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.GITFLOW_PR_TEMPLATES","title":"GITFLOW_PR_TEMPLATES  <code>module-attribute</code>","text":"<pre><code>GITFLOW_PR_TEMPLATES = {\n\t\"feature\": {\n\t\t\"title\": \"Feature: {description}\",\n\t\t\"description\": \"## Feature Description\\n\\n{description}\\n\\n## Implemented Changes\\n\\n-\\n\\n## Testing Performed\\n\\n- [ ] Unit tests\\n- [ ] Integration tests\\n- [ ] Manual testing\\n\\n## Related Issues\\n\\n- Closes #\\n\",\n\t},\n\t\"release\": {\n\t\t\"title\": \"Release {description}\",\n\t\t\"description\": \"## Release {description}\\n\\n### Features\\n\\n-\\n\\n### Bug Fixes\\n\\n-\\n\\n### Breaking Changes\\n\\n-\\n\\n## Deployment Notes\\n\\n-\\n\\n## Testing Required\\n\\n- [ ] Smoke tests\\n- [ ] Regression tests\\n- [ ] Performance tests\\n\",\n\t},\n\t\"hotfix\": {\n\t\t\"title\": \"Hotfix: {description}\",\n\t\t\"description\": \"## Hotfix: {description}\\n\\n### Issue Description\\n\\n&lt;!-- Describe the issue being fixed --&gt;\\n\\n### Fix Implementation\\n\\n&lt;!-- Describe how the issue was fixed --&gt;\\n\\n-\\n\\n### Testing Performed\\n\\n- [ ] Verified fix locally\\n- [ ] Added regression test\\n\\n### Impact Analysis\\n\\n- Affected components:\\n- Risk assessment:\\n\",\n\t},\n\t\"bugfix\": {\n\t\t\"title\": \"Fix: {description}\",\n\t\t\"description\": \"## Bug Fix\\n\\n### Issue Description\\n\\n{description}\\n\\n### Root Cause\\n\\n&lt;!-- What caused the bug? --&gt;\\n\\n### Fix Implementation\\n\\n-\\n\\n### Testing Performed\\n\\n- [ ] Added test case that reproduces the bug\\n- [ ] Verified fix locally\\n\\n### Related Issues\\n\\n- Fixes #\\n\",\n\t},\n}\n</code></pre>"},{"location":"api/git/pr_generator/utils/","title":"Utils","text":"<p>Utility functions for PR generation.</p>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.PRCreationError","title":"PRCreationError","text":"<p>               Bases: <code>GitError</code></p> <p>Error raised when there's an issue creating or updating a pull request.</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>class PRCreationError(GitError):\n\t\"\"\"Error raised when there's an issue creating or updating a pull request.\"\"\"\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.GH_TOKEN_PARTS_LEN","title":"GH_TOKEN_PARTS_LEN  <code>module-attribute</code>","text":"<pre><code>GH_TOKEN_PARTS_LEN = 2\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_token_from_gh_cli","title":"get_token_from_gh_cli","text":"<pre><code>get_token_from_gh_cli() -&gt; str | None\n</code></pre> <p>Try to get the GitHub token from the gh CLI.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The token string if found, else None.</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_token_from_gh_cli() -&gt; str | None:\n\t\"\"\"\n\tTry to get the GitHub token from the gh CLI.\n\n\tReturns:\n\t\tThe token string if found, else None.\n\t\"\"\"\n\ttry:\n\t\t# This subprocess call is safe: command and args are hardcoded, no user input\n\t\tresult = subprocess.run(  # noqa: S603\n\t\t\t[\"gh\", \"auth\", \"status\", \"--show-token\"],  # noqa: S607\n\t\t\tcapture_output=True,\n\t\t\ttext=True,\n\t\t\tcheck=True,\n\t\t)\n\t\t# Look for 'Token: ...' in output\n\t\tfor line in result.stdout.splitlines():\n\t\t\tif line.strip().startswith(\"- Token:\"):\n\t\t\t\t# Extract token\n\t\t\t\tparts = line.split(\":\", 1)\n\t\t\t\tif len(parts) == GH_TOKEN_PARTS_LEN:\n\t\t\t\t\ttoken = parts[1].strip()\n\t\t\t\t\tif token:\n\t\t\t\t\t\treturn token\n\t\treturn None\n\texcept (subprocess.CalledProcessError, FileNotFoundError):\n\t\treturn None\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_github_client","title":"get_github_client","text":"<pre><code>get_github_client(\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; tuple[Github, str]\n</code></pre> <p>Get a singleton Github client using the OAuth token from config.</p> <p>Returns:</p> Type Description <code>(Github, repo_full_name)</code> <p>Tuple of Github client and repo name</p> <p>Raises:         PRCreationError: If token is missing or repo cannot be determined</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_github_client(config_loader: ConfigLoader | None = None) -&gt; tuple[Github, str]:\n\t\"\"\"\n\tGet a singleton Github client using the OAuth token from config.\n\n\tReturns:\n\t\t(Github, repo_full_name): Tuple of Github client and repo name\n\tRaises:\n\t\tPRCreationError: If token is missing or repo cannot be determined\n\t\"\"\"\n\tglobal _github_client, _github_repo  # noqa: PLW0603\n\tif _github_client is not None and _github_repo is not None:\n\t\treturn _github_client, _github_repo\n\n\tconfig_loader = config_loader or ConfigLoader.get_instance()\n\tconfig = config_loader.get.github\n\ttoken = config.token\n\trepo_name = config.repo\n\tif not token:\n\t\t# Try to get from gh CLI\n\t\ttoken = get_token_from_gh_cli()\n\t\tif token:\n\t\t\t# Save to .env.local for future use\n\t\t\ttry:\n\t\t\t\tset_key(str(Path(\".env.local\")), \"GITHUB_TOKEN\", token)\n\t\t\t\tlogger.info(\"Saved GitHub token from gh CLI to .env.local\")\n\t\t\t# Only catch expected errors from set_key\n\t\t\texcept (OSError, ValueError) as e:\n\t\t\t\tlogger.warning(f\"Could not save GitHub token to .env.local: {e}\")\n\t\telse:\n\t\t\tlogger.error(\"GitHub OAuth token not set in config (github.token), env, or gh CLI.\")\n\t\t\tmsg = (\n\t\t\t\t\"GitHub OAuth token not set in config (github.token), env, or gh CLI. \"\n\t\t\t\t\"Please run 'gh auth login' or set GITHUB_TOKEN in your .env/.env.local.\"\n\t\t\t)\n\t\t\traise PRCreationError(msg)\n\tauth = Auth.Token(token)\n\t_github_client = Github(auth=auth)\n\tif not repo_name:\n\t\t# Try to infer from git remote\n\t\ttry:\n\t\t\tpr_git_utils = PRGitUtils.get_instance()\n\t\t\turl = pr_git_utils.repo.remotes[\"origin\"].url\n\t\t\t# Ensure url is a string\n\t\t\tif not isinstance(url, str) or not url:\n\t\t\t\tmsg = f\"Could not parse GitHub repo from remote URL: {url}\"\n\t\t\t\traise PRCreationError(msg)\n\t\t\t# Parse repo name from URL (supports git@github.com:user/repo.git and https)\n\t\t\tm = re.search(r\"github.com[:/](.+?)(?:\\\\.git)?$\", url)\n\t\t\tif m:\n\t\t\t\trepo_name = m.group(1)\n\t\t\t\t# Remove .git suffix if present\n\t\t\t\trepo_name = repo_name.removesuffix(\".git\")\n\t\t\telse:\n\t\t\t\tmsg = f\"Could not parse GitHub repo from remote URL: {url}\"\n\t\t\t\traise PRCreationError(msg)\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Could not determine GitHub repo from git remote\")\n\t\t\tmsg = \"Could not determine GitHub repo from git remote\"\n\t\t\traise PRCreationError(msg) from e\n\t_github_repo = repo_name\n\treturn _github_client, _github_repo\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_title_from_commits","title":"generate_pr_title_from_commits","text":"<pre><code>generate_pr_title_from_commits(commits: list[str]) -&gt; str\n</code></pre> <p>Generate a PR title from commit messages.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated PR title</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_title_from_commits(commits: list[str]) -&gt; str:\n\t\"\"\"\n\tGenerate a PR title from commit messages.\n\n\tArgs:\n\t    commits: List of commit messages\n\n\tReturns:\n\t    Generated PR title\n\n\t\"\"\"\n\tif not commits:\n\t\treturn \"Update branch\"\n\n\t# Use the first commit to determine the PR type\n\tfirst_commit = commits[0]\n\n\t# Define mapping from commit prefixes to PR title prefixes\n\tprefix_mapping = {\"feat\": \"Feature:\", \"fix\": \"Fix:\", \"docs\": \"Docs:\", \"refactor\": \"Refactor:\", \"perf\": \"Optimize:\"}\n\n\t# Extract commit type from first commit\n\tmatch = re.match(r\"^([a-z]+)(\\([^)]+\\))?:\", first_commit)\n\tif match:\n\t\tprefix = match.group(1)\n\t\ttitle_prefix = prefix_mapping.get(prefix, \"Update:\")\n\n\t\t# Strip the prefix and use as title\n\t\ttitle = re.sub(r\"^[a-z]+(\\([^)]+\\))?:\\s*\", \"\", first_commit)\n\t\t# Capitalize first letter and add PR type prefix\n\t\treturn f\"{title_prefix} {title[0].upper() + title[1:]}\"\n\n\t# Fallback if no conventional commit format found\n\treturn first_commit\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_title_with_llm","title":"generate_pr_title_with_llm","text":"<pre><code>generate_pr_title_with_llm(\n\tcommits: list[str], llm_client: LLMClient\n) -&gt; str\n</code></pre> <p>Generate a PR title using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <code>llm_client</code> <code>LLMClient</code> <p>LLMClient instance</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated PR title</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_title_with_llm(\n\tcommits: list[str],\n\tllm_client: LLMClient,\n) -&gt; str:\n\t\"\"\"\n\tGenerate a PR title using an LLM.\n\n\tArgs:\n\t    commits: List of commit messages\n\t    llm_client: LLMClient instance\n\n\tReturns:\n\t    Generated PR title\n\n\t\"\"\"\n\tif not commits:\n\t\treturn \"Update branch\"\n\n\ttry:\n\t\t# Format commit messages and prepare prompt\n\t\tcommit_list = format_commits_for_prompt(commits)\n\t\tprompt = PR_TITLE_PROMPT.format(commit_list=commit_list)\n\n\t\treturn llm_client.completion(\n\t\t\tmessages=[\n\t\t\t\t{\"role\": \"system\", \"content\": PR_SYSTEM_PROMPT},\n\t\t\t\t{\"role\": \"user\", \"content\": prompt},\n\t\t\t],\n\t\t)\n\n\texcept (ValueError, RuntimeError, ConnectionError) as e:\n\t\tlogger.warning(\"Failed to generate PR title with LLM: %s\", str(e))\n\t\t# Fallback to rule-based approach\n\t\treturn generate_pr_title_from_commits(commits)\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_description_from_commits","title":"generate_pr_description_from_commits","text":"<pre><code>generate_pr_description_from_commits(\n\tcommits: list[str],\n) -&gt; str\n</code></pre> <p>Generate a PR description from commit messages.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated PR description</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_description_from_commits(commits: list[str]) -&gt; str:\n\t\"\"\"\n\tGenerate a PR description from commit messages.\n\n\tArgs:\n\t    commits: List of commit messages\n\n\tReturns:\n\t    Generated PR description\n\n\t\"\"\"\n\tif not commits:\n\t\treturn \"No changes\"\n\n\t# Group commits by type\n\tfeatures = []\n\tfixes = []\n\tdocs = []\n\trefactors = []\n\toptimizations = []\n\tother = []\n\n\tfor commit in commits:\n\t\tif commit.startswith(\"feat\"):\n\t\t\tfeatures.append(commit)\n\t\telif commit.startswith(\"fix\"):\n\t\t\tfixes.append(commit)\n\t\telif commit.startswith(\"docs\"):\n\t\t\tdocs.append(commit)\n\t\telif commit.startswith(\"refactor\"):\n\t\t\trefactors.append(commit)\n\t\telif commit.startswith(\"perf\"):\n\t\t\toptimizations.append(commit)\n\t\telse:\n\t\t\tother.append(commit)\n\n\t# Determine PR type checkboxes\n\thas_refactor = bool(refactors)\n\thas_feature = bool(features)\n\thas_bug_fix = bool(fixes)\n\thas_optimization = bool(optimizations)\n\thas_docs_update = bool(docs)\n\n\t# Build description\n\tdescription = \"## What type of PR is this? (check all applicable)\\n\\n\"\n\tdescription += f\"- [{' ' if not has_refactor else 'x'}] Refactor\\n\"\n\tdescription += f\"- [{' ' if not has_feature else 'x'}] Feature\\n\"\n\tdescription += f\"- [{' ' if not has_bug_fix else 'x'}] Bug Fix\\n\"\n\tdescription += f\"- [{' ' if not has_optimization else 'x'}] Optimization\\n\"\n\tdescription += f\"- [{' ' if not has_docs_update else 'x'}] Documentation Update\\n\\n\"\n\n\tdescription += \"## Description\\n\\n\"\n\n\t# Add categorized changes to description\n\tif features:\n\t\tdescription += \"### Features\\n\\n\"\n\t\tfor feat in features:\n\t\t\t# Remove the prefix and format as a list item\n\t\t\tclean_msg = re.sub(r\"^feat(\\([^)]+\\))?:\\s*\", \"\", feat)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif fixes:\n\t\tdescription += \"### Fixes\\n\\n\"\n\t\tfor fix in fixes:\n\t\t\tclean_msg = re.sub(r\"^fix(\\([^)]+\\))?:\\s*\", \"\", fix)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif docs:\n\t\tdescription += \"### Documentation\\n\\n\"\n\t\tfor doc in docs:\n\t\t\tclean_msg = re.sub(r\"^docs(\\([^)]+\\))?:\\s*\", \"\", doc)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif refactors:\n\t\tdescription += \"### Refactors\\n\\n\"\n\t\tfor refactor in refactors:\n\t\t\tclean_msg = re.sub(r\"^refactor(\\([^)]+\\))?:\\s*\", \"\", refactor)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif optimizations:\n\t\tdescription += \"### Optimizations\\n\\n\"\n\t\tfor perf in optimizations:\n\t\t\tclean_msg = re.sub(r\"^perf(\\([^)]+\\))?:\\s*\", \"\", perf)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif other:\n\t\tdescription += \"### Other\\n\\n\"\n\t\tfor msg in other:\n\t\t\t# Try to clean up conventional commit prefixes\n\t\t\tclean_msg = re.sub(r\"^(style|test|build|ci|chore|revert)(\\([^)]+\\))?:\\s*\", \"\", msg)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tdescription += \"## Related Tickets &amp; Documents\\n\\n\"\n\tdescription += \"- Related Issue #\\n\"\n\tdescription += \"- Closes #\\n\\n\"\n\n\tdescription += \"## Added/updated tests?\\n\\n\"\n\tdescription += \"- [ ] Yes\\n\"\n\tdescription += (\n\t\t\"- [ ] No, and this is why: _please replace this line with details on why tests have not been included_\\n\"\n\t)\n\tdescription += \"- [ ] I need help with writing tests\\n\"\n\n\treturn description\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_description_with_llm","title":"generate_pr_description_with_llm","text":"<pre><code>generate_pr_description_with_llm(\n\tcommits: list[str], llm_client: LLMClient\n) -&gt; str\n</code></pre> <p>Generate a PR description using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <code>llm_client</code> <code>LLMClient</code> <p>LLMClient instance</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated PR description</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_description_with_llm(\n\tcommits: list[str],\n\tllm_client: LLMClient,\n) -&gt; str:\n\t\"\"\"\n\tGenerate a PR description using an LLM.\n\n\tArgs:\n\t    commits: List of commit messages\n\t    llm_client: LLMClient instance\n\n\tReturns:\n\t    Generated PR description\n\n\t\"\"\"\n\tif not commits:\n\t\treturn \"No changes\"\n\n\ttry:\n\t\t# Format commit messages and prepare prompt\n\t\tcommit_list = format_commits_for_prompt(commits)\n\t\tprompt = PR_DESCRIPTION_PROMPT.format(commit_list=commit_list)\n\n\t\treturn llm_client.completion(\n\t\t\tmessages=[\n\t\t\t\t{\"role\": \"system\", \"content\": PR_SYSTEM_PROMPT},\n\t\t\t\t{\"role\": \"user\", \"content\": prompt},\n\t\t\t],\n\t\t)\n\n\texcept (ValueError, RuntimeError, ConnectionError) as e:\n\t\tlogger.warning(\"Failed to generate PR description with LLM: %s\", str(e))\n\t\t# Fallback to rule-based approach\n\t\treturn generate_pr_description_from_commits(commits)\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.create_pull_request","title":"create_pull_request","text":"<pre><code>create_pull_request(\n\tbase_branch: str,\n\thead_branch: str,\n\ttitle: str,\n\tdescription: str,\n) -&gt; PullRequest\n</code></pre> <p>Create a pull request on GitHub using PyGithub.</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def create_pull_request(base_branch: str, head_branch: str, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"Create a pull request on GitHub using PyGithub.\"\"\"\n\ttry:\n\t\tgh, repo_name = get_github_client()\n\t\trepo = gh.get_repo(repo_name)\n\t\tpr = repo.create_pull(\n\t\t\tbase=base_branch,\n\t\t\thead=head_branch,\n\t\t\ttitle=title,\n\t\t\tbody=description,\n\t\t)\n\t\treturn PullRequest(\n\t\t\tbranch=head_branch,\n\t\t\ttitle=title,\n\t\t\tdescription=description,\n\t\t\turl=pr.html_url,\n\t\t\tnumber=pr.number,\n\t\t)\n\texcept GithubException as e:\n\t\tlogger.exception(\"GitHub API error during PR creation:\")\n\t\tmsg = f\"Failed to create PR: {e.data.get('message', str(e)) if hasattr(e, 'data') else str(e)}\"\n\t\traise PRCreationError(msg) from e\n\texcept Exception as e:\n\t\tlogger.exception(\"Error creating PR via PyGithub:\")\n\t\tmsg = f\"Error during PR creation: {e}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.update_pull_request","title":"update_pull_request","text":"<pre><code>update_pull_request(\n\tpr_number: int | None, title: str, description: str\n) -&gt; PullRequest\n</code></pre> <p>Update an existing pull request using PyGithub.</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def update_pull_request(pr_number: int | None, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"Update an existing pull request using PyGithub.\"\"\"\n\tif pr_number is None:\n\t\tmsg = \"PR number cannot be None\"\n\t\traise PRCreationError(msg)\n\ttry:\n\t\tgh, repo_name = get_github_client()\n\t\trepo = gh.get_repo(repo_name)\n\t\tpr = repo.get_pull(pr_number)\n\t\tpr.edit(title=title, body=description)\n\t\t# Get current branch name\n\t\tpr_git_utils = PRGitUtils.get_instance()\n\t\tbranch = pr_git_utils.get_current_branch()\n\t\treturn PullRequest(\n\t\t\tbranch=branch,\n\t\t\ttitle=title,\n\t\t\tdescription=description,\n\t\t\turl=pr.html_url,\n\t\t\tnumber=pr.number,\n\t\t)\n\texcept GithubException as e:\n\t\tlogger.exception(\"GitHub API error during PR update:\")\n\t\tmsg = f\"Failed to update PR: {e.data.get('message', str(e)) if hasattr(e, 'data') else str(e)}\"\n\t\traise PRCreationError(msg) from e\n\texcept Exception as e:\n\t\tlogger.exception(\"Error updating PR via PyGithub:\")\n\t\tmsg = f\"Error during PR update: {e}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_existing_pr","title":"get_existing_pr","text":"<pre><code>get_existing_pr(branch_name: str) -&gt; PullRequest | None\n</code></pre> <p>Get an existing PR for a branch using PyGithub.</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_existing_pr(branch_name: str) -&gt; PullRequest | None:\n\t\"\"\"Get an existing PR for a branch using PyGithub.\"\"\"\n\ttry:\n\t\tif not branch_name:\n\t\t\tlogger.debug(\"Branch name is None, cannot get existing PR.\")\n\t\t\treturn None\n\t\tgh, repo_name = get_github_client()\n\t\trepo = gh.get_repo(repo_name)\n\t\ttry:\n\t\t\tpulls = repo.get_pulls(state=\"open\", head=f\"{repo.owner.login}:{branch_name}\")\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tlogger.warning(f\"Error getting PRs from GitHub API: {e}\")\n\t\t\treturn None\n\t\tfor pr in pulls:\n\t\t\t# Return the first matching PR\n\t\t\treturn PullRequest(\n\t\t\t\tbranch=branch_name,\n\t\t\t\ttitle=pr.title,\n\t\t\t\tdescription=pr.body,\n\t\t\t\turl=pr.html_url,\n\t\t\t\tnumber=pr.number,\n\t\t\t)\n\t\treturn None\n\texcept GithubException as e:\n\t\tlogger.warning(f\"GitHub API error during get_existing_pr: {e}\")\n\t\treturn None\n\texcept (ValueError, RuntimeError, ConnectionError, TypeError) as e:\n\t\tlogger.warning(f\"Error getting existing PR via PyGithub: {e}\")\n\t\treturn None\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_content_from_template","title":"generate_pr_content_from_template","text":"<pre><code>generate_pr_content_from_template(\n\tbranch_name: str,\n\tdescription: str,\n\tstrategy_name: str = \"github-flow\",\n) -&gt; PRContent\n</code></pre> <p>Generate PR title and description using templates from the selected workflow strategy.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <code>description</code> <code>str</code> <p>Short description of the changes</p> required <code>strategy_name</code> <code>str</code> <p>Name of the workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>PRContent</code> <p>Dictionary with 'title' and 'description' fields</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_content_from_template(\n\tbranch_name: str,\n\tdescription: str,\n\tstrategy_name: str = \"github-flow\",\n) -&gt; PRContent:\n\t\"\"\"\n\tGenerate PR title and description using templates from the selected workflow strategy.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\t    description: Short description of the changes\n\t    strategy_name: Name of the workflow strategy to use\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' fields\n\n\t\"\"\"\n\t# Create the strategy\n\tstrategy = create_strategy(strategy_name)\n\n\t# Detect branch type from branch name\n\tbranch_type = strategy.detect_branch_type(branch_name) or \"feature\"\n\n\t# Get templates for this branch type\n\ttemplates = strategy.get_pr_templates(branch_type)\n\n\t# Format templates with description\n\ttitle = templates[\"title\"].format(description=description, branch_type=branch_type)\n\n\tdescription_text = templates[\"description\"].format(\n\t\tdescription=description, branch_type=branch_type, branch_name=branch_name\n\t)\n\n\treturn {\"title\": title, \"description\": description_text}\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_timestamp","title":"get_timestamp","text":"<pre><code>get_timestamp() -&gt; str\n</code></pre> <p>Get a timestamp string for branch names.</p> <p>Returns:</p> Type Description <code>str</code> <p>Timestamp string in YYYYMMDD-HHMMSS format</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_timestamp() -&gt; str:\n\t\"\"\"\n\tGet a timestamp string for branch names.\n\n\tReturns:\n\t    Timestamp string in YYYYMMDD-HHMMSS format\n\n\t\"\"\"\n\tnow = datetime.now(UTC)\n\treturn now.strftime(\"%Y%m%d-%H%M%S\")\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(message: str, workflow: str) -&gt; str\n</code></pre> <p>Suggest a branch name based on a commit message and workflow.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Commit message or description</p> required <code>workflow</code> <code>str</code> <p>Git workflow strategy to use</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def suggest_branch_name(message: str, workflow: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on a commit message and workflow.\n\n\tArgs:\n\t    message: Commit message or description\n\t    workflow: Git workflow strategy to use\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\t# For testing specific test cases\n\tif message.startswith(\"feat(api): Add new endpoint\"):\n\t\tif workflow in {\"github-flow\", \"gitflow\"}:\n\t\t\treturn \"feature/api-endpoint\"\n\t\tif workflow == \"trunk-based\":\n\t\t\treturn \"user/api-endpoint\"\n\n\t# Process typical commit messages\n\tif message == \"Update documentation and fix typos\":\n\t\tif workflow in {\"github-flow\", \"gitflow\"}:\n\t\t\treturn \"docs/update-fix-typos\"\n\t\tif workflow == \"trunk-based\":\n\t\t\treturn \"user/update-docs\"\n\n\t# Determine branch type\n\tbranch_type = \"feature\"  # Default branch type\n\n\t# Identify branch type from commit message\n\tif re.search(r\"^\\s*fix|bug|hotfix\", message, re.IGNORECASE):\n\t\tbranch_type = \"bugfix\" if workflow == \"github-flow\" else \"hotfix\"\n\telif re.search(r\"^\\s*doc|docs\", message, re.IGNORECASE):\n\t\tbranch_type = \"docs\"\n\telif re.search(r\"^\\s*feat|feature\", message, re.IGNORECASE):\n\t\tbranch_type = \"feature\"\n\telif re.search(r\"^\\s*release\", message, re.IGNORECASE):\n\t\tbranch_type = \"release\"\n\n\t# Create workflow strategy\n\tworkflow_type = cast(\"str\", workflow)\n\tstrategy = create_strategy(workflow_type)\n\n\t# Clean up description for branch name\n\tcleaned_message = re.sub(\n\t\tr\"^\\s*(?:fix|bug|hotfix|feat|feature|doc|docs|release).*?:\\s*\", \"\", message, flags=re.IGNORECASE\n\t)\n\tcleaned_message = re.sub(r\"[^\\w\\s-]\", \"\", cleaned_message)\n\n\t# Generate branch name based on workflow strategy\n\tsuggested_name = strategy.suggest_branch_name(branch_type, cleaned_message)\n\n\t# Add timestamp if needed (for release branches)\n\tif branch_type == \"release\" and not re.search(r\"\\d+\\.\\d+\\.\\d+\", suggested_name):\n\t\tsuggested_name = f\"{suggested_name}-{get_timestamp()}\"\n\n\treturn suggested_name\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_branch_description","title":"get_branch_description","text":"<pre><code>get_branch_description(branch_name: str) -&gt; str\n</code></pre> <p>Generate a description for a branch based on its commits.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the branch</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_branch_description(branch_name: str) -&gt; str:\n\t\"\"\"\n\tGenerate a description for a branch based on its commits.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\n\tReturns:\n\t    Description of the branch\n\n\t\"\"\"\n\ttry:\n\t\t# Get base branch\n\t\tbase_branch = get_default_branch()  # This is a helper from .strategies\n\n\t\t# Instantiate PRGitUtils to use the new pygit2-based get_commit_messages\n\t\t# This assumes the CWD is within a git repo.\n\t\t# A better approach would be to pass an instance of PRGitUtils or relevant context.\n\t\tgit_utils_instance = PRGitUtils()  # This will initialize ExtendedGitRepoContext\n\t\tcommits = git_utils_instance.get_commit_messages(base_branch, branch_name)\n\n\t\tif not commits:\n\t\t\treturn \"No unique commits found on this branch.\"\n\n\t\t# Return first few commits as description\n\t\tif len(commits) &lt;= MAX_COMMIT_PREVIEW:\n\t\t\treturn \"\\n\".join([f\"- {commit}\" for commit in commits])\n\n\t\tsummary = \"\\n\".join([f\"- {commit}\" for commit in commits[:MAX_COMMIT_PREVIEW]])\n\t\treturn f\"{summary}\\n- ... and {len(commits) - MAX_COMMIT_PREVIEW} more commits\"\n\texcept GitError:\n\t\treturn \"Unable to get branch description.\"\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.detect_branch_type","title":"detect_branch_type","text":"<pre><code>detect_branch_type(\n\tbranch_name: str, strategy_name: str = \"github-flow\"\n) -&gt; str\n</code></pre> <p>Detect the type of a branch based on its name and workflow strategy.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <code>strategy_name</code> <code>str</code> <p>Name of the workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>str</code> <p>Branch type or \"feature\" if not detected</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def detect_branch_type(branch_name: str, strategy_name: str = \"github-flow\") -&gt; str:\n\t\"\"\"\n\tDetect the type of a branch based on its name and workflow strategy.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\t    strategy_name: Name of the workflow strategy to use\n\n\tReturns:\n\t    Branch type or \"feature\" if not detected\n\n\t\"\"\"\n\tstrategy = create_strategy(strategy_name)\n\t# Handle None branch_name\n\tif not branch_name:\n\t\treturn \"feature\"  # Default if branch name is None\n\tbranch_type = strategy.detect_branch_type(branch_name)\n\n\treturn branch_type or \"feature\"  # Default to feature if not detected\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.list_branches","title":"list_branches","text":"<pre><code>list_branches() -&gt; list[str]\n</code></pre> <p>Get a list of all branches (local and remote).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of branch names</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def list_branches() -&gt; list[str]:\n\t\"\"\"\n\tGet a list of all branches (local and remote).\n\n\tReturns:\n\t        List of branch names\n\t\"\"\"\n\ttry:\n\t\tgit_utils_instance = PRGitUtils()\n\t\tlocal_branches = list(git_utils_instance.repo.branches.local)\n\t\tremote_branches_full_refs = list(git_utils_instance.repo.branches.remote)\n\t\tremote_branches = []\n\t\tfor ref_name in remote_branches_full_refs:\n\t\t\t# Example ref_name: \"origin/main\", \"origin/HEAD\"\n\t\t\tif not ref_name.endswith(\"/HEAD\"):  # Exclude remote HEAD pointers\n\t\t\t\t# Strip the remote name prefix, e.g., \"origin/\"\n\t\t\t\tparts = ref_name.split(\"/\", 1)\n\t\t\t\tif len(parts) &gt; 1:\n\t\t\t\t\tremote_branches.append(parts[1])\n\t\t\t\telse:  # Should not happen for valid remote branch refs like \"origin/branch\"\n\t\t\t\t\tremote_branches.append(ref_name)\n\n\t\t# Combine and remove duplicates\n\t\treturn list(set(local_branches + remote_branches))\n\texcept (GitError, Pygit2GitError) as e:\n\t\tlogger.debug(f\"Error listing branches using pygit2: {e}\")\n\t\treturn []\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.validate_branch_name","title":"validate_branch_name","text":"<pre><code>validate_branch_name(branch_name: str | None) -&gt; bool\n</code></pre> <p>Validate a branch name.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str | None</code> <p>Branch name to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid, False otherwise</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def validate_branch_name(branch_name: str | None) -&gt; bool:\n\t\"\"\"\n\tValidate a branch name.\n\n\tArgs:\n\t    branch_name: Branch name to validate\n\n\tReturns:\n\t    True if valid, False otherwise\n\n\t\"\"\"\n\t# Check if branch name is valid\n\tif not branch_name or not re.match(r\"^[a-zA-Z0-9_.-]+$\", branch_name):\n\t\t# Log error instead of showing directly, as this is now a util function\n\t\tlogger.error(\n\t\t\t\"Invalid branch name '%s'. Use only letters, numbers, underscores, dots, and hyphens.\", branch_name\n\t\t)\n\t\treturn False\n\treturn True\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_all_open_prs","title":"get_all_open_prs","text":"<pre><code>get_all_open_prs() -&gt; list[PullRequest]\n</code></pre> <p>Fetch all open pull requests for the current repository.</p> <p>Returns:</p> Type Description <code>list[PullRequest]</code> <p>List of PullRequest objects for all open PRs.</p> <p>Raises:</p> Type Description <code>PRCreationError</code> <p>If repo_name is invalid or repo cannot be found.</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_all_open_prs() -&gt; list[PullRequest]:\n\t\"\"\"\n\tFetch all open pull requests for the current repository.\n\n\tReturns:\n\t\tList of PullRequest objects for all open PRs.\n\n\tRaises:\n\t\tPRCreationError: If repo_name is invalid or repo cannot be found.\n\t\"\"\"\n\tgh, repo_name = get_github_client()\n\tif not repo_name or \"/\" not in repo_name:\n\t\tlogger.error(f\"Invalid repo_name for GitHub API: {repo_name}\")\n\t\tmsg = f\"Invalid repo_name for GitHub API: {repo_name}\"\n\t\traise PRCreationError(msg)\n\ttry:\n\t\trepo = gh.get_repo(repo_name)\n\texcept Exception as e:\n\t\tlogger.exception(f\"Could not fetch repo '{repo_name}' from GitHub.\")\n\t\tmsg = f\"Could not fetch repo '{repo_name}' from GitHub: {e}\"\n\t\traise PRCreationError(msg) from e\n\treturn [\n\t\tPullRequest(\n\t\t\tbranch=pr.head.ref,\n\t\t\ttitle=pr.title,\n\t\t\tdescription=pr.body,\n\t\t\turl=pr.html_url,\n\t\t\tnumber=pr.number,\n\t\t)\n\t\tfor pr in repo.get_pulls(state=\"open\")\n\t]\n</code></pre>"},{"location":"api/git/semantic_grouping/","title":"Semantic Grouping Overview","text":"<p>Semantic grouping implementation for the CodeMap project.</p> <ul> <li>Clusterer - Module for clustering diff chunks based on their embeddings.</li> <li>Context Processor - Context processing utilities for LLM prompts.</li> <li>Embedder - Module for generating embeddings from diff chunks.</li> <li>Group - Module for semantic grouping of diff chunks.</li> <li>Resolver - Module for resolving file integrity constraints in semantic groups.</li> </ul>"},{"location":"api/git/semantic_grouping/clusterer/","title":"Clusterer","text":"<p>Module for clustering diff chunks based on their embeddings.</p> <p>This module provides functionality to group related code changes together based on their semantic similarity, using vector embeddings and clustering algorithms. The clustering process helps identify related changes that should be committed together.</p> <p>Key components: - DiffClusterer: Main class that implements clustering algorithms for diff chunks - ClusteringParams: Type definition for parameters used by clustering algorithms</p> <p>The module supports multiple clustering methods: 1. Agglomerative (hierarchical) clustering: Builds a hierarchy of clusters based on distances    between embeddings, using a distance threshold to determine final cluster boundaries 2. DBSCAN: Density-based clustering that groups points in high-density regions,    treating low-density points as noise/outliers</p>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams","title":"ClusteringParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for clustering algorithm parameters.</p> <p>These parameters configure the behavior of the clustering algorithms:</p> <p>For agglomerative clustering: - n_clusters: Optional limit on number of clusters (None means no limit) - distance_threshold: Maximum distance for clusters to be merged (lower = more clusters) - metric: Distance metric to use (e.g., \"precomputed\" for precomputed distance matrix) - linkage: Strategy for calculating distances between clusters (\"average\", \"single\", etc.)</p> <p>For DBSCAN: - eps: Maximum distance between points in the same neighborhood - min_samples: Minimum points required to form a dense region - metric: Distance metric to use</p> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>class ClusteringParams(TypedDict, total=False):\n\t\"\"\"\n\tType definition for clustering algorithm parameters.\n\n\tThese parameters configure the behavior of the clustering algorithms:\n\n\tFor agglomerative clustering:\n\t- n_clusters: Optional limit on number of clusters (None means no limit)\n\t- distance_threshold: Maximum distance for clusters to be merged (lower = more clusters)\n\t- metric: Distance metric to use (e.g., \"precomputed\" for precomputed distance matrix)\n\t- linkage: Strategy for calculating distances between clusters (\"average\", \"single\", etc.)\n\n\tFor DBSCAN:\n\t- eps: Maximum distance between points in the same neighborhood\n\t- min_samples: Minimum points required to form a dense region\n\t- metric: Distance metric to use\n\n\t\"\"\"\n\n\tn_clusters: int | None\n\tdistance_threshold: float | None\n\tmetric: str\n\tlinkage: str\n\teps: float\n\tmin_samples: int\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.n_clusters","title":"n_clusters  <code>instance-attribute</code>","text":"<pre><code>n_clusters: int | None\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.distance_threshold","title":"distance_threshold  <code>instance-attribute</code>","text":"<pre><code>distance_threshold: float | None\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.metric","title":"metric  <code>instance-attribute</code>","text":"<pre><code>metric: str\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.linkage","title":"linkage  <code>instance-attribute</code>","text":"<pre><code>linkage: str\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps: float\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.min_samples","title":"min_samples  <code>instance-attribute</code>","text":"<pre><code>min_samples: int\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer","title":"DiffClusterer","text":"<p>Clusters diff chunks based on their semantic embeddings.</p> <p>This class provides methods to group related code changes by their semantic similarity, using vector embeddings and standard clustering algorithms from scikit-learn.</p> <p>Clustering helps identify code changes that are related to each other and should be grouped in the same commit, even if they appear in different files.</p> <p>The class supports multiple clustering algorithms: 1. Agglomerative clustering: Hierarchical clustering that's good for finding natural    groupings without needing to specify the exact number of clusters 2. DBSCAN: Density-based clustering that can identify outliers and works well with    irregularly shaped clusters</p> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>class DiffClusterer:\n\t\"\"\"\n\tClusters diff chunks based on their semantic embeddings.\n\n\tThis class provides methods to group related code changes by their semantic similarity,\n\tusing vector embeddings and standard clustering algorithms from scikit-learn.\n\n\tClustering helps identify code changes that are related to each other and should be\n\tgrouped in the same commit, even if they appear in different files.\n\n\tThe class supports multiple clustering algorithms:\n\t1. Agglomerative clustering: Hierarchical clustering that's good for finding natural\n\t   groupings without needing to specify the exact number of clusters\n\t2. DBSCAN: Density-based clustering that can identify outliers and works well with\n\t   irregularly shaped clusters\n\n\t\"\"\"\n\n\tdef __init__(self, config_loader: \"ConfigLoader\", **kwargs: object) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the clusterer.\n\n\t\tArgs:\n\t\t    config_loader: ConfigLoader to use for configuration (follows DI pattern)\n\t\t    **kwargs: Additional parameters for the clustering algorithm:\n\t\t        - For agglomerative: distance_threshold, linkage, etc.\n\t\t        - For DBSCAN: eps, min_samples, etc.\n\n\t\tRaises:\n\t\t    ImportError: If scikit-learn is not installed\n\n\t\t\"\"\"\n\t\tself.config = config_loader.get.embedding.clustering\n\t\tself.method = self.config.method\n\t\tself.kwargs = kwargs\n\n\t\t# Import here to avoid making sklearn a hard dependency\n\t\ttry:\n\t\t\tfrom sklearn.cluster import DBSCAN, AgglomerativeClustering\n\t\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\t\tself.AgglomerativeClustering = AgglomerativeClustering\n\t\t\tself.DBSCAN = DBSCAN\n\t\t\tself.cosine_similarity = cosine_similarity\n\t\texcept ImportError as e:\n\t\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\t\tmsg = \"scikit-learn is required for clustering\"\n\t\t\traise ImportError(msg) from e\n\n\tdef cluster(self, chunk_embeddings: list[tuple[DiffChunk, np.ndarray]]) -&gt; list[list[DiffChunk]]:\n\t\t\"\"\"\n\t\tCluster chunks based on their embeddings.\n\n\t\t              Process:\n\t\t              1. Extracts chunks and embeddings from input tuples\n\t\t              2. Computes a similarity matrix using cosine similarity\n\t\t              3. Converts similarity to distance matrix (1 - similarity)\n\t\t              4. Applies clustering algorithm based on the chosen method\n\t\t              5. Organizes chunks into clusters based on labels\n\t\t              6. Handles special cases like noise points in DBSCAN\n\n\t\tArgs:\n\t\t    chunk_embeddings: List of (chunk, embedding) tuples where each embedding\n\t\t        is a numpy array representing the semantic vector of a code chunk\n\n\t\tReturns:\n\t\t    List of lists, where each inner list contains chunks in the same cluster.\n\t\t    With DBSCAN, noise points (label -1) are returned as individual single-item clusters.\n\n\t\tExamples:\n\t\t    &gt;&gt;&gt; embedder = DiffEmbedder()\n\t\t    &gt;&gt;&gt; chunk_embeddings = embedder.embed_chunks(diff_chunks)\n\t\t    &gt;&gt;&gt; clusterer = DiffClusterer(method=\"agglomerative\", distance_threshold=0.5)\n\t\t    &gt;&gt;&gt; clusters = clusterer.cluster(chunk_embeddings)\n\t\t    &gt;&gt;&gt; for i, cluster in enumerate(clusters):\n\t\t    ...     print(f\"Cluster {i} has {len(cluster)} chunks\")\n\n\t\t\"\"\"\n\t\tif not chunk_embeddings:\n\t\t\treturn []\n\n\t\t# Extract chunks and embeddings\n\t\tchunks = [ce[0] for ce in chunk_embeddings]\n\t\tembeddings = np.array([ce[1] for ce in chunk_embeddings])\n\n\t\t# Compute similarity matrix (1 - cosine distance)\n\t\tsimilarity_matrix = self.cosine_similarity(embeddings)\n\n\t\t# Convert to distance matrix (1 - similarity)\n\t\tdistance_matrix = 1 - similarity_matrix\n\n\t\t# Apply clustering\n\t\tif self.method == \"agglomerative\":\n\t\t\t# Default parameters if not provided\n\t\t\tparams = {\n\t\t\t\t\"n_clusters\": None,\n\t\t\t\t\"distance_threshold\": self.config.agglomerative.distance_threshold,\n\t\t\t\t\"metric\": self.config.agglomerative.metric,\n\t\t\t\t\"linkage\": self.config.agglomerative.linkage,\n\t\t\t}\n\t\t\tparams.update(cast(\"dict[str, float | str | None]\", self.kwargs))\n\n\t\t\tclustering = self.AgglomerativeClustering(**params)\n\t\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\t\telif self.method == \"dbscan\":\n\t\t\t# Default parameters if not provided\n\t\t\tparams = {\n\t\t\t\t\"eps\": self.config.dbscan.eps,\n\t\t\t\t\"min_samples\": self.config.dbscan.min_samples,\n\t\t\t\t\"metric\": self.config.dbscan.metric,\n\t\t\t}\n\t\t\tparams.update(cast(\"dict[str, float | int | str]\", self.kwargs))\n\n\t\t\tclustering = self.DBSCAN(**params)\n\t\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\t\telse:\n\t\t\tmsg = f\"Unsupported clustering method: {self.method}\"\n\t\t\traise ValueError(msg)\n\n\t\t# Group chunks by cluster label\n\t\tclusters: dict[int, list[DiffChunk]] = {}\n\t\tlabels_list: list[int] = labels.tolist()  # Convert numpy array to list for type safety\n\t\tfor i, label in enumerate(labels_list):\n\t\t\t# Convert numpy integer to Python int\n\t\t\tlabel_key = int(label)\n\t\t\tif label_key not in clusters:\n\t\t\t\tclusters[label_key] = []\n\t\t\tclusters[label_key].append(chunks[i])\n\n\t\t# Convert to list of lists and handle noise points (-1 label in DBSCAN)\n\t\tresult: list[list[DiffChunk]] = []\n\t\tfor label, cluster_chunks in sorted(clusters.items()):\n\t\t\tif label != -1:  # Regular cluster\n\t\t\t\tresult.append(cluster_chunks)\n\t\t\telse:  # Noise points - each forms its own cluster\n\t\t\t\tresult.extend([[chunk] for chunk in cluster_chunks])\n\n\t\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig_loader: ConfigLoader, **kwargs: object\n) -&gt; None\n</code></pre> <p>Initialize the clusterer.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader to use for configuration (follows DI pattern)</p> required <code>**kwargs</code> <code>object</code> <p>Additional parameters for the clustering algorithm: - For agglomerative: distance_threshold, linkage, etc. - For DBSCAN: eps, min_samples, etc.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If scikit-learn is not installed</p> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>def __init__(self, config_loader: \"ConfigLoader\", **kwargs: object) -&gt; None:\n\t\"\"\"\n\tInitialize the clusterer.\n\n\tArgs:\n\t    config_loader: ConfigLoader to use for configuration (follows DI pattern)\n\t    **kwargs: Additional parameters for the clustering algorithm:\n\t        - For agglomerative: distance_threshold, linkage, etc.\n\t        - For DBSCAN: eps, min_samples, etc.\n\n\tRaises:\n\t    ImportError: If scikit-learn is not installed\n\n\t\"\"\"\n\tself.config = config_loader.get.embedding.clustering\n\tself.method = self.config.method\n\tself.kwargs = kwargs\n\n\t# Import here to avoid making sklearn a hard dependency\n\ttry:\n\t\tfrom sklearn.cluster import DBSCAN, AgglomerativeClustering\n\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\tself.AgglomerativeClustering = AgglomerativeClustering\n\t\tself.DBSCAN = DBSCAN\n\t\tself.cosine_similarity = cosine_similarity\n\texcept ImportError as e:\n\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\tmsg = \"scikit-learn is required for clustering\"\n\t\traise ImportError(msg) from e\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = clustering\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method = method\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.AgglomerativeClustering","title":"AgglomerativeClustering  <code>instance-attribute</code>","text":"<pre><code>AgglomerativeClustering = AgglomerativeClustering\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.DBSCAN","title":"DBSCAN  <code>instance-attribute</code>","text":"<pre><code>DBSCAN = DBSCAN\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.cosine_similarity","title":"cosine_similarity  <code>instance-attribute</code>","text":"<pre><code>cosine_similarity = cosine_similarity\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.cluster","title":"cluster","text":"<pre><code>cluster(\n\tchunk_embeddings: list[tuple[DiffChunk, ndarray]],\n) -&gt; list[list[DiffChunk]]\n</code></pre> <p>Cluster chunks based on their embeddings.</p> <pre><code>          Process:\n          1. Extracts chunks and embeddings from input tuples\n          2. Computes a similarity matrix using cosine similarity\n          3. Converts similarity to distance matrix (1 - similarity)\n          4. Applies clustering algorithm based on the chosen method\n          5. Organizes chunks into clusters based on labels\n          6. Handles special cases like noise points in DBSCAN\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_embeddings</code> <code>list[tuple[DiffChunk, ndarray]]</code> <p>List of (chunk, embedding) tuples where each embedding is a numpy array representing the semantic vector of a code chunk</p> required <p>Returns:</p> Type Description <code>list[list[DiffChunk]]</code> <p>List of lists, where each inner list contains chunks in the same cluster.</p> <code>list[list[DiffChunk]]</code> <p>With DBSCAN, noise points (label -1) are returned as individual single-item clusters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; embedder = DiffEmbedder()\n&gt;&gt;&gt; chunk_embeddings = embedder.embed_chunks(diff_chunks)\n&gt;&gt;&gt; clusterer = DiffClusterer(method=\"agglomerative\", distance_threshold=0.5)\n&gt;&gt;&gt; clusters = clusterer.cluster(chunk_embeddings)\n&gt;&gt;&gt; for i, cluster in enumerate(clusters):\n...     print(f\"Cluster {i} has {len(cluster)} chunks\")\n</code></pre> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>def cluster(self, chunk_embeddings: list[tuple[DiffChunk, np.ndarray]]) -&gt; list[list[DiffChunk]]:\n\t\"\"\"\n\tCluster chunks based on their embeddings.\n\n\t              Process:\n\t              1. Extracts chunks and embeddings from input tuples\n\t              2. Computes a similarity matrix using cosine similarity\n\t              3. Converts similarity to distance matrix (1 - similarity)\n\t              4. Applies clustering algorithm based on the chosen method\n\t              5. Organizes chunks into clusters based on labels\n\t              6. Handles special cases like noise points in DBSCAN\n\n\tArgs:\n\t    chunk_embeddings: List of (chunk, embedding) tuples where each embedding\n\t        is a numpy array representing the semantic vector of a code chunk\n\n\tReturns:\n\t    List of lists, where each inner list contains chunks in the same cluster.\n\t    With DBSCAN, noise points (label -1) are returned as individual single-item clusters.\n\n\tExamples:\n\t    &gt;&gt;&gt; embedder = DiffEmbedder()\n\t    &gt;&gt;&gt; chunk_embeddings = embedder.embed_chunks(diff_chunks)\n\t    &gt;&gt;&gt; clusterer = DiffClusterer(method=\"agglomerative\", distance_threshold=0.5)\n\t    &gt;&gt;&gt; clusters = clusterer.cluster(chunk_embeddings)\n\t    &gt;&gt;&gt; for i, cluster in enumerate(clusters):\n\t    ...     print(f\"Cluster {i} has {len(cluster)} chunks\")\n\n\t\"\"\"\n\tif not chunk_embeddings:\n\t\treturn []\n\n\t# Extract chunks and embeddings\n\tchunks = [ce[0] for ce in chunk_embeddings]\n\tembeddings = np.array([ce[1] for ce in chunk_embeddings])\n\n\t# Compute similarity matrix (1 - cosine distance)\n\tsimilarity_matrix = self.cosine_similarity(embeddings)\n\n\t# Convert to distance matrix (1 - similarity)\n\tdistance_matrix = 1 - similarity_matrix\n\n\t# Apply clustering\n\tif self.method == \"agglomerative\":\n\t\t# Default parameters if not provided\n\t\tparams = {\n\t\t\t\"n_clusters\": None,\n\t\t\t\"distance_threshold\": self.config.agglomerative.distance_threshold,\n\t\t\t\"metric\": self.config.agglomerative.metric,\n\t\t\t\"linkage\": self.config.agglomerative.linkage,\n\t\t}\n\t\tparams.update(cast(\"dict[str, float | str | None]\", self.kwargs))\n\n\t\tclustering = self.AgglomerativeClustering(**params)\n\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\telif self.method == \"dbscan\":\n\t\t# Default parameters if not provided\n\t\tparams = {\n\t\t\t\"eps\": self.config.dbscan.eps,\n\t\t\t\"min_samples\": self.config.dbscan.min_samples,\n\t\t\t\"metric\": self.config.dbscan.metric,\n\t\t}\n\t\tparams.update(cast(\"dict[str, float | int | str]\", self.kwargs))\n\n\t\tclustering = self.DBSCAN(**params)\n\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\telse:\n\t\tmsg = f\"Unsupported clustering method: {self.method}\"\n\t\traise ValueError(msg)\n\n\t# Group chunks by cluster label\n\tclusters: dict[int, list[DiffChunk]] = {}\n\tlabels_list: list[int] = labels.tolist()  # Convert numpy array to list for type safety\n\tfor i, label in enumerate(labels_list):\n\t\t# Convert numpy integer to Python int\n\t\tlabel_key = int(label)\n\t\tif label_key not in clusters:\n\t\t\tclusters[label_key] = []\n\t\tclusters[label_key].append(chunks[i])\n\n\t# Convert to list of lists and handle noise points (-1 label in DBSCAN)\n\tresult: list[list[DiffChunk]] = []\n\tfor label, cluster_chunks in sorted(clusters.items()):\n\t\tif label != -1:  # Regular cluster\n\t\t\tresult.append(cluster_chunks)\n\t\telse:  # Noise points - each forms its own cluster\n\t\t\tresult.extend([[chunk] for chunk in cluster_chunks])\n\n\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/","title":"Context Processor","text":"<p>Context processing utilities for LLM prompts.</p> <p>This module provides functionality to process and format code contexts for LLM prompts using tree-sitter analysis and Level of Detail (LOD) to optimize context length while preserving meaningful content.</p>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.DEFAULT_MAX_TOKENS","title":"DEFAULT_MAX_TOKENS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_MAX_TOKENS = 4000\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.CHUNK_TOKEN_ESTIMATE","title":"CHUNK_TOKEN_ESTIMATE  <code>module-attribute</code>","text":"<pre><code>CHUNK_TOKEN_ESTIMATE = 500\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.MAX_CHUNKS","title":"MAX_CHUNKS  <code>module-attribute</code>","text":"<pre><code>MAX_CHUNKS = 6\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.MAX_SIMPLE_CHUNKS","title":"MAX_SIMPLE_CHUNKS  <code>module-attribute</code>","text":"<pre><code>MAX_SIMPLE_CHUNKS = 3\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.process_chunks_with_lod","title":"process_chunks_with_lod","text":"<pre><code>process_chunks_with_lod(\n\tchunks: list[DiffChunk],\n\tmax_tokens: int = DEFAULT_MAX_TOKENS,\n) -&gt; str\n</code></pre> <p>Process diff chunks using LOD to create optimized context for LLM prompts.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of diff chunks to process</p> required <code>max_tokens</code> <code>int</code> <p>Maximum tokens allowed in the formatted context</p> <code>DEFAULT_MAX_TOKENS</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown context optimized for token usage</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def process_chunks_with_lod(chunks: list[DiffChunk], max_tokens: int = DEFAULT_MAX_TOKENS) -&gt; str:\n\t\"\"\"\n\tProcess diff chunks using LOD to create optimized context for LLM prompts.\n\n\tArgs:\n\t    chunks: List of diff chunks to process\n\t    max_tokens: Maximum tokens allowed in the formatted context\n\n\tReturns:\n\t    Formatted markdown context optimized for token usage\n\n\t\"\"\"\n\t# If chunks list is small, we might not need LOD processing\n\tif len(chunks) &lt;= MAX_SIMPLE_CHUNKS:\n\t\treturn format_regular_chunks(chunks[:MAX_CHUNKS])\n\n\t# Set up LOD generator and estimate number of chunks we can include\n\tlod_generator = LODGenerator()\n\testimated_chunk_count = min(max_tokens // CHUNK_TOKEN_ESTIMATE, len(chunks))\n\tprioritized_chunks = prioritize_chunks(chunks, min(estimated_chunk_count, MAX_CHUNKS))\n\n\t# Start with highest LOD level and progressively reduce if needed\n\tlod_levels = [LODLevel.STRUCTURE, LODLevel.SIGNATURES]\n\tformatted_chunks = []\n\tcurrent_level_index = 0\n\n\twhile current_level_index &lt; len(lod_levels):\n\t\tcurrent_level = lod_levels[current_level_index]\n\t\tformatted_chunks = []\n\n\t\tfor chunk in prioritized_chunks:\n\t\t\t# Get file paths from chunk\n\t\t\tfile_paths = get_file_paths_from_chunk(chunk)\n\n\t\t\tif not file_paths:\n\t\t\t\t# If we can't extract paths, use regular formatting for this chunk\n\t\t\t\tformatted_chunks.append(format_chunk(chunk))\n\t\t\t\tcontinue\n\n\t\t\t# Process each file in the chunk with LOD\n\t\t\tlod_formatted = []\n\t\t\tfor file_path in file_paths:\n\t\t\t\tpath = Path(file_path)\n\t\t\t\tif not path.exists():\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Generate LOD representation\n\t\t\t\tlod_entity = lod_generator.generate_lod(path, level=current_level)\n\t\t\t\tif lod_entity:\n\t\t\t\t\tlod_formatted.append(format_lod_entity(lod_entity, file_path, current_level))\n\n\t\t\tif lod_formatted:\n\t\t\t\tformatted_chunks.append(\"\\n\".join(lod_formatted))\n\t\t\telse:\n\t\t\t\t# Fallback to regular formatting\n\t\t\t\tformatted_chunks.append(format_chunk(chunk))\n\n\t\t# Estimate if we're within token limit\n\t\ttotal_context = \"\\n\\n\".join(formatted_chunks)\n\t\testimated_tokens = estimate_tokens(total_context)\n\n\t\tif estimated_tokens &lt;= max_tokens or current_level_index == len(lod_levels) - 1:\n\t\t\tbreak\n\n\t\t# Try with lower LOD level\n\t\tcurrent_level_index += 1\n\n\t# If we still exceed the token limit, truncate\n\ttotal_context = \"\\n\\n\".join(formatted_chunks)\n\tif estimate_tokens(total_context) &gt; max_tokens:\n\t\ttotal_context = truncate_context(total_context, max_tokens)\n\n\treturn total_context\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.prioritize_chunks","title":"prioritize_chunks","text":"<pre><code>prioritize_chunks(\n\tchunks: list[DiffChunk], max_count: int\n) -&gt; list[DiffChunk]\n</code></pre> <p>Prioritize chunks based on heuristics (file types, changes, etc.).</p> <p>This is a simple implementation that could be extended with more sophisticated dissimilarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of chunks to prioritize</p> required <code>max_count</code> <code>int</code> <p>Maximum number of chunks to return</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>Prioritized list of chunks</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def prioritize_chunks(chunks: list[DiffChunk], max_count: int) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tPrioritize chunks based on heuristics (file types, changes, etc.).\n\n\tThis is a simple implementation that could be extended with more\n\tsophisticated dissimilarity metrics.\n\n\tArgs:\n\t    chunks: List of chunks to prioritize\n\t    max_count: Maximum number of chunks to return\n\n\tReturns:\n\t    Prioritized list of chunks\n\n\t\"\"\"\n\t# Simple heuristics for now:\n\t# 1. Prefer chunks with code files over non-code files\n\t# 2. Prefer chunks with more files (more central changes)\n\t# 3. Prefer chunks with more added/changed lines\n\n\tdef chunk_score(chunk: DiffChunk) -&gt; float:\n\t\t\"\"\"Calculates a priority score for a diff chunk based on heuristics.\n\n\t\tThe score is calculated using three factors:\n\t\t1. Presence of code files (60% weight)\n\t\t2. Number of files affected (20% weight)\n\t\t3. Size of content changes (20% weight)\n\n\t\tArgs:\n\t\t\tchunk: The diff chunk to score\n\n\t\tReturns:\n\t\t\tfloat: A score between 0 and 1 representing the chunk's priority\n\t\t\"\"\"\n\t\t# Check if any files are code files\n\t\tcode_file_score = 0\n\t\tfor file in chunk.files:\n\t\t\tif any(file.endswith(ext) for ext in [\".py\", \".js\", \".ts\", \".java\", \".c\", \".cpp\", \".go\"]):\n\t\t\t\tcode_file_score = 1\n\t\t\t\tbreak\n\n\t\t# Score based on number of files\n\t\tfile_count_score = min(len(chunk.files), 3) / 3\n\n\t\t# Score based on content size (as proxy for changes)\n\t\tcontent_score = min(len(chunk.content), 1000) / 1000\n\n\t\treturn code_file_score * 0.6 + file_count_score * 0.2 + content_score * 0.2\n\n\t# Sort chunks by score and return top max_count\n\treturn sorted(chunks, key=chunk_score, reverse=True)[:max_count]\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.get_file_paths_from_chunk","title":"get_file_paths_from_chunk","text":"<pre><code>get_file_paths_from_chunk(chunk: DiffChunk) -&gt; list[str]\n</code></pre> <p>Extract file paths from a diff chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>The diff chunk to process</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of file paths</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def get_file_paths_from_chunk(chunk: DiffChunk) -&gt; list[str]:\n\t\"\"\"\n\tExtract file paths from a diff chunk.\n\n\tArgs:\n\t    chunk: The diff chunk to process\n\n\tReturns:\n\t    List of file paths\n\n\t\"\"\"\n\treturn [file for file in chunk.files if file]\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_lod_entity","title":"format_lod_entity","text":"<pre><code>format_lod_entity(\n\tentity: LODEntity, file_path: str, level: LODLevel\n) -&gt; str\n</code></pre> <p>Format an LOD entity as GitHub-flavored markdown.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>LODEntity</code> <p>The LOD entity to format</p> required <code>file_path</code> <code>str</code> <p>Path to the source file</p> required <code>level</code> <code>LODLevel</code> <p>LOD level used</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown string</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_lod_entity(entity: LODEntity, file_path: str, level: LODLevel) -&gt; str:\n\t\"\"\"\n\tFormat an LOD entity as GitHub-flavored markdown.\n\n\tArgs:\n\t    entity: The LOD entity to format\n\t    file_path: Path to the source file\n\t    level: LOD level used\n\n\tReturns:\n\t    Formatted markdown string\n\n\t\"\"\"\n\t# Start with file header\n\tresult = f\"## {file_path}\\n\\n\"\n\n\t# Format the entity based on LOD level\n\tif level == LODLevel.STRUCTURE:\n\t\tresult += format_entity_structure(entity, 0)\n\telif level == LODLevel.SIGNATURES:\n\t\tresult += format_entity_signatures(entity, 0)\n\n\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_entity_structure","title":"format_entity_structure","text":"<pre><code>format_entity_structure(\n\tentity: LODEntity, indent: int\n) -&gt; str\n</code></pre> <p>Format entity with structure (signatures and hierarchy).</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_entity_structure(entity: LODEntity, indent: int) -&gt; str:\n\t\"\"\"Format entity with structure (signatures and hierarchy).\"\"\"\n\tindent_str = \"  \" * indent\n\tresult = f\"{indent_str}- **{entity.entity_type.name}**: `{entity.name}`\"\n\n\tif entity.signature:\n\t\tresult += f\"\\n{indent_str}  ```\\n{indent_str}  {entity.signature}\\n{indent_str}  ```\"\n\n\tif entity.children:\n\t\tresult += \"\\n\"\n\t\tfor child in entity.children:\n\t\t\tresult += format_entity_structure(child, indent + 1)\n\n\treturn result + \"\\n\"\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_entity_signatures","title":"format_entity_signatures","text":"<pre><code>format_entity_signatures(\n\tentity: LODEntity, indent: int\n) -&gt; str\n</code></pre> <p>Format entity with just signatures.</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_entity_signatures(entity: LODEntity, indent: int) -&gt; str:\n\t\"\"\"Format entity with just signatures.\"\"\"\n\tindent_str = \"  \" * indent\n\tresult = f\"{indent_str}- **{entity.entity_type.name}**: `{entity.name}`\"\n\n\tif entity.signature:\n\t\tresult += f\" - `{entity.signature}`\"\n\n\tif entity.children:\n\t\tresult += \"\\n\"\n\t\tfor child in entity.children:\n\t\t\tresult += format_entity_signatures(child, indent + 1)\n\n\treturn result + \"\\n\"\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_regular_chunks","title":"format_regular_chunks","text":"<pre><code>format_regular_chunks(chunks: list[DiffChunk]) -&gt; str\n</code></pre> <p>Format chunks using the regular approach when LOD is not necessary.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of chunks to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown string</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_regular_chunks(chunks: list[DiffChunk]) -&gt; str:\n\t\"\"\"\n\tFormat chunks using the regular approach when LOD is not necessary.\n\n\tArgs:\n\t    chunks: List of chunks to format\n\n\tReturns:\n\t    Formatted markdown string\n\n\t\"\"\"\n\tformatted_chunks = [format_chunk(chunk) for chunk in chunks]\n\treturn \"\\n\\n\".join(formatted_chunks)\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_chunk","title":"format_chunk","text":"<pre><code>format_chunk(chunk: DiffChunk) -&gt; str\n</code></pre> <p>Format a single diff chunk as markdown.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>The diff chunk to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown string</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_chunk(chunk: DiffChunk) -&gt; str:\n\t\"\"\"\n\tFormat a single diff chunk as markdown.\n\n\tArgs:\n\t    chunk: The diff chunk to format\n\n\tReturns:\n\t    Formatted markdown string\n\n\t\"\"\"\n\t# Format file paths\n\tfile_section = \"## Files\\n\"\n\tfor file in chunk.files:\n\t\tif file:\n\t\t\tfile_section += f\"- {file}\\n\"\n\n\t# Format content\n\tcontent_section = \"### Changes\\n```diff\\n\" + chunk.content + \"\\n```\"\n\n\treturn file_section + \"\\n\" + content_section\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate the number of tokens in a text.</p> <p>This is a simple estimation that can be improved with actual tokenizer implementations if needed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to estimate tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def estimate_tokens(text: str) -&gt; int:\n\t\"\"\"\n\tEstimate the number of tokens in a text.\n\n\tThis is a simple estimation that can be improved with\n\tactual tokenizer implementations if needed.\n\n\tArgs:\n\t    text: Text to estimate tokens for\n\n\tReturns:\n\t    Estimated token count\n\n\t\"\"\"\n\t# Simple estimation: 4 characters per token on average\n\treturn len(text) // 4\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.truncate_context","title":"truncate_context","text":"<pre><code>truncate_context(context: str, max_tokens: int) -&gt; str\n</code></pre> <p>Truncate context to fit within token limit.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>Context to truncate</p> required <code>max_tokens</code> <code>int</code> <p>Maximum allowed tokens</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated context</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def truncate_context(context: str, max_tokens: int) -&gt; str:\n\t\"\"\"\n\tTruncate context to fit within token limit.\n\n\tArgs:\n\t    context: Context to truncate\n\t    max_tokens: Maximum allowed tokens\n\n\tReturns:\n\t    Truncated context\n\n\t\"\"\"\n\t# Simple truncation by estimating tokens\n\tif estimate_tokens(context) &lt;= max_tokens:\n\t\treturn context\n\n\t# Split into chunks and preserve as many complete chunks as possible\n\tchunks = context.split(\"\\n\\n\")\n\tresult_chunks = []\n\tcurrent_token_count = 0\n\n\tfor chunk in chunks:\n\t\tchunk_tokens = estimate_tokens(chunk)\n\t\tif current_token_count + chunk_tokens &lt;= max_tokens - 100:  # Reserve 100 tokens for truncation marker\n\t\t\tresult_chunks.append(chunk)\n\t\t\tcurrent_token_count += chunk_tokens\n\t\telse:\n\t\t\t# Add truncation marker and stop\n\t\t\tresult_chunks.append(\"\\n\\n[...TRUNCATED...]\\n\\n\")\n\t\t\tbreak\n\n\treturn \"\\n\\n\".join(result_chunks)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/","title":"Embedder","text":"<p>Module for generating embeddings from diff chunks.</p>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder","title":"DiffEmbedder","text":"<p>Generates embeddings for diff chunks.</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>class DiffEmbedder:\n\t\"\"\"Generates embeddings for diff chunks.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: \"ConfigLoader\",\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the embedder with configuration.\n\n\t\tArgs:\n\t\t    config_loader: ConfigLoader instance for embedding configuration.\n\t\t\"\"\"\n\t\tself.config_loader = config_loader\n\n\tdef preprocess_diff(self, diff_text: str) -&gt; str:\n\t\t\"\"\"\n\t\tPreprocess diff text to make it more suitable for embedding.\n\n\t\tArgs:\n\t\t    diff_text: Raw diff text\n\n\t\tReturns:\n\t\t    Preprocessed text\n\n\t\t\"\"\"\n\t\t# Remove diff headers, line numbers, etc.\n\t\t# Focus on actual content changes\n\t\tlines = []\n\t\tfor line in diff_text.splitlines():\n\t\t\t# Skip diff metadata lines\n\t\t\tif line.startswith((\"diff --git\", \"index \", \"+++\", \"---\")):\n\t\t\t\tcontinue\n\n\t\t\t# Keep actual content changes, removing the +/- prefix\n\t\t\tif line.startswith((\"+\", \"-\", \" \")):\n\t\t\t\tlines.append(line[1:])\n\n\t\treturn \"\\n\".join(lines)\n\n\tasync def embed_chunk(self, chunk: DiffChunk) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tGenerate an embedding for a diff chunk using Voyage AI.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk object\n\n\t\tReturns:\n\t\t    numpy.ndarray: Embedding vector\n\n\t\t\"\"\"\n\t\t# Get the diff content from the chunk\n\t\tdiff_text = chunk.content\n\n\t\t# Preprocess the diff text\n\t\tprocessed_text = self.preprocess_diff(diff_text)\n\n\t\t# If the processed text is empty, use the file paths as context\n\t\tif not processed_text.strip():\n\t\t\tprocessed_text = \" \".join(chunk.files)\n\n\t\t# Generate embeddings in batch (of 1)\n\t\tembeddings = generate_embedding([processed_text], self.config_loader)\n\n\t\tif not embeddings:\n\t\t\tmessage = f\"Failed to generate embedding for chunk with files: {', '.join(chunk.files)}\"\n\t\t\tlogger.error(message)\n\t\t\t# Return a zero vector as a fallback\n\t\t\treturn np.zeros(1024)  # Using default dimension of 1024\n\n\t\treturn np.array(embeddings[0])\n\n\tasync def embed_contents(self, contents: list[str]) -&gt; list[list[float] | None]:\n\t\t\"\"\"\n\t\tGenerate embeddings for multiple content strings.\n\n\t\tArgs:\n\t\t    contents: List of text content strings to embed\n\n\t\tReturns:\n\t\t    List of embedding vectors or None for each content\n\t\t\"\"\"\n\t\t# Filter out empty contents\n\t\tcontents_to_embed = []\n\t\tvalid_indices = []\n\n\t\tfor i, content in enumerate(contents):\n\t\t\tif content and content.strip():\n\t\t\t\t# Preprocess if it looks like diff content\n\t\t\t\tif content.startswith((\"diff --git\", \"+\", \"-\", \" \")):\n\t\t\t\t\tprocessed = self.preprocess_diff(content)\n\t\t\t\t\tif processed.strip():\n\t\t\t\t\t\tcontents_to_embed.append(processed)\n\t\t\t\t\t\tvalid_indices.append(i)\n\t\t\t\telse:\n\t\t\t\t\t# Use as-is if it doesn't look like a diff\n\t\t\t\t\tcontents_to_embed.append(content)\n\t\t\t\t\tvalid_indices.append(i)\n\n\t\t# Return early if no valid contents\n\t\tif not contents_to_embed:\n\t\t\treturn cast(\"list[list[float] | None]\", [None] * len(contents))\n\n\t\t# Generate embeddings in batch\n\t\ttry:\n\t\t\tembeddings_batch = generate_embedding(contents_to_embed, self.config_loader)\n\n\t\t\t# Rebuild result list with None for invalid contents\n\t\t\tresult: list[list[float] | None] = cast(\"list[list[float] | None]\", [None] * len(contents))\n\t\t\tif embeddings_batch:\n\t\t\t\tfor idx, valid_idx in enumerate(valid_indices):\n\t\t\t\t\tif idx &lt; len(embeddings_batch):\n\t\t\t\t\t\tresult[valid_idx] = embeddings_batch[idx]\n\t\t\treturn result\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Unexpected error during embedding generation\")\n\t\t\treturn cast(\"list[list[float] | None]\", [None] * len(contents))\n\n\tasync def embed_chunks(self, chunks: list[DiffChunk]) -&gt; list[tuple[DiffChunk, np.ndarray]]:\n\t\t\"\"\"\n\t\tGenerate embeddings for multiple chunks using efficient batch processing.\n\n\t\tArgs:\n\t\t    chunks: List of DiffChunk objects\n\n\t\tReturns:\n\t\t    List of (chunk, embedding) tuples\n\n\t\t\"\"\"\n\t\tif not chunks:\n\t\t\treturn []\n\n\t\t# Preprocess all chunk texts\n\t\tpreprocessed_texts = []\n\t\tfor chunk in chunks:\n\t\t\tdiff_text = chunk.content\n\t\t\tprocessed_text = self.preprocess_diff(diff_text)\n\n\t\t\t# If the processed text is empty, use the file paths as context\n\t\t\tif not processed_text.strip():\n\t\t\t\tprocessed_text = \" \".join(chunk.files)\n\n\t\t\tpreprocessed_texts.append(processed_text)\n\n\t\t# Generate embeddings in batch\n\t\tembeddings = generate_embedding(preprocessed_texts, self.config_loader)\n\n\t\t# Create result tuples\n\t\tresult = []\n\t\tif embeddings:\n\t\t\tfor i, chunk in enumerate(chunks):\n\t\t\t\tif i &lt; len(embeddings):\n\t\t\t\t\tembedding = np.array(embeddings[i])\n\t\t\t\telse:\n\t\t\t\t\tlogger.error(f\"Missing embedding for chunk with files: {', '.join(chunk.files)}\")\n\t\t\t\t\tembedding = np.zeros(1024)  # Fallback\n\t\t\t\tresult.append((chunk, embedding))\n\t\telse:\n\t\t\t# Fallback if batch embedding failed\n\t\t\tlogger.error(\"Batch embedding generation failed, using fallback zeros\")\n\t\t\tresult.extend((chunk, np.zeros(1024)) for chunk in chunks)\n\n\t\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.__init__","title":"__init__","text":"<pre><code>__init__(config_loader: ConfigLoader) -&gt; None\n</code></pre> <p>Initialize the embedder with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance for embedding configuration.</p> required Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: \"ConfigLoader\",\n) -&gt; None:\n\t\"\"\"\n\tInitialize the embedder with configuration.\n\n\tArgs:\n\t    config_loader: ConfigLoader instance for embedding configuration.\n\t\"\"\"\n\tself.config_loader = config_loader\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.preprocess_diff","title":"preprocess_diff","text":"<pre><code>preprocess_diff(diff_text: str) -&gt; str\n</code></pre> <p>Preprocess diff text to make it more suitable for embedding.</p> <p>Parameters:</p> Name Type Description Default <code>diff_text</code> <code>str</code> <p>Raw diff text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Preprocessed text</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>def preprocess_diff(self, diff_text: str) -&gt; str:\n\t\"\"\"\n\tPreprocess diff text to make it more suitable for embedding.\n\n\tArgs:\n\t    diff_text: Raw diff text\n\n\tReturns:\n\t    Preprocessed text\n\n\t\"\"\"\n\t# Remove diff headers, line numbers, etc.\n\t# Focus on actual content changes\n\tlines = []\n\tfor line in diff_text.splitlines():\n\t\t# Skip diff metadata lines\n\t\tif line.startswith((\"diff --git\", \"index \", \"+++\", \"---\")):\n\t\t\tcontinue\n\n\t\t# Keep actual content changes, removing the +/- prefix\n\t\tif line.startswith((\"+\", \"-\", \" \")):\n\t\t\tlines.append(line[1:])\n\n\treturn \"\\n\".join(lines)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.embed_chunk","title":"embed_chunk  <code>async</code>","text":"<pre><code>embed_chunk(chunk: DiffChunk) -&gt; ndarray\n</code></pre> <p>Generate an embedding for a diff chunk using Voyage AI.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>DiffChunk object</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Embedding vector</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>async def embed_chunk(self, chunk: DiffChunk) -&gt; np.ndarray:\n\t\"\"\"\n\tGenerate an embedding for a diff chunk using Voyage AI.\n\n\tArgs:\n\t    chunk: DiffChunk object\n\n\tReturns:\n\t    numpy.ndarray: Embedding vector\n\n\t\"\"\"\n\t# Get the diff content from the chunk\n\tdiff_text = chunk.content\n\n\t# Preprocess the diff text\n\tprocessed_text = self.preprocess_diff(diff_text)\n\n\t# If the processed text is empty, use the file paths as context\n\tif not processed_text.strip():\n\t\tprocessed_text = \" \".join(chunk.files)\n\n\t# Generate embeddings in batch (of 1)\n\tembeddings = generate_embedding([processed_text], self.config_loader)\n\n\tif not embeddings:\n\t\tmessage = f\"Failed to generate embedding for chunk with files: {', '.join(chunk.files)}\"\n\t\tlogger.error(message)\n\t\t# Return a zero vector as a fallback\n\t\treturn np.zeros(1024)  # Using default dimension of 1024\n\n\treturn np.array(embeddings[0])\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.embed_contents","title":"embed_contents  <code>async</code>","text":"<pre><code>embed_contents(\n\tcontents: list[str],\n) -&gt; list[list[float] | None]\n</code></pre> <p>Generate embeddings for multiple content strings.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>list[str]</code> <p>List of text content strings to embed</p> required <p>Returns:</p> Type Description <code>list[list[float] | None]</code> <p>List of embedding vectors or None for each content</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>async def embed_contents(self, contents: list[str]) -&gt; list[list[float] | None]:\n\t\"\"\"\n\tGenerate embeddings for multiple content strings.\n\n\tArgs:\n\t    contents: List of text content strings to embed\n\n\tReturns:\n\t    List of embedding vectors or None for each content\n\t\"\"\"\n\t# Filter out empty contents\n\tcontents_to_embed = []\n\tvalid_indices = []\n\n\tfor i, content in enumerate(contents):\n\t\tif content and content.strip():\n\t\t\t# Preprocess if it looks like diff content\n\t\t\tif content.startswith((\"diff --git\", \"+\", \"-\", \" \")):\n\t\t\t\tprocessed = self.preprocess_diff(content)\n\t\t\t\tif processed.strip():\n\t\t\t\t\tcontents_to_embed.append(processed)\n\t\t\t\t\tvalid_indices.append(i)\n\t\t\telse:\n\t\t\t\t# Use as-is if it doesn't look like a diff\n\t\t\t\tcontents_to_embed.append(content)\n\t\t\t\tvalid_indices.append(i)\n\n\t# Return early if no valid contents\n\tif not contents_to_embed:\n\t\treturn cast(\"list[list[float] | None]\", [None] * len(contents))\n\n\t# Generate embeddings in batch\n\ttry:\n\t\tembeddings_batch = generate_embedding(contents_to_embed, self.config_loader)\n\n\t\t# Rebuild result list with None for invalid contents\n\t\tresult: list[list[float] | None] = cast(\"list[list[float] | None]\", [None] * len(contents))\n\t\tif embeddings_batch:\n\t\t\tfor idx, valid_idx in enumerate(valid_indices):\n\t\t\t\tif idx &lt; len(embeddings_batch):\n\t\t\t\t\tresult[valid_idx] = embeddings_batch[idx]\n\t\treturn result\n\n\texcept Exception:\n\t\tlogger.exception(\"Unexpected error during embedding generation\")\n\t\treturn cast(\"list[list[float] | None]\", [None] * len(contents))\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.embed_chunks","title":"embed_chunks  <code>async</code>","text":"<pre><code>embed_chunks(\n\tchunks: list[DiffChunk],\n) -&gt; list[tuple[DiffChunk, ndarray]]\n</code></pre> <p>Generate embeddings for multiple chunks using efficient batch processing.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of DiffChunk objects</p> required <p>Returns:</p> Type Description <code>list[tuple[DiffChunk, ndarray]]</code> <p>List of (chunk, embedding) tuples</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>async def embed_chunks(self, chunks: list[DiffChunk]) -&gt; list[tuple[DiffChunk, np.ndarray]]:\n\t\"\"\"\n\tGenerate embeddings for multiple chunks using efficient batch processing.\n\n\tArgs:\n\t    chunks: List of DiffChunk objects\n\n\tReturns:\n\t    List of (chunk, embedding) tuples\n\n\t\"\"\"\n\tif not chunks:\n\t\treturn []\n\n\t# Preprocess all chunk texts\n\tpreprocessed_texts = []\n\tfor chunk in chunks:\n\t\tdiff_text = chunk.content\n\t\tprocessed_text = self.preprocess_diff(diff_text)\n\n\t\t# If the processed text is empty, use the file paths as context\n\t\tif not processed_text.strip():\n\t\t\tprocessed_text = \" \".join(chunk.files)\n\n\t\tpreprocessed_texts.append(processed_text)\n\n\t# Generate embeddings in batch\n\tembeddings = generate_embedding(preprocessed_texts, self.config_loader)\n\n\t# Create result tuples\n\tresult = []\n\tif embeddings:\n\t\tfor i, chunk in enumerate(chunks):\n\t\t\tif i &lt; len(embeddings):\n\t\t\t\tembedding = np.array(embeddings[i])\n\t\t\telse:\n\t\t\t\tlogger.error(f\"Missing embedding for chunk with files: {', '.join(chunk.files)}\")\n\t\t\t\tembedding = np.zeros(1024)  # Fallback\n\t\t\tresult.append((chunk, embedding))\n\telse:\n\t\t# Fallback if batch embedding failed\n\t\tlogger.error(\"Batch embedding generation failed, using fallback zeros\")\n\t\tresult.extend((chunk, np.zeros(1024)) for chunk in chunks)\n\n\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/group/","title":"Group","text":"<p>Module for semantic grouping of diff chunks.</p>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup","title":"SemanticGroup","text":"<p>Represents a group of semantically related diff chunks.</p> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>class SemanticGroup:\n\t\"\"\"Represents a group of semantically related diff chunks.\"\"\"\n\n\tdef __init__(self, chunks: list[DiffChunk] | None = None, name: str | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize a semantic group.\n\n\t\tArgs:\n\t\t    chunks: List of DiffChunk objects\n\t\t    name: Optional name for the group\n\n\t\t\"\"\"\n\t\tself.chunks = chunks or []\n\t\tself.name = name\n\t\tself.message: str | None = None\n\t\tself.approved = False\n\n\t@property\n\tdef files(self) -&gt; list[str]:\n\t\t\"\"\"Get the set of files affected by this group.\"\"\"\n\t\tfiles: set[str] = set()\n\t\tfor chunk in self.chunks:\n\t\t\tfiles.update(chunk.files)\n\t\treturn sorted(files)\n\n\t@property\n\tdef content(self) -&gt; str:\n\t\t\"\"\"Get the combined diff content of all chunks.\"\"\"\n\t\treturn \"\\n\".join(chunk.content for chunk in self.chunks)\n\n\tdef merge_with(self, other_group: \"SemanticGroup\") -&gt; \"SemanticGroup\":\n\t\t\"\"\"\n\t\tMerge this group with another group.\n\n\t\tArgs:\n\t\t    other_group: Another SemanticGroup to merge with\n\n\t\tReturns:\n\t\t    A new SemanticGroup containing chunks from both groups\n\n\t\t\"\"\"\n\t\treturn SemanticGroup(\n\t\t\tchunks=self.chunks + other_group.chunks, name=f\"Merged: {self.name or ''} + {other_group.name or ''}\"\n\t\t)\n\n\tdef __repr__(self) -&gt; str:\n\t\t\"\"\"Return a string representation of the group with file and chunk counts.\"\"\"\n\t\treturn f\"SemanticGroup(files={len(self.files)}, chunks={len(self.chunks)})\"\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.__init__","title":"__init__","text":"<pre><code>__init__(\n\tchunks: list[DiffChunk] | None = None,\n\tname: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize a semantic group.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk] | None</code> <p>List of DiffChunk objects</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the group</p> <code>None</code> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>def __init__(self, chunks: list[DiffChunk] | None = None, name: str | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize a semantic group.\n\n\tArgs:\n\t    chunks: List of DiffChunk objects\n\t    name: Optional name for the group\n\n\t\"\"\"\n\tself.chunks = chunks or []\n\tself.name = name\n\tself.message: str | None = None\n\tself.approved = False\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.chunks","title":"chunks  <code>instance-attribute</code>","text":"<pre><code>chunks = chunks or []\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str | None = None\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.approved","title":"approved  <code>instance-attribute</code>","text":"<pre><code>approved = False\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.files","title":"files  <code>property</code>","text":"<pre><code>files: list[str]\n</code></pre> <p>Get the set of files affected by this group.</p>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Get the combined diff content of all chunks.</p>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.merge_with","title":"merge_with","text":"<pre><code>merge_with(other_group: SemanticGroup) -&gt; SemanticGroup\n</code></pre> <p>Merge this group with another group.</p> <p>Parameters:</p> Name Type Description Default <code>other_group</code> <code>SemanticGroup</code> <p>Another SemanticGroup to merge with</p> required <p>Returns:</p> Type Description <code>SemanticGroup</code> <p>A new SemanticGroup containing chunks from both groups</p> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>def merge_with(self, other_group: \"SemanticGroup\") -&gt; \"SemanticGroup\":\n\t\"\"\"\n\tMerge this group with another group.\n\n\tArgs:\n\t    other_group: Another SemanticGroup to merge with\n\n\tReturns:\n\t    A new SemanticGroup containing chunks from both groups\n\n\t\"\"\"\n\treturn SemanticGroup(\n\t\tchunks=self.chunks + other_group.chunks, name=f\"Merged: {self.name or ''} + {other_group.name or ''}\"\n\t)\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return a string representation of the group with file and chunk counts.</p> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>def __repr__(self) -&gt; str:\n\t\"\"\"Return a string representation of the group with file and chunk counts.\"\"\"\n\treturn f\"SemanticGroup(files={len(self.files)}, chunks={len(self.chunks)})\"\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/","title":"Resolver","text":"<p>Module for resolving file integrity constraints in semantic groups.</p> <p>This module provides functionality for ensuring that changes to the same file are kept in the same commit, even when semantic clustering might separate them. This ensures that file integrity is maintained during the commit process.</p> <p>Key components: - FileIntegrityResolver: Main class that analyzes file overlaps between semantic groups   and decides whether to merge groups or reassign chunks to maintain file integrity</p> <p>The resolution process involves: 1. Detecting violations (files that appear in multiple semantic groups) 2. Calculating semantic similarity between groups with overlapping files 3. Deciding whether to merge groups (if sufficiently similar) or reassign chunks 4. Iteratively resolving violations until all files are in exactly one group</p>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T', bound=DiffChunk)\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver","title":"FileIntegrityResolver","text":"<p>Resolves file integrity constraints for semantic groups.</p> <p>File integrity refers to the requirement that all changes to a specific file should be included in the same commit, even if they are semantically different. This prevents fragmented changes to the same file across multiple commits, which can lead to broken builds or inconsistent states.</p> <p>The resolver works by: 1. Identifying files that appear in multiple semantic groups 2. Calculating the semantic similarity between these overlapping groups 3. Either merging similar groups or reassigning chunks from less relevant groups    to the most appropriate group</p> <p>This process ensures that each file is modified in exactly one commit, while still maintaining semantic coherence within commits when possible.</p> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>class FileIntegrityResolver:\n\t\"\"\"\n\tResolves file integrity constraints for semantic groups.\n\n\tFile integrity refers to the requirement that all changes to a specific file should\n\tbe included in the same commit, even if they are semantically different. This prevents\n\tfragmented changes to the same file across multiple commits, which can lead to broken builds\n\tor inconsistent states.\n\n\tThe resolver works by:\n\t1. Identifying files that appear in multiple semantic groups\n\t2. Calculating the semantic similarity between these overlapping groups\n\t3. Either merging similar groups or reassigning chunks from less relevant groups\n\t   to the most appropriate group\n\n\tThis process ensures that each file is modified in exactly one commit, while still\n\tmaintaining semantic coherence within commits when possible.\n\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tsimilarity_threshold: float = 0.6,\n\t\tconfig_loader: \"ConfigLoader | None\" = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the resolver.\n\n\t\tArgs:\n\t\t    similarity_threshold: Threshold for group similarity to trigger merging (0.0-1.0).\n\t\t    config_loader: Optional ConfigLoader instance.\n\t\t\"\"\"\n\t\tif config_loader:\n\t\t\tself.config_loader = config_loader\n\t\telse:\n\t\t\tfrom codemap.config import ConfigLoader\n\n\t\t\tself.config_loader = ConfigLoader()\n\n\t\tself.similarity_threshold = similarity_threshold\n\n\t\t# Import here to avoid making sklearn a hard dependency\n\t\ttry:\n\t\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\t\tself.cosine_similarity = cosine_similarity\n\t\texcept ImportError as e:\n\t\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\t\tmsg = \"scikit-learn is required for file integrity resolution\"\n\t\t\traise ImportError(msg) from e\n\n\tdef calculate_group_similarity(\n\t\tself, group1: \"SemanticGroup\", group2: \"SemanticGroup\", chunk_embeddings: dict[DiffChunk, np.ndarray]\n\t) -&gt; float:\n\t\t\"\"\"\n\t\tCalculate similarity between two groups based on their chunks' embeddings.\n\n\t\tThis method computes the average pairwise cosine similarity between all combinations\n\t\tof chunks from the two groups. The similarity is based on the semantic embeddings\n\t\tof the chunks' content.\n\n\t\tProcess:\n\t\t1. Extract embeddings for all chunks in both groups\n\t\t2. Compute pairwise cosine similarities between each pair of chunks\n\t\t3. Return the average similarity score\n\n\t\tArgs:\n\t\t    group1: First semantic group to compare\n\t\t    group2: Second semantic group to compare\n\t\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\t\tReturns:\n\t\t    float: Similarity score between 0 and 1, where:\n\t\t        - 0 indicates completely unrelated changes\n\t\t        - 1 indicates identical or extremely similar changes\n\t\t        - Values around 0.6-0.8 typically indicate related functionality\n\n\t\t\"\"\"\n\t\t# Get embeddings for chunks in each group\n\t\tembeddings1 = [chunk_embeddings[chunk] for chunk in group1.chunks if chunk in chunk_embeddings]\n\t\tembeddings2 = [chunk_embeddings[chunk] for chunk in group2.chunks if chunk in chunk_embeddings]\n\n\t\tif not embeddings1 or not embeddings2:\n\t\t\treturn 0.0\n\n\t\t# Calculate pairwise similarities\n\t\tsimilarities = []\n\t\tfor emb1 in embeddings1:\n\t\t\tfor emb2 in embeddings2:\n\t\t\t\tsim = self.cosine_similarity([emb1], [emb2])[0][0]\n\t\t\t\tsimilarities.append(sim)\n\n\t\t# Return average similarity\n\t\treturn sum(similarities) / len(similarities) if similarities else 0.0\n\n\tdef resolve_violations(\n\t\tself, groups: list[\"SemanticGroup\"], chunk_embeddings: dict[DiffChunk, np.ndarray]\n\t) -&gt; list[\"SemanticGroup\"]:\n\t\t\"\"\"\n\t\tResolve file integrity violations by merging or reassigning chunks.\n\n\t\tA violation occurs when the same file appears in multiple semantic groups.\n\t\tThis needs to be resolved because a file should be modified in only one commit.\n\n\t\tArgs:\n\t\t    groups: List of SemanticGroup objects to resolve\n\t\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\t\tReturns:\n\t\t    List of SemanticGroup objects with all violations resolved\n\n\t\t\"\"\"\n\t\t# Keep iterating until no violations remain\n\t\twhile True:\n\t\t\t# Build file to groups mapping\n\t\t\tfile_to_groups: dict[str, list[int]] = {}\n\t\t\tfor i, group in enumerate(groups):\n\t\t\t\tfor file in group.files:\n\t\t\t\t\tif file not in file_to_groups:\n\t\t\t\t\t\tfile_to_groups[file] = []\n\t\t\t\t\tfile_to_groups[file].append(i)\n\n\t\t\t# Find violations (files in multiple groups)\n\t\t\tviolations = {file: indices for file, indices in file_to_groups.items() if len(indices) &gt; 1}\n\n\t\t\tif not violations:\n\t\t\t\tbreak  # No violations, we're done\n\n\t\t\t# Process the first violation\n\t\t\tfile = next(iter(violations))\n\t\t\tgroup_indices = violations[file]\n\n\t\t\t# Try to find groups to merge based on similarity\n\t\t\tmax_similarity = 0\n\t\t\tgroups_to_merge = None\n\n\t\t\t# Calculate similarities between all pairs of groups containing this file\n\t\t\tfor i in range(len(group_indices)):\n\t\t\t\tfor j in range(i + 1, len(group_indices)):\n\t\t\t\t\tidx1, idx2 = group_indices[i], group_indices[j]\n\t\t\t\t\tsimilarity = self.calculate_group_similarity(groups[idx1], groups[idx2], chunk_embeddings)\n\n\t\t\t\t\tif similarity &gt; max_similarity:\n\t\t\t\t\t\tmax_similarity = similarity\n\t\t\t\t\t\tgroups_to_merge = (idx1, idx2)\n\n\t\t\t# Decide whether to merge or reassign based on similarity threshold\n\t\t\tif max_similarity &gt;= self.similarity_threshold and groups_to_merge:\n\t\t\t\t# STRATEGY 1: Merge groups if they're similar enough\n\t\t\t\tidx1, idx2 = groups_to_merge\n\t\t\t\tmerged_group = groups[idx1].merge_with(groups[idx2])\n\n\t\t\t\t# Replace the first group with the merged one and remove the second\n\t\t\t\tgroups[idx1] = merged_group\n\t\t\t\tgroups.pop(idx2)\n\t\t\telse:\n\t\t\t\t# STRATEGY 2: Reassign chunks to the primary group for this file\n\t\t\t\t# Find the primary group (group with most chunks containing this file)\n\t\t\t\tfile_chunks_count = []\n\t\t\t\tfor idx in group_indices:\n\t\t\t\t\tcount = sum(1 for chunk in groups[idx].chunks if file in chunk.files)\n\t\t\t\t\tfile_chunks_count.append((idx, count))\n\n\t\t\t\t# Sort by count descending\n\t\t\t\tfile_chunks_count.sort(key=lambda x: x[1], reverse=True)\n\t\t\t\tprimary_idx = file_chunks_count[0][0]\n\n\t\t\t\t# Move chunks containing this file to the primary group\n\t\t\t\tfor idx in group_indices:\n\t\t\t\t\tif idx != primary_idx:\n\t\t\t\t\t\t# Find chunks containing this file\n\t\t\t\t\t\tchunks_to_move = [chunk for chunk in groups[idx].chunks if file in chunk.files]\n\n\t\t\t\t\t\t# Move chunks to primary group\n\t\t\t\t\t\tgroups[primary_idx].chunks.extend(chunks_to_move)\n\n\t\t\t\t\t\t# Remove moved chunks from original group\n\t\t\t\t\t\tgroups[idx].chunks = [chunk for chunk in groups[idx].chunks if file not in chunk.files]\n\n\t\t\t\t# Remove empty groups\n\t\t\t\tgroups = [group for group in groups if group.chunks]\n\n\t\treturn groups\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.__init__","title":"__init__","text":"<pre><code>__init__(\n\tsimilarity_threshold: float = 0.6,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the resolver.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_threshold</code> <code>float</code> <p>Threshold for group similarity to trigger merging (0.0-1.0).</p> <code>0.6</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance.</p> <code>None</code> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>def __init__(\n\tself,\n\tsimilarity_threshold: float = 0.6,\n\tconfig_loader: \"ConfigLoader | None\" = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the resolver.\n\n\tArgs:\n\t    similarity_threshold: Threshold for group similarity to trigger merging (0.0-1.0).\n\t    config_loader: Optional ConfigLoader instance.\n\t\"\"\"\n\tif config_loader:\n\t\tself.config_loader = config_loader\n\telse:\n\t\tfrom codemap.config import ConfigLoader\n\n\t\tself.config_loader = ConfigLoader()\n\n\tself.similarity_threshold = similarity_threshold\n\n\t# Import here to avoid making sklearn a hard dependency\n\ttry:\n\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\tself.cosine_similarity = cosine_similarity\n\texcept ImportError as e:\n\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\tmsg = \"scikit-learn is required for file integrity resolution\"\n\t\traise ImportError(msg) from e\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.similarity_threshold","title":"similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.cosine_similarity","title":"cosine_similarity  <code>instance-attribute</code>","text":"<pre><code>cosine_similarity = cosine_similarity\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.calculate_group_similarity","title":"calculate_group_similarity","text":"<pre><code>calculate_group_similarity(\n\tgroup1: SemanticGroup,\n\tgroup2: SemanticGroup,\n\tchunk_embeddings: dict[DiffChunk, ndarray],\n) -&gt; float\n</code></pre> <p>Calculate similarity between two groups based on their chunks' embeddings.</p> <p>This method computes the average pairwise cosine similarity between all combinations of chunks from the two groups. The similarity is based on the semantic embeddings of the chunks' content.</p> <p>Process: 1. Extract embeddings for all chunks in both groups 2. Compute pairwise cosine similarities between each pair of chunks 3. Return the average similarity score</p> <p>Parameters:</p> Name Type Description Default <code>group1</code> <code>SemanticGroup</code> <p>First semantic group to compare</p> required <code>group2</code> <code>SemanticGroup</code> <p>Second semantic group to compare</p> required <code>chunk_embeddings</code> <code>dict[DiffChunk, ndarray]</code> <p>Dict mapping chunks to their embeddings</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Similarity score between 0 and 1, where: - 0 indicates completely unrelated changes - 1 indicates identical or extremely similar changes - Values around 0.6-0.8 typically indicate related functionality</p> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>def calculate_group_similarity(\n\tself, group1: \"SemanticGroup\", group2: \"SemanticGroup\", chunk_embeddings: dict[DiffChunk, np.ndarray]\n) -&gt; float:\n\t\"\"\"\n\tCalculate similarity between two groups based on their chunks' embeddings.\n\n\tThis method computes the average pairwise cosine similarity between all combinations\n\tof chunks from the two groups. The similarity is based on the semantic embeddings\n\tof the chunks' content.\n\n\tProcess:\n\t1. Extract embeddings for all chunks in both groups\n\t2. Compute pairwise cosine similarities between each pair of chunks\n\t3. Return the average similarity score\n\n\tArgs:\n\t    group1: First semantic group to compare\n\t    group2: Second semantic group to compare\n\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\tReturns:\n\t    float: Similarity score between 0 and 1, where:\n\t        - 0 indicates completely unrelated changes\n\t        - 1 indicates identical or extremely similar changes\n\t        - Values around 0.6-0.8 typically indicate related functionality\n\n\t\"\"\"\n\t# Get embeddings for chunks in each group\n\tembeddings1 = [chunk_embeddings[chunk] for chunk in group1.chunks if chunk in chunk_embeddings]\n\tembeddings2 = [chunk_embeddings[chunk] for chunk in group2.chunks if chunk in chunk_embeddings]\n\n\tif not embeddings1 or not embeddings2:\n\t\treturn 0.0\n\n\t# Calculate pairwise similarities\n\tsimilarities = []\n\tfor emb1 in embeddings1:\n\t\tfor emb2 in embeddings2:\n\t\t\tsim = self.cosine_similarity([emb1], [emb2])[0][0]\n\t\t\tsimilarities.append(sim)\n\n\t# Return average similarity\n\treturn sum(similarities) / len(similarities) if similarities else 0.0\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.resolve_violations","title":"resolve_violations","text":"<pre><code>resolve_violations(\n\tgroups: list[SemanticGroup],\n\tchunk_embeddings: dict[DiffChunk, ndarray],\n) -&gt; list[SemanticGroup]\n</code></pre> <p>Resolve file integrity violations by merging or reassigning chunks.</p> <p>A violation occurs when the same file appears in multiple semantic groups. This needs to be resolved because a file should be modified in only one commit.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>list[SemanticGroup]</code> <p>List of SemanticGroup objects to resolve</p> required <code>chunk_embeddings</code> <code>dict[DiffChunk, ndarray]</code> <p>Dict mapping chunks to their embeddings</p> required <p>Returns:</p> Type Description <code>list[SemanticGroup]</code> <p>List of SemanticGroup objects with all violations resolved</p> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>def resolve_violations(\n\tself, groups: list[\"SemanticGroup\"], chunk_embeddings: dict[DiffChunk, np.ndarray]\n) -&gt; list[\"SemanticGroup\"]:\n\t\"\"\"\n\tResolve file integrity violations by merging or reassigning chunks.\n\n\tA violation occurs when the same file appears in multiple semantic groups.\n\tThis needs to be resolved because a file should be modified in only one commit.\n\n\tArgs:\n\t    groups: List of SemanticGroup objects to resolve\n\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\tReturns:\n\t    List of SemanticGroup objects with all violations resolved\n\n\t\"\"\"\n\t# Keep iterating until no violations remain\n\twhile True:\n\t\t# Build file to groups mapping\n\t\tfile_to_groups: dict[str, list[int]] = {}\n\t\tfor i, group in enumerate(groups):\n\t\t\tfor file in group.files:\n\t\t\t\tif file not in file_to_groups:\n\t\t\t\t\tfile_to_groups[file] = []\n\t\t\t\tfile_to_groups[file].append(i)\n\n\t\t# Find violations (files in multiple groups)\n\t\tviolations = {file: indices for file, indices in file_to_groups.items() if len(indices) &gt; 1}\n\n\t\tif not violations:\n\t\t\tbreak  # No violations, we're done\n\n\t\t# Process the first violation\n\t\tfile = next(iter(violations))\n\t\tgroup_indices = violations[file]\n\n\t\t# Try to find groups to merge based on similarity\n\t\tmax_similarity = 0\n\t\tgroups_to_merge = None\n\n\t\t# Calculate similarities between all pairs of groups containing this file\n\t\tfor i in range(len(group_indices)):\n\t\t\tfor j in range(i + 1, len(group_indices)):\n\t\t\t\tidx1, idx2 = group_indices[i], group_indices[j]\n\t\t\t\tsimilarity = self.calculate_group_similarity(groups[idx1], groups[idx2], chunk_embeddings)\n\n\t\t\t\tif similarity &gt; max_similarity:\n\t\t\t\t\tmax_similarity = similarity\n\t\t\t\t\tgroups_to_merge = (idx1, idx2)\n\n\t\t# Decide whether to merge or reassign based on similarity threshold\n\t\tif max_similarity &gt;= self.similarity_threshold and groups_to_merge:\n\t\t\t# STRATEGY 1: Merge groups if they're similar enough\n\t\t\tidx1, idx2 = groups_to_merge\n\t\t\tmerged_group = groups[idx1].merge_with(groups[idx2])\n\n\t\t\t# Replace the first group with the merged one and remove the second\n\t\t\tgroups[idx1] = merged_group\n\t\t\tgroups.pop(idx2)\n\t\telse:\n\t\t\t# STRATEGY 2: Reassign chunks to the primary group for this file\n\t\t\t# Find the primary group (group with most chunks containing this file)\n\t\t\tfile_chunks_count = []\n\t\t\tfor idx in group_indices:\n\t\t\t\tcount = sum(1 for chunk in groups[idx].chunks if file in chunk.files)\n\t\t\t\tfile_chunks_count.append((idx, count))\n\n\t\t\t# Sort by count descending\n\t\t\tfile_chunks_count.sort(key=lambda x: x[1], reverse=True)\n\t\t\tprimary_idx = file_chunks_count[0][0]\n\n\t\t\t# Move chunks containing this file to the primary group\n\t\t\tfor idx in group_indices:\n\t\t\t\tif idx != primary_idx:\n\t\t\t\t\t# Find chunks containing this file\n\t\t\t\t\tchunks_to_move = [chunk for chunk in groups[idx].chunks if file in chunk.files]\n\n\t\t\t\t\t# Move chunks to primary group\n\t\t\t\t\tgroups[primary_idx].chunks.extend(chunks_to_move)\n\n\t\t\t\t\t# Remove moved chunks from original group\n\t\t\t\t\tgroups[idx].chunks = [chunk for chunk in groups[idx].chunks if file not in chunk.files]\n\n\t\t\t# Remove empty groups\n\t\t\tgroups = [group for group in groups if group.chunks]\n\n\treturn groups\n</code></pre>"},{"location":"api/llm/","title":"Llm Overview","text":"<p>LLM module for CodeMap.</p> <ul> <li>Api - API interaction for LLM services.</li> <li>Client - LLM client for unified access to language models.</li> <li>Errors - Error classes for LLM-related operations.</li> <li>Rag - RAG (Retrieval-Augmented Generation) functionalities for CodeMap.</li> <li>Utils - Utility functions for working with LLMs.</li> </ul>"},{"location":"api/llm/api/","title":"Api","text":"<p>API interaction for LLM services.</p>"},{"location":"api/llm/api/#codemap.llm.api.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.PydanticModelT","title":"PydanticModelT  <code>module-attribute</code>","text":"<pre><code>PydanticModelT = TypeVar('PydanticModelT', bound=BaseModel)\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.MessageDict","title":"MessageDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>Typed dictionary for LLM message structure.</p> Source code in <code>src/codemap/llm/api.py</code> <pre><code>class MessageDict(TypedDict):\n\t\"\"\"Typed dictionary for LLM message structure.\"\"\"\n\n\trole: Literal[\"user\", \"system\"]\n\tcontent: str\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.MessageDict.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Literal['user', 'system']\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.MessageDict.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(\n\tmodel: type[PydanticModelT], input_data: str | object\n) -&gt; PydanticModelT\n</code></pre> <p>Validate the schema of the input data.</p> Source code in <code>src/codemap/llm/api.py</code> <pre><code>def validate_schema(model: type[PydanticModelT], input_data: str | object) -&gt; PydanticModelT:\n\t\"\"\"Validate the schema of the input data.\"\"\"\n\tif isinstance(input_data, str):\n\t\treturn cast(\"PydanticModelT\", model.model_validate_json(input_data))\n\treturn cast(\"PydanticModelT\", model.model_validate(input_data))\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.call_llm_api","title":"call_llm_api","text":"<pre><code>call_llm_api(\n\tmessages: list[MessageDict],\n\tconfig_loader: ConfigLoader,\n\ttools: list[Tool] | None = None,\n\tpydantic_model: type[PydanticModelT] | None = None,\n) -&gt; str | PydanticModelT\n</code></pre> <p>Call an LLM API using pydantic-ai.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[MessageDict]</code> <p>The list of messages to send to the LLM</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance for additional configuration</p> required <code>tools</code> <code>list[Tool] | None</code> <p>Optional list of tools to use.</p> <code>None</code> <code>pydantic_model</code> <code>type[PydanticModelT] | None</code> <p>Optional Pydantic model class to structure the output.           If provided, the function will return an instance of this model.           Otherwise, it returns a string.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | PydanticModelT</code> <p>The generated response, either as a string or an instance of the pydantic_model.</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If pydantic-ai is not installed or the API call fails.</p> Source code in <code>src/codemap/llm/api.py</code> <pre><code>def call_llm_api(\n\tmessages: list[MessageDict],\n\tconfig_loader: ConfigLoader,\n\ttools: list[Tool] | None = None,\n\tpydantic_model: type[PydanticModelT] | None = None,\n) -&gt; str | PydanticModelT:\n\t\"\"\"\n\tCall an LLM API using pydantic-ai.\n\n\tArgs:\n\t    messages: The list of messages to send to the LLM\n\t    config_loader: ConfigLoader instance for additional configuration\n\t    tools: Optional list of tools to use.\n\t    pydantic_model: Optional Pydantic model class to structure the output.\n\t                  If provided, the function will return an instance of this model.\n\t                  Otherwise, it returns a string.\n\n\tReturns:\n\t    The generated response, either as a string or an instance of the pydantic_model.\n\n\tRaises:\n\t    LLMError: If pydantic-ai is not installed or the API call fails.\n\t\"\"\"\n\tif Agent is None or End is None or FinalResult is None:  # Check all imports\n\t\tmsg = \"Pydantic-AI library or its required types (AgentNode, End, FinalResult) not installed/found.\"\n\t\tlogger.exception(msg)\n\t\traise LLMError(msg) from None\n\n\t# Determine system prompt\n\tsystem_prompt_str = (\n\t\t\"You are an AI programming assistant. Follow the user's requirements carefully and to the letter.\"\n\t)\n\n\tfor msg in messages:\n\t\tif msg[\"role\"] == \"system\":\n\t\t\tsystem_prompt_str = msg[\"content\"]\n\t\t\tbreak\n\n\t# If an output_model is specified, pydantic-ai handles instructing the LLM for structured output.\n\t# So, no need to manually add schema instructions to the system_prompt_str here.\n\n\t# Determine the output_type for the Pydantic-AI Agent\n\tagent_output_type: type = pydantic_model if pydantic_model else str\n\n\t# Convert None to empty list if tools is None\n\tagent_tools: list[Tool] = tools or []\n\n\ttry:\n\t\t# Initialize Pydantic-AI Agent\n\t\tmodel_name = config_loader.get.llm.model\n\n\t\tif is_ollama_model(model_name):\n\t\t\tfrom pydantic_ai.models.openai import OpenAIModel\n\t\t\tfrom pydantic_ai.providers.openai import OpenAIProvider\n\n\t\t\tmodel_name = model_name.split(\":\", 1)[1]\n\n\t\t\tbase_url = config_loader.get.llm.base_url\n\t\t\tif base_url is None:\n\t\t\t\tbase_url = \"http://localhost:11434/v1\"\n\n\t\t\tollama_model = OpenAIModel(model_name=model_name, provider=OpenAIProvider(base_url=base_url))\n\n\t\t\tagent = Agent(\n\t\t\t\tollama_model,\n\t\t\t\ttools=agent_tools,\n\t\t\t\tsystem_prompt=system_prompt_str,\n\t\t\t\toutput_type=agent_output_type,\n\t\t\t)\n\t\telse:\n\t\t\tagent = Agent(\n\t\t\t\tmodel=config_loader.get.llm.model,\n\t\t\t\ttools=agent_tools,\n\t\t\t\tsystem_prompt=system_prompt_str,\n\t\t\t\toutput_type=agent_output_type,\n\t\t\t)\n\n\t\tif not any(msg.get(\"role\") == \"user\" for msg in messages):\n\t\t\tmsg = \"No user content found in messages for Pydantic-AI agent.\"\n\t\t\tlogger.exception(msg)\n\t\t\traise LLMError(msg)\n\n\t\tif not messages or messages[-1].get(\"role\") != \"user\":\n\t\t\tmsg = \"Last message is not an user prompt\"\n\t\t\tlogger.exception(msg)\n\t\t\traise LLMError(msg)\n\n\t\tuser_prompt = messages[-1][\"content\"]\n\n\t\tif ModelSettings is None:\n\t\t\tmsg = \"ModelSettings not found in pydantic-ai. Install the correct version.\"\n\t\t\tlogger.exception(msg)\n\t\t\traise LLMError(msg)\n\n\t\t# Run the agent and validate the output\n\t\tmodel_settings = ModelSettings(\n\t\t\ttemperature=float(config_loader.get.llm.temperature),\n\t\t\tmax_tokens=int(config_loader.get.llm.max_output_tokens),\n\t\t)\n\t\trun = agent.run_sync(user_prompt=user_prompt, model_settings=model_settings)\n\n\t\tif run.output is not None:\n\t\t\tif pydantic_model:\n\t\t\t\ttry:\n\t\t\t\t\treturn validate_schema(pydantic_model, run.output)\n\t\t\t\texcept ValidationError as e:\n\t\t\t\t\traise LLMError from e\n\t\t\telif isinstance(run.output, str):\n\t\t\t\treturn run.output\n\t\t\telif isinstance(run.output, BaseModel):\n\t\t\t\t# This shouldn't happen when pydantic_model is None, but handle it\n\t\t\t\treturn str(run.output)\n\n\t\tmsg = \"Pydantic-AI call succeeded but returned no structured data or text.\"\n\t\tlogger.error(msg)\n\t\traise LLMError(msg)\n\n\texcept ImportError:\n\t\tmsg = \"Pydantic-AI library not installed. Install it with 'uv add pydantic-ai'.\"\n\t\tlogger.exception(msg)\n\t\traise LLMError(msg) from None\n\texcept Exception as e:\n\t\tlogger.exception(\"Pydantic-AI LLM API call failed\")\n\t\tmsg = f\"Pydantic-AI LLM API call failed: {e}\"\n\t\traise LLMError(msg) from e\n</code></pre>"},{"location":"api/llm/client/","title":"Client","text":"<p>LLM client for unified access to language models.</p>"},{"location":"api/llm/client/#codemap.llm.client.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient","title":"LLMClient","text":"<p>Client for interacting with LLM services in a unified way.</p> Source code in <code>src/codemap/llm/client.py</code> <pre><code>class LLMClient:\n\t\"\"\"Client for interacting with LLM services in a unified way.\"\"\"\n\n\t# Default templates - empty in base class\n\tDEFAULT_TEMPLATES: ClassVar[dict[str, str]] = {}\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: ConfigLoader,\n\t\trepo_path: Path | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the LLM client.\n\n\t\tArgs:\n\t\t    config_loader: ConfigLoader instance to use\n\t\t    repo_path: Path to the repository (for loading configuration)\n\t\t\"\"\"\n\t\tself.repo_path = repo_path\n\t\tself.config_loader = config_loader\n\t\tself._templates = self.DEFAULT_TEMPLATES.copy()\n\n\tdef set_template(self, name: str, template: str) -&gt; None:\n\t\t\"\"\"\n\t\tSet a prompt template.\n\n\t\tArgs:\n\t\t    name: Template name\n\t\t    template: Template content\n\n\t\t\"\"\"\n\t\tself._templates[name] = template\n\n\tdef completion(\n\t\tself,\n\t\tmessages: list[MessageDict],\n\t\ttools: list[Tool] | None = None,\n\t\tpydantic_model: type[PydanticModelT] | None = None,\n\t) -&gt; str | PydanticModelT:\n\t\t\"\"\"\n\t\tGenerate text using the configured LLM.\n\n\t\tArgs:\n\t\t    messages: List of messages to send to the LLM\n\t\t    tools: Optional list of tools to use.\n\t\t    pydantic_model: Optional Pydantic model for response validation\n\n\t\tReturns:\n\t\t    Generated text or Pydantic model instance\n\n\t\tRaises:\n\t\t    LLMError: If the API call fails\n\n\t\t\"\"\"\n\t\t# Call the API\n\t\treturn call_llm_api(\n\t\t\tmessages=messages,\n\t\t\ttools=tools,\n\t\t\tpydantic_model=pydantic_model,\n\t\t\tconfig_loader=self.config_loader,\n\t\t)\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.DEFAULT_TEMPLATES","title":"DEFAULT_TEMPLATES  <code>class-attribute</code>","text":"<pre><code>DEFAULT_TEMPLATES: dict[str, str] = {}\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig_loader: ConfigLoader,\n\trepo_path: Path | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance to use</p> required <code>repo_path</code> <code>Path | None</code> <p>Path to the repository (for loading configuration)</p> <code>None</code> Source code in <code>src/codemap/llm/client.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: ConfigLoader,\n\trepo_path: Path | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the LLM client.\n\n\tArgs:\n\t    config_loader: ConfigLoader instance to use\n\t    repo_path: Path to the repository (for loading configuration)\n\t\"\"\"\n\tself.repo_path = repo_path\n\tself.config_loader = config_loader\n\tself._templates = self.DEFAULT_TEMPLATES.copy()\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.set_template","title":"set_template","text":"<pre><code>set_template(name: str, template: str) -&gt; None\n</code></pre> <p>Set a prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Template name</p> required <code>template</code> <code>str</code> <p>Template content</p> required Source code in <code>src/codemap/llm/client.py</code> <pre><code>def set_template(self, name: str, template: str) -&gt; None:\n\t\"\"\"\n\tSet a prompt template.\n\n\tArgs:\n\t    name: Template name\n\t    template: Template content\n\n\t\"\"\"\n\tself._templates[name] = template\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.completion","title":"completion","text":"<pre><code>completion(\n\tmessages: list[MessageDict],\n\ttools: list[Tool] | None = None,\n\tpydantic_model: type[PydanticModelT] | None = None,\n) -&gt; str | PydanticModelT\n</code></pre> <p>Generate text using the configured LLM.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[MessageDict]</code> <p>List of messages to send to the LLM</p> required <code>tools</code> <code>list[Tool] | None</code> <p>Optional list of tools to use.</p> <code>None</code> <code>pydantic_model</code> <code>type[PydanticModelT] | None</code> <p>Optional Pydantic model for response validation</p> <code>None</code> <p>Returns:</p> Type Description <code>str | PydanticModelT</code> <p>Generated text or Pydantic model instance</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If the API call fails</p> Source code in <code>src/codemap/llm/client.py</code> <pre><code>def completion(\n\tself,\n\tmessages: list[MessageDict],\n\ttools: list[Tool] | None = None,\n\tpydantic_model: type[PydanticModelT] | None = None,\n) -&gt; str | PydanticModelT:\n\t\"\"\"\n\tGenerate text using the configured LLM.\n\n\tArgs:\n\t    messages: List of messages to send to the LLM\n\t    tools: Optional list of tools to use.\n\t    pydantic_model: Optional Pydantic model for response validation\n\n\tReturns:\n\t    Generated text or Pydantic model instance\n\n\tRaises:\n\t    LLMError: If the API call fails\n\n\t\"\"\"\n\t# Call the API\n\treturn call_llm_api(\n\t\tmessages=messages,\n\t\ttools=tools,\n\t\tpydantic_model=pydantic_model,\n\t\tconfig_loader=self.config_loader,\n\t)\n</code></pre>"},{"location":"api/llm/errors/","title":"Errors","text":"<p>Error classes for LLM-related operations.</p>"},{"location":"api/llm/errors/#codemap.llm.errors.LLMError","title":"LLMError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for LLM-related errors.</p> Source code in <code>src/codemap/llm/errors.py</code> <pre><code>class LLMError(Exception):\n\t\"\"\"Base exception for LLM-related errors.\"\"\"\n</code></pre>"},{"location":"api/llm/utils/","title":"Utils","text":"<p>Utility functions for working with LLMs.</p>"},{"location":"api/llm/utils/#codemap.llm.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.load_prompt_template","title":"load_prompt_template","text":"<pre><code>load_prompt_template(\n\ttemplate_path: str | None,\n) -&gt; str | None\n</code></pre> <p>Load custom prompt template from file.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str | None</code> <p>Path to prompt template file</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Loaded template or None if loading failed</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def load_prompt_template(template_path: str | None) -&gt; str | None:\n\t\"\"\"\n\tLoad custom prompt template from file.\n\n\tArgs:\n\t    template_path: Path to prompt template file\n\n\tReturns:\n\t    Loaded template or None if loading failed\n\n\t\"\"\"\n\tif not template_path:\n\t\treturn None\n\n\ttry:\n\t\ttemplate_file = Path(template_path)\n\t\twith template_file.open(\"r\") as f:\n\t\t\treturn f.read()\n\texcept OSError:\n\t\tlogger.warning(\"Could not load prompt template: %s\", template_path)\n\t\treturn None\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.LLMResponseType","title":"LLMResponseType  <code>module-attribute</code>","text":"<pre><code>LLMResponseType = (\n\tdict[str, Any] | Mapping[str, Any] | object\n)\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.is_ollama_model","title":"is_ollama_model","text":"<pre><code>is_ollama_model(model_name: str) -&gt; bool\n</code></pre> <p>Check if the model name is an Ollama model.</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def is_ollama_model(model_name: str) -&gt; bool:\n\t\"\"\"Check if the model name is an Ollama model.\"\"\"\n\treturn model_name.startswith(\"ollama:\")\n</code></pre>"},{"location":"api/llm/rag/","title":"Rag Overview","text":"<p>RAG (Retrieval-Augmented Generation) functionalities for CodeMap.</p> <ul> <li>Agents - Agent Implementations for the RAG Commands.</li> <li>Ask - Ask command for the RAG.</li> <li>Do - Do command for the RAG.</li> <li>Interactive - Interactive LLM interface for CodeMap.</li> <li>Tools - Tool Implementations for the RAG-commands.</li> </ul>"},{"location":"api/llm/rag/interactive/","title":"Interactive","text":"<p>Interactive LLM interface for CodeMap.</p>"},{"location":"api/llm/rag/interactive/#codemap.llm.rag.interactive.RagUI","title":"RagUI","text":"<p>Interactive UI for the RAG process.</p> Source code in <code>src/codemap/llm/rag/interactive.py</code> <pre><code>class RagUI:\n\t\"\"\"Interactive UI for the RAG process.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the RAG UI.\"\"\"\n\t\tself.console = Console()\n\n\tdef format_ask_response(self, response_text: str | None) -&gt; Markdown:\n\t\t\"\"\"\n\t\tFormats the AI's response text using Rich Markdown.\n\n\t\tArgs:\n\t\t\tresponse_text (Optional[str]): The text response from the AI.\n\n\t\tReturns:\n\t\t\tMarkdown: A Rich Markdown object ready for printing.\n\n\t\t\"\"\"\n\t\tif response_text is None:\n\t\t\tresponse_text = \"*No response generated.*\"\n\t\t# Basic Markdown formatting. Can be enhanced later to detect code blocks,\n\t\t# file paths, etc., and apply specific styling or links.\n\t\treturn Markdown(response_text)\n\n\tdef print_ask_result(self, result: dict[str, Any]) -&gt; None:\n\t\t\"\"\"\n\t\tPrints the structured result of the ask command using Rich.\n\n\t\tArgs:\n\t\t\tresult (Dict[str, Any]): The structured result containing 'answer' and 'context'.\n\n\t\t\"\"\"\n\t\tanswer = result.get(\"answer\")\n\t\tcontext = result.get(\"context\", [])\n\n\t\t# Print the main answer\n\t\trich_print(Panel(self.format_ask_response(answer), title=\"[bold green]Answer[/]\", border_style=\"green\"))\n\n\t\t# Print the context used if there are any items\n\t\tif context:\n\t\t\t# Build a single string with all context items as a numbered list\n\t\t\tcontext_list = []\n\t\t\tfor i, item in enumerate(context, 1):\n\t\t\t\tfile_path = item.get(\"file_path\", \"Unknown\")\n\t\t\t\tstart_line = item.get(\"start_line\", -1)\n\t\t\t\tend_line = item.get(\"end_line\", -1)\n\t\t\t\tdistance = item.get(\"distance\", -1.0)\n\n\t\t\t\t# Create the list item text\n\t\t\t\tlocation = f\"{file_path}\"\n\t\t\t\tif start_line &gt; 0 and end_line &gt; 0:\n\t\t\t\t\tlocation += f\" (lines {start_line}-{end_line})\"\n\n\t\t\t\t# Format with relevance info\n\t\t\t\trelevance = f\"(similarity: {1 - distance:.2f})\" if distance &gt;= 0 else \"\"\n\t\t\t\tlist_item = f\"[bold cyan]{i}.[/bold cyan] {location} [dim]{relevance}[/dim]\"\n\t\t\t\tcontext_list.append(list_item)\n\n\t\t\t# Join all list items into a single string\n\t\t\tcontext_content = \"\\n\".join(context_list)\n\n\t\t\t# Print a single panel with all context items\n\t\t\trich_print(\n\t\t\t\tPanel(\n\t\t\t\t\tcontext_content, title=\"[bold yellow]Context Used[/]\", border_style=\"yellow\", title_align=\"center\"\n\t\t\t\t)\n\t\t\t)\n\n\t\t\trich_print()\n\n\tdef format_content_for_context(self, context_items: list[dict[str, Any]]) -&gt; str:\n\t\t\"\"\"\n\t\tFormat context items into a string suitable for inclusion in prompts.\n\n\t\tArgs:\n\t\t\tcontext_items: List of context dictionaries with file_path, start_line, end_line, and content\n\n\t\tReturns:\n\t\t\tFormatted string with code snippets and file information\n\n\t\t\"\"\"\n\t\tif not context_items:\n\t\t\treturn \"No relevant code found in the repository.\"\n\n\t\tformatted_parts = []\n\n\t\tfor i, item in enumerate(context_items, 1):\n\t\t\t# Extract file information\n\t\t\tfile_path = item.get(\"file_path\", \"Unknown file\")\n\t\t\tstart_line = item.get(\"start_line\", -1)\n\t\t\tend_line = item.get(\"end_line\", -1)\n\t\t\tcontent = item.get(\"content\", \"\")\n\n\t\t\t# Create a header with file info\n\t\t\theader = f\"[{i}] {file_path}\"\n\t\t\tif start_line &gt; 0 and end_line &gt; 0:\n\t\t\t\theader += f\" (lines {start_line}-{end_line})\"\n\n\t\t\t# Format the code snippet with the header\n\t\t\tformatted_parts.append(f\"{header}\\n{'-' * len(header)}\\n{content}\\n\")\n\n\t\treturn \"\\n\\n\".join(formatted_parts)\n</code></pre>"},{"location":"api/llm/rag/interactive/#codemap.llm.rag.interactive.RagUI.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the RAG UI.</p> Source code in <code>src/codemap/llm/rag/interactive.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the RAG UI.\"\"\"\n\tself.console = Console()\n</code></pre>"},{"location":"api/llm/rag/interactive/#codemap.llm.rag.interactive.RagUI.console","title":"console  <code>instance-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/llm/rag/interactive/#codemap.llm.rag.interactive.RagUI.format_ask_response","title":"format_ask_response","text":"<pre><code>format_ask_response(response_text: str | None) -&gt; Markdown\n</code></pre> <p>Formats the AI's response text using Rich Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>response_text</code> <code>Optional[str]</code> <p>The text response from the AI.</p> required <p>Returns:</p> Name Type Description <code>Markdown</code> <code>Markdown</code> <p>A Rich Markdown object ready for printing.</p> Source code in <code>src/codemap/llm/rag/interactive.py</code> <pre><code>def format_ask_response(self, response_text: str | None) -&gt; Markdown:\n\t\"\"\"\n\tFormats the AI's response text using Rich Markdown.\n\n\tArgs:\n\t\tresponse_text (Optional[str]): The text response from the AI.\n\n\tReturns:\n\t\tMarkdown: A Rich Markdown object ready for printing.\n\n\t\"\"\"\n\tif response_text is None:\n\t\tresponse_text = \"*No response generated.*\"\n\t# Basic Markdown formatting. Can be enhanced later to detect code blocks,\n\t# file paths, etc., and apply specific styling or links.\n\treturn Markdown(response_text)\n</code></pre>"},{"location":"api/llm/rag/interactive/#codemap.llm.rag.interactive.RagUI.print_ask_result","title":"print_ask_result","text":"<pre><code>print_ask_result(result: dict[str, Any]) -&gt; None\n</code></pre> <p>Prints the structured result of the ask command using Rich.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Dict[str, Any]</code> <p>The structured result containing 'answer' and 'context'.</p> required Source code in <code>src/codemap/llm/rag/interactive.py</code> <pre><code>def print_ask_result(self, result: dict[str, Any]) -&gt; None:\n\t\"\"\"\n\tPrints the structured result of the ask command using Rich.\n\n\tArgs:\n\t\tresult (Dict[str, Any]): The structured result containing 'answer' and 'context'.\n\n\t\"\"\"\n\tanswer = result.get(\"answer\")\n\tcontext = result.get(\"context\", [])\n\n\t# Print the main answer\n\trich_print(Panel(self.format_ask_response(answer), title=\"[bold green]Answer[/]\", border_style=\"green\"))\n\n\t# Print the context used if there are any items\n\tif context:\n\t\t# Build a single string with all context items as a numbered list\n\t\tcontext_list = []\n\t\tfor i, item in enumerate(context, 1):\n\t\t\tfile_path = item.get(\"file_path\", \"Unknown\")\n\t\t\tstart_line = item.get(\"start_line\", -1)\n\t\t\tend_line = item.get(\"end_line\", -1)\n\t\t\tdistance = item.get(\"distance\", -1.0)\n\n\t\t\t# Create the list item text\n\t\t\tlocation = f\"{file_path}\"\n\t\t\tif start_line &gt; 0 and end_line &gt; 0:\n\t\t\t\tlocation += f\" (lines {start_line}-{end_line})\"\n\n\t\t\t# Format with relevance info\n\t\t\trelevance = f\"(similarity: {1 - distance:.2f})\" if distance &gt;= 0 else \"\"\n\t\t\tlist_item = f\"[bold cyan]{i}.[/bold cyan] {location} [dim]{relevance}[/dim]\"\n\t\t\tcontext_list.append(list_item)\n\n\t\t# Join all list items into a single string\n\t\tcontext_content = \"\\n\".join(context_list)\n\n\t\t# Print a single panel with all context items\n\t\trich_print(\n\t\t\tPanel(\n\t\t\t\tcontext_content, title=\"[bold yellow]Context Used[/]\", border_style=\"yellow\", title_align=\"center\"\n\t\t\t)\n\t\t)\n\n\t\trich_print()\n</code></pre>"},{"location":"api/llm/rag/interactive/#codemap.llm.rag.interactive.RagUI.format_content_for_context","title":"format_content_for_context","text":"<pre><code>format_content_for_context(\n\tcontext_items: list[dict[str, Any]],\n) -&gt; str\n</code></pre> <p>Format context items into a string suitable for inclusion in prompts.</p> <p>Parameters:</p> Name Type Description Default <code>context_items</code> <code>list[dict[str, Any]]</code> <p>List of context dictionaries with file_path, start_line, end_line, and content</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string with code snippets and file information</p> Source code in <code>src/codemap/llm/rag/interactive.py</code> <pre><code>def format_content_for_context(self, context_items: list[dict[str, Any]]) -&gt; str:\n\t\"\"\"\n\tFormat context items into a string suitable for inclusion in prompts.\n\n\tArgs:\n\t\tcontext_items: List of context dictionaries with file_path, start_line, end_line, and content\n\n\tReturns:\n\t\tFormatted string with code snippets and file information\n\n\t\"\"\"\n\tif not context_items:\n\t\treturn \"No relevant code found in the repository.\"\n\n\tformatted_parts = []\n\n\tfor i, item in enumerate(context_items, 1):\n\t\t# Extract file information\n\t\tfile_path = item.get(\"file_path\", \"Unknown file\")\n\t\tstart_line = item.get(\"start_line\", -1)\n\t\tend_line = item.get(\"end_line\", -1)\n\t\tcontent = item.get(\"content\", \"\")\n\n\t\t# Create a header with file info\n\t\theader = f\"[{i}] {file_path}\"\n\t\tif start_line &gt; 0 and end_line &gt; 0:\n\t\t\theader += f\" (lines {start_line}-{end_line})\"\n\n\t\t# Format the code snippet with the header\n\t\tformatted_parts.append(f\"{header}\\n{'-' * len(header)}\\n{content}\\n\")\n\n\treturn \"\\n\\n\".join(formatted_parts)\n</code></pre>"},{"location":"api/llm/rag/agents/","title":"Agents Overview","text":"<p>Agent Implementations for the RAG Commands.</p>"},{"location":"api/llm/rag/ask/","title":"Ask Overview","text":"<p>Ask command for the RAG.</p> <ul> <li>Command - Command for asking questions about the codebase using RAG.</li> <li>Prompts - Prompts for the ask command.</li> </ul>"},{"location":"api/llm/rag/ask/command/","title":"Command","text":"<p>Command for asking questions about the codebase using RAG.</p>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskResult","title":"AskResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>Structured result for the ask command.</p> Source code in <code>src/codemap/llm/rag/ask/command.py</code> <pre><code>class AskResult(TypedDict):\n\t\"\"\"Structured result for the ask command.\"\"\"\n\n\tanswer: str | None\n\tcontext: list[dict[str, Any]]\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskResult.answer","title":"answer  <code>instance-attribute</code>","text":"<pre><code>answer: str | None\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskResult.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: list[dict[str, Any]]\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand","title":"AskCommand","text":"<p>Handles the logic for the <code>codemap ask</code> command.</p> <p>Interacts with the ProcessingPipeline, DatabaseClient, and an LLM to answer questions about the codebase using RAG. Maintains conversation history for interactive sessions.</p> Source code in <code>src/codemap/llm/rag/ask/command.py</code> <pre><code>class AskCommand:\n\t\"\"\"\n\tHandles the logic for the `codemap ask` command.\n\n\tInteracts with the ProcessingPipeline, DatabaseClient, and an LLM to\n\tanswer questions about the codebase using RAG. Maintains conversation\n\thistory for interactive sessions.\n\n\t\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the AskCommand with lazy-loaded dependencies.\"\"\"\n\t\tself.config_loader = ConfigLoader.get_instance()\n\t\tself.ui = RagUI()\n\t\tself.session_id = str(uuid.uuid4())\n\t\tself._db_client: DatabaseClient | None = None\n\t\tself._llm_client: LLMClient | None = None\n\t\tself._pipeline: ProcessingPipeline | None = None\n\t\tself._max_context_length: int = 8000  # Default value\n\t\tself._max_context_results: int = 10  # Default value\n\n\t@property\n\tdef db_client(self) -&gt; DatabaseClient:\n\t\t\"\"\"Lazily initialize and return a DatabaseClient instance.\"\"\"\n\t\tif self._db_client is None:\n\t\t\tself._db_client = DatabaseClient()\n\t\treturn self._db_client\n\n\t@property\n\tdef llm_client(self) -&gt; LLMClient:\n\t\t\"\"\"Lazily initialize and return an LLMClient instance.\"\"\"\n\t\tif self._llm_client is None:\n\t\t\tself._llm_client = LLMClient(config_loader=self.config_loader)\n\t\treturn self._llm_client\n\n\t@property\n\tdef pipeline(self) -&gt; ProcessingPipeline | None:\n\t\t\"\"\"Lazily initialize and return a ProcessingPipeline instance, or None if initialization fails.\"\"\"\n\t\tif self._pipeline is None:\n\t\t\ttry:\n\t\t\t\twith progress_indicator(message=\"Initializing processing pipeline...\", style=\"spinner\", transient=True):\n\t\t\t\t\tself._pipeline = ProcessingPipeline(config_loader=self.config_loader)\n\t\t\t\tlogger.info(\"ProcessingPipeline initialization complete.\")\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Failed to initialize ProcessingPipeline\")\n\n\t\treturn self._pipeline\n\n\t@property\n\tdef max_context_length(self) -&gt; int:\n\t\t\"\"\"Return the maximum context length for RAG, using config or default.\"\"\"\n\t\tcached = getattr(self, \"_max_context_length\", None)\n\t\tif cached is not None:\n\t\t\treturn cached\n\t\ttry:\n\t\t\trag_config = self.config_loader.get.rag\n\t\t\tvalue = getattr(rag_config, \"max_context_length\", None)\n\t\t\tif value is not None:\n\t\t\t\tself._max_context_length = value\n\t\t\t\treturn value\n\t\texcept (AttributeError, TypeError) as e:\n\t\t\tlogger.debug(\"Error reading max_context_length from config: %s\", e)\n\t\treturn self._max_context_length\n\n\t@property\n\tdef max_context_results(self) -&gt; int:\n\t\t\"\"\"Return the maximum number of context results for RAG, using config or default.\"\"\"\n\t\tcached = getattr(self, \"_max_context_results\", None)\n\t\tif cached is not None:\n\t\t\treturn cached\n\t\ttry:\n\t\t\trag_config = self.config_loader.get.rag\n\t\t\tvalue = getattr(rag_config, \"max_context_results\", None)\n\t\t\tif value is not None:\n\t\t\t\tself._max_context_results = value\n\t\t\t\treturn value\n\t\texcept (AttributeError, TypeError) as e:\n\t\t\tlogger.debug(\"Error reading max_context_results from config: %s\", e)\n\t\treturn self._max_context_results\n\n\tasync def initialize(self) -&gt; None:\n\t\t\"\"\"Perform asynchronous initialization for the command, especially the pipeline.\"\"\"\n\t\tif self.pipeline and not self.pipeline.is_async_initialized:\n\t\t\ttry:\n\t\t\t\t# Show a spinner while initializing the pipeline asynchronously\n\t\t\t\twith progress_indicator(\n\t\t\t\t\tmessage=\"Initializing async components (pipeline)...\", style=\"spinner\", transient=True\n\t\t\t\t):\n\t\t\t\t\tawait self.pipeline.async_init(sync_on_init=True)\n\t\t\t\tlogger.info(\"ProcessingPipeline async initialization complete.\")\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Failed during async initialization of ProcessingPipeline\")\n\t\t\t\t# Optionally set pipeline to None or handle the error appropriately\n\t\t\t\tself._pipeline = None\n\t\telif not self.pipeline:\n\t\t\tlogger.error(\"Cannot perform async initialization: ProcessingPipeline failed to initialize earlier.\")\n\t\telse:\n\t\t\tlogger.info(\"AskCommand async components already initialized.\")\n\n\tasync def run(self, question: str) -&gt; AskResult:\n\t\t\"\"\"Executes one turn of the ask command, returning the answer and context.\"\"\"\n\t\tlogger.info(f\"Processing question for session {self.session_id}: '{question}'\")\n\n\t\t# Ensure async initialization happened (idempotent check inside)\n\t\tawait self.initialize()\n\n\t\tif not self.pipeline:\n\t\t\treturn AskResult(answer=\"Processing pipeline not available.\", context=[])\n\n\t\t# Construct prompt text from the context and question\n\t\tmessages: list[MessageDict] = [\n\t\t\t{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n\t\t\t{\"role\": \"user\", \"content\": f\"Here's my question about the codebase: {question}\"},\n\t\t]\n\n\t\t# Store user query in DB\n\t\tdb_entry_id = None\n\t\ttry:\n\t\t\tdb_entry = self.db_client.add_chat_message(session_id=self.session_id, user_query=question)\n\t\t\tdb_entry_id = db_entry.id if db_entry else None\n\t\t\tif db_entry_id:\n\t\t\t\tlogger.debug(f\"Stored current query turn with DB ID: {db_entry_id}\")\n\t\t\telse:\n\t\t\t\tlogger.warning(\"Failed to get DB entry ID for current query turn.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to store current query turn in DB\")\n\n\t\t# Call LLM with context\n\t\ttry:\n\t\t\twith progress_indicator(\"Waiting for LLM response...\"):\n\t\t\t\tanswer = self.llm_client.completion(\n\t\t\t\t\tmessages=messages,\n\t\t\t\t\ttools=[read_file_tool, semantic_retrieval_tool, web_search_tool()],\n\t\t\t\t)\n\t\t\tlogger.debug(f\"LLM response: {answer}\")\n\n\t\t\t# Update DB with answer using the dedicated client method\n\t\t\tif db_entry_id and answer:\n\t\t\t\t# The update_chat_response method handles its own exceptions and returns success/failure\n\t\t\t\tsuccess = self.db_client.update_chat_response(message_id=db_entry_id, ai_response=answer)\n\t\t\t\tif not success:\n\t\t\t\t\tlogger.warning(f\"Failed to update DB entry {db_entry_id} via client method.\")\n\n\t\t\treturn AskResult(answer=answer, context=[])\n\t\texcept Exception as e:  # Keep the outer exception for LLM call errors\n\t\t\tlogger.exception(\"Error during LLM completion\")\n\t\t\treturn AskResult(answer=f\"Error: {e!s}\", context=[])\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the AskCommand with lazy-loaded dependencies.</p> Source code in <code>src/codemap/llm/rag/ask/command.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the AskCommand with lazy-loaded dependencies.\"\"\"\n\tself.config_loader = ConfigLoader.get_instance()\n\tself.ui = RagUI()\n\tself.session_id = str(uuid.uuid4())\n\tself._db_client: DatabaseClient | None = None\n\tself._llm_client: LLMClient | None = None\n\tself._pipeline: ProcessingPipeline | None = None\n\tself._max_context_length: int = 8000  # Default value\n\tself._max_context_results: int = 10  # Default value\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = get_instance()\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.ui","title":"ui  <code>instance-attribute</code>","text":"<pre><code>ui = RagUI()\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id = str(uuid4())\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.db_client","title":"db_client  <code>property</code>","text":"<pre><code>db_client: DatabaseClient\n</code></pre> <p>Lazily initialize and return a DatabaseClient instance.</p>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.llm_client","title":"llm_client  <code>property</code>","text":"<pre><code>llm_client: LLMClient\n</code></pre> <p>Lazily initialize and return an LLMClient instance.</p>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.pipeline","title":"pipeline  <code>property</code>","text":"<pre><code>pipeline: ProcessingPipeline | None\n</code></pre> <p>Lazily initialize and return a ProcessingPipeline instance, or None if initialization fails.</p>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.max_context_length","title":"max_context_length  <code>property</code>","text":"<pre><code>max_context_length: int\n</code></pre> <p>Return the maximum context length for RAG, using config or default.</p>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.max_context_results","title":"max_context_results  <code>property</code>","text":"<pre><code>max_context_results: int\n</code></pre> <p>Return the maximum number of context results for RAG, using config or default.</p>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize() -&gt; None\n</code></pre> <p>Perform asynchronous initialization for the command, especially the pipeline.</p> Source code in <code>src/codemap/llm/rag/ask/command.py</code> <pre><code>async def initialize(self) -&gt; None:\n\t\"\"\"Perform asynchronous initialization for the command, especially the pipeline.\"\"\"\n\tif self.pipeline and not self.pipeline.is_async_initialized:\n\t\ttry:\n\t\t\t# Show a spinner while initializing the pipeline asynchronously\n\t\t\twith progress_indicator(\n\t\t\t\tmessage=\"Initializing async components (pipeline)...\", style=\"spinner\", transient=True\n\t\t\t):\n\t\t\t\tawait self.pipeline.async_init(sync_on_init=True)\n\t\t\tlogger.info(\"ProcessingPipeline async initialization complete.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed during async initialization of ProcessingPipeline\")\n\t\t\t# Optionally set pipeline to None or handle the error appropriately\n\t\t\tself._pipeline = None\n\telif not self.pipeline:\n\t\tlogger.error(\"Cannot perform async initialization: ProcessingPipeline failed to initialize earlier.\")\n\telse:\n\t\tlogger.info(\"AskCommand async components already initialized.\")\n</code></pre>"},{"location":"api/llm/rag/ask/command/#codemap.llm.rag.ask.command.AskCommand.run","title":"run  <code>async</code>","text":"<pre><code>run(question: str) -&gt; AskResult\n</code></pre> <p>Executes one turn of the ask command, returning the answer and context.</p> Source code in <code>src/codemap/llm/rag/ask/command.py</code> <pre><code>async def run(self, question: str) -&gt; AskResult:\n\t\"\"\"Executes one turn of the ask command, returning the answer and context.\"\"\"\n\tlogger.info(f\"Processing question for session {self.session_id}: '{question}'\")\n\n\t# Ensure async initialization happened (idempotent check inside)\n\tawait self.initialize()\n\n\tif not self.pipeline:\n\t\treturn AskResult(answer=\"Processing pipeline not available.\", context=[])\n\n\t# Construct prompt text from the context and question\n\tmessages: list[MessageDict] = [\n\t\t{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n\t\t{\"role\": \"user\", \"content\": f\"Here's my question about the codebase: {question}\"},\n\t]\n\n\t# Store user query in DB\n\tdb_entry_id = None\n\ttry:\n\t\tdb_entry = self.db_client.add_chat_message(session_id=self.session_id, user_query=question)\n\t\tdb_entry_id = db_entry.id if db_entry else None\n\t\tif db_entry_id:\n\t\t\tlogger.debug(f\"Stored current query turn with DB ID: {db_entry_id}\")\n\t\telse:\n\t\t\tlogger.warning(\"Failed to get DB entry ID for current query turn.\")\n\texcept Exception:\n\t\tlogger.exception(\"Failed to store current query turn in DB\")\n\n\t# Call LLM with context\n\ttry:\n\t\twith progress_indicator(\"Waiting for LLM response...\"):\n\t\t\tanswer = self.llm_client.completion(\n\t\t\t\tmessages=messages,\n\t\t\t\ttools=[read_file_tool, semantic_retrieval_tool, web_search_tool()],\n\t\t\t)\n\t\tlogger.debug(f\"LLM response: {answer}\")\n\n\t\t# Update DB with answer using the dedicated client method\n\t\tif db_entry_id and answer:\n\t\t\t# The update_chat_response method handles its own exceptions and returns success/failure\n\t\t\tsuccess = self.db_client.update_chat_response(message_id=db_entry_id, ai_response=answer)\n\t\t\tif not success:\n\t\t\t\tlogger.warning(f\"Failed to update DB entry {db_entry_id} via client method.\")\n\n\t\treturn AskResult(answer=answer, context=[])\n\texcept Exception as e:  # Keep the outer exception for LLM call errors\n\t\tlogger.exception(\"Error during LLM completion\")\n\t\treturn AskResult(answer=f\"Error: {e!s}\", context=[])\n</code></pre>"},{"location":"api/llm/rag/ask/prompts/","title":"Prompts","text":"<p>Prompts for the ask command.</p>"},{"location":"api/llm/rag/ask/prompts/#codemap.llm.rag.ask.prompts.SYSTEM_PROMPT","title":"SYSTEM_PROMPT  <code>module-attribute</code>","text":"<pre><code>SYSTEM_PROMPT = \"\\nYou are a helpful AI assistant integrated into the CodeMap tool.\\nYou'll be given a user question about their codebase along with relevant code chunks from the codebase.\\nProvide concise answers based on the context provided.\\nInclude relevant file paths and code snippets in your response when applicable.\\nFocus on answering the question based *only* on the provided context.\\nIf the provided context doesn't contain enough information to answer the question, say so clearly.\\nDo not make assumptions or provide information not directly present in the provided context.\\n\"\n</code></pre>"},{"location":"api/llm/rag/do/","title":"Do Overview","text":"<p>Do command for the RAG.</p>"},{"location":"api/llm/rag/tools/","title":"Tools Overview","text":"<p>Tool Implementations for the RAG-commands.</p> <ul> <li>Read File - Read file tool for PydanticAI agents to search and read file content.</li> <li>Semantic Retrieval - Retrieval tool for PydanticAI agents to search and retrieve code context.</li> <li>Web Search - Web search tool for PydanticAI agents to search the web.</li> </ul>"},{"location":"api/llm/rag/tools/read_file/","title":"Read File","text":"<p>Read file tool for PydanticAI agents to search and read file content.</p>"},{"location":"api/llm/rag/tools/read_file/#codemap.llm.rag.tools.read_file.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/rag/tools/read_file/#codemap.llm.rag.tools.read_file.MAX_FILES_TO_DISPLAY","title":"MAX_FILES_TO_DISPLAY  <code>module-attribute</code>","text":"<pre><code>MAX_FILES_TO_DISPLAY = 5\n</code></pre>"},{"location":"api/llm/rag/tools/read_file/#codemap.llm.rag.tools.read_file.search_files_by_name","title":"search_files_by_name","text":"<pre><code>search_files_by_name(\n\tfilename: str, search_root: Path | None = None\n) -&gt; list[Path]\n</code></pre> <p>Search for files by name in the codebase.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name or partial name of the file to search for</p> required <code>search_root</code> <code>Path | None</code> <p>Root directory to search from (defaults to current working directory)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of matching file paths</p> Source code in <code>src/codemap/llm/rag/tools/read_file.py</code> <pre><code>def search_files_by_name(filename: str, search_root: Path | None = None) -&gt; list[Path]:\n\t\"\"\"Search for files by name in the codebase.\n\n\tArgs:\n\t    filename: Name or partial name of the file to search for\n\t    search_root: Root directory to search from (defaults to current working directory)\n\n\tReturns:\n\t    List of matching file paths\n\t\"\"\"\n\tif search_root is None:\n\t\tsearch_root = Path.cwd()\n\n\t# Search for exact matches first\n\tmatching_files = [file_path for file_path in search_root.rglob(filename) if file_path.is_file()]\n\n\t# If no exact matches, search for partial matches\n\tif not matching_files:\n\t\tmatching_files.extend(\n\t\t\t[\n\t\t\t\tfile_path\n\t\t\t\tfor file_path in search_root.rglob(\"*\")\n\t\t\t\tif file_path.is_file() and filename.lower() in file_path.name.lower()\n\t\t\t]\n\t\t)\n\n\treturn matching_files\n</code></pre>"},{"location":"api/llm/rag/tools/read_file/#codemap.llm.rag.tools.read_file.read_file_content","title":"read_file_content  <code>async</code>","text":"<pre><code>read_file_content(filename: str) -&gt; str\n</code></pre> <p>Read file content by searching for the file name in the codebase.</p> <p>This tool searches for files matching the given filename and returns their content. If multiple files match, it returns content for all matches.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name or partial name of the file to read</p> required <p>Returns:</p> Type Description <code>str</code> <p>String containing the file content(s) with formatting</p> Source code in <code>src/codemap/llm/rag/tools/read_file.py</code> <pre><code>async def read_file_content(filename: str) -&gt; str:\n\t\"\"\"Read file content by searching for the file name in the codebase.\n\n\tThis tool searches for files matching the given filename and returns their content.\n\tIf multiple files match, it returns content for all matches.\n\n\tArgs:\n\t    filename: Name or partial name of the file to read\n\n\tReturns:\n\t    String containing the file content(s) with formatting\n\t\"\"\"\n\ttry:\n\t\t# Search for matching files\n\t\tmatching_files = search_files_by_name(filename)\n\n\t\tif not matching_files:\n\t\t\treturn f\"No files found matching '{filename}'\"\n\n\t\t# If too many matches, limit and inform user\n\t\tif len(matching_files) &gt; MAX_FILES_TO_DISPLAY:\n\t\t\tmatching_files = matching_files[:MAX_FILES_TO_DISPLAY]\n\t\t\tresult = f\"Found {len(matching_files)} files matching '{filename}' (showing first 5):\\n\\n\"\n\t\telif len(matching_files) &gt; 1:\n\t\t\tresult = f\"Found {len(matching_files)} files matching '{filename}':\\n\\n\"\n\t\telse:\n\t\t\tresult = \"\"\n\n\t\t# Read and format content for each matching file\n\t\tfor i, file_path in enumerate(matching_files):\n\t\t\ttry:\n\t\t\t\t# Read file content asynchronously\n\t\t\t\tasync with aiofiles.open(file_path, encoding=\"utf-8\") as f:\n\t\t\t\t\tcontent = await f.read()\n\n\t\t\t\t# Get relative path for display\n\t\t\t\ttry:\n\t\t\t\t\tdisplay_path = file_path.relative_to(Path.cwd())\n\t\t\t\texcept ValueError:\n\t\t\t\t\tdisplay_path = file_path\n\n\t\t\t\t# Add file header and content\n\t\t\t\tif len(matching_files) &gt; 1:\n\t\t\t\t\tresult += f\"## File {i + 1}: {display_path}\\n\\n\"\n\t\t\t\telse:\n\t\t\t\t\tresult += f\"## {display_path}\\n\\n\"\n\n\t\t\t\t# Detect file extension for syntax highlighting\n\t\t\t\tfile_ext = file_path.suffix[1:] if file_path.suffix else \"text\"\n\n\t\t\t\tresult += f\"```{file_ext}\\n{content}\\n```\\n\\n\"\n\n\t\t\texcept (OSError, UnicodeDecodeError) as e:\n\t\t\t\tresult += f\"## {file_path}\\n\\n*Error reading file: {e}*\\n\\n\"\n\n\t\treturn result.strip()\n\n\texcept Exception as e:\n\t\tlogger.exception(\"Error in read_file_content\")\n\t\treturn f\"Error searching for or reading file '{filename}': {e}\"\n</code></pre>"},{"location":"api/llm/rag/tools/read_file/#codemap.llm.rag.tools.read_file.read_file_tool","title":"read_file_tool  <code>module-attribute</code>","text":"<pre><code>read_file_tool = Tool(\n\tread_file_content,\n\ttakes_ctx=False,\n\tname=\"read_file\",\n\tdescription=\"Search for and read file content from the codebase by filename. Provide the filename or partial filename to search for. Returns the complete file content with syntax highlighting. Can handle multiple matches if the filename is ambiguous.\",\n\tprepare=None,\n)\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/","title":"Semantic Retrieval","text":"<p>Retrieval tool for PydanticAI agents to search and retrieve code context.</p>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.MIN_FILE_LINES_FOR_PROCESSING","title":"MIN_FILE_LINES_FOR_PROCESSING  <code>module-attribute</code>","text":"<pre><code>MIN_FILE_LINES_FOR_PROCESSING = 3\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.MAX_METADATA_ONLY_LINES","title":"MAX_METADATA_ONLY_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_METADATA_ONLY_LINES = 2\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.RetrievalContext","title":"RetrievalContext","text":"<p>               Bases: <code>TypedDict</code></p> <p>Context information for the retrieval tool.</p> Source code in <code>src/codemap/llm/rag/tools/semantic_retrieval.py</code> <pre><code>class RetrievalContext(TypedDict):\n\t\"\"\"Context information for the retrieval tool.\"\"\"\n\n\tfile_path: str\n\tstart_line: int\n\tend_line: int\n\tcontent: str\n\tscore: float\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.RetrievalContext.file_path","title":"file_path  <code>instance-attribute</code>","text":"<pre><code>file_path: str\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.RetrievalContext.start_line","title":"start_line  <code>instance-attribute</code>","text":"<pre><code>start_line: int\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.RetrievalContext.end_line","title":"end_line  <code>instance-attribute</code>","text":"<pre><code>end_line: int\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.RetrievalContext.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.RetrievalContext.score","title":"score  <code>instance-attribute</code>","text":"<pre><code>score: float\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.gen_doc","title":"gen_doc","text":"<pre><code>gen_doc(target_path: Path) -&gt; str\n</code></pre> <p>Generate documentation for the given target path.</p> Source code in <code>src/codemap/llm/rag/tools/semantic_retrieval.py</code> <pre><code>def gen_doc(target_path: Path) -&gt; str:\n\t\"\"\"Generate documentation for the given target path.\"\"\"\n\tfile_tree: bool = False\n\tinclude_entity_graph: bool = False\n\n\tif target_path.is_dir():\n\t\tfile_tree = True\n\t\tinclude_entity_graph = True\n\n\tconfig = GenSchema(\n\t\tlod_level=\"skeleton\",\n\t\tinclude_entity_graph=include_entity_graph,\n\t\tinclude_tree=file_tree,\n\t\tmermaid_show_legend=False,\n\t\tmermaid_remove_unconnected=True,\n\t\tmermaid_styled=False,\n\t)\n\n\tconfig_loader = ConfigLoader.get_instance()\n\tgenerator = CodeMapGenerator(config)\n\n\tentities, metadata = process_codebase(target_path, config, config_loader=config_loader)\n\n\treturn generator.generate_documentation(entities, metadata)\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.consolidate_paths","title":"consolidate_paths","text":"<pre><code>consolidate_paths(\n\tpaths: list[Path], threshold: float = 0.8\n) -&gt; list[Path]\n</code></pre> <p>Consolidate file paths to directory paths when appropriate.</p> <p>If a combination of paths includes all files in a directory or over threshold of files in a directory, removes individual file paths and includes the directory path instead.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[Path]</code> <p>List of file paths to consolidate</p> required <code>threshold</code> <code>float</code> <p>Threshold for consolidation (default 0.8)</p> <code>0.8</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of consolidated paths (mix of files and directories)</p> Source code in <code>src/codemap/llm/rag/tools/semantic_retrieval.py</code> <pre><code>def consolidate_paths(paths: list[Path], threshold: float = 0.8) -&gt; list[Path]:\n\t\"\"\"Consolidate file paths to directory paths when appropriate.\n\n\tIf a combination of paths includes all files in a directory or over threshold\n\tof files in a directory, removes individual file paths and includes\n\tthe directory path instead.\n\n\tArgs:\n\t    paths: List of file paths to consolidate\n\t    threshold: Threshold for consolidation (default 0.8)\n\n\tReturns:\n\t    List of consolidated paths (mix of files and directories)\n\t\"\"\"\n\tfrom collections import defaultdict\n\n\tif not paths:\n\t\treturn []\n\n\t# Group paths by their immediate parent directory only\n\tdir_to_files: dict[Path, set[Path]] = defaultdict(set)\n\n\tfor path in paths:\n\t\tif path.is_file():\n\t\t\tparent_dir = path.parent\n\t\t\tdir_to_files[parent_dir].add(path)\n\n\tconsolidated_paths: list[Path] = []\n\tfiles_to_exclude: set[Path] = set()\n\n\t# Check each immediate parent directory for consolidation opportunities\n\tfor directory, files_in_dir in dir_to_files.items():\n\t\tif not directory.exists() or not directory.is_dir():\n\t\t\tcontinue\n\n\t\t# Get all files in the immediate directory only (excluding subdirectories)\n\t\ttry:\n\t\t\tall_files_in_dir = [f for f in directory.iterdir() if f.is_file()]\n\t\texcept (PermissionError, OSError):\n\t\t\t# Skip directories we can't read\n\t\t\tcontinue\n\n\t\tif not all_files_in_dir:\n\t\t\tcontinue\n\n\t\ttotal_files = len(all_files_in_dir)\n\t\tmatched_files = len(files_in_dir)\n\t\tcoverage_ratio = matched_files / total_files\n\n\t\t# If we have all files or over threshold coverage, consolidate to directory\n\t\tif coverage_ratio &gt;= threshold:\n\t\t\tconsolidated_paths.append(directory)\n\t\t\tfiles_to_exclude.update(files_in_dir)\n\n\t# Add remaining individual files that weren't consolidated\n\tfor path in paths:\n\t\tif path not in files_to_exclude:\n\t\t\tif path.is_file():\n\t\t\t\tconsolidated_paths.append(path)\n\t\t\telif path.is_dir():\n\t\t\t\t# Keep directory paths as-is\n\t\t\t\tconsolidated_paths.append(path)\n\n\treturn consolidated_paths\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.retrieve_code_context","title":"retrieve_code_context  <code>async</code>","text":"<pre><code>retrieve_code_context(query: str) -&gt; str\n</code></pre> <p>Retrieve relevant code chunks based on the query.</p> <p>This tool performs semantic search on the codebase and returns a formatted markdown string with the retrieved code context.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query to find relevant code</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string containing the formatted markdown of retrieved context.</p> Source code in <code>src/codemap/llm/rag/tools/semantic_retrieval.py</code> <pre><code>async def retrieve_code_context(query: str) -&gt; str:\n\t\"\"\"Retrieve relevant code chunks based on the query.\n\n\tThis tool performs semantic search on the codebase and returns\n\ta formatted markdown string with the retrieved code context.\n\n\tArgs:\n\t    query: Search query to find relevant code\n\n\tReturns:\n\t    A string containing the formatted markdown of retrieved context.\n\t\"\"\"\n\tpipeline = await ProcessingPipeline.get_instance()\n\tconfig_loader = ConfigLoader.get_instance()\n\n\tif not pipeline:\n\t\tlogger.warning(\"ProcessingPipeline not available, no context will be retrieved.\")\n\t\treturn \"Error: Could not retrieve or process context.\"\n\n\t# Use provided limit or configured default\n\tactual_limit = config_loader.get.rag.max_context_results\n\n\ttry:\n\t\tfrom codemap.processor.vector.schema import ChunkMetadataSchema\n\n\t\tlogger.info(f\"Retrieving context for query: '{query}', limit: {actual_limit}\")\n\n\t\t# Perform semantic search\n\t\tresults = await pipeline.semantic_search(query, k=actual_limit)\n\n\t\tpaths: list[dict[Path, float]] = []\n\n\t\tif not results:\n\t\t\tlogger.debug(\"Semantic search returned no results.\")\n\t\t\treturn \"No relevant code context found.\"\n\n\t\tfor r in results:\n\t\t\t# Extract relevant fields from payload\n\t\t\tpayload: ChunkMetadataSchema = r.get(\"payload\", {})\n\n\t\t\t# Get file metadata\n\t\t\tfile_path = payload.file_metadata.file_path\n\t\t\tstart_line = payload.start_line\n\t\t\tend_line = payload.end_line\n\t\t\tnum_lines = end_line - start_line + 1\n\n\t\t\t# Calculate coverage as lines covered vs total lines in file\n\t\t\ttry:\n\t\t\t\tasync with aiofiles.open(file_path, encoding=\"utf-8\") as f:\n\t\t\t\t\ttotal_lines = sum([1 async for _ in f])\n\t\t\t\tcoverage = num_lines / total_lines if total_lines &gt; 0 else 0\n\t\t\texcept (OSError, UnicodeDecodeError):\n\t\t\t\t# If we can't read the file, assign a default coverage\n\t\t\t\tcoverage = 0.1\n\n\t\t\t# Check if file_path already exists in paths\n\t\t\tfound = False\n\t\t\tfor _, path_dict in enumerate(paths):\n\t\t\t\tif Path(file_path) in path_dict:\n\t\t\t\t\t# Update existing coverage with maximum value\n\t\t\t\t\tcurrent_coverage = path_dict[Path(file_path)]\n\t\t\t\t\tpath_dict[Path(file_path)] = max(current_coverage, coverage)\n\t\t\t\t\tfound = True\n\t\t\t\t\tbreak\n\n\t\t\tif not found:\n\t\t\t\tpaths.append({Path(file_path): coverage})\n\n\t\tlogger.debug(f\"Semantic search returned {len(paths)} raw results.\")\n\n\t\t# Extract paths with coverage &gt; 0.5 threshold\n\t\tfiltered_paths: list[Path] = []\n\t\tfor path_dict in paths:\n\t\t\tfor path, coverage in path_dict.items():\n\t\t\t\tif coverage &gt; 0.5:  # noqa: PLR2004\n\t\t\t\t\tfiltered_paths.append(path)\n\n\t\tconsolidated_paths = consolidate_paths(filtered_paths)\n\t\tdocs = [gen_doc(path) for path in consolidated_paths]\n\n\t\t# Generate markdown context directly\n\t\treturn \"\\n\\n---\\n\\n\".join(docs)\n\n\texcept Exception:\n\t\tlogger.exception(\"Error retrieving context\")\n\t\treturn \"Error: Could not retrieve or process context.\"\n</code></pre>"},{"location":"api/llm/rag/tools/semantic_retrieval/#codemap.llm.rag.tools.semantic_retrieval.semantic_retrieval_tool","title":"semantic_retrieval_tool  <code>module-attribute</code>","text":"<pre><code>semantic_retrieval_tool = Tool(\n\tretrieve_code_context,\n\ttakes_ctx=False,\n\tname=\"semantic_retrieval\",\n\tdescription=\"Retrieve relevant context from the codebase using semantic search for the given query. The query should be technical and specific to the codebase. Mention keywords like functions, classes, modules, etc. if applicable.\",\n\tprepare=None,\n)\n</code></pre>"},{"location":"api/llm/rag/tools/web_search/","title":"Web Search","text":"<p>Web search tool for PydanticAI agents to search the web.</p>"},{"location":"api/llm/rag/tools/web_search/#codemap.llm.rag.tools.web_search.web_search_tool","title":"web_search_tool","text":"<pre><code>web_search_tool(\n\tduckduckgo_client: DDGS | None = None,\n\tmax_results: int | None = None,\n) -&gt; Tool\n</code></pre> <p>Creates a DuckDuckGo search tool.</p> <p>Parameters:</p> Name Type Description Default <code>duckduckgo_client</code> <code>DDGS | None</code> <p>The DuckDuckGo search client.</p> <code>None</code> <code>max_results</code> <code>int | None</code> <p>The maximum number of results. If None, returns results only from the first response.</p> <code>None</code> Source code in <code>src/codemap/llm/rag/tools/web_search.py</code> <pre><code>def web_search_tool(duckduckgo_client: DDGS | None = None, max_results: int | None = None) -&gt; Tool:\n\t\"\"\"Creates a DuckDuckGo search tool.\n\n\tArgs:\n\t    duckduckgo_client: The DuckDuckGo search client.\n\t    max_results: The maximum number of results. If None, returns results only from the first response.\n\t\"\"\"\n\treturn Tool(\n\t\tDuckDuckGoSearchTool(client=duckduckgo_client or DDGS(), max_results=max_results).__call__,\n\t\tname=\"web_search\",\n\t\tdescription=\"Searches the web for the given query and returns the results.\",\n\t)\n</code></pre>"},{"location":"api/processor/","title":"Processor Overview","text":"<p>CodeMap processor module.</p> <ul> <li>Hash Calculation - Module for calculating hierarchical repository checksums.</li> <li>Lod - Level of Detail (LOD) implementation for code analysis.</li> <li>Pipeline - Unified pipeline for CodeMap data processing, synchronization, and retrieval.</li> <li>Tree Sitter - Tree-sitter based code analysis.</li> <li>Utils - Processor Utilities Package.</li> <li>Vector - Vector processing package for CodeMap.</li> </ul>"},{"location":"api/processor/hash_calculation/","title":"Hash Calculation","text":"<p>Module for calculating hierarchical repository checksums.</p>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator","title":"RepoChecksumCalculator","text":"<p>Calculates a hierarchical checksum for a repository.</p> <p>Directory hashes are derived from the names and hashes of their children, making the checksum sensitive to content changes, additions, deletions, and renames.</p> Source code in <code>src/codemap/processor/hash_calculation.py</code> <pre><code>class RepoChecksumCalculator:\n\t\"\"\"\n\tCalculates a hierarchical checksum for a repository.\n\n\tDirectory hashes are derived from the names and hashes of their children,\n\tmaking the checksum sensitive to content changes, additions, deletions,\n\tand renames.\n\t\"\"\"\n\n\t_instances: ClassVar[dict[Path, \"RepoChecksumCalculator\"]] = {}\n\n\tdef __init__(\n\t\tself, repo_path: Path, git_context: \"GitRepoContext | None\" = None, config_loader: \"ConfigLoader | None\" = None\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the checksum calculator.\n\n\t\tPrefer using get_instance() to create or retrieve instances.\n\n\t\tArgs:\n\t\t    repo_path: Absolute path to the repository root.\n\t\t    git_context: Optional GitRepoContext, used for context like branch names\n\t\t                 (for future storage strategies) and potentially accessing\n\t\t                 configuration for checksum paths.\n\t\t    config_loader: Optional ConfigLoader, used for configuration\n\t\t\"\"\"\n\t\tif not repo_path.is_dir():\n\t\t\tmsg = f\"Repository path {repo_path} is not a valid directory.\"\n\t\t\traise ValueError(msg)\n\t\tself.repo_path = repo_path.resolve()\n\t\tself.git_context = git_context\n\t\tself.config_loader = config_loader\n\t\tself.all_nodes_map: dict[str, dict[str, str]] | None = None  # path -&gt; {\"type\": \"file\"|\"dir\", \"hash\": hash_val}\n\n\t\tif not self.config_loader:\n\t\t\t# Ensure we have a config loader instance to fetch default/user configs\n\t\t\tfrom codemap.config.config_loader import ConfigLoader\n\n\t\t\tself.config_loader = ConfigLoader().get_instance()\n\n\t\tself.checksums_base_dir = self.repo_path / \".codemap_cache\" / \"checksums\"\n\t\tself.checksums_base_dir.mkdir(parents=True, exist_ok=True)\n\n\t\t# Fetch exclude patterns from SyncSchema via ConfigLoader\n\t\t# This allows user overrides from .codemap.yml to be respected.\n\t\t# If no config file or specific settings, defaults from SyncSchema are used.\n\t\tapp_config = self.config_loader.get\n\t\tself.exclude_patterns_str: list[str] = list(app_config.sync.exclude_patterns[:])  # Start with config patterns\n\n\t\t# Custom .gitignore parsing is removed. We will use pygit2.path_is_ignored later.\n\n\t\t# Ensure .codemap_cache (or configured equivalent) is always excluded.\n\t\t# This specific path should ideally be part of the default config in SyncSchema\n\t\t# or managed via a dedicated configuration setting if its name/location is dynamic.\n\t\t# For now, adding it directly here if not already present via a generic pattern.\n\t\tcodemap_cache_pattern = r\"^\\.codemap_cache/\"\n\t\tif codemap_cache_pattern not in self.exclude_patterns_str:\n\t\t\tself.exclude_patterns_str.append(codemap_cache_pattern)\n\n\t\t# Also explicitly exclude the checksums directory we just defined\n\t\tchecksums_dir_relative = self.checksums_base_dir.relative_to(self.repo_path).as_posix()\n\t\tchecksums_dir_pattern = f\"^{re.escape(checksums_dir_relative)}/\"\n\t\tif checksums_dir_pattern not in self.exclude_patterns_str:\n\t\t\tself.exclude_patterns_str.append(checksums_dir_pattern)\n\n\t\tself.compiled_exclude_patterns: list[Pattern[str]] = [re.compile(p) for p in self.exclude_patterns_str]\n\t\tpatterns = [p.pattern for p in self.compiled_exclude_patterns]\n\t\tlogger.info(f\"RepoChecksumCalculator compiled CodeMap exclude patterns: {patterns}\")\n\n\t@classmethod\n\tdef get_instance(\n\t\tcls,\n\t\trepo_path: Path,\n\t\tgit_context: \"GitRepoContext | None\" = None,\n\t\tconfig_loader: \"ConfigLoader | None\" = None,\n\t) -&gt; \"RepoChecksumCalculator\":\n\t\t\"\"\"\n\t\tGets a cached instance of RepoChecksumCalculator for the given repo_path..\n\n\t\tArgs:\n\t\t    repo_path: Absolute or relative path to the repository root.\n\t\t    git_context: Optional GitRepoContext for the new instance if created.\n\t\t    config_loader: Optional ConfigLoader for the new instance if created.\n\n\t\tReturns:\n\t\t    An instance of RepoChecksumCalculator.\n\t\t\"\"\"\n\t\tresolved_path = repo_path.resolve()\n\t\tif resolved_path not in cls._instances:\n\t\t\tlogger.debug(f\"Creating new RepoChecksumCalculator instance for {resolved_path}\")\n\t\t\tinstance = cls(resolved_path, git_context, config_loader)\n\t\t\tcls._instances[resolved_path] = instance\n\t\telse:\n\t\t\tlogger.debug(f\"Reusing existing RepoChecksumCalculator instance for {resolved_path}\")\n\t\t\t# Update context if provided, as it might have changed (e.g., branch switch)\n\t\t\texisting_instance = cls._instances[resolved_path]\n\t\t\tif git_context is not None:\n\t\t\t\texisting_instance.git_context = git_context\n\t\t\tif config_loader is not None:\n\t\t\t\texisting_instance.config_loader = config_loader\n\t\treturn cls._instances[resolved_path]\n\n\tdef _hash_string(self, data: str) -&gt; str:\n\t\t\"\"\"Helper to hash a string using xxhash.xxh3_128_hexdigest for consistency.\"\"\"\n\t\thasher = xxhash.xxh3_128()\n\t\thasher.update(data.encode(\"utf-8\"))\n\t\treturn hasher.hexdigest()\n\n\tasync def _hash_file_content(self, file_path: Path) -&gt; str:\n\t\t\"\"\"Calculates xxhash.xxh3_128_hexdigest for a file's content.\n\n\t\tReads the file in chunks for efficiency with large files.\n\t\t\"\"\"\n\t\thasher = xxhash.xxh3_128()\n\t\ttry:\n\t\t\tasync with aiofiles.open(file_path, \"rb\") as f:\n\t\t\t\twhile True:\n\t\t\t\t\tchunk = await f.read(8192)  # 8KB chunks\n\t\t\t\t\tif not chunk:\n\t\t\t\t\t\tbreak\n\t\t\t\t\thasher.update(chunk)\n\t\t\treturn hasher.hexdigest()\n\t\texcept OSError:\n\t\t\tlogger.exception(f\"Error reading file content for {file_path}\")\n\t\t\treturn self._hash_string(f\"ERROR_READING_FILE:{file_path.name}\")\n\n\tdef _is_path_explicitly_excluded(self, path_to_check: Path) -&gt; tuple[bool, str]:\n\t\t\"\"\"\n\t\tChecks if a path should be excluded based on configured regex patterns.\n\n\t\tPatterns are matched against the relative path from the repository root.\n\n\t\tReturns a tuple: (is_excluded, reason_for_hash_if_excluded).\n\t\t\"\"\"\n\t\trelative_path_str: str\n\t\tif self.repo_path == path_to_check:  # Root itself cannot be excluded by patterns matching children\n\t\t\trelative_path_str = \".\"\n\t\telse:\n\t\t\ttry:\n\t\t\t\trelative_path_str = str(path_to_check.relative_to(self.repo_path).as_posix())\n\t\t\texcept ValueError:\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"Path {path_to_check} is not relative to repo root {self.repo_path}. Not excluding by pattern.\"\n\t\t\t\t)\n\t\t\t\treturn False, \"\"\n\n\t\tif relative_path_str.startswith(\".cache/\"):\n\t\t\tlogger.info(f\"Checking exclusion for .cache path: '{relative_path_str}'\")\n\n\t\t# 1. Check against CodeMap-specific patterns (from .codemap.yml and hardcoded)\n\t\tfor pattern_idx, compiled_pattern in enumerate(self.compiled_exclude_patterns):\n\t\t\tif compiled_pattern.search(relative_path_str):\n\t\t\t\toriginal_pattern_str = self.exclude_patterns_str[pattern_idx]\n\t\t\t\treason = f\"EXCLUDED_BY_CODEMAP_CONFIG_PATTERN:{original_pattern_str}:{relative_path_str}\"\n\t\t\t\tif relative_path_str == \".\":\n\t\t\t\t\tlogger.error(\n\t\t\t\t\t\tf\"Repository root ('.') EXCLUDED by CodeMap config pattern: \"\n\t\t\t\t\t\tf\"'{original_pattern_str}' (Regex: '{compiled_pattern.pattern}')\"\n\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\tf\"Path '{relative_path_str}' excluded by CodeMap config pattern '{original_pattern_str}'\"\n\t\t\t\t\t)\n\t\t\t\treturn True, reason\n\n\t\t# 2. If not excluded by CodeMap patterns, check Git's ignore status via pygit2\n\t\tif self.git_context and self.git_context.repo:\n\t\t\ttry:\n\t\t\t\t# For the repository root (\".\"), trust CodeMap config patterns primarily.\n\t\t\t\t# Avoid excluding the root based on path_is_ignored(\".\") due to observed discrepancies\n\t\t\t\t# where `git check-ignore -v .` says not ignored, but pygit2 says it is.\n\t\t\t\tif relative_path_str == \".\":\n\t\t\t\t\t# We already logged the result of path_is_ignored(\".\") earlier if it was True.\n\t\t\t\t\t# Here, we explicitly decide NOT to exclude the root based on that specific check.\n\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\"Skipping pygit2.path_is_ignored check for root '.' due to \"\n\t\t\t\t\t\t\"potential discrepancies. Only CodeMap config can exclude root.\"\n\t\t\t\t\t)\n\n\t\t\t\telif self.git_context.repo.path_is_ignored(relative_path_str):\n\t\t\t\t\t# This is for paths OTHER than the root \".\"\n\t\t\t\t\treason = f\"EXCLUDED_BY_GITIGNORE:{relative_path_str}\"\n\t\t\t\t\tlogger.debug(f\"Path '{relative_path_str}' is ignored by Git (.gitignore or similar).\")\n\t\t\t\t\treturn True, reason\n\t\t\texcept GitError as e:  # Specifically catch GitError\n\t\t\t\t# path_is_ignored can raise GitError for various reasons.\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"GitError checking git ignore status for '{relative_path_str}': {e}. \"\n\t\t\t\t\t\"Treating as not ignored by Git.\"\n\t\t\t\t)\n\t\t\texcept TypeError as e:  # Example of another specific error if relevant\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"TypeError checking git ignore status for '{relative_path_str}': {e}. Path type might be an issue.\"\n\t\t\t\t)\n\t\t\t# Add other specific exceptions if pygit2.path_is_ignored is known to raise them.\n\t\t\t# For truly unexpected errors, it might be better to let them propagate if they indicate a severe issue.\n\t\telse:\n\t\t\tlogger.debug(\"GitContext not available, skipping .gitignore check for path: %s\", relative_path_str)\n\n\t\t# If it wasn't excluded by any CodeMap pattern and (if GitContext was available) not by Git's ignore rules\n\t\tif relative_path_str.startswith(\".cache/\"):\n\t\t\tlogger.info(f\"Path '{relative_path_str}' (under .cache/) was NOT excluded by any method.\")\n\n\t\treturn False, \"\"\n\n\tasync def _calculate_node_hash_recursive(\n\t\tself, current_path: Path, current_nodes_map: dict[str, dict[str, str]]\n\t) -&gt; str:\n\t\t\"\"\"Recursively calculates the hash for a file or directory.\n\n\t\tPopulates current_nodes_map with {relative_path: {\"type\": \"file\"|\"dir\"|\"excluded\"|\"error_dir\"|\"unknown\",\n\t\t\"hash\": hash_val}} for all processed nodes.\n\t\tReturns the hash of the current_path node.\n\t\t\"\"\"\n\t\t# Use POSIX-style paths for consistency across OS, relative to repo root.\n\t\trelative_path_str = str(current_path.relative_to(self.repo_path).as_posix())\n\n\t\tif relative_path_str == \".\":  # Represent root as empty string for map keys if preferred, or \".\"\n\t\t\trelative_path_str = \"\"\n\n\t\tis_excluded, exclusion_hash_reason = self._is_path_explicitly_excluded(current_path)\n\t\tif is_excluded:\n\t\t\tnode_hash = self._hash_string(exclusion_hash_reason)\n\t\t\t# For excluded items, we still record them as 'excluded' type for completeness if needed.\n\t\t\t# Or simply don't add them to the map if they shouldn't affect parent hashes.\n\t\t\t# Current logic: excluded items affect parent hash via their unique exclusion_hash_reason.\n\t\t\tcurrent_nodes_map[relative_path_str] = {\"type\": \"excluded\", \"hash\": node_hash}\n\t\t\treturn node_hash\n\n\t\tif current_path.is_file():\n\t\t\tnode_hash = await self._hash_file_content(current_path)\n\t\t\tcurrent_nodes_map[relative_path_str] = {\"type\": \"file\", \"hash\": node_hash}\n\t\t\treturn node_hash\n\n\t\tif current_path.is_dir():\n\t\t\tchildren_info_for_hash: list[str] = []\n\t\t\ttry:\n\t\t\t\t# Sort children by name for deterministic hashing.\n\t\t\t\tchildren_paths_sync = list(current_path.iterdir())  # Sync part\n\t\t\t\tchildren_paths = sorted(children_paths_sync, key=lambda p: p.name)\n\t\t\texcept OSError:\n\t\t\t\tlogger.exception(f\"Error listing directory {current_path}\")\n\t\t\t\tnode_hash = self._hash_string(f\"ERROR_LISTING_DIR:{current_path.name}\")\n\t\t\t\tcurrent_nodes_map[relative_path_str] = {\"type\": \"error_dir\", \"hash\": node_hash}\n\t\t\t\treturn node_hash\n\n\t\t\tfor child_path in children_paths:\n\t\t\t\t# The recursive call populates current_nodes_map for the child and its descendants.\n\t\t\t\tchild_hash = await self._calculate_node_hash_recursive(child_path, current_nodes_map)\n\t\t\t\t# The directory's hash depends on its children's names and their hashes.\n\t\t\t\tchildren_info_for_hash.append(str(f\"{child_path.name}:{child_hash}\"))\n\n\t\t\t# Concatenate all children's \"name:hash\" strings.\n\t\t\t# An empty directory will hash an empty string.\n\t\t\tdir_content_representation = \"\".join(children_info_for_hash)\n\t\t\tnode_hash = self._hash_string(dir_content_representation)\n\n\t\t\tcurrent_nodes_map[relative_path_str] = {\"type\": \"dir\", \"hash\": node_hash}\n\t\t\treturn node_hash\n\t\t# Handles symlinks (if is_file/is_dir is false), broken links, or other types.\n\t\tlogger.warning(\n\t\t\tf\"Path {current_path} is not a file or directory (or is a broken symlink). Assigning a fixed hash.\"\n\t\t)\n\t\tnode_hash = self._hash_string(f\"UNKNOWN_TYPE:{current_path.name}\")\n\t\t# Store its hash if it needs to be part of the map.\n\t\t# Ensure relative_path_str is correctly derived for the root path itself if it's of unknown type\n\t\tmap_key = relative_path_str if relative_path_str else \".\"  # Use \".\" if relative_path_str became empty (root)\n\t\tcurrent_nodes_map[map_key] = {\"type\": \"unknown\", \"hash\": node_hash}\n\t\treturn node_hash\n\n\tdef _get_current_branch_checksum_dir(self) -&gt; Path | None:\n\t\tif not self.git_context:\n\t\t\tlogger.warning(\"GitContext not available, cannot determine current branch for checksum storage.\")\n\t\t\treturn None\n\n\t\tbranch_name = self.git_context.get_current_branch()\n\t\tsanitized_branch_name = self.sanitize_branch_name(branch_name)\n\n\t\tbranch_dir = self.checksums_base_dir / sanitized_branch_name\n\t\tbranch_dir.mkdir(parents=True, exist_ok=True)\n\t\treturn branch_dir\n\n\tdef _write_checksum_data(self, root_hash: str, nodes_map: dict[str, dict[str, str]]) -&gt; Path | None:\n\t\tbranch_dir = self._get_current_branch_checksum_dir()\n\t\tif not branch_dir:\n\t\t\tlogger.error(\"Could not determine branch-specific directory. Cannot write checksum data.\")\n\t\t\treturn None\n\n\t\ttimestamp = datetime.now(UTC).strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n\t\t# Optionally include part of root_hash in filename for quick identification,\n\t\t# though timestamp should be unique enough.\n\t\t# short_root_hash = root_hash[:8]\n\t\t# checksum_file_name = f\"{timestamp}_{short_root_hash}.json\"\n\t\tchecksum_file_name = f\"{timestamp}.json\"\n\t\tchecksum_file_path = branch_dir / checksum_file_name\n\n\t\tdata_to_write = {\"root_hash\": root_hash, \"nodes\": nodes_map}\n\n\t\ttry:\n\t\t\twith checksum_file_path.open(\"w\", encoding=\"utf-8\") as f:\n\t\t\t\tjson.dump(data_to_write, f, indent=2)\n\t\t\tlogger.info(f\"Checksum data written to {checksum_file_path}\")\n\t\t\treturn checksum_file_path\n\t\texcept OSError:\n\t\t\tlogger.exception(f\"Error writing checksum data to {checksum_file_path}\")\n\t\t\treturn None\n\n\tdef _get_latest_checksum_file_for_current_branch(self) -&gt; Path | None:\n\t\tbranch_dir = self._get_current_branch_checksum_dir()\n\t\tif not branch_dir or not branch_dir.exists():\n\t\t\treturn None\n\n\t\tjson_files = sorted(\n\t\t\t[f for f in branch_dir.iterdir() if f.is_file() and f.suffix == \".json\"],\n\t\t\tkey=lambda f: f.name,  # Relies on lexicographical sort of YYYY-MM-DD_HH-MM-SS-ffffff.json\n\t\t\treverse=True,\n\t\t)\n\n\t\tif json_files:\n\t\t\treturn json_files[0]\n\t\treturn None\n\n\tdef read_latest_checksum_data_for_current_branch(self) -&gt; tuple[str | None, dict[str, dict[str, str]] | None]:\n\t\t\"\"\"Reads the most recent checksum data file for the current git branch.\n\n\t\tAttempts to locate and read the latest checksum JSON file in the branch-specific\n\t\tchecksum directory. The file contains repository checksum information including\n\t\tthe root hash and a map of all node checksums.\n\n\t\tReturns:\n\t\t\ttuple[str | None, dict[str, dict[str, str]] | None]:\n\t\t\t\tA tuple containing:\n\t\t\t\t- The root hash string if successfully read, otherwise None\n\t\t\t\t- A dictionary mapping paths to their checksum data if successfully read, otherwise None\n\t\t\t\tBoth values will be None if no checksum file exists or if reading fails.\n\t\t\"\"\"\n\t\tlatest_file = self._get_latest_checksum_file_for_current_branch()\n\t\tif not latest_file:\n\t\t\tlogger.info(\"No previous checksum file found for the current branch.\")\n\t\t\treturn None, None\n\n\t\ttry:\n\t\t\twith latest_file.open(\"r\", encoding=\"utf-8\") as f:\n\t\t\t\tdata = json.load(f)\n\n\t\t\troot_hash = data.get(\"root_hash\")\n\t\t\tnodes_map = data.get(\"nodes\")\n\n\t\t\tif isinstance(root_hash, str) and isinstance(nodes_map, dict):\n\t\t\t\tlogger.info(f\"Successfully read checksum data from {latest_file}\")\n\t\t\t\treturn root_hash, nodes_map\n\t\t\tlogger.error(f\"Invalid format in checksum file {latest_file}. Missing 'root_hash' or 'nodes'.\")\n\t\t\treturn None, None\n\t\texcept (OSError, json.JSONDecodeError):\n\t\t\tlogger.exception(f\"Error reading or parsing checksum file {latest_file}\")\n\t\t\treturn None, None\n\t\texcept Exception:  # Catch any other unexpected error\n\t\t\tlogger.exception(f\"Unexpected error reading checksum file {latest_file}\")\n\t\t\treturn None, None\n\n\tasync def calculate_repo_checksum(self) -&gt; tuple[str, dict[str, dict[str, str]]]:\n\t\t\"\"\"Calculates the checksum for the entire repository and all its constituents.\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t        - str: The checksum of the repository root.\n\t\t        - dict[str, dict[str, str]]: A dictionary mapping relative paths (files and dirs)\n\t\t                          to their calculated checksums. Paths use POSIX separators.\n\t\t\"\"\"\n\t\tlocal_nodes_map: dict[str, dict[str, str]] = {}  # Use a local var for population\n\t\tlogger.info(f\"Starting checksum calculation for repository: {self.repo_path}\")\n\n\t\t# The recursive call for the repo_path itself will calculate its hash\n\t\t# based on its children and populate local_nodes_map.\n\t\trepo_root_checksum = await self._calculate_node_hash_recursive(self.repo_path, local_nodes_map)\n\n\t\tself.all_nodes_map = local_nodes_map  # Store the populated map\n\n\t\t# Write the new checksum data\n\t\tself._write_checksum_data(repo_root_checksum, self.all_nodes_map)\n\n\t\tlogger.info(f\"Finished checksum calculation. Root checksum: {repo_root_checksum}\")\n\t\treturn repo_root_checksum, self.all_nodes_map  # Return the stored map\n\n\tdef get_file_checksum(self, relative_path_str: str) -&gt; str | None:\n\t\t\"\"\"\n\t\tRetrieves the pre-calculated checksum for a specific file.\n\n\t\tArgs:\n\t\t    relative_path_str: The POSIX-style relative path of the file from the repo root.\n\n\t\tReturns:\n\t\t    The checksum string if the file was found in the calculated map, else None.\n\t\t\"\"\"\n\t\tif self.all_nodes_map is None:\n\t\t\t# Try to load from latest if map isn't populated (e.g., if only get_file_checksum is called)\n\t\t\t_, nodes_map = self.read_latest_checksum_data_for_current_branch()\n\t\t\tif nodes_map is None:  # Still none after trying to read\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"Checksum map not calculated or readable. \"\n\t\t\t\t\t\"Call calculate_repo_checksum() or ensure a \"\n\t\t\t\t\t\"valid checksum file exists.\"\n\t\t\t\t)\n\t\t\t\treturn None\n\t\t\tself.all_nodes_map = nodes_map\n\n\t\tnode_info = self.all_nodes_map.get(relative_path_str)\n\t\tif node_info and node_info.get(\"type\") == \"file\":\n\t\t\treturn node_info.get(\"hash\")\n\n\t\t# If path uses OS-specific separators, try converting to POSIX\n\t\tposix_path_str = Path(relative_path_str).as_posix()\n\t\tif posix_path_str != relative_path_str:\n\t\t\tnode_info = self.all_nodes_map.get(posix_path_str)\n\t\t\tif node_info and node_info.get(\"type\") == \"file\":\n\t\t\t\treturn node_info.get(\"hash\")\n\n\t\tlogger.debug(f\"No file checksum found for '{relative_path_str}' in the map.\")\n\t\treturn None\n\n\t@staticmethod\n\tdef sanitize_branch_name(branch_name: str) -&gt; str:\n\t\t\"\"\"Sanitizes a branch name to be safe for directory path construction.\n\n\t\tReplaces typical path separators and other problematic characters.\n\t\t\"\"\"\n\t\tif not branch_name:\n\t\t\treturn \"unnamed_branch\"\n\n\t\t# Replace common separators like / and \\\\ with an underscore\n\t\tsanitized = branch_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n\n\t\t# Remove or replace any characters not suitable for directory names.\n\t\t# Whitelist approach: allow alphanumeric, underscore, hyphen, dot.\n\t\tsanitized = re.sub(r\"[^a-zA-Z0-9_.-]\", \"\", sanitized)\n\n\t\t# Prevent names that are just dots or empty after sanitization\n\t\tif not sanitized or all(c == \".\" for c in sanitized):\n\t\t\treturn \"invalid_branch_name_after_sanitize\"\n\n\t\t# Limit length if necessary (OS path limits)\n\t\tmax_len = 50  # Arbitrary reasonable limit for a directory name component\n\t\tif len(sanitized) &gt; max_len:\n\t\t\tsanitized = sanitized[:max_len]\n\n\t\treturn sanitized\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_path: Path,\n\tgit_context: GitRepoContext | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the checksum calculator.</p> <p>Prefer using get_instance() to create or retrieve instances.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Absolute path to the repository root.</p> required <code>git_context</code> <code>GitRepoContext | None</code> <p>Optional GitRepoContext, used for context like branch names          (for future storage strategies) and potentially accessing          configuration for checksum paths.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader, used for configuration</p> <code>None</code> Source code in <code>src/codemap/processor/hash_calculation.py</code> <pre><code>def __init__(\n\tself, repo_path: Path, git_context: \"GitRepoContext | None\" = None, config_loader: \"ConfigLoader | None\" = None\n) -&gt; None:\n\t\"\"\"\n\tInitialize the checksum calculator.\n\n\tPrefer using get_instance() to create or retrieve instances.\n\n\tArgs:\n\t    repo_path: Absolute path to the repository root.\n\t    git_context: Optional GitRepoContext, used for context like branch names\n\t                 (for future storage strategies) and potentially accessing\n\t                 configuration for checksum paths.\n\t    config_loader: Optional ConfigLoader, used for configuration\n\t\"\"\"\n\tif not repo_path.is_dir():\n\t\tmsg = f\"Repository path {repo_path} is not a valid directory.\"\n\t\traise ValueError(msg)\n\tself.repo_path = repo_path.resolve()\n\tself.git_context = git_context\n\tself.config_loader = config_loader\n\tself.all_nodes_map: dict[str, dict[str, str]] | None = None  # path -&gt; {\"type\": \"file\"|\"dir\", \"hash\": hash_val}\n\n\tif not self.config_loader:\n\t\t# Ensure we have a config loader instance to fetch default/user configs\n\t\tfrom codemap.config.config_loader import ConfigLoader\n\n\t\tself.config_loader = ConfigLoader().get_instance()\n\n\tself.checksums_base_dir = self.repo_path / \".codemap_cache\" / \"checksums\"\n\tself.checksums_base_dir.mkdir(parents=True, exist_ok=True)\n\n\t# Fetch exclude patterns from SyncSchema via ConfigLoader\n\t# This allows user overrides from .codemap.yml to be respected.\n\t# If no config file or specific settings, defaults from SyncSchema are used.\n\tapp_config = self.config_loader.get\n\tself.exclude_patterns_str: list[str] = list(app_config.sync.exclude_patterns[:])  # Start with config patterns\n\n\t# Custom .gitignore parsing is removed. We will use pygit2.path_is_ignored later.\n\n\t# Ensure .codemap_cache (or configured equivalent) is always excluded.\n\t# This specific path should ideally be part of the default config in SyncSchema\n\t# or managed via a dedicated configuration setting if its name/location is dynamic.\n\t# For now, adding it directly here if not already present via a generic pattern.\n\tcodemap_cache_pattern = r\"^\\.codemap_cache/\"\n\tif codemap_cache_pattern not in self.exclude_patterns_str:\n\t\tself.exclude_patterns_str.append(codemap_cache_pattern)\n\n\t# Also explicitly exclude the checksums directory we just defined\n\tchecksums_dir_relative = self.checksums_base_dir.relative_to(self.repo_path).as_posix()\n\tchecksums_dir_pattern = f\"^{re.escape(checksums_dir_relative)}/\"\n\tif checksums_dir_pattern not in self.exclude_patterns_str:\n\t\tself.exclude_patterns_str.append(checksums_dir_pattern)\n\n\tself.compiled_exclude_patterns: list[Pattern[str]] = [re.compile(p) for p in self.exclude_patterns_str]\n\tpatterns = [p.pattern for p in self.compiled_exclude_patterns]\n\tlogger.info(f\"RepoChecksumCalculator compiled CodeMap exclude patterns: {patterns}\")\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = resolve()\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.git_context","title":"git_context  <code>instance-attribute</code>","text":"<pre><code>git_context = git_context\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.all_nodes_map","title":"all_nodes_map  <code>instance-attribute</code>","text":"<pre><code>all_nodes_map: dict[str, dict[str, str]] | None = None\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.checksums_base_dir","title":"checksums_base_dir  <code>instance-attribute</code>","text":"<pre><code>checksums_base_dir = (\n\trepo_path / \".codemap_cache\" / \"checksums\"\n)\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.exclude_patterns_str","title":"exclude_patterns_str  <code>instance-attribute</code>","text":"<pre><code>exclude_patterns_str: list[str] = list(exclude_patterns[:])\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.compiled_exclude_patterns","title":"compiled_exclude_patterns  <code>instance-attribute</code>","text":"<pre><code>compiled_exclude_patterns: list[Pattern[str]] = [\n\tcompile(p) for p in exclude_patterns_str\n]\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.get_instance","title":"get_instance  <code>classmethod</code>","text":"<pre><code>get_instance(\n\trepo_path: Path,\n\tgit_context: GitRepoContext | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; RepoChecksumCalculator\n</code></pre> <p>Gets a cached instance of RepoChecksumCalculator for the given repo_path..</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Absolute or relative path to the repository root.</p> required <code>git_context</code> <code>GitRepoContext | None</code> <p>Optional GitRepoContext for the new instance if created.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader for the new instance if created.</p> <code>None</code> <p>Returns:</p> Type Description <code>RepoChecksumCalculator</code> <p>An instance of RepoChecksumCalculator.</p> Source code in <code>src/codemap/processor/hash_calculation.py</code> <pre><code>@classmethod\ndef get_instance(\n\tcls,\n\trepo_path: Path,\n\tgit_context: \"GitRepoContext | None\" = None,\n\tconfig_loader: \"ConfigLoader | None\" = None,\n) -&gt; \"RepoChecksumCalculator\":\n\t\"\"\"\n\tGets a cached instance of RepoChecksumCalculator for the given repo_path..\n\n\tArgs:\n\t    repo_path: Absolute or relative path to the repository root.\n\t    git_context: Optional GitRepoContext for the new instance if created.\n\t    config_loader: Optional ConfigLoader for the new instance if created.\n\n\tReturns:\n\t    An instance of RepoChecksumCalculator.\n\t\"\"\"\n\tresolved_path = repo_path.resolve()\n\tif resolved_path not in cls._instances:\n\t\tlogger.debug(f\"Creating new RepoChecksumCalculator instance for {resolved_path}\")\n\t\tinstance = cls(resolved_path, git_context, config_loader)\n\t\tcls._instances[resolved_path] = instance\n\telse:\n\t\tlogger.debug(f\"Reusing existing RepoChecksumCalculator instance for {resolved_path}\")\n\t\t# Update context if provided, as it might have changed (e.g., branch switch)\n\t\texisting_instance = cls._instances[resolved_path]\n\t\tif git_context is not None:\n\t\t\texisting_instance.git_context = git_context\n\t\tif config_loader is not None:\n\t\t\texisting_instance.config_loader = config_loader\n\treturn cls._instances[resolved_path]\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.read_latest_checksum_data_for_current_branch","title":"read_latest_checksum_data_for_current_branch","text":"<pre><code>read_latest_checksum_data_for_current_branch() -&gt; tuple[\n\tstr | None, dict[str, dict[str, str]] | None\n]\n</code></pre> <p>Reads the most recent checksum data file for the current git branch.</p> <p>Attempts to locate and read the latest checksum JSON file in the branch-specific checksum directory. The file contains repository checksum information including the root hash and a map of all node checksums.</p> <p>Returns:</p> Type Description <code>tuple[str | None, dict[str, dict[str, str]] | None]</code> <p>tuple[str | None, dict[str, dict[str, str]] | None]: A tuple containing: - The root hash string if successfully read, otherwise None - A dictionary mapping paths to their checksum data if successfully read, otherwise None Both values will be None if no checksum file exists or if reading fails.</p> Source code in <code>src/codemap/processor/hash_calculation.py</code> <pre><code>def read_latest_checksum_data_for_current_branch(self) -&gt; tuple[str | None, dict[str, dict[str, str]] | None]:\n\t\"\"\"Reads the most recent checksum data file for the current git branch.\n\n\tAttempts to locate and read the latest checksum JSON file in the branch-specific\n\tchecksum directory. The file contains repository checksum information including\n\tthe root hash and a map of all node checksums.\n\n\tReturns:\n\t\ttuple[str | None, dict[str, dict[str, str]] | None]:\n\t\t\tA tuple containing:\n\t\t\t- The root hash string if successfully read, otherwise None\n\t\t\t- A dictionary mapping paths to their checksum data if successfully read, otherwise None\n\t\t\tBoth values will be None if no checksum file exists or if reading fails.\n\t\"\"\"\n\tlatest_file = self._get_latest_checksum_file_for_current_branch()\n\tif not latest_file:\n\t\tlogger.info(\"No previous checksum file found for the current branch.\")\n\t\treturn None, None\n\n\ttry:\n\t\twith latest_file.open(\"r\", encoding=\"utf-8\") as f:\n\t\t\tdata = json.load(f)\n\n\t\troot_hash = data.get(\"root_hash\")\n\t\tnodes_map = data.get(\"nodes\")\n\n\t\tif isinstance(root_hash, str) and isinstance(nodes_map, dict):\n\t\t\tlogger.info(f\"Successfully read checksum data from {latest_file}\")\n\t\t\treturn root_hash, nodes_map\n\t\tlogger.error(f\"Invalid format in checksum file {latest_file}. Missing 'root_hash' or 'nodes'.\")\n\t\treturn None, None\n\texcept (OSError, json.JSONDecodeError):\n\t\tlogger.exception(f\"Error reading or parsing checksum file {latest_file}\")\n\t\treturn None, None\n\texcept Exception:  # Catch any other unexpected error\n\t\tlogger.exception(f\"Unexpected error reading checksum file {latest_file}\")\n\t\treturn None, None\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.calculate_repo_checksum","title":"calculate_repo_checksum  <code>async</code>","text":"<pre><code>calculate_repo_checksum() -&gt; tuple[\n\tstr, dict[str, dict[str, str]]\n]\n</code></pre> <p>Calculates the checksum for the entire repository and all its constituents.</p> <p>Returns:</p> Type Description <code>tuple[str, dict[str, dict[str, str]]]</code> <p>A tuple containing: - str: The checksum of the repository root. - dict[str, dict[str, str]]: A dictionary mapping relative paths (files and dirs)                   to their calculated checksums. Paths use POSIX separators.</p> Source code in <code>src/codemap/processor/hash_calculation.py</code> <pre><code>async def calculate_repo_checksum(self) -&gt; tuple[str, dict[str, dict[str, str]]]:\n\t\"\"\"Calculates the checksum for the entire repository and all its constituents.\n\n\tReturns:\n\t    A tuple containing:\n\t        - str: The checksum of the repository root.\n\t        - dict[str, dict[str, str]]: A dictionary mapping relative paths (files and dirs)\n\t                          to their calculated checksums. Paths use POSIX separators.\n\t\"\"\"\n\tlocal_nodes_map: dict[str, dict[str, str]] = {}  # Use a local var for population\n\tlogger.info(f\"Starting checksum calculation for repository: {self.repo_path}\")\n\n\t# The recursive call for the repo_path itself will calculate its hash\n\t# based on its children and populate local_nodes_map.\n\trepo_root_checksum = await self._calculate_node_hash_recursive(self.repo_path, local_nodes_map)\n\n\tself.all_nodes_map = local_nodes_map  # Store the populated map\n\n\t# Write the new checksum data\n\tself._write_checksum_data(repo_root_checksum, self.all_nodes_map)\n\n\tlogger.info(f\"Finished checksum calculation. Root checksum: {repo_root_checksum}\")\n\treturn repo_root_checksum, self.all_nodes_map  # Return the stored map\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.get_file_checksum","title":"get_file_checksum","text":"<pre><code>get_file_checksum(relative_path_str: str) -&gt; str | None\n</code></pre> <p>Retrieves the pre-calculated checksum for a specific file.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path_str</code> <code>str</code> <p>The POSIX-style relative path of the file from the repo root.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The checksum string if the file was found in the calculated map, else None.</p> Source code in <code>src/codemap/processor/hash_calculation.py</code> <pre><code>def get_file_checksum(self, relative_path_str: str) -&gt; str | None:\n\t\"\"\"\n\tRetrieves the pre-calculated checksum for a specific file.\n\n\tArgs:\n\t    relative_path_str: The POSIX-style relative path of the file from the repo root.\n\n\tReturns:\n\t    The checksum string if the file was found in the calculated map, else None.\n\t\"\"\"\n\tif self.all_nodes_map is None:\n\t\t# Try to load from latest if map isn't populated (e.g., if only get_file_checksum is called)\n\t\t_, nodes_map = self.read_latest_checksum_data_for_current_branch()\n\t\tif nodes_map is None:  # Still none after trying to read\n\t\t\tlogger.warning(\n\t\t\t\t\"Checksum map not calculated or readable. \"\n\t\t\t\t\"Call calculate_repo_checksum() or ensure a \"\n\t\t\t\t\"valid checksum file exists.\"\n\t\t\t)\n\t\t\treturn None\n\t\tself.all_nodes_map = nodes_map\n\n\tnode_info = self.all_nodes_map.get(relative_path_str)\n\tif node_info and node_info.get(\"type\") == \"file\":\n\t\treturn node_info.get(\"hash\")\n\n\t# If path uses OS-specific separators, try converting to POSIX\n\tposix_path_str = Path(relative_path_str).as_posix()\n\tif posix_path_str != relative_path_str:\n\t\tnode_info = self.all_nodes_map.get(posix_path_str)\n\t\tif node_info and node_info.get(\"type\") == \"file\":\n\t\t\treturn node_info.get(\"hash\")\n\n\tlogger.debug(f\"No file checksum found for '{relative_path_str}' in the map.\")\n\treturn None\n</code></pre>"},{"location":"api/processor/hash_calculation/#codemap.processor.hash_calculation.RepoChecksumCalculator.sanitize_branch_name","title":"sanitize_branch_name  <code>staticmethod</code>","text":"<pre><code>sanitize_branch_name(branch_name: str) -&gt; str\n</code></pre> <p>Sanitizes a branch name to be safe for directory path construction.</p> <p>Replaces typical path separators and other problematic characters.</p> Source code in <code>src/codemap/processor/hash_calculation.py</code> <pre><code>@staticmethod\ndef sanitize_branch_name(branch_name: str) -&gt; str:\n\t\"\"\"Sanitizes a branch name to be safe for directory path construction.\n\n\tReplaces typical path separators and other problematic characters.\n\t\"\"\"\n\tif not branch_name:\n\t\treturn \"unnamed_branch\"\n\n\t# Replace common separators like / and \\\\ with an underscore\n\tsanitized = branch_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n\n\t# Remove or replace any characters not suitable for directory names.\n\t# Whitelist approach: allow alphanumeric, underscore, hyphen, dot.\n\tsanitized = re.sub(r\"[^a-zA-Z0-9_.-]\", \"\", sanitized)\n\n\t# Prevent names that are just dots or empty after sanitization\n\tif not sanitized or all(c == \".\" for c in sanitized):\n\t\treturn \"invalid_branch_name_after_sanitize\"\n\n\t# Limit length if necessary (OS path limits)\n\tmax_len = 50  # Arbitrary reasonable limit for a directory name component\n\tif len(sanitized) &gt; max_len:\n\t\tsanitized = sanitized[:max_len]\n\n\treturn sanitized\n</code></pre>"},{"location":"api/processor/lod/","title":"Lod","text":"<p>Level of Detail (LOD) implementation for code analysis.</p> <p>This module provides functionality for generating different levels of detail from source code using tree-sitter analysis. The LOD approach provides a hierarchical view of code, from high-level entity names to detailed implementations.</p> <p>LOD levels: - LOD1: Just entity names and types in files (classes, functions, etc.) - LOD2: Entity names with docstrings - LOD3: Entity names, docstrings, and signatures - LOD4: Complete entity implementations</p>"},{"location":"api/processor/lod/#codemap.processor.lod.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel","title":"LODLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of Level of Detail levels.</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>class LODLevel(Enum):\n\t\"\"\"Enumeration of Level of Detail levels.\"\"\"\n\n\tSIGNATURES = 1  # Top-level entity names, docstrings, and signatures\n\tSTRUCTURE = 2  # All entity signatures, indented structure\n\tDOCS = 3  # Level 2 + Docstrings for all entities\n\tSKELETON = 4  # Level 3 + Implementation skeleton (key patterns, control flow, important assignments)\n\tFULL = 5  # Level 4 + Full implementation content\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.SIGNATURES","title":"SIGNATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SIGNATURES = 1\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.STRUCTURE","title":"STRUCTURE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STRUCTURE = 2\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.DOCS","title":"DOCS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DOCS = 3\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.SKELETON","title":"SKELETON  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKELETON = 4\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.FULL","title":"FULL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FULL = 5\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity","title":"LODEntity  <code>dataclass</code>","text":"<p>Represents a code entity at a specific level of detail.</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>@dataclass\nclass LODEntity:\n\t\"\"\"Represents a code entity at a specific level of detail.\"\"\"\n\n\tname: str\n\t\"\"\"Name of the entity.\"\"\"\n\n\tentity_type: EntityType\n\t\"\"\"Type of entity (class, function, etc.).\"\"\"\n\n\tstart_line: int\n\t\"\"\"Starting line number (1-indexed).\"\"\"\n\n\tend_line: int\n\t\"\"\"Ending line number (1-indexed).\"\"\"\n\n\tdocstring: str = \"\"\n\t\"\"\"Entity docstring, if available.\"\"\"\n\n\tsignature: str = \"\"\n\t\"\"\"Entity signature (e.g., function parameters), if available.\"\"\"\n\n\tcontent: str = \"\"\n\t\"\"\"Complete entity content/implementation.\"\"\"\n\n\tchildren: list[LODEntity] = field(default_factory=list)\n\t\"\"\"Child entities contained within this entity.\"\"\"\n\n\tlanguage: str = \"\"\n\t\"\"\"Programming language of the entity.\"\"\"\n\n\tmetadata: dict[str, Any] = field(default_factory=dict)\n\t\"\"\"Additional metadata about the entity.\"\"\"\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.__init__","title":"__init__","text":"<pre><code>__init__(\n\tname: str,\n\tentity_type: EntityType,\n\tstart_line: int,\n\tend_line: int,\n\tdocstring: str = \"\",\n\tsignature: str = \"\",\n\tcontent: str = \"\",\n\tchildren: list[LODEntity] = list(),\n\tlanguage: str = \"\",\n\tmetadata: dict[str, Any] = dict(),\n) -&gt; None\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Name of the entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.entity_type","title":"entity_type  <code>instance-attribute</code>","text":"<pre><code>entity_type: EntityType\n</code></pre> <p>Type of entity (class, function, etc.).</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.start_line","title":"start_line  <code>instance-attribute</code>","text":"<pre><code>start_line: int\n</code></pre> <p>Starting line number (1-indexed).</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.end_line","title":"end_line  <code>instance-attribute</code>","text":"<pre><code>end_line: int\n</code></pre> <p>Ending line number (1-indexed).</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.docstring","title":"docstring  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>docstring: str = ''\n</code></pre> <p>Entity docstring, if available.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.signature","title":"signature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signature: str = ''\n</code></pre> <p>Entity signature (e.g., function parameters), if available.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: str = ''\n</code></pre> <p>Complete entity content/implementation.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.children","title":"children  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>children: list[LODEntity] = field(default_factory=list)\n</code></pre> <p>Child entities contained within this entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.language","title":"language  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>language: str = ''\n</code></pre> <p>Programming language of the entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Additional metadata about the entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator","title":"LODGenerator","text":"<p>Generates different levels of detail from source code.</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>class LODGenerator:\n\t\"\"\"Generates different levels of detail from source code.\"\"\"\n\n\tdef __init__(self, analyzer: TreeSitterAnalyzer | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the LOD generator.\n\n\t\tArgs:\n\t\t\tanalyzer: Optional shared TreeSitterAnalyzer instance. If None, a new one is created.\n\t\t\"\"\"\n\t\tself.analyzer = analyzer or TreeSitterAnalyzer()\n\n\tdef generate_lod(self, file_path: Path, level: LODLevel = LODLevel.STRUCTURE) -&gt; LODEntity | None:\n\t\t\"\"\"\n\t\tGenerate LOD representation for a file.\n\n\t\tArgs:\n\t\t    file_path: Path to the file to analyze\n\t\t    level: Level of detail to generate (default changed to STRUCTURE)\n\n\t\tReturns:\n\t\t    LODEntity representing the file, or None if analysis failed\n\n\t\t\"\"\"\n\t\t# Analyze file with tree-sitter - analyzer now handles content reading &amp; caching\n\t\tanalysis_result = self.analyzer.analyze_file(file_path)  # Pass only file_path\n\t\tif not analysis_result:\n\t\t\tlogger.warning(f\"Failed to analyze {file_path}\")\n\t\t\treturn None\n\n\t\t# Convert analysis result to LOD, passing the file_path\n\t\treturn self._convert_to_lod(analysis_result, level, file_path)\n\n\tdef _convert_to_lod(\n\t\tself, analysis_result: dict[str, Any], level: LODLevel, file_path: Path | None = None, is_root: bool = True\n\t) -&gt; LODEntity:\n\t\t\"\"\"\n\t\tConvert tree-sitter analysis to LOD format.\n\n\t\tArgs:\n\t\t    analysis_result: Tree-sitter analysis result\n\t\t    level: Level of detail to generate\n\t\t    file_path: Path to the file being analyzed (present for the root entity)\n\t\t    is_root: Whether the entity is the root entity for the file\n\n\t\tReturns:\n\t\t    LODEntity representation\n\n\t\t\"\"\"\n\t\tentity_type_str = analysis_result.get(\"type\", \"UNKNOWN\")\n\t\ttry:\n\t\t\tentity_type = getattr(EntityType, entity_type_str)\n\t\texcept AttributeError:\n\t\t\tentity_type = EntityType.UNKNOWN\n\n\t\tlocation = analysis_result.get(\"location\", {})\n\t\tstart_line = location.get(\"start_line\", 1)\n\t\tend_line = location.get(\"end_line\", 1)\n\n\t\t# Get the name from analysis result\n\t\tentity_name = analysis_result.get(\"name\", \"\")\n\n\t\t# For modules with placeholder names, use the filename instead\n\t\tif entity_type == EntityType.MODULE and entity_name.startswith(\"&lt;anonymous-\") and file_path:\n\t\t\tentity_name = file_path.stem  # Get filename without extension\n\n\t\tentity = LODEntity(\n\t\t\tname=entity_name,\n\t\t\tentity_type=entity_type,\n\t\t\tstart_line=start_line,\n\t\t\tend_line=end_line,\n\t\t\tlanguage=analysis_result.get(\"language\", \"\"),\n\t\t)\n\n\t\t# Store file_path for all entities for node ID generation, but mark as root only for the top entity\n\t\tif file_path:\n\t\t\tentity.metadata[\"file_path\"] = str(file_path)\n\t\t\tif is_root and \"full_content_str\" in analysis_result:\n\t\t\t\t# If full_content_str is available from analyzer, store it in root entity metadata\n\t\t\t\tentity.metadata[\"full_content_str\"] = analysis_result[\"full_content_str\"]\n\n\t\tif level.value &gt;= LODLevel.DOCS.value:\n\t\t\tentity.docstring = analysis_result.get(\"docstring\", \"\")\n\n\t\tif level.value &gt;= LODLevel.SIGNATURES.value:\n\t\t\t# Extract signature from content if available\n\t\t\tcontent = analysis_result.get(\"content\", \"\")\n\t\t\tentity.signature = self._extract_signature(content, entity_type, entity.language)\n\n\t\tif level.value &gt;= LODLevel.SKELETON.value or entity_type in {EntityType.COMMENT, EntityType.CONSTANT}:\n\t\t\tentity.content = analysis_result.get(\"content\", \"\")\n\n\t\t# Process children recursively (propagate file_path to children but mark as non-root)\n\t\tchildren = analysis_result.get(\"children\", [])\n\t\tfor child in children:\n\t\t\tchild_entity = self._convert_to_lod(child, level, file_path, is_root=False)\n\t\t\tentity.children.append(child_entity)\n\n\t\t# Add any additional metadata\n\t\tif \"dependencies\" in analysis_result:\n\t\t\tentity.metadata[\"dependencies\"] = analysis_result[\"dependencies\"]\n\t\tif \"calls\" in analysis_result:\n\t\t\tentity.metadata[\"calls\"] = analysis_result[\"calls\"]\n\n\t\treturn entity\n\n\tdef _extract_signature(self, content: str, entity_type: EntityType, _language: str) -&gt; str:\n\t\t\"\"\"\n\t\tExtract function/method signature from content.\n\n\t\tThis is a simple implementation; ideally, the language-specific handlers\n\t\tshould provide this functionality.\n\n\t\tArgs:\n\t\t    content: Full entity content\n\t\t    entity_type: Type of entity\n\t\t    _language: Programming language (unused currently)\n\n\t\tReturns:\n\t\t    Signature string\n\n\t\t\"\"\"\n\t\tif not content:\n\t\t\treturn \"\"\n\n\t\t# For functions and methods, extract the first line (declaration)\n\t\tif entity_type in [EntityType.FUNCTION, EntityType.METHOD, EntityType.CLASS, EntityType.INTERFACE]:\n\t\t\tlines = content.split(\"\\n\")\n\t\t\tif lines:\n\t\t\t\t# Return first line without trailing characters\n\t\t\t\treturn lines[0].rstrip(\":{\")\n\n\t\treturn \"\"\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator.__init__","title":"__init__","text":"<pre><code>__init__(\n\tanalyzer: TreeSitterAnalyzer | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the LOD generator.</p> <p>Parameters:</p> Name Type Description Default <code>analyzer</code> <code>TreeSitterAnalyzer | None</code> <p>Optional shared TreeSitterAnalyzer instance. If None, a new one is created.</p> <code>None</code> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>def __init__(self, analyzer: TreeSitterAnalyzer | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize the LOD generator.\n\n\tArgs:\n\t\tanalyzer: Optional shared TreeSitterAnalyzer instance. If None, a new one is created.\n\t\"\"\"\n\tself.analyzer = analyzer or TreeSitterAnalyzer()\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator.analyzer","title":"analyzer  <code>instance-attribute</code>","text":"<pre><code>analyzer = analyzer or TreeSitterAnalyzer()\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator.generate_lod","title":"generate_lod","text":"<pre><code>generate_lod(\n\tfile_path: Path, level: LODLevel = STRUCTURE\n) -&gt; LODEntity | None\n</code></pre> <p>Generate LOD representation for a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to analyze</p> required <code>level</code> <code>LODLevel</code> <p>Level of detail to generate (default changed to STRUCTURE)</p> <code>STRUCTURE</code> <p>Returns:</p> Type Description <code>LODEntity | None</code> <p>LODEntity representing the file, or None if analysis failed</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>def generate_lod(self, file_path: Path, level: LODLevel = LODLevel.STRUCTURE) -&gt; LODEntity | None:\n\t\"\"\"\n\tGenerate LOD representation for a file.\n\n\tArgs:\n\t    file_path: Path to the file to analyze\n\t    level: Level of detail to generate (default changed to STRUCTURE)\n\n\tReturns:\n\t    LODEntity representing the file, or None if analysis failed\n\n\t\"\"\"\n\t# Analyze file with tree-sitter - analyzer now handles content reading &amp; caching\n\tanalysis_result = self.analyzer.analyze_file(file_path)  # Pass only file_path\n\tif not analysis_result:\n\t\tlogger.warning(f\"Failed to analyze {file_path}\")\n\t\treturn None\n\n\t# Convert analysis result to LOD, passing the file_path\n\treturn self._convert_to_lod(analysis_result, level, file_path)\n</code></pre>"},{"location":"api/processor/pipeline/","title":"Pipeline","text":"<p>Unified pipeline for CodeMap data processing, synchronization, and retrieval.</p> <p>This module defines the <code>ProcessingPipeline</code>, which acts as the central orchestrator for managing and interacting with the HNSW vector database. It handles initialization, synchronization with the Git repository, and provides semantic search capabilities.</p>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline","title":"ProcessingPipeline","text":"<p>Orchestrates data processing, synchronization, and retrieval for CodeMap using Qdrant.</p> <p>Manages connections and interactions with the Qdrant vector database, ensuring it is synchronized with the Git repository state. Provides methods for semantic search. Uses asyncio for database and embedding operations.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>class ProcessingPipeline:\n\t\"\"\"\n\tOrchestrates data processing, synchronization, and retrieval for CodeMap using Qdrant.\n\n\tManages connections and interactions with the Qdrant vector database,\n\tensuring it is synchronized with the Git repository state. Provides\n\tmethods for semantic search. Uses asyncio for database and embedding\n\toperations.\n\n\t\"\"\"\n\n\t_instance: ProcessingPipeline | None = None\n\t_lock = asyncio.Lock()\n\n\t@classmethod\n\tasync def get_instance(\n\t\tcls,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; ProcessingPipeline:\n\t\t\"\"\"\n\t\tGet or create a singleton instance of ProcessingPipeline.\n\n\t\tArgs:\n\t\t    config_loader: Application configuration loader. If None, a default one is created.\n\n\t\tReturns:\n\t\t    The singleton ProcessingPipeline instance.\n\t\t\"\"\"\n\t\tasync with cls._lock:\n\t\t\tif cls._instance is None:\n\t\t\t\tcls._instance = cls(config_loader=config_loader)\n\t\t\t\tawait cls._instance.async_init()\n\t\t\treturn cls._instance\n\n\t@classmethod\n\tdef reset_instance(cls) -&gt; None:\n\t\t\"\"\"Reset the singleton instance. Useful for testing.\"\"\"\n\t\tcls._instance = None\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the processing pipeline synchronously.\n\n\t\tCore async initialization is done via `async_init`.\n\n\t\tArgs:\n\t\t    config_loader: Application configuration loader. If None, a default one is created.\n\t\t\"\"\"\n\t\t# Import ConfigLoader at the beginning to ensure it's always available\n\t\tfrom codemap.config import ConfigLoader\n\n\t\tif config_loader:\n\t\t\tself.config_loader = config_loader\n\t\telse:\n\t\t\tself.config_loader = ConfigLoader.get_instance()\n\n\t\tself.git_context = GitRepoContext.get_instance()\n\n\t\tself.repo_path = self.config_loader.get.repo_root\n\n\t\tif not self.repo_path:\n\t\t\tself.repo_path = self.git_context.repo_root\n\n\t\tif not self.repo_path:\n\t\t\tself.repo_path = self.git_context.get_repo_root()\n\n\t\tif not self.repo_path:\n\t\t\tmsg = \"Repository path could not be determined. Please ensure it's a git repository or set in config.\"\n\t\t\tlogger.critical(msg)\n\n\t\tif self.repo_path:\n\t\t\tfrom pathlib import Path\n\n\t\t\tself.repo_path = Path(self.repo_path)\n\t\telse:\n\t\t\tlogger.error(\"Critical: repo_path is None, RepoChecksumCalculator cannot be initialized.\")\n\n\t\tif not isinstance(self.config_loader, ConfigLoader):\n\t\t\tfrom codemap.config import ConfigError\n\n\t\t\tlogger.error(f\"Config loading failed or returned unexpected type: {type(self.config_loader)}\")\n\t\t\tmsg = \"Failed to load a valid Config object.\"\n\t\t\traise ConfigError(msg)\n\n\t\tself.repo_checksum_calculator: RepoChecksumCalculator | None = None\n\t\tif self.repo_path and self.repo_path.is_dir():\n\t\t\tself.repo_checksum_calculator = RepoChecksumCalculator.get_instance(\n\t\t\t\trepo_path=self.repo_path, git_context=self.git_context, config_loader=self.config_loader\n\t\t\t)\n\t\t\tlogger.info(f\"RepoChecksumCalculator initialized for {self.repo_path}\")\n\t\telse:\n\t\t\tlogger.warning(\n\t\t\t\t\"RepoChecksumCalculator could not be initialized because repo_path is invalid or not set. \"\n\t\t\t\t\"Checksum-based quick sync will be skipped.\"\n\t\t\t)\n\n\t\t# --- Defer Shared Components Initialization --- #\n\t\tself._analyzer: TreeSitterAnalyzer | None = None\n\t\tself._chunker: TreeSitterChunker | None = None\n\t\tself._db_client: DatabaseClient | None = None\n\n\t\t# --- Load Configuration --- #\n\t\tembedding_config = self.config_loader.get.embedding\n\t\tembedding_model = embedding_config.model_name\n\t\tqdrant_dimension = embedding_config.dimension\n\t\tdistance_metric = embedding_config.dimension_metric\n\n\t\tself.embedding_model_name: str = \"minishlab/potion-base-8M\"\n\t\tif embedding_model and isinstance(embedding_model, str):\n\t\t\tself.embedding_model_name = embedding_model\n\n\t\tif not qdrant_dimension:\n\t\t\tlogger.warning(\"Missing qdrant dimension in configuration, using default 256\")\n\t\t\tqdrant_dimension = 256\n\n\t\tlogger.info(f\"Using embedding model: {self.embedding_model_name} with dimension: {qdrant_dimension}\")\n\n\t\tvector_config = self.config_loader.get.embedding\n\n\t\tif self.repo_path:\n\t\t\tqdrant_location = self.repo_path / \".codemap_cache\" / \"qdrant\"\n\t\t\tqdrant_location.mkdir(parents=True, exist_ok=True)\n\n\t\tqdrant_url = vector_config.url\n\t\tqdrant_api_key = vector_config.api_key\n\n\t\tdistance_enum = qdrant_models.Distance.COSINE\n\t\tif distance_metric and distance_metric.upper() in [\"COSINE\", \"EUCLID\", \"DOT\", \"MANHATTAN\"]:\n\t\t\tdistance_enum = getattr(qdrant_models.Distance, distance_metric.upper())\n\n\t\tstr(self.repo_path) if self.repo_path else \"no_repo_path\"\n\t\tbranch_str = self.git_context.branch or \"no_branch\"\n\n\t\tstable_repo_id = str(self.repo_path.resolve()) if self.repo_path else \"unknown_repo\"\n\t\tcollection_base_name = hashlib.sha256(stable_repo_id.encode()).hexdigest()[:16]\n\t\tcollection_name = f\"codemap_{collection_base_name}_{branch_str}\"\n\n\t\timport re\n\n\t\tsafe_branch_str = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", branch_str)\n\t\tcollection_name = f\"codemap_{collection_base_name}_{safe_branch_str}\"\n\n\t\tlogger.info(f\"Configuring Qdrant client for URL: {qdrant_url}, Collection: {collection_name}\")\n\n\t\tself.qdrant_manager = QdrantManager(\n\t\t\tconfig_loader=self.config_loader,\n\t\t\tcollection_name=collection_name,\n\t\t\tdim=qdrant_dimension,\n\t\t\tdistance=distance_enum,\n\t\t\turl=qdrant_url,\n\t\t\tapi_key=qdrant_api_key,\n\t\t)\n\t\tself._vector_synchronizer: VectorSynchronizer | None = None\n\n\t\tlogger.info(f\"ProcessingPipeline synchronous initialization complete for repo: {self.repo_path}\")\n\t\tself.is_async_initialized = False\n\t\tself.watcher: Watcher | None = None\n\t\tself._watcher_task: asyncio.Task | None = None\n\t\tself._sync_lock = asyncio.Lock()\n\n\t@property\n\tdef analyzer(self) -&gt; TreeSitterAnalyzer:\n\t\t\"\"\"\n\t\tLazily initialize and return a shared TreeSitterAnalyzer instance.\n\n\t\tReturns:\n\t\t\tTreeSitterAnalyzer: The shared analyzer instance.\n\t\t\"\"\"\n\t\tif self._analyzer is None:\n\t\t\tfrom codemap.processor.tree_sitter import TreeSitterAnalyzer\n\n\t\t\tself._analyzer = TreeSitterAnalyzer()\n\t\treturn self._analyzer\n\n\t@property\n\tdef chunker(self) -&gt; TreeSitterChunker:\n\t\t\"\"\"\n\t\tLazily initialize and return a TreeSitterChunker.\n\n\t\tReturns:\n\t\t\tTreeSitterChunker: The chunker instance.\n\t\t\"\"\"\n\t\tif self._chunker is None:\n\t\t\tfrom codemap.processor.lod import LODGenerator\n\t\t\tfrom codemap.processor.vector.chunking import TreeSitterChunker\n\n\t\t\tlod_generator = LODGenerator(analyzer=self.analyzer)\n\t\t\tself._chunker = TreeSitterChunker(\n\t\t\t\tlod_generator=lod_generator,\n\t\t\t\tconfig_loader=self.config_loader,\n\t\t\t\tgit_context=self.git_context,\n\t\t\t\trepo_checksum_calculator=self.repo_checksum_calculator,\n\t\t\t)\n\t\treturn self._chunker\n\n\t@property\n\tdef db_client(self) -&gt; DatabaseClient:\n\t\t\"\"\"\n\t\tLazily initialize and return a DatabaseClient instance.\n\n\t\tReturns:\n\t\t\tDatabaseClient: The database client instance.\n\n\t\tRaises:\n\t\t\tRuntimeError: If the DatabaseClient cannot be initialized.\n\t\t\"\"\"\n\t\tif self._db_client is None:  # Only attempt initialization if not already done\n\t\t\ttry:\n\t\t\t\tfrom codemap.db.client import DatabaseClient\n\n\t\t\t\tself._db_client = DatabaseClient()  # Add necessary args if any\n\t\t\texcept ImportError:\n\t\t\t\tlogger.exception(\n\t\t\t\t\t\"DatabaseClient could not be imported. DB features will be unavailable. \"\n\t\t\t\t\t\"Ensure database dependencies are installed if needed.\"\n\t\t\t\t)\n\t\t\t\t# We will raise a RuntimeError below if _db_client is still None.\n\t\t\t\t# Allow to proceed to the check below\n\t\t\texcept Exception:\n\t\t\t\t# Catch other potential errors during DatabaseClient instantiation\n\t\t\t\tlogger.exception(\"Error initializing DatabaseClient\")\n\t\t\t\t# We will raise a RuntimeError below if _db_client is still None.\n\n\t\t# After attempting initialization, check if it was successful.\n\t\tif self._db_client is None:\n\t\t\tmsg = (\n\t\t\t\t\"Failed to initialize DatabaseClient. It remains None after attempting import and instantiation. \"\n\t\t\t\t\"Check logs for import errors or instantiation issues.\"\n\t\t\t)\n\t\t\tlogger.critical(msg)  # Use critical for such a failure\n\t\t\traise RuntimeError(msg)\n\n\t\treturn self._db_client\n\n\t@property\n\tdef vector_synchronizer(self) -&gt; VectorSynchronizer:\n\t\t\"\"\"\n\t\tLazily initialize and return a VectorSynchronizer.\n\n\t\tReturns:\n\t\t\tVectorSynchronizer: The synchronizer instance.\n\t\t\"\"\"\n\t\tif self._vector_synchronizer is None:\n\t\t\tfrom codemap.processor.vector.synchronizer import VectorSynchronizer\n\n\t\t\tif self.repo_path is None:\n\t\t\t\tmsg = \"repo_path must not be None for VectorSynchronizer\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise RuntimeError(msg)\n\t\t\tif self.qdrant_manager is None:\n\t\t\t\tmsg = \"qdrant_manager must not be None for VectorSynchronizer\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise RuntimeError(msg)\n\n\t\t\tself._vector_synchronizer = VectorSynchronizer(\n\t\t\t\trepo_path=self.repo_path,\n\t\t\t\tqdrant_manager=self.qdrant_manager,\n\t\t\t\tchunker=self.chunker,\n\t\t\t\tembedding_model_name=self.embedding_model_name,\n\t\t\t\tanalyzer=self.analyzer,\n\t\t\t\tconfig_loader=self.config_loader,\n\t\t\t\trepo_checksum_calculator=self.repo_checksum_calculator,\n\t\t\t)\n\t\treturn self._vector_synchronizer\n\n\tasync def async_init(self, sync_on_init: bool = True) -&gt; None:\n\t\t\"\"\"\n\t\tPerform asynchronous initialization steps, including Qdrant connection and initial sync.\n\n\t\tArgs:\n\t\t    sync_on_init: If True, run database synchronization during initialization.\n\t\t    update_progress: Optional ProgressUpdater instance for progress updates.\n\n\t\t\"\"\"\n\t\tif self.is_async_initialized:\n\t\t\tlogger.info(\"Pipeline already async initialized.\")\n\t\t\treturn\n\n\t\twith progress_indicator(\"Initializing pipeline components...\"):\n\t\t\ttry:\n\t\t\t\t# Get embedding configuration for Qdrant URL\n\t\t\t\tembedding_config = self.config_loader.get.embedding\n\t\t\t\tqdrant_url = embedding_config.url\n\n\t\t\t\t# Check for Docker containers\n\t\t\t\tif qdrant_url:\n\t\t\t\t\twith progress_indicator(\"Checking Docker containers...\"):\n\t\t\t\t\t\t# Only check Docker if we're using a URL that looks like localhost/127.0.0.1\n\t\t\t\t\t\tif \"localhost\" in qdrant_url or \"127.0.0.1\" in qdrant_url:\n\t\t\t\t\t\t\tlogger.info(\"Ensuring Qdrant container is running\")\n\t\t\t\t\t\t\tsuccess, message = await ensure_qdrant_running(wait_for_health=True, qdrant_url=qdrant_url)\n\n\t\t\t\t\t\t\tif not success:\n\t\t\t\t\t\t\t\tlogger.warning(f\"Docker check failed: {message}\")\n\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tlogger.info(f\"Docker container check: {message}\")\n\n\t\t\t\t# Initialize Qdrant client (connects, creates collection if needed)\n\t\t\t\tif self.qdrant_manager:\n\t\t\t\t\twith progress_indicator(\"Initializing Qdrant manager...\"):\n\t\t\t\t\t\tawait self.qdrant_manager.initialize()\n\t\t\t\t\t\tlogger.info(\"Qdrant manager initialized asynchronously.\")\n\t\t\t\telse:\n\t\t\t\t\t# This case should theoretically not happen if __init__ succeeded\n\t\t\t\t\tmsg = \"QdrantManager was not initialized in __init__.\"\n\t\t\t\t\tlogger.error(msg)\n\t\t\t\t\traise RuntimeError(msg)\n\n\t\t\t\tneeds_sync = False\n\t\t\t\tif sync_on_init:\n\t\t\t\t\tneeds_sync = True\n\t\t\t\t\tlogger.info(\"`sync_on_init` is True. Performing index synchronization...\")\n\t\t\t\telse:\n\t\t\t\t\t# Optional: Could add a check here if Qdrant collection is empty\n\t\t\t\t\t# requires another call to qdrant_manager, e.g., get_count()\n\t\t\t\t\tlogger.info(\"Skipping sync on init as requested.\")\n\t\t\t\t\tneeds_sync = False\n\n\t\t\t\t# Set initialized flag *before* potentially long sync operation\n\t\t\t\tself.is_async_initialized = True\n\t\t\t\tlogger.info(\"ProcessingPipeline async core components initialized.\")\n\n\t\t\t\tif needs_sync:\n\t\t\t\t\tawait self.sync_databases()\n\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Failed during async initialization\")\n\t\t\t\t# Optionally re-raise or handle specific exceptions\n\t\t\t\traise\n\n\tasync def stop(self) -&gt; None:\n\t\t\"\"\"Stops the pipeline and releases resources, including closing Qdrant connection.\"\"\"\n\t\tlogger.info(\"Stopping ProcessingPipeline asynchronously...\")\n\t\tif self.qdrant_manager:\n\t\t\tawait self.qdrant_manager.close()\n\t\t\tself.qdrant_manager = None  # type: ignore[assignment]\n\t\telse:\n\t\t\tlogger.warning(\"Qdrant Manager already None during stop.\")\n\n\t\t# Stop the watcher if it's running\n\t\tif self._watcher_task and not self._watcher_task.done():\n\t\t\tlogger.info(\"Stopping file watcher...\")\n\t\t\tself._watcher_task.cancel()\n\t\t\ttry:\n\t\t\t\tawait self._watcher_task  # Allow cancellation to propagate\n\t\t\texcept asyncio.CancelledError:\n\t\t\t\tlogger.info(\"File watcher task cancelled.\")\n\t\t\tif self.watcher:\n\t\t\t\tself.watcher.stop()\n\t\t\t\tlogger.info(\"File watcher stopped.\")\n\t\t\tself.watcher = None\n\t\t\tself._watcher_task = None\n\n\t\t# Other cleanup if needed\n\t\tself.is_async_initialized = False\n\t\tlogger.info(\"ProcessingPipeline stopped.\")\n\n\t# --- Synchronization --- #\n\n\tasync def _sync_callback_wrapper(self) -&gt; None:\n\t\t\"\"\"Async wrapper for the sync callback to handle locking.\"\"\"\n\t\tif self._sync_lock.locked():\n\t\t\tlogger.info(\"Sync already in progress, skipping watcher-triggered sync.\")\n\t\t\treturn\n\n\t\tasync with self._sync_lock:\n\t\t\tlogger.info(\"Watcher triggered sync starting...\")\n\t\t\t# Run sync without progress bars from watcher\n\t\t\tawait self.sync_databases()\n\t\t\tlogger.info(\"Watcher triggered sync finished.\")\n\n\tasync def sync_databases(self) -&gt; None:\n\t\t\"\"\"\n\t\tAsynchronously synchronize the Qdrant index with the Git repository state.\n\n\t\tArgs:\n\t\t    update_progress: Optional ProgressUpdater instance for progress updates.\n\n\t\t\"\"\"\n\t\tif not self.is_async_initialized:\n\t\t\tlogger.error(\"Cannot sync databases, async initialization not complete.\")\n\t\t\treturn\n\n\t\t# Acquire lock only if not already held (for watcher calls)\n\t\tif not self._sync_lock.locked():\n\t\t\tasync with self._sync_lock:\n\t\t\t\tlogger.info(\"Starting vector index synchronization using VectorSynchronizer...\")\n\t\t\t\t# VectorSynchronizer handles its own progress updates internally now\n\t\t\t\tawait self.vector_synchronizer.sync_index()\n\t\t\t\t# Final status message/logging is handled by sync_index\n\t\telse:\n\t\t\t# If lock is already held (likely by watcher call), just run it\n\t\t\tlogger.info(\"Starting vector index synchronization (lock already held)...\")\n\t\t\tawait self.vector_synchronizer.sync_index()\n\n\t# --- Watcher Methods --- #\n\n\tdef initialize_watcher(self, debounce_delay: float = 2.0) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the file watcher.\n\n\t\tArgs:\n\t\t    debounce_delay: Delay in seconds before triggering sync after a file change.\n\n\t\t\"\"\"\n\t\tif not self.repo_path:\n\t\t\tlogger.error(\"Cannot initialize watcher without a repository path.\")\n\t\t\treturn\n\n\t\tif self.watcher:\n\t\t\tlogger.warning(\"Watcher already initialized.\")\n\t\t\treturn\n\n\t\tlogger.info(f\"Initializing file watcher for path: {self.repo_path}\")\n\t\ttry:\n\t\t\tself.watcher = Watcher(\n\t\t\t\tpath_to_watch=self.repo_path,\n\t\t\t\ton_change_callback=self._sync_callback_wrapper,  # Use the lock wrapper\n\t\t\t\tdebounce_delay=debounce_delay,\n\t\t\t)\n\t\t\tlogger.info(\"File watcher initialized.\")\n\t\texcept ValueError:\n\t\t\tlogger.exception(\"Failed to initialize watcher\")\n\t\t\tself.watcher = None\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Unexpected error initializing watcher.\")\n\t\t\tself.watcher = None\n\n\tasync def start_watcher(self) -&gt; None:\n\t\t\"\"\"\n\t\tStart the file watcher in the background.\n\n\t\t`initialize_watcher` must be called first.\n\n\t\t\"\"\"\n\t\tif not self.watcher:\n\t\t\tlogger.error(\"Watcher not initialized. Call initialize_watcher() first.\")\n\t\t\treturn\n\n\t\tif self._watcher_task and not self._watcher_task.done():\n\t\t\tlogger.warning(\"Watcher task is already running.\")\n\t\t\treturn\n\n\t\tlogger.info(\"Starting file watcher task in the background...\")\n\t\t# Create a task to run the watcher's start method asynchronously\n\t\tself._watcher_task = asyncio.create_task(self.watcher.start())\n\t\t# We don't await the task here; it runs independently.\n\t\t# Error handling within the watcher's start method logs issues.\n\n\t# --- Retrieval Methods --- #\n\n\tasync def semantic_search(\n\t\tself,\n\t\tquery: str,\n\t\tk: int = 5,\n\t\tfilter_params: dict[str, Any] | None = None,\n\t) -&gt; list[dict[str, Any]] | None:\n\t\t\"\"\"\n\t\tPerform semantic search for code chunks similar to the query using Qdrant.\n\n\t\tArgs:\n\t\t    query: The search query string.\n\t\t    k: The number of top similar results to retrieve.\n\t\t    filter_params: Optional dictionary for filtering results. Supports:\n\t\t        - exact match: {\"field\": \"value\"} or {\"match\": {\"field\": \"value\"}}\n\t\t        - multiple values: {\"match_any\": {\"field\": [\"value1\", \"value2\"]}}\n\t\t        - range: {\"range\": {\"field\": {\"gt\": value, \"lt\": value}}}\n\t\t        - complex: {\"must\": [...], \"should\": [...], \"must_not\": [...]}\n\n\t\tReturns:\n\t\t    A list of search result dictionaries (Qdrant ScoredPoint converted to dict),\n\t\t    or None if an error occurs.\n\n\t\t\"\"\"\n\t\tif not self.is_async_initialized or not self.qdrant_manager:\n\t\t\tlogger.error(\"QdrantManager not available for semantic search.\")\n\t\t\treturn None\n\n\t\tlogger.debug(\"Performing semantic search for query: '%s', k=%d\", query, k)\n\n\t\ttry:\n\t\t\t# 1. Generate query embedding (must be async)\n\t\t\tquery_embedding = generate_embedding([query], self.config_loader)\n\t\t\tif query_embedding is None:\n\t\t\t\tlogger.error(\"Failed to generate embedding for query.\")\n\t\t\t\treturn None\n\n\t\t\t# Convert to numpy array if needed by Qdrant client, though list is often fine\n\t\t\t# query_vector = np.array(query_embedding, dtype=np.float32)\n\t\t\tquery_vector = query_embedding[0]  # Qdrant client typically accepts list[float]\n\n\t\t\t# 2. Process filter parameters to Qdrant filter format\n\t\t\tquery_filter = None\n\t\t\tif filter_params:\n\t\t\t\tquery_filter = self._build_qdrant_filter(filter_params)\n\t\t\t\tlogger.debug(\"Using filter for search: %s\", query_filter)\n\n\t\t\t# 3. Query Qdrant index (must be async)\n\t\t\tsearch_results: list[qdrant_models.ScoredPoint] = await self.qdrant_manager.search(\n\t\t\t\tquery_vector, k, query_filter=query_filter\n\t\t\t)\n\n\t\t\tif not search_results:\n\t\t\t\tlogger.debug(\"Qdrant search returned no results.\")\n\t\t\t\treturn []\n\n\t\t\t# 4. Format results (convert ScoredPoint to dictionary)\n\t\t\tformatted_results = []\n\t\t\tfor scored_point in search_results:\n\t\t\t\t# Convert Qdrant model to dict for consistent output\n\t\t\t\t# Include score (similarity) and payload\n\t\t\t\tfrom codemap.processor.vector.schema import ChunkMetadataSchema\n\n\t\t\t\tpayload = ChunkMetadataSchema.model_validate(scored_point.payload)\n\n\t\t\t\tresult_dict = {\n\t\t\t\t\t\"id\": str(scored_point.id),  # Ensure ID is string\n\t\t\t\t\t\"score\": scored_point.score,\n\t\t\t\t\t\"payload\": payload,\n\t\t\t\t}\n\t\t\t\tformatted_results.append(result_dict)\n\n\t\t\tlogger.debug(\"Semantic search found %d results.\", len(formatted_results))\n\t\t\treturn formatted_results\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error during semantic search.\")\n\t\t\treturn None\n\n\tdef _build_qdrant_filter(self, filter_params: dict[str, Any]) -&gt; qdrant_models.Filter:\n\t\t\"\"\"\n\t\tConvert filter parameters to Qdrant filter format.\n\n\t\tArgs:\n\t\t    filter_params: Dictionary of filter parameters\n\n\t\tReturns:\n\t\t    Qdrant filter object\n\n\t\t\"\"\"\n\t\t# If already a proper Qdrant filter, return as is\n\t\tif isinstance(filter_params, qdrant_models.Filter):\n\t\t\treturn filter_params\n\n\t\t# Check for clause-based filter (must, should, must_not)\n\t\tif any(key in filter_params for key in [\"must\", \"should\", \"must_not\"]):\n\t\t\tfilter_obj = {}\n\n\t\t\t# Process must conditions (AND)\n\t\t\tif \"must\" in filter_params:\n\t\t\t\tfilter_obj[\"must\"] = [self._build_qdrant_filter(cond) for cond in filter_params[\"must\"]]\n\n\t\t\t# Process should conditions (OR)\n\t\t\tif \"should\" in filter_params:\n\t\t\t\tfilter_obj[\"should\"] = [self._build_qdrant_filter(cond) for cond in filter_params[\"should\"]]\n\n\t\t\t# Process must_not conditions (NOT)\n\t\t\tif \"must_not\" in filter_params:\n\t\t\t\tfilter_obj[\"must_not\"] = [self._build_qdrant_filter(cond) for cond in filter_params[\"must_not\"]]\n\n\t\t\treturn qdrant_models.Filter(**filter_obj)\n\n\t\t# Check for condition-based filter (match, range, etc.)\n\t\tif \"match\" in filter_params:\n\t\t\tfield, value = next(iter(filter_params[\"match\"].items()))\n\t\t\treturn qdrant_models.Filter(\n\t\t\t\tmust=[qdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value))]\n\t\t\t)\n\n\t\tif \"match_any\" in filter_params:\n\t\t\tfield, values = next(iter(filter_params[\"match_any\"].items()))\n\t\t\t# For string values\n\t\t\tif (values and isinstance(values[0], str)) or (values and isinstance(values[0], (int, float))):\n\t\t\t\treturn qdrant_models.Filter(\n\t\t\t\t\tshould=[\n\t\t\t\t\t\tqdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value))\n\t\t\t\t\t\tfor value in values\n\t\t\t\t\t]\n\t\t\t\t)\n\t\t\t# Default case\n\t\t\treturn qdrant_models.Filter(\n\t\t\t\tshould=[\n\t\t\t\t\tqdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value))\n\t\t\t\t\tfor value in values\n\t\t\t\t]\n\t\t\t)\n\n\t\tif \"range\" in filter_params:\n\t\t\tfield, range_values = next(iter(filter_params[\"range\"].items()))\n\t\t\treturn qdrant_models.Filter(\n\t\t\t\tmust=[qdrant_models.FieldCondition(key=field, range=qdrant_models.Range(**range_values))]\n\t\t\t)\n\n\t\t# Default: treat as simple field-value pairs (exact match)\n\t\tmust_conditions = []\n\t\tfor field, value in filter_params.items():\n\t\t\tmust_conditions.append(qdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value)))\n\n\t\treturn qdrant_models.Filter(must=must_conditions)\n\n\t# Context manager support for async operations\n\tasync def __aenter__(self) -&gt; Self:\n\t\t\"\"\"Return self for use as async context manager.\"\"\"\n\t\t# Basic initialization is sync, async init must be called separately\n\t\t# Consider if automatic async_init here is desired, or keep it explicit\n\t\t# await self.async_init() # Example if auto-init is desired\n\t\treturn self\n\n\tasync def __aexit__(\n\t\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n\t) -&gt; None:\n\t\t\"\"\"Clean up resources when exiting the async context manager.\"\"\"\n\t\tawait self.stop()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.get_instance","title":"get_instance  <code>async</code> <code>classmethod</code>","text":"<pre><code>get_instance(\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; ProcessingPipeline\n</code></pre> <p>Get or create a singleton instance of ProcessingPipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader | None</code> <p>Application configuration loader. If None, a default one is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>ProcessingPipeline</code> <p>The singleton ProcessingPipeline instance.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>@classmethod\nasync def get_instance(\n\tcls,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; ProcessingPipeline:\n\t\"\"\"\n\tGet or create a singleton instance of ProcessingPipeline.\n\n\tArgs:\n\t    config_loader: Application configuration loader. If None, a default one is created.\n\n\tReturns:\n\t    The singleton ProcessingPipeline instance.\n\t\"\"\"\n\tasync with cls._lock:\n\t\tif cls._instance is None:\n\t\t\tcls._instance = cls(config_loader=config_loader)\n\t\t\tawait cls._instance.async_init()\n\t\treturn cls._instance\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.reset_instance","title":"reset_instance  <code>classmethod</code>","text":"<pre><code>reset_instance() -&gt; None\n</code></pre> <p>Reset the singleton instance. Useful for testing.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>@classmethod\ndef reset_instance(cls) -&gt; None:\n\t\"\"\"Reset the singleton instance. Useful for testing.\"\"\"\n\tcls._instance = None\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.__init__","title":"__init__","text":"<pre><code>__init__(config_loader: ConfigLoader | None = None) -&gt; None\n</code></pre> <p>Initialize the processing pipeline synchronously.</p> <p>Core async initialization is done via <code>async_init</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader | None</code> <p>Application configuration loader. If None, a default one is created.</p> <code>None</code> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the processing pipeline synchronously.\n\n\tCore async initialization is done via `async_init`.\n\n\tArgs:\n\t    config_loader: Application configuration loader. If None, a default one is created.\n\t\"\"\"\n\t# Import ConfigLoader at the beginning to ensure it's always available\n\tfrom codemap.config import ConfigLoader\n\n\tif config_loader:\n\t\tself.config_loader = config_loader\n\telse:\n\t\tself.config_loader = ConfigLoader.get_instance()\n\n\tself.git_context = GitRepoContext.get_instance()\n\n\tself.repo_path = self.config_loader.get.repo_root\n\n\tif not self.repo_path:\n\t\tself.repo_path = self.git_context.repo_root\n\n\tif not self.repo_path:\n\t\tself.repo_path = self.git_context.get_repo_root()\n\n\tif not self.repo_path:\n\t\tmsg = \"Repository path could not be determined. Please ensure it's a git repository or set in config.\"\n\t\tlogger.critical(msg)\n\n\tif self.repo_path:\n\t\tfrom pathlib import Path\n\n\t\tself.repo_path = Path(self.repo_path)\n\telse:\n\t\tlogger.error(\"Critical: repo_path is None, RepoChecksumCalculator cannot be initialized.\")\n\n\tif not isinstance(self.config_loader, ConfigLoader):\n\t\tfrom codemap.config import ConfigError\n\n\t\tlogger.error(f\"Config loading failed or returned unexpected type: {type(self.config_loader)}\")\n\t\tmsg = \"Failed to load a valid Config object.\"\n\t\traise ConfigError(msg)\n\n\tself.repo_checksum_calculator: RepoChecksumCalculator | None = None\n\tif self.repo_path and self.repo_path.is_dir():\n\t\tself.repo_checksum_calculator = RepoChecksumCalculator.get_instance(\n\t\t\trepo_path=self.repo_path, git_context=self.git_context, config_loader=self.config_loader\n\t\t)\n\t\tlogger.info(f\"RepoChecksumCalculator initialized for {self.repo_path}\")\n\telse:\n\t\tlogger.warning(\n\t\t\t\"RepoChecksumCalculator could not be initialized because repo_path is invalid or not set. \"\n\t\t\t\"Checksum-based quick sync will be skipped.\"\n\t\t)\n\n\t# --- Defer Shared Components Initialization --- #\n\tself._analyzer: TreeSitterAnalyzer | None = None\n\tself._chunker: TreeSitterChunker | None = None\n\tself._db_client: DatabaseClient | None = None\n\n\t# --- Load Configuration --- #\n\tembedding_config = self.config_loader.get.embedding\n\tembedding_model = embedding_config.model_name\n\tqdrant_dimension = embedding_config.dimension\n\tdistance_metric = embedding_config.dimension_metric\n\n\tself.embedding_model_name: str = \"minishlab/potion-base-8M\"\n\tif embedding_model and isinstance(embedding_model, str):\n\t\tself.embedding_model_name = embedding_model\n\n\tif not qdrant_dimension:\n\t\tlogger.warning(\"Missing qdrant dimension in configuration, using default 256\")\n\t\tqdrant_dimension = 256\n\n\tlogger.info(f\"Using embedding model: {self.embedding_model_name} with dimension: {qdrant_dimension}\")\n\n\tvector_config = self.config_loader.get.embedding\n\n\tif self.repo_path:\n\t\tqdrant_location = self.repo_path / \".codemap_cache\" / \"qdrant\"\n\t\tqdrant_location.mkdir(parents=True, exist_ok=True)\n\n\tqdrant_url = vector_config.url\n\tqdrant_api_key = vector_config.api_key\n\n\tdistance_enum = qdrant_models.Distance.COSINE\n\tif distance_metric and distance_metric.upper() in [\"COSINE\", \"EUCLID\", \"DOT\", \"MANHATTAN\"]:\n\t\tdistance_enum = getattr(qdrant_models.Distance, distance_metric.upper())\n\n\tstr(self.repo_path) if self.repo_path else \"no_repo_path\"\n\tbranch_str = self.git_context.branch or \"no_branch\"\n\n\tstable_repo_id = str(self.repo_path.resolve()) if self.repo_path else \"unknown_repo\"\n\tcollection_base_name = hashlib.sha256(stable_repo_id.encode()).hexdigest()[:16]\n\tcollection_name = f\"codemap_{collection_base_name}_{branch_str}\"\n\n\timport re\n\n\tsafe_branch_str = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", branch_str)\n\tcollection_name = f\"codemap_{collection_base_name}_{safe_branch_str}\"\n\n\tlogger.info(f\"Configuring Qdrant client for URL: {qdrant_url}, Collection: {collection_name}\")\n\n\tself.qdrant_manager = QdrantManager(\n\t\tconfig_loader=self.config_loader,\n\t\tcollection_name=collection_name,\n\t\tdim=qdrant_dimension,\n\t\tdistance=distance_enum,\n\t\turl=qdrant_url,\n\t\tapi_key=qdrant_api_key,\n\t)\n\tself._vector_synchronizer: VectorSynchronizer | None = None\n\n\tlogger.info(f\"ProcessingPipeline synchronous initialization complete for repo: {self.repo_path}\")\n\tself.is_async_initialized = False\n\tself.watcher: Watcher | None = None\n\tself._watcher_task: asyncio.Task | None = None\n\tself._sync_lock = asyncio.Lock()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.git_context","title":"git_context  <code>instance-attribute</code>","text":"<pre><code>git_context = get_instance()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_root\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.repo_checksum_calculator","title":"repo_checksum_calculator  <code>instance-attribute</code>","text":"<pre><code>repo_checksum_calculator: RepoChecksumCalculator | None = (\n\tNone\n)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.embedding_model_name","title":"embedding_model_name  <code>instance-attribute</code>","text":"<pre><code>embedding_model_name: str = 'minishlab/potion-base-8M'\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.qdrant_manager","title":"qdrant_manager  <code>instance-attribute</code>","text":"<pre><code>qdrant_manager = QdrantManager(\n\tconfig_loader=config_loader,\n\tcollection_name=collection_name,\n\tdim=qdrant_dimension,\n\tdistance=distance_enum,\n\turl=qdrant_url,\n\tapi_key=qdrant_api_key,\n)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.is_async_initialized","title":"is_async_initialized  <code>instance-attribute</code>","text":"<pre><code>is_async_initialized = False\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.watcher","title":"watcher  <code>instance-attribute</code>","text":"<pre><code>watcher: Watcher | None = None\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.analyzer","title":"analyzer  <code>property</code>","text":"<pre><code>analyzer: TreeSitterAnalyzer\n</code></pre> <p>Lazily initialize and return a shared TreeSitterAnalyzer instance.</p> <p>Returns:</p> Name Type Description <code>TreeSitterAnalyzer</code> <code>TreeSitterAnalyzer</code> <p>The shared analyzer instance.</p>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.chunker","title":"chunker  <code>property</code>","text":"<pre><code>chunker: TreeSitterChunker\n</code></pre> <p>Lazily initialize and return a TreeSitterChunker.</p> <p>Returns:</p> Name Type Description <code>TreeSitterChunker</code> <code>TreeSitterChunker</code> <p>The chunker instance.</p>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.db_client","title":"db_client  <code>property</code>","text":"<pre><code>db_client: DatabaseClient\n</code></pre> <p>Lazily initialize and return a DatabaseClient instance.</p> <p>Returns:</p> Name Type Description <code>DatabaseClient</code> <code>DatabaseClient</code> <p>The database client instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the DatabaseClient cannot be initialized.</p>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.vector_synchronizer","title":"vector_synchronizer  <code>property</code>","text":"<pre><code>vector_synchronizer: VectorSynchronizer\n</code></pre> <p>Lazily initialize and return a VectorSynchronizer.</p> <p>Returns:</p> Name Type Description <code>VectorSynchronizer</code> <code>VectorSynchronizer</code> <p>The synchronizer instance.</p>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.async_init","title":"async_init  <code>async</code>","text":"<pre><code>async_init(sync_on_init: bool = True) -&gt; None\n</code></pre> <p>Perform asynchronous initialization steps, including Qdrant connection and initial sync.</p> <p>Parameters:</p> Name Type Description Default <code>sync_on_init</code> <code>bool</code> <p>If True, run database synchronization during initialization.</p> <code>True</code> <code>update_progress</code> <p>Optional ProgressUpdater instance for progress updates.</p> required Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def async_init(self, sync_on_init: bool = True) -&gt; None:\n\t\"\"\"\n\tPerform asynchronous initialization steps, including Qdrant connection and initial sync.\n\n\tArgs:\n\t    sync_on_init: If True, run database synchronization during initialization.\n\t    update_progress: Optional ProgressUpdater instance for progress updates.\n\n\t\"\"\"\n\tif self.is_async_initialized:\n\t\tlogger.info(\"Pipeline already async initialized.\")\n\t\treturn\n\n\twith progress_indicator(\"Initializing pipeline components...\"):\n\t\ttry:\n\t\t\t# Get embedding configuration for Qdrant URL\n\t\t\tembedding_config = self.config_loader.get.embedding\n\t\t\tqdrant_url = embedding_config.url\n\n\t\t\t# Check for Docker containers\n\t\t\tif qdrant_url:\n\t\t\t\twith progress_indicator(\"Checking Docker containers...\"):\n\t\t\t\t\t# Only check Docker if we're using a URL that looks like localhost/127.0.0.1\n\t\t\t\t\tif \"localhost\" in qdrant_url or \"127.0.0.1\" in qdrant_url:\n\t\t\t\t\t\tlogger.info(\"Ensuring Qdrant container is running\")\n\t\t\t\t\t\tsuccess, message = await ensure_qdrant_running(wait_for_health=True, qdrant_url=qdrant_url)\n\n\t\t\t\t\t\tif not success:\n\t\t\t\t\t\t\tlogger.warning(f\"Docker check failed: {message}\")\n\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tlogger.info(f\"Docker container check: {message}\")\n\n\t\t\t# Initialize Qdrant client (connects, creates collection if needed)\n\t\t\tif self.qdrant_manager:\n\t\t\t\twith progress_indicator(\"Initializing Qdrant manager...\"):\n\t\t\t\t\tawait self.qdrant_manager.initialize()\n\t\t\t\t\tlogger.info(\"Qdrant manager initialized asynchronously.\")\n\t\t\telse:\n\t\t\t\t# This case should theoretically not happen if __init__ succeeded\n\t\t\t\tmsg = \"QdrantManager was not initialized in __init__.\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise RuntimeError(msg)\n\n\t\t\tneeds_sync = False\n\t\t\tif sync_on_init:\n\t\t\t\tneeds_sync = True\n\t\t\t\tlogger.info(\"`sync_on_init` is True. Performing index synchronization...\")\n\t\t\telse:\n\t\t\t\t# Optional: Could add a check here if Qdrant collection is empty\n\t\t\t\t# requires another call to qdrant_manager, e.g., get_count()\n\t\t\t\tlogger.info(\"Skipping sync on init as requested.\")\n\t\t\t\tneeds_sync = False\n\n\t\t\t# Set initialized flag *before* potentially long sync operation\n\t\t\tself.is_async_initialized = True\n\t\t\tlogger.info(\"ProcessingPipeline async core components initialized.\")\n\n\t\t\tif needs_sync:\n\t\t\t\tawait self.sync_databases()\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed during async initialization\")\n\t\t\t# Optionally re-raise or handle specific exceptions\n\t\t\traise\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stops the pipeline and releases resources, including closing Qdrant connection.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def stop(self) -&gt; None:\n\t\"\"\"Stops the pipeline and releases resources, including closing Qdrant connection.\"\"\"\n\tlogger.info(\"Stopping ProcessingPipeline asynchronously...\")\n\tif self.qdrant_manager:\n\t\tawait self.qdrant_manager.close()\n\t\tself.qdrant_manager = None  # type: ignore[assignment]\n\telse:\n\t\tlogger.warning(\"Qdrant Manager already None during stop.\")\n\n\t# Stop the watcher if it's running\n\tif self._watcher_task and not self._watcher_task.done():\n\t\tlogger.info(\"Stopping file watcher...\")\n\t\tself._watcher_task.cancel()\n\t\ttry:\n\t\t\tawait self._watcher_task  # Allow cancellation to propagate\n\t\texcept asyncio.CancelledError:\n\t\t\tlogger.info(\"File watcher task cancelled.\")\n\t\tif self.watcher:\n\t\t\tself.watcher.stop()\n\t\t\tlogger.info(\"File watcher stopped.\")\n\t\tself.watcher = None\n\t\tself._watcher_task = None\n\n\t# Other cleanup if needed\n\tself.is_async_initialized = False\n\tlogger.info(\"ProcessingPipeline stopped.\")\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.sync_databases","title":"sync_databases  <code>async</code>","text":"<pre><code>sync_databases() -&gt; None\n</code></pre> <p>Asynchronously synchronize the Qdrant index with the Git repository state.</p> <p>Parameters:</p> Name Type Description Default <code>update_progress</code> <p>Optional ProgressUpdater instance for progress updates.</p> required Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def sync_databases(self) -&gt; None:\n\t\"\"\"\n\tAsynchronously synchronize the Qdrant index with the Git repository state.\n\n\tArgs:\n\t    update_progress: Optional ProgressUpdater instance for progress updates.\n\n\t\"\"\"\n\tif not self.is_async_initialized:\n\t\tlogger.error(\"Cannot sync databases, async initialization not complete.\")\n\t\treturn\n\n\t# Acquire lock only if not already held (for watcher calls)\n\tif not self._sync_lock.locked():\n\t\tasync with self._sync_lock:\n\t\t\tlogger.info(\"Starting vector index synchronization using VectorSynchronizer...\")\n\t\t\t# VectorSynchronizer handles its own progress updates internally now\n\t\t\tawait self.vector_synchronizer.sync_index()\n\t\t\t# Final status message/logging is handled by sync_index\n\telse:\n\t\t# If lock is already held (likely by watcher call), just run it\n\t\tlogger.info(\"Starting vector index synchronization (lock already held)...\")\n\t\tawait self.vector_synchronizer.sync_index()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.initialize_watcher","title":"initialize_watcher","text":"<pre><code>initialize_watcher(debounce_delay: float = 2.0) -&gt; None\n</code></pre> <p>Initialize the file watcher.</p> <p>Parameters:</p> Name Type Description Default <code>debounce_delay</code> <code>float</code> <p>Delay in seconds before triggering sync after a file change.</p> <code>2.0</code> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>def initialize_watcher(self, debounce_delay: float = 2.0) -&gt; None:\n\t\"\"\"\n\tInitialize the file watcher.\n\n\tArgs:\n\t    debounce_delay: Delay in seconds before triggering sync after a file change.\n\n\t\"\"\"\n\tif not self.repo_path:\n\t\tlogger.error(\"Cannot initialize watcher without a repository path.\")\n\t\treturn\n\n\tif self.watcher:\n\t\tlogger.warning(\"Watcher already initialized.\")\n\t\treturn\n\n\tlogger.info(f\"Initializing file watcher for path: {self.repo_path}\")\n\ttry:\n\t\tself.watcher = Watcher(\n\t\t\tpath_to_watch=self.repo_path,\n\t\t\ton_change_callback=self._sync_callback_wrapper,  # Use the lock wrapper\n\t\t\tdebounce_delay=debounce_delay,\n\t\t)\n\t\tlogger.info(\"File watcher initialized.\")\n\texcept ValueError:\n\t\tlogger.exception(\"Failed to initialize watcher\")\n\t\tself.watcher = None\n\texcept Exception:\n\t\tlogger.exception(\"Unexpected error initializing watcher.\")\n\t\tself.watcher = None\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.start_watcher","title":"start_watcher  <code>async</code>","text":"<pre><code>start_watcher() -&gt; None\n</code></pre> <p>Start the file watcher in the background.</p> <p><code>initialize_watcher</code> must be called first.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def start_watcher(self) -&gt; None:\n\t\"\"\"\n\tStart the file watcher in the background.\n\n\t`initialize_watcher` must be called first.\n\n\t\"\"\"\n\tif not self.watcher:\n\t\tlogger.error(\"Watcher not initialized. Call initialize_watcher() first.\")\n\t\treturn\n\n\tif self._watcher_task and not self._watcher_task.done():\n\t\tlogger.warning(\"Watcher task is already running.\")\n\t\treturn\n\n\tlogger.info(\"Starting file watcher task in the background...\")\n\t# Create a task to run the watcher's start method asynchronously\n\tself._watcher_task = asyncio.create_task(self.watcher.start())\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.semantic_search","title":"semantic_search  <code>async</code>","text":"<pre><code>semantic_search(\n\tquery: str,\n\tk: int = 5,\n\tfilter_params: dict[str, Any] | None = None,\n) -&gt; list[dict[str, Any]] | None\n</code></pre> <p>Perform semantic search for code chunks similar to the query using Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query string.</p> required <code>k</code> <code>int</code> <p>The number of top similar results to retrieve.</p> <code>5</code> <code>filter_params</code> <code>dict[str, Any] | None</code> <p>Optional dictionary for filtering results. Supports: - exact match: {\"field\": \"value\"} or {\"match\": {\"field\": \"value\"}} - multiple values: {\"match_any\": {\"field\": [\"value1\", \"value2\"]}} - range: {\"range\": {\"field\": {\"gt\": value, \"lt\": value}}} - complex: {\"must\": [...], \"should\": [...], \"must_not\": [...]}</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]] | None</code> <p>A list of search result dictionaries (Qdrant ScoredPoint converted to dict),</p> <code>list[dict[str, Any]] | None</code> <p>or None if an error occurs.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def semantic_search(\n\tself,\n\tquery: str,\n\tk: int = 5,\n\tfilter_params: dict[str, Any] | None = None,\n) -&gt; list[dict[str, Any]] | None:\n\t\"\"\"\n\tPerform semantic search for code chunks similar to the query using Qdrant.\n\n\tArgs:\n\t    query: The search query string.\n\t    k: The number of top similar results to retrieve.\n\t    filter_params: Optional dictionary for filtering results. Supports:\n\t        - exact match: {\"field\": \"value\"} or {\"match\": {\"field\": \"value\"}}\n\t        - multiple values: {\"match_any\": {\"field\": [\"value1\", \"value2\"]}}\n\t        - range: {\"range\": {\"field\": {\"gt\": value, \"lt\": value}}}\n\t        - complex: {\"must\": [...], \"should\": [...], \"must_not\": [...]}\n\n\tReturns:\n\t    A list of search result dictionaries (Qdrant ScoredPoint converted to dict),\n\t    or None if an error occurs.\n\n\t\"\"\"\n\tif not self.is_async_initialized or not self.qdrant_manager:\n\t\tlogger.error(\"QdrantManager not available for semantic search.\")\n\t\treturn None\n\n\tlogger.debug(\"Performing semantic search for query: '%s', k=%d\", query, k)\n\n\ttry:\n\t\t# 1. Generate query embedding (must be async)\n\t\tquery_embedding = generate_embedding([query], self.config_loader)\n\t\tif query_embedding is None:\n\t\t\tlogger.error(\"Failed to generate embedding for query.\")\n\t\t\treturn None\n\n\t\t# Convert to numpy array if needed by Qdrant client, though list is often fine\n\t\t# query_vector = np.array(query_embedding, dtype=np.float32)\n\t\tquery_vector = query_embedding[0]  # Qdrant client typically accepts list[float]\n\n\t\t# 2. Process filter parameters to Qdrant filter format\n\t\tquery_filter = None\n\t\tif filter_params:\n\t\t\tquery_filter = self._build_qdrant_filter(filter_params)\n\t\t\tlogger.debug(\"Using filter for search: %s\", query_filter)\n\n\t\t# 3. Query Qdrant index (must be async)\n\t\tsearch_results: list[qdrant_models.ScoredPoint] = await self.qdrant_manager.search(\n\t\t\tquery_vector, k, query_filter=query_filter\n\t\t)\n\n\t\tif not search_results:\n\t\t\tlogger.debug(\"Qdrant search returned no results.\")\n\t\t\treturn []\n\n\t\t# 4. Format results (convert ScoredPoint to dictionary)\n\t\tformatted_results = []\n\t\tfor scored_point in search_results:\n\t\t\t# Convert Qdrant model to dict for consistent output\n\t\t\t# Include score (similarity) and payload\n\t\t\tfrom codemap.processor.vector.schema import ChunkMetadataSchema\n\n\t\t\tpayload = ChunkMetadataSchema.model_validate(scored_point.payload)\n\n\t\t\tresult_dict = {\n\t\t\t\t\"id\": str(scored_point.id),  # Ensure ID is string\n\t\t\t\t\"score\": scored_point.score,\n\t\t\t\t\"payload\": payload,\n\t\t\t}\n\t\t\tformatted_results.append(result_dict)\n\n\t\tlogger.debug(\"Semantic search found %d results.\", len(formatted_results))\n\t\treturn formatted_results\n\n\texcept Exception:\n\t\tlogger.exception(\"Error during semantic search.\")\n\t\treturn None\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; Self\n</code></pre> <p>Return self for use as async context manager.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def __aenter__(self) -&gt; Self:\n\t\"\"\"Return self for use as async context manager.\"\"\"\n\t# Basic initialization is sync, async init must be called separately\n\t# Consider if automatic async_init here is desired, or keep it explicit\n\t# await self.async_init() # Example if auto-init is desired\n\treturn self\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n\texc_type: type[BaseException] | None,\n\texc_val: BaseException | None,\n\texc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Clean up resources when exiting the async context manager.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def __aexit__(\n\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n) -&gt; None:\n\t\"\"\"Clean up resources when exiting the async context manager.\"\"\"\n\tawait self.stop()\n</code></pre>"},{"location":"api/processor/tree_sitter/","title":"Tree Sitter Overview","text":"<p>Tree-sitter based code analysis.</p> <ul> <li>Analyzer - Tree-sitter based code analysis.</li> <li>Base - Base classes and interfaces for tree-sitter analysis.</li> <li>Languages - Language-specific configurations and handlers for tree-sitter analysis.</li> </ul>"},{"location":"api/processor/tree_sitter/analyzer/","title":"Analyzer","text":"<p>Tree-sitter based code analysis.</p> <p>This module provides functionality for analyzing source code using tree- sitter.</p>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.LANGUAGE_NAMES","title":"LANGUAGE_NAMES  <code>module-attribute</code>","text":"<pre><code>LANGUAGE_NAMES: dict[str, SupportedLanguage] = {\n\t\"python\": \"python\",\n\t\"javascript\": \"javascript\",\n\t\"typescript\": \"typescript\",\n}\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.get_language_by_extension","title":"get_language_by_extension","text":"<pre><code>get_language_by_extension(file_path: Path) -&gt; str | None\n</code></pre> <p>Get language name from file extension.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Language name if supported, None otherwise</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def get_language_by_extension(file_path: Path) -&gt; str | None:\n\t\"\"\"\n\tGet language name from file extension.\n\n\tArgs:\n\t    file_path: Path to the file\n\n\tReturns:\n\t    Language name if supported, None otherwise\n\n\t\"\"\"\n\text = file_path.suffix\n\tfor lang, config in LANGUAGE_CONFIGS.items():\n\t\tif ext in config.file_extensions:\n\t\t\treturn lang\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer","title":"TreeSitterAnalyzer","text":"<p>Analyzer for source code using tree-sitter.</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>class TreeSitterAnalyzer:\n\t\"\"\"Analyzer for source code using tree-sitter.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the tree-sitter analyzer.\"\"\"\n\t\tself.parsers: dict[str, Parser] = {}\n\t\tself.ast_cache: dict[Path, tuple[float, Node, Parser]] = {}\n\n\tdef get_parser(self, language: str) -&gt; Parser | None:\n\t\t\"\"\"\n\t\tGet the parser for a language, loading it if necessary.\n\n\t\tArgs:\n\t\t    language: The language to get a parser for\n\n\t\tReturns:\n\t\t    A tree-sitter parser or None if not supported\n\t\t\"\"\"\n\t\tif language in self.parsers:\n\t\t\treturn self.parsers[language]\n\n\t\t# Lazy load the parser\n\t\tts_lang_name = LANGUAGE_NAMES.get(language)\n\t\tif not ts_lang_name:\n\t\t\treturn None\n\t\ttry:\n\t\t\tlang_obj: Language = get_language(ts_lang_name)\n\t\t\tparser: Parser = Parser()\n\t\t\tparser.language = lang_obj\n\t\t\tself.parsers[language] = parser\n\t\t\treturn parser\n\t\texcept (ValueError, RuntimeError, ImportError) as e:\n\t\t\tlogger.debug(\"Failed to load language %s: %s\", language, str(e))\n\t\t\treturn None\n\n\tdef parse_file(\n\t\tself, file_path: Path, language: str | None = None\n\t) -&gt; tuple[Node | None, str, Parser | None, bytes | None]:\n\t\t\"\"\"\n\t\tParse a file and return its root node, determined language, the parser used, and content_bytes.\n\n\t\tUtilizes a cache to avoid re-parsing unchanged files.\n\n\t\tArgs:\n\t\t    file_path: Path to the file to parse\n\t\t    language: Optional language override\n\n\t\tReturns:\n\t\t    A tuple containing the parse tree root node (or None if parsing failed),\n\t\t    the determined language string, the parser instance, and the file content as bytes (if read).\n\t\t\"\"\"\n\t\t# Determine language if not provided\n\t\tdetermined_language = language\n\t\tif not determined_language:\n\t\t\tdetermined_language = get_language_by_extension(file_path)\n\t\t\tif not determined_language:\n\t\t\t\tlogger.debug(\"Could not determine language for file %s\", file_path)\n\t\t\t\treturn None, \"\", None, None\n\n\t\ttry:\n\t\t\tcurrent_mtime = file_path.stat().st_mtime\n\t\t\tif file_path in self.ast_cache:\n\t\t\t\t# Assuming cache stores (mtime, ast_root, parser, content_bytes)\n\t\t\t\t# For now, let's simplify and not cache content_bytes directly with AST to avoid large cache items\n\t\t\t\t# We will re-read if cache hit for AST, but this is still better than re-parsing.\n\t\t\t\t# A more advanced cache could handle content_bytes.\n\t\t\t\tcached_mtime, cached_tree_root, cached_parser = self.ast_cache[file_path]\n\t\t\t\tif current_mtime == cached_mtime:\n\t\t\t\t\tlogger.debug(\"AST cache hit for: %s. Content will be re-read by caller if needed.\", file_path)\n\t\t\t\t\t# To return content_bytes here, we would need to have cached it or re-read it.\n\t\t\t\t\t# For simplicity of this step, parse_file will return None for content_bytes\n\t\t\t\t\t# on a pure AST cache hit.\n\t\t\t\t\t# The caller (analyze_file) will then read it. The main win (no re-parse) is achieved.\n\t\t\t\t\treturn cached_tree_root, determined_language, cached_parser, None\n\t\texcept FileNotFoundError:\n\t\t\tlogger.warning(\"File not found during mtime check: %s\", file_path)\n\t\t\tif file_path in self.ast_cache:\n\t\t\t\tdel self.ast_cache[file_path]\n\t\t\treturn None, determined_language, None, None\n\n\t\tparser = self.get_parser(determined_language)\n\t\tif not parser:\n\t\t\tlogger.debug(\"No parser for language %s\", determined_language)\n\t\t\treturn None, determined_language, None, None\n\n\t\tcontent_bytes_read: bytes | None = None  # Initialize here\n\t\ttry:\n\t\t\twith file_path.open(\"rb\") as f:\n\t\t\t\tcontent_bytes_read = f.read()\n\t\t\ttree = parser.parse(content_bytes_read)\n\t\t\troot_node = tree.root_node\n\t\t\tcurrent_mtime_after_read = file_path.stat().st_mtime\n\t\t\tself.ast_cache[file_path] = (\n\t\t\t\tcurrent_mtime_after_read,\n\t\t\t\troot_node,\n\t\t\t\tparser,\n\t\t\t)  # Not caching content_bytes in AST cache for now\n\t\t\tlogger.debug(\"Parsed and cached AST for: %s\", file_path)\n\t\t\treturn root_node, determined_language, parser, content_bytes_read  # Return read content_bytes\n\t\texcept FileNotFoundError:\n\t\t\tlogger.warning(\"File not found during parsing: %s\", file_path)\n\t\t\tif file_path in self.ast_cache:\n\t\t\t\tdel self.ast_cache[file_path]\n\t\t\t# If file not found, content_bytes_read would not have been assigned (or remains None)\n\t\t\treturn None, determined_language, parser, None\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to parse file %s\", file_path)\n\t\t\t# content_bytes_read will be None if open failed, or the read bytes if parse failed\n\t\t\treturn None, determined_language, parser, content_bytes_read\n\n\tdef get_syntax_handler(self, language: str) -&gt; LanguageSyntaxHandler | None:\n\t\t\"\"\"\n\t\tGet the syntax handler for a language.\n\n\t\tArgs:\n\t\t    language: The language to get a handler for\n\n\t\tReturns:\n\t\t    A syntax handler or None if not supported\n\n\t\t\"\"\"\n\t\thandler_class = LANGUAGE_HANDLERS.get(language)\n\t\tif not handler_class:\n\t\t\treturn None\n\t\treturn handler_class()\n\n\tdef analyze_node(\n\t\tself,\n\t\tnode: Node,\n\t\tcontent_bytes: bytes,\n\t\tfile_path: Path,\n\t\tlanguage: str,\n\t\thandler: LanguageSyntaxHandler,\n\t\tparent_node: Node | None = None,\n\t\tprocessed_nodes: set[int] | None = None,\n\t) -&gt; dict:\n\t\t\"\"\"\n\t\tAnalyze a tree-sitter node and return structured information.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\t\t    file_path: Path to the source file\n\t\t    language: Programming language\n\t\t    handler: Language-specific syntax handler\n\t\t    parent_node: Parent node if any\n\t\t    processed_nodes: Set of node IDs that have already been processed (for deduplication)\n\n\t\tReturns:\n\t\t    Dict with node analysis information\n\n\t\t\"\"\"\n\t\t# Initialize processed_nodes set if not provided (root call)\n\t\tif processed_nodes is None:\n\t\t\tprocessed_nodes = set()\n\n\t\t# Check if this node has already been processed (deduplication)\n\t\tnode_id = id(node)\n\t\tif node_id in processed_nodes:\n\t\t\tlogger.debug(\n\t\t\t\t\"Skipping already processed node: %s at %s:%s\", node.type, node.start_point[0] + 1, node.start_point[1]\n\t\t\t)\n\t\t\treturn {}\n\n\t\t# Check if we should skip this node\n\t\tif handler.should_skip_node(node):\n\t\t\treturn {}\n\n\t\t# Mark this node as processed to prevent double-processing\n\t\tprocessed_nodes.add(node_id)\n\n\t\t# Get entity type for this node from the handler\n\t\tentity_type = handler.get_entity_type(node, parent_node, content_bytes)\n\n\t\t# Skip unknown/uninteresting nodes unless they might contain interesting children\n\t\tif entity_type == EntityType.UNKNOWN and not node.named_child_count &gt; 0:\n\t\t\treturn {}\n\n\t\t# Get name and other metadata\n\t\tname = handler.extract_name(node, content_bytes)\n\t\tdocstring_text, docstring_node = handler.find_docstring(node, content_bytes)\n\n\t\t# Get node content\n\t\ttry:\n\t\t\tnode_content = content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\tnode_content = \"\"\n\n\t\t# Extract dependencies from import statements\n\t\tdependencies = []\n\t\tif entity_type == EntityType.IMPORT:\n\t\t\ttry:\n\t\t\t\tdependencies = handler.extract_imports(node, content_bytes)\n\t\t\texcept (AttributeError, UnicodeDecodeError, IndexError, ValueError) as e:\n\t\t\t\tlogger.debug(\"Failed to extract dependencies: %s\", e)\n\n\t\t# Build result\n\t\tresult: dict[str, Any] = {\n\t\t\t\"type\": entity_type.name if entity_type != EntityType.UNKNOWN else \"UNKNOWN\",\n\t\t\t\"name\": name,\n\t\t\t\"location\": {\n\t\t\t\t\"start_line\": node.start_point[0] + 1,  # Convert to 1-based\n\t\t\t\t\"end_line\": node.end_point[0] + 1,\n\t\t\t\t\"start_col\": node.start_point[1],\n\t\t\t\t\"end_col\": node.end_point[1],\n\t\t\t},\n\t\t\t\"docstring\": docstring_text,\n\t\t\t\"content\": node_content,\n\t\t\t\"children\": [],\n\t\t\t\"language\": language,\n\t\t}\n\n\t\t# Add dependencies only if they exist to keep the output clean\n\t\tif dependencies:\n\t\t\tresult[\"dependencies\"] = dependencies\n\n\t\t# Extract function calls if the entity is a function or method\n\t\tcalls = []\n\t\tif entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\t\tbody_node = handler.get_body_node(node)\n\t\t\tif body_node:\n\t\t\t\ttry:\n\t\t\t\t\tcalls = handler.extract_calls(body_node, content_bytes)\n\t\t\t\texcept (AttributeError, IndexError, UnicodeDecodeError, ValueError) as e:\n\t\t\t\t\tlogger.debug(\"Failed to extract calls for %s: %s\", name or \"&lt;anonymous&gt;\", e)\n\n\t\t# Add calls only if they exist\n\t\tif calls:\n\t\t\tresult[\"calls\"] = calls\n\n\t\t# Process child nodes\n\t\tbody_node = handler.get_body_node(node)\n\t\tchildren_to_process = handler.get_children_to_process(node, body_node)\n\n\t\tfor child in children_to_process:\n\t\t\tif docstring_node and child == docstring_node:\n\t\t\t\tcontinue  # Skip docstring node\n\n\t\t\tchild_result = self.analyze_node(child, content_bytes, file_path, language, handler, node, processed_nodes)\n\n\t\t\tif child_result:  # Only add non-empty results\n\t\t\t\tresult[\"children\"].append(child_result)\n\n\t\treturn result\n\n\tdef analyze_file(\n\t\tself,\n\t\tfile_path: Path,\n\t\tlanguage: str | None = None,\n\t) -&gt; dict:\n\t\t\"\"\"\n\t\tAnalyze a file and return its structural information.\n\n\t\tUses cached ASTs.\n\t\tThe returned dictionary will include a 'full_content_str' key\n\t\tif the file content was successfully read.\n\n\t\tArgs:\n\t\t    file_path: Path to the file\n\t\t    language: Optional language override\n\n\t\tReturns:\n\t\t    Structured analysis of the file or an empty dict on failure.\n\t\t    Includes 'full_content_str' with the file's decoded content if read.\n\t\t\"\"\"\n\t\troot_node, resolved_language, _, read_content_bytes = self.parse_file(file_path, language)\n\n\t\tif not root_node or not resolved_language:\n\t\t\treturn {}\n\n\t\thandler = self.get_syntax_handler(resolved_language)\n\t\tif not handler:\n\t\t\tlogger.debug(\"No syntax handler for language %s\", resolved_language)\n\t\t\treturn {}\n\n\t\tcontent_bytes_for_analysis = read_content_bytes\n\t\tdecoded_full_content_str = None\n\n\t\tif content_bytes_for_analysis is None:\n\t\t\ttry:\n\t\t\t\twith file_path.open(\"rb\") as f:\n\t\t\t\t\tcontent_bytes_for_analysis = f.read()\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Failed to re-read file content for analysis %s\", file_path)\n\t\t\t\treturn {}\n\n\t\tif content_bytes_for_analysis is not None:\n\t\t\ttry:\n\t\t\t\tdecoded_full_content_str = content_bytes_for_analysis.decode(\"utf-8\", errors=\"ignore\")\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Failed to decode file content for %s\", file_path)\n\t\t\t\t# Continue without decoded_full_content_str if decoding fails\n\n\t\t# Perform node-level analysis (recursive)\n\t\tanalysis_data = self.analyze_node(root_node, content_bytes_for_analysis, file_path, resolved_language, handler)\n\n\t\t# Add the full decoded content to the top-level result if available\n\t\tif decoded_full_content_str is not None:\n\t\t\tanalysis_data[\"full_content_str\"] = decoded_full_content_str\n\n\t\treturn analysis_data\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the tree-sitter analyzer.</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the tree-sitter analyzer.\"\"\"\n\tself.parsers: dict[str, Parser] = {}\n\tself.ast_cache: dict[Path, tuple[float, Node, Parser]] = {}\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.parsers","title":"parsers  <code>instance-attribute</code>","text":"<pre><code>parsers: dict[str, Parser] = {}\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.ast_cache","title":"ast_cache  <code>instance-attribute</code>","text":"<pre><code>ast_cache: dict[Path, tuple[float, Node, Parser]] = {}\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.get_parser","title":"get_parser","text":"<pre><code>get_parser(language: str) -&gt; Parser | None\n</code></pre> <p>Get the parser for a language, loading it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>The language to get a parser for</p> required <p>Returns:</p> Type Description <code>Parser | None</code> <p>A tree-sitter parser or None if not supported</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def get_parser(self, language: str) -&gt; Parser | None:\n\t\"\"\"\n\tGet the parser for a language, loading it if necessary.\n\n\tArgs:\n\t    language: The language to get a parser for\n\n\tReturns:\n\t    A tree-sitter parser or None if not supported\n\t\"\"\"\n\tif language in self.parsers:\n\t\treturn self.parsers[language]\n\n\t# Lazy load the parser\n\tts_lang_name = LANGUAGE_NAMES.get(language)\n\tif not ts_lang_name:\n\t\treturn None\n\ttry:\n\t\tlang_obj: Language = get_language(ts_lang_name)\n\t\tparser: Parser = Parser()\n\t\tparser.language = lang_obj\n\t\tself.parsers[language] = parser\n\t\treturn parser\n\texcept (ValueError, RuntimeError, ImportError) as e:\n\t\tlogger.debug(\"Failed to load language %s: %s\", language, str(e))\n\t\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.parse_file","title":"parse_file","text":"<pre><code>parse_file(\n\tfile_path: Path, language: str | None = None\n) -&gt; tuple[Node | None, str, Parser | None, bytes | None]\n</code></pre> <p>Parse a file and return its root node, determined language, the parser used, and content_bytes.</p> <p>Utilizes a cache to avoid re-parsing unchanged files.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to parse</p> required <code>language</code> <code>str | None</code> <p>Optional language override</p> <code>None</code> <p>Returns:</p> Type Description <code>Node | None</code> <p>A tuple containing the parse tree root node (or None if parsing failed),</p> <code>str</code> <p>the determined language string, the parser instance, and the file content as bytes (if read).</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def parse_file(\n\tself, file_path: Path, language: str | None = None\n) -&gt; tuple[Node | None, str, Parser | None, bytes | None]:\n\t\"\"\"\n\tParse a file and return its root node, determined language, the parser used, and content_bytes.\n\n\tUtilizes a cache to avoid re-parsing unchanged files.\n\n\tArgs:\n\t    file_path: Path to the file to parse\n\t    language: Optional language override\n\n\tReturns:\n\t    A tuple containing the parse tree root node (or None if parsing failed),\n\t    the determined language string, the parser instance, and the file content as bytes (if read).\n\t\"\"\"\n\t# Determine language if not provided\n\tdetermined_language = language\n\tif not determined_language:\n\t\tdetermined_language = get_language_by_extension(file_path)\n\t\tif not determined_language:\n\t\t\tlogger.debug(\"Could not determine language for file %s\", file_path)\n\t\t\treturn None, \"\", None, None\n\n\ttry:\n\t\tcurrent_mtime = file_path.stat().st_mtime\n\t\tif file_path in self.ast_cache:\n\t\t\t# Assuming cache stores (mtime, ast_root, parser, content_bytes)\n\t\t\t# For now, let's simplify and not cache content_bytes directly with AST to avoid large cache items\n\t\t\t# We will re-read if cache hit for AST, but this is still better than re-parsing.\n\t\t\t# A more advanced cache could handle content_bytes.\n\t\t\tcached_mtime, cached_tree_root, cached_parser = self.ast_cache[file_path]\n\t\t\tif current_mtime == cached_mtime:\n\t\t\t\tlogger.debug(\"AST cache hit for: %s. Content will be re-read by caller if needed.\", file_path)\n\t\t\t\t# To return content_bytes here, we would need to have cached it or re-read it.\n\t\t\t\t# For simplicity of this step, parse_file will return None for content_bytes\n\t\t\t\t# on a pure AST cache hit.\n\t\t\t\t# The caller (analyze_file) will then read it. The main win (no re-parse) is achieved.\n\t\t\t\treturn cached_tree_root, determined_language, cached_parser, None\n\texcept FileNotFoundError:\n\t\tlogger.warning(\"File not found during mtime check: %s\", file_path)\n\t\tif file_path in self.ast_cache:\n\t\t\tdel self.ast_cache[file_path]\n\t\treturn None, determined_language, None, None\n\n\tparser = self.get_parser(determined_language)\n\tif not parser:\n\t\tlogger.debug(\"No parser for language %s\", determined_language)\n\t\treturn None, determined_language, None, None\n\n\tcontent_bytes_read: bytes | None = None  # Initialize here\n\ttry:\n\t\twith file_path.open(\"rb\") as f:\n\t\t\tcontent_bytes_read = f.read()\n\t\ttree = parser.parse(content_bytes_read)\n\t\troot_node = tree.root_node\n\t\tcurrent_mtime_after_read = file_path.stat().st_mtime\n\t\tself.ast_cache[file_path] = (\n\t\t\tcurrent_mtime_after_read,\n\t\t\troot_node,\n\t\t\tparser,\n\t\t)  # Not caching content_bytes in AST cache for now\n\t\tlogger.debug(\"Parsed and cached AST for: %s\", file_path)\n\t\treturn root_node, determined_language, parser, content_bytes_read  # Return read content_bytes\n\texcept FileNotFoundError:\n\t\tlogger.warning(\"File not found during parsing: %s\", file_path)\n\t\tif file_path in self.ast_cache:\n\t\t\tdel self.ast_cache[file_path]\n\t\t# If file not found, content_bytes_read would not have been assigned (or remains None)\n\t\treturn None, determined_language, parser, None\n\texcept Exception:\n\t\tlogger.exception(\"Failed to parse file %s\", file_path)\n\t\t# content_bytes_read will be None if open failed, or the read bytes if parse failed\n\t\treturn None, determined_language, parser, content_bytes_read\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.get_syntax_handler","title":"get_syntax_handler","text":"<pre><code>get_syntax_handler(\n\tlanguage: str,\n) -&gt; LanguageSyntaxHandler | None\n</code></pre> <p>Get the syntax handler for a language.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>The language to get a handler for</p> required <p>Returns:</p> Type Description <code>LanguageSyntaxHandler | None</code> <p>A syntax handler or None if not supported</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def get_syntax_handler(self, language: str) -&gt; LanguageSyntaxHandler | None:\n\t\"\"\"\n\tGet the syntax handler for a language.\n\n\tArgs:\n\t    language: The language to get a handler for\n\n\tReturns:\n\t    A syntax handler or None if not supported\n\n\t\"\"\"\n\thandler_class = LANGUAGE_HANDLERS.get(language)\n\tif not handler_class:\n\t\treturn None\n\treturn handler_class()\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.analyze_node","title":"analyze_node","text":"<pre><code>analyze_node(\n\tnode: Node,\n\tcontent_bytes: bytes,\n\tfile_path: Path,\n\tlanguage: str,\n\thandler: LanguageSyntaxHandler,\n\tparent_node: Node | None = None,\n\tprocessed_nodes: set[int] | None = None,\n) -&gt; dict\n</code></pre> <p>Analyze a tree-sitter node and return structured information.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <code>file_path</code> <code>Path</code> <p>Path to the source file</p> required <code>language</code> <code>str</code> <p>Programming language</p> required <code>handler</code> <code>LanguageSyntaxHandler</code> <p>Language-specific syntax handler</p> required <code>parent_node</code> <code>Node | None</code> <p>Parent node if any</p> <code>None</code> <code>processed_nodes</code> <code>set[int] | None</code> <p>Set of node IDs that have already been processed (for deduplication)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with node analysis information</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def analyze_node(\n\tself,\n\tnode: Node,\n\tcontent_bytes: bytes,\n\tfile_path: Path,\n\tlanguage: str,\n\thandler: LanguageSyntaxHandler,\n\tparent_node: Node | None = None,\n\tprocessed_nodes: set[int] | None = None,\n) -&gt; dict:\n\t\"\"\"\n\tAnalyze a tree-sitter node and return structured information.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\t    file_path: Path to the source file\n\t    language: Programming language\n\t    handler: Language-specific syntax handler\n\t    parent_node: Parent node if any\n\t    processed_nodes: Set of node IDs that have already been processed (for deduplication)\n\n\tReturns:\n\t    Dict with node analysis information\n\n\t\"\"\"\n\t# Initialize processed_nodes set if not provided (root call)\n\tif processed_nodes is None:\n\t\tprocessed_nodes = set()\n\n\t# Check if this node has already been processed (deduplication)\n\tnode_id = id(node)\n\tif node_id in processed_nodes:\n\t\tlogger.debug(\n\t\t\t\"Skipping already processed node: %s at %s:%s\", node.type, node.start_point[0] + 1, node.start_point[1]\n\t\t)\n\t\treturn {}\n\n\t# Check if we should skip this node\n\tif handler.should_skip_node(node):\n\t\treturn {}\n\n\t# Mark this node as processed to prevent double-processing\n\tprocessed_nodes.add(node_id)\n\n\t# Get entity type for this node from the handler\n\tentity_type = handler.get_entity_type(node, parent_node, content_bytes)\n\n\t# Skip unknown/uninteresting nodes unless they might contain interesting children\n\tif entity_type == EntityType.UNKNOWN and not node.named_child_count &gt; 0:\n\t\treturn {}\n\n\t# Get name and other metadata\n\tname = handler.extract_name(node, content_bytes)\n\tdocstring_text, docstring_node = handler.find_docstring(node, content_bytes)\n\n\t# Get node content\n\ttry:\n\t\tnode_content = content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\texcept (UnicodeDecodeError, IndexError):\n\t\tnode_content = \"\"\n\n\t# Extract dependencies from import statements\n\tdependencies = []\n\tif entity_type == EntityType.IMPORT:\n\t\ttry:\n\t\t\tdependencies = handler.extract_imports(node, content_bytes)\n\t\texcept (AttributeError, UnicodeDecodeError, IndexError, ValueError) as e:\n\t\t\tlogger.debug(\"Failed to extract dependencies: %s\", e)\n\n\t# Build result\n\tresult: dict[str, Any] = {\n\t\t\"type\": entity_type.name if entity_type != EntityType.UNKNOWN else \"UNKNOWN\",\n\t\t\"name\": name,\n\t\t\"location\": {\n\t\t\t\"start_line\": node.start_point[0] + 1,  # Convert to 1-based\n\t\t\t\"end_line\": node.end_point[0] + 1,\n\t\t\t\"start_col\": node.start_point[1],\n\t\t\t\"end_col\": node.end_point[1],\n\t\t},\n\t\t\"docstring\": docstring_text,\n\t\t\"content\": node_content,\n\t\t\"children\": [],\n\t\t\"language\": language,\n\t}\n\n\t# Add dependencies only if they exist to keep the output clean\n\tif dependencies:\n\t\tresult[\"dependencies\"] = dependencies\n\n\t# Extract function calls if the entity is a function or method\n\tcalls = []\n\tif entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\tbody_node = handler.get_body_node(node)\n\t\tif body_node:\n\t\t\ttry:\n\t\t\t\tcalls = handler.extract_calls(body_node, content_bytes)\n\t\t\texcept (AttributeError, IndexError, UnicodeDecodeError, ValueError) as e:\n\t\t\t\tlogger.debug(\"Failed to extract calls for %s: %s\", name or \"&lt;anonymous&gt;\", e)\n\n\t# Add calls only if they exist\n\tif calls:\n\t\tresult[\"calls\"] = calls\n\n\t# Process child nodes\n\tbody_node = handler.get_body_node(node)\n\tchildren_to_process = handler.get_children_to_process(node, body_node)\n\n\tfor child in children_to_process:\n\t\tif docstring_node and child == docstring_node:\n\t\t\tcontinue  # Skip docstring node\n\n\t\tchild_result = self.analyze_node(child, content_bytes, file_path, language, handler, node, processed_nodes)\n\n\t\tif child_result:  # Only add non-empty results\n\t\t\tresult[\"children\"].append(child_result)\n\n\treturn result\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.analyze_file","title":"analyze_file","text":"<pre><code>analyze_file(\n\tfile_path: Path, language: str | None = None\n) -&gt; dict\n</code></pre> <p>Analyze a file and return its structural information.</p> <p>Uses cached ASTs. The returned dictionary will include a 'full_content_str' key if the file content was successfully read.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file</p> required <code>language</code> <code>str | None</code> <p>Optional language override</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Structured analysis of the file or an empty dict on failure.</p> <code>dict</code> <p>Includes 'full_content_str' with the file's decoded content if read.</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def analyze_file(\n\tself,\n\tfile_path: Path,\n\tlanguage: str | None = None,\n) -&gt; dict:\n\t\"\"\"\n\tAnalyze a file and return its structural information.\n\n\tUses cached ASTs.\n\tThe returned dictionary will include a 'full_content_str' key\n\tif the file content was successfully read.\n\n\tArgs:\n\t    file_path: Path to the file\n\t    language: Optional language override\n\n\tReturns:\n\t    Structured analysis of the file or an empty dict on failure.\n\t    Includes 'full_content_str' with the file's decoded content if read.\n\t\"\"\"\n\troot_node, resolved_language, _, read_content_bytes = self.parse_file(file_path, language)\n\n\tif not root_node or not resolved_language:\n\t\treturn {}\n\n\thandler = self.get_syntax_handler(resolved_language)\n\tif not handler:\n\t\tlogger.debug(\"No syntax handler for language %s\", resolved_language)\n\t\treturn {}\n\n\tcontent_bytes_for_analysis = read_content_bytes\n\tdecoded_full_content_str = None\n\n\tif content_bytes_for_analysis is None:\n\t\ttry:\n\t\t\twith file_path.open(\"rb\") as f:\n\t\t\t\tcontent_bytes_for_analysis = f.read()\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to re-read file content for analysis %s\", file_path)\n\t\t\treturn {}\n\n\tif content_bytes_for_analysis is not None:\n\t\ttry:\n\t\t\tdecoded_full_content_str = content_bytes_for_analysis.decode(\"utf-8\", errors=\"ignore\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to decode file content for %s\", file_path)\n\t\t\t# Continue without decoded_full_content_str if decoding fails\n\n\t# Perform node-level analysis (recursive)\n\tanalysis_data = self.analyze_node(root_node, content_bytes_for_analysis, file_path, resolved_language, handler)\n\n\t# Add the full decoded content to the top-level result if available\n\tif decoded_full_content_str is not None:\n\t\tanalysis_data[\"full_content_str\"] = decoded_full_content_str\n\n\treturn analysis_data\n</code></pre>"},{"location":"api/processor/tree_sitter/base/","title":"Base","text":"<p>Base classes and interfaces for tree-sitter analysis.</p> <p>This module defines the core data structures and interfaces for tree-sitter analysis. It provides: - Entity type definitions for tree-sitter nodes - Metadata structures for tree-sitter nodes. - Base tree-sitter analysis interface</p>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType","title":"EntityType","text":"<p>               Bases: <code>Enum</code></p> <p>Types of code entities that can be extracted.</p> Source code in <code>src/codemap/processor/tree_sitter/base.py</code> <pre><code>class EntityType(Enum):\n\t\"\"\"Types of code entities that can be extracted.\"\"\"\n\n\t# File-level entities\n\tMODULE = auto()\n\tNAMESPACE = auto()\n\tPACKAGE = auto()\n\n\t# Type definitions\n\tCLASS = auto()\n\tINTERFACE = auto()\n\tPROTOCOL = auto()  # Similar to interface but for structural typing\n\tSTRUCT = auto()\n\tENUM = auto()\n\tTYPE_ALIAS = auto()\n\n\t# Functions and methods\n\tFUNCTION = auto()\n\tMETHOD = auto()\n\tPROPERTY = auto()  # For getter/setter methods\n\tTEST_CASE = auto()\n\tTEST_SUITE = auto()\n\n\t# Variables and constants\n\tVARIABLE = auto()\n\tCONSTANT = auto()\n\tCLASS_FIELD = auto()  # For class-level variables/fields\n\n\t# Code organization\n\tIMPORT = auto()\n\tDECORATOR = auto()\n\n\t# Documentation\n\tCOMMENT = auto()\n\tDOCSTRING = auto()\n\n\t# Special cases\n\tUNKNOWN = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.MODULE","title":"MODULE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODULE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.NAMESPACE","title":"NAMESPACE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NAMESPACE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.PACKAGE","title":"PACKAGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PACKAGE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.CLASS","title":"CLASS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLASS = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.INTERFACE","title":"INTERFACE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INTERFACE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.PROTOCOL","title":"PROTOCOL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROTOCOL = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.STRUCT","title":"STRUCT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STRUCT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.ENUM","title":"ENUM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENUM = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.TYPE_ALIAS","title":"TYPE_ALIAS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TYPE_ALIAS = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.FUNCTION","title":"FUNCTION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FUNCTION = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.METHOD","title":"METHOD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>METHOD = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.PROPERTY","title":"PROPERTY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROPERTY = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.TEST_CASE","title":"TEST_CASE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST_CASE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.TEST_SUITE","title":"TEST_SUITE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST_SUITE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.VARIABLE","title":"VARIABLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VARIABLE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.CONSTANT","title":"CONSTANT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONSTANT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.CLASS_FIELD","title":"CLASS_FIELD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLASS_FIELD = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.IMPORT","title":"IMPORT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMPORT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.DECORATOR","title":"DECORATOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DECORATOR = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.COMMENT","title":"COMMENT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMMENT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.DOCSTRING","title":"DOCSTRING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DOCSTRING = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.UNKNOWN","title":"UNKNOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNKNOWN = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/","title":"Languages Overview","text":"<p>Language-specific configurations and handlers for tree-sitter analysis.</p> <ul> <li>Base - Base configuration for language-specific syntax chunking.</li> <li>Javascript - JavaScript-specific configuration for syntax chunking.</li> <li>Python - Python-specific configuration for syntax chunking.</li> <li>Typescript - TypeScript-specific configuration for syntax chunking.</li> </ul>"},{"location":"api/processor/tree_sitter/languages/base/","title":"Base","text":"<p>Base configuration for language-specific syntax chunking.</p> <p>This module provides the base configuration class for defining how different programming languages map their syntax elements to code chunks.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig","title":"LanguageConfig  <code>dataclass</code>","text":"<p>Configuration for language-specific syntax chunking.</p> <p>This class defines how a specific programming language's syntax elements map to different types of code chunks. Each field is a list of syntax node types that represent that kind of entity in the language's AST.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@dataclass(frozen=True)\nclass LanguageConfig:\n\t\"\"\"\n\tConfiguration for language-specific syntax chunking.\n\n\tThis class defines how a specific programming language's syntax\n\telements map to different types of code chunks. Each field is a list of\n\tsyntax node types that represent that kind of entity in the language's\n\tAST.\n\n\t\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]]\n\t\"\"\"Node types that represent entire modules/files.\"\"\"\n\n\tnamespace: ClassVar[list[str]]\n\t\"\"\"Node types for namespace/package declarations.\"\"\"\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]]\n\t\"\"\"Node types for class definitions.\"\"\"\n\n\tinterface: ClassVar[list[str]]\n\t\"\"\"Node types for interface definitions.\"\"\"\n\n\tprotocol: ClassVar[list[str]]\n\t\"\"\"Node types for protocol/trait definitions.\"\"\"\n\n\tstruct: ClassVar[list[str]]\n\t\"\"\"Node types for struct definitions.\"\"\"\n\n\tenum: ClassVar[list[str]]\n\t\"\"\"Node types for enum declarations.\"\"\"\n\n\ttype_alias: ClassVar[list[str]]\n\t\"\"\"Node types for type aliases/typedefs.\"\"\"\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]]\n\t\"\"\"Node types for function declarations.\"\"\"\n\n\tmethod: ClassVar[list[str]]\n\t\"\"\"Node types for method declarations.\"\"\"\n\n\tproperty_def: ClassVar[list[str]]\n\t\"\"\"Node types for property/getter/setter declarations.\"\"\"\n\n\ttest_case: ClassVar[list[str]]\n\t\"\"\"Node types that identify test functions.\"\"\"\n\n\ttest_suite: ClassVar[list[str]]\n\t\"\"\"Node types that identify test classes/suites.\"\"\"\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]]\n\t\"\"\"Node types for variable declarations.\"\"\"\n\n\tconstant: ClassVar[list[str]]\n\t\"\"\"Node types for constant declarations.\"\"\"\n\n\tclass_field: ClassVar[list[str]]\n\t\"\"\"Node types for class field declarations.\"\"\"\n\n\t# Code organization\n\timport_: ClassVar[list[str]]\n\t\"\"\"Node types for import statements.\"\"\"\n\n\tdecorator: ClassVar[list[str]]\n\t\"\"\"Node types for decorators/annotations.\"\"\"\n\n\t# Documentation\n\tcomment: ClassVar[list[str]]\n\t\"\"\"Node types for general comments.\"\"\"\n\n\tdocstring: ClassVar[list[str]]\n\t\"\"\"Node types for documentation strings.\"\"\"\n\n\t# Language-specific metadata\n\tfile_extensions: ClassVar[list[str]]\n\t\"\"\"File extensions associated with this language (e.g., ['.py', '.pyi']).\"\"\"\n\n\ttree_sitter_name: ClassVar[str] = \"\"\n\t\"\"\"Tree-sitter language identifier.\"\"\"\n\n\t# Optional node types that might be language-specific\n\tdecorators: ClassVar[list[str] | None] = None\n\tclass_fields: ClassVar[list[str] | None] = None\n\n\t@property\n\tdef all_node_types(self) -&gt; set[str]:\n\t\t\"\"\"\n\t\tGet all node types defined in this configuration.\n\n\t\tReturns:\n\t\t    A set of all node types from all categories.\n\n\t\t\"\"\"\n\t\tall_types = set()\n\t\tfor attr in [\n\t\t\tself.module,\n\t\t\tself.namespace,\n\t\t\tself.class_,\n\t\t\tself.interface,\n\t\t\tself.protocol,\n\t\t\tself.struct,\n\t\t\tself.enum,\n\t\t\tself.type_alias,\n\t\t\tself.function,\n\t\t\tself.method,\n\t\t\tself.property_def,\n\t\t\tself.test_case,\n\t\t\tself.test_suite,\n\t\t\tself.variable,\n\t\t\tself.constant,\n\t\t\tself.class_field,\n\t\t\tself.import_,\n\t\t\tself.decorator,\n\t\t\tself.comment,\n\t\t\tself.docstring,\n\t\t\tself.decorators,\n\t\t\tself.class_fields,\n\t\t]:\n\t\t\tif attr:  # Skip None values\n\t\t\t\tall_types.update(attr)\n\t\treturn all_types\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str]\n</code></pre> <p>Node types that represent entire modules/files.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str]\n</code></pre> <p>Node types for namespace/package declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str]\n</code></pre> <p>Node types for class definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str]\n</code></pre> <p>Node types for interface definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str]\n</code></pre> <p>Node types for protocol/trait definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str]\n</code></pre> <p>Node types for struct definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str]\n</code></pre> <p>Node types for enum declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str]\n</code></pre> <p>Node types for type aliases/typedefs.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str]\n</code></pre> <p>Node types for function declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str]\n</code></pre> <p>Node types for method declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str]\n</code></pre> <p>Node types for property/getter/setter declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str]\n</code></pre> <p>Node types that identify test functions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str]\n</code></pre> <p>Node types that identify test classes/suites.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str]\n</code></pre> <p>Node types for variable declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str]\n</code></pre> <p>Node types for constant declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str]\n</code></pre> <p>Node types for class field declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str]\n</code></pre> <p>Node types for import statements.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str]\n</code></pre> <p>Node types for decorators/annotations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str]\n</code></pre> <p>Node types for general comments.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str]\n</code></pre> <p>Node types for documentation strings.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str]\n</code></pre> <p>File extensions associated with this language (e.g., ['.py', '.pyi']).</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = ''\n</code></pre> <p>Tree-sitter language identifier.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.decorators","title":"decorators  <code>class-attribute</code>","text":"<pre><code>decorators: list[str] | None = None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.class_fields","title":"class_fields  <code>class-attribute</code>","text":"<pre><code>class_fields: list[str] | None = None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.all_node_types","title":"all_node_types  <code>property</code>","text":"<pre><code>all_node_types: set[str]\n</code></pre> <p>Get all node types defined in this configuration.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>A set of all node types from all categories.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler","title":"LanguageSyntaxHandler","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for language-specific syntax handling.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class LanguageSyntaxHandler(abc.ABC):\n\t\"\"\"Abstract base class for language-specific syntax handling.\"\"\"\n\n\tdef __init__(self, config: LanguageConfig) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize with language configuration.\n\n\t\tArgs:\n\t\t    config: Language-specific configuration\n\n\t\t\"\"\"\n\t\tself.config = config\n\n\t@abc.abstractmethod\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a given node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\t\"\"\"\n\t\tFind the docstring associated with a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t    - The extracted docstring text (or None).\n\t\t    - The specific AST node representing the docstring that should be skipped\n\t\t      during child processing (or None).\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"\n\t\tGet the main body node for a function, method, or class.\n\n\t\tThis should be overridden by subclasses to find the appropriate block node.\n\n\t\tArgs:\n\t\t    node: The node representing the function, method, or class.\n\n\t\tReturns:\n\t\t    The body node, or None if not applicable/found.\n\n\t\t\"\"\"\n\t\t# Default implementation: returns None or the node itself as a naive fallback\n\t\t# Subclasses should find specific body nodes like 'block', 'statement_block' etc.\n\t\treturn node  # Naive fallback - subclasses MUST override\n\n\t@abc.abstractmethod\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef should_skip_node(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tDetermine if a node should be skipped entirely during processing.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node should be skipped\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported dependency names from an import node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (should be an import type)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported names\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract names of functions/methods called within a node's scope.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (e.g., function/method body)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of called function/method names\n\n\t\t\"\"\"\n\n\tdef extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\t\"\"\"\n\t\tExtract the signature (definition line without body) for a function, class, etc.\n\n\t\tArgs:\n\t\t    node: The node to extract the signature from.\n\t\t    content_bytes: Source code content as bytes.\n\n\t\tReturns:\n\t\t    The signature string, or None if not applicable.\n\n\t\t\"\"\"\n\t\t# Default implementation: return the first line of the node's text\n\t\ttry:\n\t\t\tfirst_line = content_bytes[node.start_byte : node.end_byte].split(b\"\\n\", 1)[0]\n\t\t\treturn first_line.decode(\"utf-8\", errors=\"ignore\").strip()\n\t\texcept (IndexError, UnicodeDecodeError):\n\t\t\t# Catch specific errors related to slicing and decoding\n\t\t\treturn None\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"\n\t\tFind the first ancestor node that matches the target entity type.\n\n\t\tArgs:\n\t\t    node: The starting node.\n\t\t    target_type: The EntityType to search for in ancestors.\n\n\t\tReturns:\n\t\t    The ancestor node if found, otherwise None.\n\n\t\t\"\"\"\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\t# We need content_bytes to determine the type accurately, but we don't have it here.\n\t\t\t# This highlights a limitation of doing this purely structurally without context.\n\t\t\t# Subclasses might need a different approach or access to the analyzer/content.\n\t\t\t# For a basic structural check:\n\t\t\t# entity_type = self.get_entity_type(current, current.parent, ???) # Need content_bytes\n\t\t\t# if entity_type == target_type:\n\t\t\t#    return current\n\n\t\t\t# Simplistic check based on node type name (less reliable)\n\t\t\ttarget_name = str(target_type.name).lower()  # Extract name explicitly for type checker\n\t\t\tif target_name in current.type.lower():  # Very rough check\n\t\t\t\treturn current\n\t\t\tcurrent = current.parent\n\t\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__(config: LanguageConfig) -&gt; None\n</code></pre> <p>Initialize with language configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LanguageConfig</code> <p>Language-specific configuration</p> required Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>def __init__(self, config: LanguageConfig) -&gt; None:\n\t\"\"\"\n\tInitialize with language configuration.\n\n\tArgs:\n\t    config: Language-specific configuration\n\n\t\"\"\"\n\tself.config = config\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_entity_type","title":"get_entity_type  <code>abstractmethod</code>","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a given node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.find_docstring","title":"find_docstring  <code>abstractmethod</code>","text":"<pre><code>find_docstring(\n\tnode: Node, content_bytes: bytes\n) -&gt; tuple[str | None, Node | None]\n</code></pre> <p>Find the docstring associated with a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A tuple containing:</p> <code>Node | None</code> <ul> <li>The extracted docstring text (or None).</li> </ul> <code>tuple[str | None, Node | None]</code> <ul> <li>The specific AST node representing the docstring that should be skipped during child processing (or None).</li> </ul> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\"\"\"\n\tFind the docstring associated with a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    A tuple containing:\n\t    - The extracted docstring text (or None).\n\t    - The specific AST node representing the docstring that should be skipped\n\t      during child processing (or None).\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_name","title":"extract_name  <code>abstractmethod</code>","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_body_node","title":"get_body_node  <code>abstractmethod</code>","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the main body node for a function, method, or class.</p> <p>This should be overridden by subclasses to find the appropriate block node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node representing the function, method, or class.</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The body node, or None if not applicable/found.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"\n\tGet the main body node for a function, method, or class.\n\n\tThis should be overridden by subclasses to find the appropriate block node.\n\n\tArgs:\n\t    node: The node representing the function, method, or class.\n\n\tReturns:\n\t    The body node, or None if not applicable/found.\n\n\t\"\"\"\n\t# Default implementation: returns None or the node itself as a naive fallback\n\t# Subclasses should find specific body nodes like 'block', 'statement_block' etc.\n\treturn node  # Naive fallback - subclasses MUST override\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_children_to_process","title":"get_children_to_process  <code>abstractmethod</code>","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.should_skip_node","title":"should_skip_node  <code>abstractmethod</code>","text":"<pre><code>should_skip_node(node: Node) -&gt; bool\n</code></pre> <p>Determine if a node should be skipped entirely during processing.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be skipped</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef should_skip_node(self, node: Node) -&gt; bool:\n\t\"\"\"\n\tDetermine if a node should be skipped entirely during processing.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    True if the node should be skipped\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_imports","title":"extract_imports  <code>abstractmethod</code>","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported dependency names from an import node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (should be an import type)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported dependency names from an import node.\n\n\tArgs:\n\t    node: The tree-sitter node (should be an import type)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported names\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_calls","title":"extract_calls  <code>abstractmethod</code>","text":"<pre><code>extract_calls(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract names of functions/methods called within a node's scope.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (e.g., function/method body)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of called function/method names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract names of functions/methods called within a node's scope.\n\n\tArgs:\n\t    node: The tree-sitter node (e.g., function/method body)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of called function/method names\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_signature","title":"extract_signature","text":"<pre><code>extract_signature(\n\tnode: Node, content_bytes: bytes\n) -&gt; str | None\n</code></pre> <p>Extract the signature (definition line without body) for a function, class, etc.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to extract the signature from.</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The signature string, or None if not applicable.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>def extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\"\"\"\n\tExtract the signature (definition line without body) for a function, class, etc.\n\n\tArgs:\n\t    node: The node to extract the signature from.\n\t    content_bytes: Source code content as bytes.\n\n\tReturns:\n\t    The signature string, or None if not applicable.\n\n\t\"\"\"\n\t# Default implementation: return the first line of the node's text\n\ttry:\n\t\tfirst_line = content_bytes[node.start_byte : node.end_byte].split(b\"\\n\", 1)[0]\n\t\treturn first_line.decode(\"utf-8\", errors=\"ignore\").strip()\n\texcept (IndexError, UnicodeDecodeError):\n\t\t# Catch specific errors related to slicing and decoding\n\t\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node that matches the target entity type.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The starting node.</p> required <code>target_type</code> <code>EntityType</code> <p>The EntityType to search for in ancestors.</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The ancestor node if found, otherwise None.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"\n\tFind the first ancestor node that matches the target entity type.\n\n\tArgs:\n\t    node: The starting node.\n\t    target_type: The EntityType to search for in ancestors.\n\n\tReturns:\n\t    The ancestor node if found, otherwise None.\n\n\t\"\"\"\n\tcurrent = node.parent\n\twhile current:\n\t\t# We need content_bytes to determine the type accurately, but we don't have it here.\n\t\t# This highlights a limitation of doing this purely structurally without context.\n\t\t# Subclasses might need a different approach or access to the analyzer/content.\n\t\t# For a basic structural check:\n\t\t# entity_type = self.get_entity_type(current, current.parent, ???) # Need content_bytes\n\t\t# if entity_type == target_type:\n\t\t#    return current\n\n\t\t# Simplistic check based on node type name (less reliable)\n\t\ttarget_name = str(target_type.name).lower()  # Extract name explicitly for type checker\n\t\tif target_name in current.type.lower():  # Very rough check\n\t\t\treturn current\n\t\tcurrent = current.parent\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig","title":"PythonConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for Python language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class PythonConfig(LanguageConfig):\n\t\"\"\"Configuration for Python language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"module\"]\n\tclass_: ClassVar[list[str]] = [\"class_definition\"]\n\tfunction: ClassVar[list[str]] = [\"function_definition\"]\n\tproperty_def: ClassVar[list[str]] = [\"decorated_definition\"]\n\tstruct: ClassVar[list[str]] = []\n\tdocstring: ClassVar[list[str]] = [\"string\"]\n\tfile_extensions: ClassVar[list[str]] = [\".py\", \".pyi\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['module']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = ['decorated_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['string']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.py', '.pyi']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig","title":"JavaScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for JavaScript language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class JavaScriptConfig(LanguageConfig):\n\t\"\"\"Configuration for JavaScript language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tfunction: ClassVar[list[str]] = [\"function_declaration\", \"method_definition\", \"function\"]\n\tproperty_def: ClassVar[list[str]] = [\"property_definition\", \"property_identifier\"]\n\tstruct: ClassVar[list[str]] = []\n\tdocstring: ClassVar[list[str]] = [\"comment\"]\n\tfile_extensions: ClassVar[list[str]] = [\".js\", \".jsx\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [\n\t\"function_declaration\",\n\t\"method_definition\",\n\t\"function\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t\"property_definition\",\n\t\"property_identifier\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.js', '.jsx']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig","title":"TypeScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for TypeScript language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class TypeScriptConfig(LanguageConfig):\n\t\"\"\"Configuration for TypeScript language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tfunction: ClassVar[list[str]] = [\"function_declaration\", \"method_definition\", \"function\"]\n\tproperty_def: ClassVar[list[str]] = [\"property_definition\", \"property_identifier\"]\n\tstruct: ClassVar[list[str]] = []\n\tdocstring: ClassVar[list[str]] = [\"comment\"]\n\tfile_extensions: ClassVar[list[str]] = [\".ts\", \".tsx\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [\n\t\"function_declaration\",\n\t\"method_definition\",\n\t\"function\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t\"property_definition\",\n\t\"property_identifier\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.ts', '.tsx']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/","title":"Javascript","text":"<p>JavaScript-specific configuration for syntax chunking.</p>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig","title":"JavaScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>JavaScript-specific syntax chunking configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>class JavaScriptConfig(LanguageConfig):\n\t\"\"\"JavaScript-specific syntax chunking configuration.\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tnamespace: ClassVar[list[str]] = [\"export_statement\"]  # Using export as namespace indicator\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tinterface: ClassVar[list[str]] = []  # Pure JS doesn't have interfaces\n\tprotocol: ClassVar[list[str]] = []  # Pure JS doesn't have protocols\n\tstruct: ClassVar[list[str]] = []  # Pure JS doesn't have structs\n\tenum: ClassVar[list[str]] = []  # Pure JS doesn't have enums\n\ttype_alias: ClassVar[list[str]] = []  # Pure JS doesn't have type aliases\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]] = [\n\t\t\"function_declaration\",\n\t\t\"function\",\n\t\t\"arrow_function\",\n\t\t\"generator_function_declaration\",\n\t]\n\tmethod: ClassVar[list[str]] = [\"method_definition\"]\n\tproperty_def: ClassVar[list[str]] = [\"property_identifier\", \"public_field_definition\"]\n\ttest_case: ClassVar[list[str]] = [\"call_expression\"]  # Special detection for test frameworks\n\ttest_suite: ClassVar[list[str]] = [\"call_expression\"]  # Special detection for test frameworks\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]] = [\"variable_declaration\", \"lexical_declaration\"]\n\tconstant: ClassVar[list[str]] = [\"variable_declaration\", \"lexical_declaration\"]  # const declarations\n\tclass_field: ClassVar[list[str]] = [\"public_field_definition\"]\n\n\t# Code organization\n\timport_: ClassVar[list[str]] = [\"import_statement\"]\n\tdecorator: ClassVar[list[str]] = [\"decorator\"]\n\n\t# Documentation\n\tcomment: ClassVar[list[str]] = [\"comment\"]\n\tdocstring: ClassVar[list[str]] = [\"comment\"]  # JS uses comments for documentation\n\n\tfile_extensions: ClassVar[list[str]] = [\".js\", \".jsx\", \".mjs\", \".cjs\"]\n\ttree_sitter_name: ClassVar[str] = \"javascript\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str] = ['export_statement']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [\n\t\"function_declaration\",\n\t\"function\",\n\t\"arrow_function\",\n\t\"generator_function_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str] = ['method_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t\"property_identifier\",\n\t\"public_field_definition\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str] = ['call_expression']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str] = ['call_expression']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str] = [\n\t\"variable_declaration\",\n\t\"lexical_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str] = [\n\t\"variable_declaration\",\n\t\"lexical_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str] = ['public_field_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str] = ['import_statement']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str] = ['decorator']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = [\".js\", \".jsx\", \".mjs\", \".cjs\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = 'javascript'\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JAVASCRIPT_CONFIG","title":"JAVASCRIPT_CONFIG  <code>module-attribute</code>","text":"<pre><code>JAVASCRIPT_CONFIG = JavaScriptConfig()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler","title":"JavaScriptSyntaxHandler","text":"<p>               Bases: <code>LanguageSyntaxHandler</code></p> <p>JavaScript-specific syntax handling logic.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>class JavaScriptSyntaxHandler(LanguageSyntaxHandler):\n\t\"\"\"JavaScript-specific syntax handling logic.\"\"\"\n\n\t@staticmethod\n\tdef _get_node_text(node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"Helper to get node text safely.\"\"\"\n\t\ttry:\n\t\t\treturn content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept IndexError:\n\t\t\treturn \"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize with JavaScript configuration.\"\"\"\n\t\tsuper().__init__(JAVASCRIPT_CONFIG)\n\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a JavaScript node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\t\tnode_type = node.type\n\t\tlogger.debug(\n\t\t\t\"Getting entity type for JavaScript node: type=%s, parent_type=%s\",\n\t\t\tnode_type,\n\t\t\tparent.type if parent else None,\n\t\t)\n\n\t\t# Module-level\n\t\tif node_type in self.config.module:\n\t\t\treturn EntityType.MODULE\n\t\tif node_type in self.config.namespace:\n\t\t\treturn EntityType.NAMESPACE\n\n\t\t# Documentation\n\t\tif node_type in self.config.comment:\n\t\t\t# Check if this is a JSDoc comment (starts with /**)\n\t\t\tif self._is_jsdoc_comment(node, content_bytes):\n\t\t\t\treturn EntityType.DOCSTRING\n\t\t\treturn EntityType.COMMENT\n\n\t\t# Type definitions\n\t\tif node_type in self.config.class_:\n\t\t\treturn EntityType.CLASS\n\n\t\t# Functions and methods\n\t\tif node_type in self.config.function:\n\t\t\t# Check if this is a test function (for frameworks like Jest, Mocha)\n\t\t\tif self._is_test_function(node, content_bytes):\n\t\t\t\treturn EntityType.TEST_CASE\n\t\t\treturn EntityType.FUNCTION\n\n\t\tif node_type in self.config.method:\n\t\t\treturn EntityType.METHOD\n\n\t\t# Check for test suite declarations (describe blocks in Jest/Mocha)\n\t\tif node_type in self.config.test_suite and self._is_test_suite(node, content_bytes):\n\t\t\treturn EntityType.TEST_SUITE\n\n\t\t# Property definitions\n\t\tif node_type in self.config.property_def:\n\t\t\treturn EntityType.PROPERTY\n\n\t\t# Variables and constants\n\t\tif node_type in self.config.variable:\n\t\t\t# Check if it's a const declaration\n\t\t\tif self._is_constant(node, content_bytes):\n\t\t\t\treturn EntityType.CONSTANT\n\t\t\treturn EntityType.VARIABLE\n\n\t\t# Class fields\n\t\tif node_type in self.config.class_field:\n\t\t\treturn EntityType.CLASS_FIELD\n\n\t\t# Code organization\n\t\tif node_type in self.config.import_:\n\t\t\treturn EntityType.IMPORT\n\n\t\treturn EntityType.UNKNOWN\n\n\tdef _is_jsdoc_comment(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a comment node is a JSDoc comment.\n\n\t\tArgs:\n\t\t    node: The comment node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a JSDoc comment\n\n\t\t\"\"\"\n\t\tif node.type != \"comment\":\n\t\t\treturn False\n\n\t\ttry:\n\t\t\tcomment_text = content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\treturn comment_text.startswith(\"/**\") and comment_text.endswith(\"*/\")\n\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\treturn False\n\n\tdef _is_constant(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a variable declaration is a constant.\n\n\t\tArgs:\n\t\t    node: The variable declaration node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a constant declaration\n\n\t\t\"\"\"\n\t\tif node.type not in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t\treturn False\n\n\t\ttry:\n\t\t\tdecl_text = content_bytes[node.start_byte : node.start_byte + 5].decode(\"utf-8\", errors=\"ignore\")\n\t\t\treturn decl_text.startswith(\"const\")\n\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\treturn False\n\n\tdef _is_test_function(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a function is a test function.\n\n\t\tArgs:\n\t\t    node: The function node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a test function\n\n\t\t\"\"\"\n\t\tif node.type == \"call_expression\":\n\t\t\tcallee = node.child_by_field_name(\"function\")\n\t\t\tif callee:\n\t\t\t\ttry:\n\t\t\t\t\tcallee_text = content_bytes[callee.start_byte : callee.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\treturn callee_text in [\"it\", \"test\"]\n\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\tpass\n\t\treturn False\n\n\tdef _is_test_suite(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a node is a test suite declaration.\n\n\t\tArgs:\n\t\t    node: The node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a test suite declaration\n\n\t\t\"\"\"\n\t\tif node.type == \"call_expression\":\n\t\t\tcallee = node.child_by_field_name(\"function\")\n\t\t\tif callee:\n\t\t\t\ttry:\n\t\t\t\t\tcallee_text = content_bytes[callee.start_byte : callee.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\treturn callee_text == \"describe\"\n\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\tpass\n\t\treturn False\n\n\tdef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\t\"\"\"\n\t\tFind the docstring associated with a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t    - The extracted docstring text (or None).\n\t\t    - The specific AST node representing the docstring (or None).\n\n\t\t\"\"\"\n\t\t# For functions, classes, and other definition nodes\n\t\tparent_node = node.parent\n\n\t\t# Look for JSDoc comments immediately preceding the node\n\t\tif parent_node:\n\t\t\tindex = None\n\t\t\tfor i, child in enumerate(parent_node.children):\n\t\t\t\tif child == node:\n\t\t\t\t\tindex = i\n\t\t\t\t\tbreak\n\n\t\t\tif index is not None and index &gt; 0:\n\t\t\t\tprev_node = parent_node.children[index - 1]\n\t\t\t\tif prev_node.type == \"comment\" and self._is_jsdoc_comment(prev_node, content_bytes):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcomment_text = content_bytes[prev_node.start_byte : prev_node.end_byte].decode(\n\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t# Clean JSDoc format: remove /** */ and trim\n\t\t\t\t\t\tcomment_text = comment_text.strip()\n\t\t\t\t\t\tcomment_text = comment_text.removeprefix(\"/**\")\n\t\t\t\t\t\tcomment_text = comment_text.removesuffix(\"*/\")\n\t\t\t\t\t\treturn comment_text.strip(), prev_node\n\t\t\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\t\t\tlogger.warning(\"Failed to decode JavaScript comment: %s\", e)\n\n\t\treturn None, None\n\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\t\t# Try to find the name field based on node type\n\t\tname_node = None\n\n\t\tif node.type in [\"function_declaration\", \"class_declaration\", \"method_definition\"]:\n\t\t\tname_node = node.child_by_field_name(\"name\")\n\t\telif node.type == \"property_identifier\":\n\t\t\tname_node = node\n\t\telif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t\t# Get the first declarator and its name\n\t\t\tdeclarator = node.child_by_field_name(\"declarations\")\n\t\t\tif declarator and declarator.named_child_count &gt; 0:\n\t\t\t\tfirst_declarator = declarator.named_children[0]\n\t\t\t\tname_node = first_declarator.child_by_field_name(\"name\")\n\t\telif node.type == \"public_field_definition\":\n\t\t\tname_node = node.child_by_field_name(\"name\")\n\n\t\tif name_node:\n\t\t\ttry:\n\t\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode JavaScript name: %s\", e)\n\t\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t\t# For call expressions that represent tests or suites\n\t\tif node.type == \"call_expression\":\n\t\t\tcallee = node.child_by_field_name(\"function\")\n\t\t\targuments = node.child_by_field_name(\"arguments\")\n\n\t\t\tif callee and arguments and arguments.named_child_count &gt; 0:\n\t\t\t\t# First argument is typically the test/suite name\n\t\t\t\tfirst_arg = arguments.named_children[0]\n\t\t\t\tif first_arg.type == \"string\":\n\t\t\t\t\ttry:\n\t\t\t\t\t\tname = content_bytes[first_arg.start_byte : first_arg.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\t\t# Remove quotes\n\t\t\t\t\t\treturn name.strip(\"\\\"'\")\n\t\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\t\tpass\n\n\t\treturn f\"&lt;anonymous-{node.type}&gt;\"\n\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"Get the statement block node for JS/TS function/class/method body.\"\"\"\n\t\t# Common body node type in JS/TS grammar\n\t\tbody_field_names = [\"body\", \"statement_block\"]\n\t\tfor field_name in body_field_names:\n\t\t\tbody_node = node.child_by_field_name(field_name)\n\t\t\tif body_node:\n\t\t\t\t# Sometimes the direct body is an expression (arrow functions)\n\t\t\t\t# Check if the found node is a block type\n\t\t\t\tif body_node.type == \"statement_block\":\n\t\t\t\t\treturn body_node\n\t\t\t\tif body_node.type == \"expression_statement\":  # Arrow function returning object literal\n\t\t\t\t\tif body_node.child_count &gt; 0 and body_node.children[0].type == \"object\":\n\t\t\t\t\t\treturn body_node  # Treat expression as body\n\t\t\t\telif node.type == \"arrow_function\":  # Direct expression in arrow function\n\t\t\t\t\treturn body_node\n\t\t# Fallback for classes where body might be direct children within curly braces\n\t\tif node.type in (\"class_declaration\", \"class\"):\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"class_body\":\n\t\t\t\t\treturn child\n\t\treturn None\n\n\tdef extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\t\"\"\"Extract the signature up to the opening curly brace '{' for JS/TS.\"\"\"\n\t\t# Find the body node first\n\t\tbody_node = self.get_body_node(node)\n\n\t\tif body_node:\n\t\t\t# Signature is everything from the start of the node up to the start of the body\n\t\t\tstart_byte = node.start_byte\n\t\t\tend_byte = body_node.start_byte\n\t\t\t# Adjust end_byte to exclude trailing whitespace before the body\n\t\t\twhile end_byte &gt; start_byte and content_bytes[end_byte - 1 : end_byte].isspace():\n\t\t\t\tend_byte -= 1\n\t\t\ttry:\n\t\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\t\texcept IndexError:\n\t\t\t\treturn None\n\t\telse:\n\t\t\t# Fallback: if no body found (e.g., abstract method, interface?), return the first line\n\t\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"Find the first ancestor node matching the target JS/TS entity type.\"\"\"\n\t\ttarget_node_types = []\n\t\tif target_type == EntityType.CLASS:\n\t\t\ttarget_node_types = [\"class_declaration\", \"class\", \"class_expression\"]\n\t\telif target_type == EntityType.FUNCTION:\n\t\t\t# Includes function declarations, arrow functions, function expressions\n\t\t\ttarget_node_types = [\"function_declaration\", \"arrow_function\", \"function\"]\n\t\telif target_type == EntityType.METHOD:\n\t\t\ttarget_node_types = [\"method_definition\"]\n\t\telif target_type == EntityType.MODULE:\n\t\t\t# Module is typically the root 'program' node\n\t\t\tcurrent = node\n\t\t\twhile current.parent:\n\t\t\t\tcurrent = current.parent\n\t\t\treturn current if current.type == \"program\" else None\n\t\t# Add other types if needed (e.g., INTERFACE for TS)\n\n\t\tif not target_node_types:\n\t\t\treturn None\n\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type in target_node_types:\n\t\t\t\treturn current\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\t\t# Process children of the body node if it exists, otherwise process direct children\n\t\tif body_node:\n\t\t\treturn list(body_node.children)\n\n\t\t# Special handling for certain nodes\n\t\tif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t\t# Process the declarations field\n\t\t\tdeclarations = node.child_by_field_name(\"declarations\")\n\t\t\treturn [declarations] if declarations else []\n\n\t\treturn list(node.children)\n\n\tdef should_skip_node(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tDetermine if a node should be skipped entirely during processing.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node should be skipped\n\n\t\t\"\"\"\n\t\t# Skip non-named nodes (like punctuation, operators)\n\t\tif not node.is_named:\n\t\t\treturn True\n\n\t\t# Skip syntax nodes that don't contribute to code structure\n\t\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\", \":\", \"=&gt;\"]\n\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported module names from a JavaScript import statement.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node representing an import statement\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported module names as strings\n\n\t\t\"\"\"\n\t\tif node.type not in self.config.import_:\n\t\t\treturn []\n\n\t\timported_names = []\n\n\t\ttry:\n\t\t\t# Find the source (module path) of the import\n\t\t\tsource_node = node.child_by_field_name(\"source\")\n\t\t\tif not source_node:\n\t\t\t\treturn []\n\n\t\t\t# Extract the module path from the string literal\n\t\t\tmodule_path = content_bytes[source_node.start_byte : source_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t# Remove quotes\n\t\t\tmodule_path = module_path.strip(\"\\\"'\")\n\n\t\t\t# Check for different import patterns:\n\n\t\t\t# 1. Default import: \"import Name from 'module'\"\n\t\t\tdefault_import = node.child_by_field_name(\"default\")\n\t\t\tif default_import:\n\t\t\t\tname = content_bytes[default_import.start_byte : default_import.end_byte].decode(\n\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t)\n\t\t\t\timported_names.append(f\"{module_path}.default\")\n\n\t\t\t# 2. Named imports: \"import { foo, bar as baz } from 'module'\"\n\t\t\tnamed_imports = node.child_by_field_name(\"named_imports\")\n\t\t\tif named_imports:\n\t\t\t\tfor child in named_imports.children:\n\t\t\t\t\tif child.type == \"import_specifier\":\n\t\t\t\t\t\timported_name = child.child_by_field_name(\"name\")\n\t\t\t\t\t\tif imported_name:\n\t\t\t\t\t\t\tname = content_bytes[imported_name.start_byte : imported_name.end_byte].decode(\n\t\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\timported_names.append(f\"{module_path}.{name}\")\n\n\t\t\t# 3. Namespace import: \"import * as Name from 'module'\"\n\t\t\tnamespace_import = node.child_by_field_name(\"namespace_import\")\n\t\t\tif namespace_import:\n\t\t\t\timported_names.append(f\"{module_path}.*\")\n\n\t\t\t# If no specific imports found but we have a module, add the whole module\n\t\t\tif not imported_names and module_path:\n\t\t\t\timported_names.append(module_path)\n\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode JavaScript imports: %s\", e)\n\n\t\treturn imported_names\n\n\tdef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract names of functions/methods called within a JS node's scope.\n\n\t\tRecursively searches for 'call_expression' nodes and extracts the function identifier.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (e.g., function/method body)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of called function/method names\n\n\t\t\"\"\"\n\t\tcalls = []\n\t\tfor child in node.children:\n\t\t\tif child.type == \"call_expression\":\n\t\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\t\tif function_node:\n\t\t\t\t\t# Extract the identifier (e.g., funcName, obj.methodName)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcall_name = content_bytes[function_node.start_byte : function_node.end_byte].decode(\n\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\tcalls.append(call_name)\n\t\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\t\tpass  # Ignore decoding errors\n\t\t\t# Recursively search deeper within non-call children\n\t\t\telse:\n\t\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\t\treturn list(set(calls))  # Return unique calls\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize with JavaScript configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize with JavaScript configuration.\"\"\"\n\tsuper().__init__(JAVASCRIPT_CONFIG)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_entity_type","title":"get_entity_type","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a JavaScript node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a JavaScript node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n\tnode_type = node.type\n\tlogger.debug(\n\t\t\"Getting entity type for JavaScript node: type=%s, parent_type=%s\",\n\t\tnode_type,\n\t\tparent.type if parent else None,\n\t)\n\n\t# Module-level\n\tif node_type in self.config.module:\n\t\treturn EntityType.MODULE\n\tif node_type in self.config.namespace:\n\t\treturn EntityType.NAMESPACE\n\n\t# Documentation\n\tif node_type in self.config.comment:\n\t\t# Check if this is a JSDoc comment (starts with /**)\n\t\tif self._is_jsdoc_comment(node, content_bytes):\n\t\t\treturn EntityType.DOCSTRING\n\t\treturn EntityType.COMMENT\n\n\t# Type definitions\n\tif node_type in self.config.class_:\n\t\treturn EntityType.CLASS\n\n\t# Functions and methods\n\tif node_type in self.config.function:\n\t\t# Check if this is a test function (for frameworks like Jest, Mocha)\n\t\tif self._is_test_function(node, content_bytes):\n\t\t\treturn EntityType.TEST_CASE\n\t\treturn EntityType.FUNCTION\n\n\tif node_type in self.config.method:\n\t\treturn EntityType.METHOD\n\n\t# Check for test suite declarations (describe blocks in Jest/Mocha)\n\tif node_type in self.config.test_suite and self._is_test_suite(node, content_bytes):\n\t\treturn EntityType.TEST_SUITE\n\n\t# Property definitions\n\tif node_type in self.config.property_def:\n\t\treturn EntityType.PROPERTY\n\n\t# Variables and constants\n\tif node_type in self.config.variable:\n\t\t# Check if it's a const declaration\n\t\tif self._is_constant(node, content_bytes):\n\t\t\treturn EntityType.CONSTANT\n\t\treturn EntityType.VARIABLE\n\n\t# Class fields\n\tif node_type in self.config.class_field:\n\t\treturn EntityType.CLASS_FIELD\n\n\t# Code organization\n\tif node_type in self.config.import_:\n\t\treturn EntityType.IMPORT\n\n\treturn EntityType.UNKNOWN\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.find_docstring","title":"find_docstring","text":"<pre><code>find_docstring(\n\tnode: Node, content_bytes: bytes\n) -&gt; tuple[str | None, Node | None]\n</code></pre> <p>Find the docstring associated with a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A tuple containing:</p> <code>Node | None</code> <ul> <li>The extracted docstring text (or None).</li> </ul> <code>tuple[str | None, Node | None]</code> <ul> <li>The specific AST node representing the docstring (or None).</li> </ul> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\"\"\"\n\tFind the docstring associated with a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    A tuple containing:\n\t    - The extracted docstring text (or None).\n\t    - The specific AST node representing the docstring (or None).\n\n\t\"\"\"\n\t# For functions, classes, and other definition nodes\n\tparent_node = node.parent\n\n\t# Look for JSDoc comments immediately preceding the node\n\tif parent_node:\n\t\tindex = None\n\t\tfor i, child in enumerate(parent_node.children):\n\t\t\tif child == node:\n\t\t\t\tindex = i\n\t\t\t\tbreak\n\n\t\tif index is not None and index &gt; 0:\n\t\t\tprev_node = parent_node.children[index - 1]\n\t\t\tif prev_node.type == \"comment\" and self._is_jsdoc_comment(prev_node, content_bytes):\n\t\t\t\ttry:\n\t\t\t\t\tcomment_text = content_bytes[prev_node.start_byte : prev_node.end_byte].decode(\n\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t)\n\t\t\t\t\t# Clean JSDoc format: remove /** */ and trim\n\t\t\t\t\tcomment_text = comment_text.strip()\n\t\t\t\t\tcomment_text = comment_text.removeprefix(\"/**\")\n\t\t\t\t\tcomment_text = comment_text.removesuffix(\"*/\")\n\t\t\t\t\treturn comment_text.strip(), prev_node\n\t\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\t\tlogger.warning(\"Failed to decode JavaScript comment: %s\", e)\n\n\treturn None, None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_name","title":"extract_name","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n\t# Try to find the name field based on node type\n\tname_node = None\n\n\tif node.type in [\"function_declaration\", \"class_declaration\", \"method_definition\"]:\n\t\tname_node = node.child_by_field_name(\"name\")\n\telif node.type == \"property_identifier\":\n\t\tname_node = node\n\telif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t# Get the first declarator and its name\n\t\tdeclarator = node.child_by_field_name(\"declarations\")\n\t\tif declarator and declarator.named_child_count &gt; 0:\n\t\t\tfirst_declarator = declarator.named_children[0]\n\t\t\tname_node = first_declarator.child_by_field_name(\"name\")\n\telif node.type == \"public_field_definition\":\n\t\tname_node = node.child_by_field_name(\"name\")\n\n\tif name_node:\n\t\ttry:\n\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode JavaScript name: %s\", e)\n\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t# For call expressions that represent tests or suites\n\tif node.type == \"call_expression\":\n\t\tcallee = node.child_by_field_name(\"function\")\n\t\targuments = node.child_by_field_name(\"arguments\")\n\n\t\tif callee and arguments and arguments.named_child_count &gt; 0:\n\t\t\t# First argument is typically the test/suite name\n\t\t\tfirst_arg = arguments.named_children[0]\n\t\t\tif first_arg.type == \"string\":\n\t\t\t\ttry:\n\t\t\t\t\tname = content_bytes[first_arg.start_byte : first_arg.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\t# Remove quotes\n\t\t\t\t\treturn name.strip(\"\\\"'\")\n\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\tpass\n\n\treturn f\"&lt;anonymous-{node.type}&gt;\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_body_node","title":"get_body_node","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the statement block node for JS/TS function/class/method body.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"Get the statement block node for JS/TS function/class/method body.\"\"\"\n\t# Common body node type in JS/TS grammar\n\tbody_field_names = [\"body\", \"statement_block\"]\n\tfor field_name in body_field_names:\n\t\tbody_node = node.child_by_field_name(field_name)\n\t\tif body_node:\n\t\t\t# Sometimes the direct body is an expression (arrow functions)\n\t\t\t# Check if the found node is a block type\n\t\t\tif body_node.type == \"statement_block\":\n\t\t\t\treturn body_node\n\t\t\tif body_node.type == \"expression_statement\":  # Arrow function returning object literal\n\t\t\t\tif body_node.child_count &gt; 0 and body_node.children[0].type == \"object\":\n\t\t\t\t\treturn body_node  # Treat expression as body\n\t\t\telif node.type == \"arrow_function\":  # Direct expression in arrow function\n\t\t\t\treturn body_node\n\t# Fallback for classes where body might be direct children within curly braces\n\tif node.type in (\"class_declaration\", \"class\"):\n\t\tfor child in node.children:\n\t\t\tif child.type == \"class_body\":\n\t\t\t\treturn child\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_signature","title":"extract_signature","text":"<pre><code>extract_signature(\n\tnode: Node, content_bytes: bytes\n) -&gt; str | None\n</code></pre> <p>Extract the signature up to the opening curly brace '{' for JS/TS.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\"\"\"Extract the signature up to the opening curly brace '{' for JS/TS.\"\"\"\n\t# Find the body node first\n\tbody_node = self.get_body_node(node)\n\n\tif body_node:\n\t\t# Signature is everything from the start of the node up to the start of the body\n\t\tstart_byte = node.start_byte\n\t\tend_byte = body_node.start_byte\n\t\t# Adjust end_byte to exclude trailing whitespace before the body\n\t\twhile end_byte &gt; start_byte and content_bytes[end_byte - 1 : end_byte].isspace():\n\t\t\tend_byte -= 1\n\t\ttry:\n\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\texcept IndexError:\n\t\t\treturn None\n\telse:\n\t\t# Fallback: if no body found (e.g., abstract method, interface?), return the first line\n\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node matching the target JS/TS entity type.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"Find the first ancestor node matching the target JS/TS entity type.\"\"\"\n\ttarget_node_types = []\n\tif target_type == EntityType.CLASS:\n\t\ttarget_node_types = [\"class_declaration\", \"class\", \"class_expression\"]\n\telif target_type == EntityType.FUNCTION:\n\t\t# Includes function declarations, arrow functions, function expressions\n\t\ttarget_node_types = [\"function_declaration\", \"arrow_function\", \"function\"]\n\telif target_type == EntityType.METHOD:\n\t\ttarget_node_types = [\"method_definition\"]\n\telif target_type == EntityType.MODULE:\n\t\t# Module is typically the root 'program' node\n\t\tcurrent = node\n\t\twhile current.parent:\n\t\t\tcurrent = current.parent\n\t\treturn current if current.type == \"program\" else None\n\t# Add other types if needed (e.g., INTERFACE for TS)\n\n\tif not target_node_types:\n\t\treturn None\n\n\tcurrent = node.parent\n\twhile current:\n\t\tif current.type in target_node_types:\n\t\t\treturn current\n\t\tcurrent = current.parent\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_children_to_process","title":"get_children_to_process","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n\t# Process children of the body node if it exists, otherwise process direct children\n\tif body_node:\n\t\treturn list(body_node.children)\n\n\t# Special handling for certain nodes\n\tif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t# Process the declarations field\n\t\tdeclarations = node.child_by_field_name(\"declarations\")\n\t\treturn [declarations] if declarations else []\n\n\treturn list(node.children)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.should_skip_node","title":"should_skip_node","text":"<pre><code>should_skip_node(node: Node) -&gt; bool\n</code></pre> <p>Determine if a node should be skipped entirely during processing.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be skipped</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def should_skip_node(self, node: Node) -&gt; bool:\n\t\"\"\"\n\tDetermine if a node should be skipped entirely during processing.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    True if the node should be skipped\n\n\t\"\"\"\n\t# Skip non-named nodes (like punctuation, operators)\n\tif not node.is_named:\n\t\treturn True\n\n\t# Skip syntax nodes that don't contribute to code structure\n\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\", \":\", \"=&gt;\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_imports","title":"extract_imports","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported module names from a JavaScript import statement.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node representing an import statement</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported module names as strings</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported module names from a JavaScript import statement.\n\n\tArgs:\n\t    node: The tree-sitter node representing an import statement\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported module names as strings\n\n\t\"\"\"\n\tif node.type not in self.config.import_:\n\t\treturn []\n\n\timported_names = []\n\n\ttry:\n\t\t# Find the source (module path) of the import\n\t\tsource_node = node.child_by_field_name(\"source\")\n\t\tif not source_node:\n\t\t\treturn []\n\n\t\t# Extract the module path from the string literal\n\t\tmodule_path = content_bytes[source_node.start_byte : source_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t# Remove quotes\n\t\tmodule_path = module_path.strip(\"\\\"'\")\n\n\t\t# Check for different import patterns:\n\n\t\t# 1. Default import: \"import Name from 'module'\"\n\t\tdefault_import = node.child_by_field_name(\"default\")\n\t\tif default_import:\n\t\t\tname = content_bytes[default_import.start_byte : default_import.end_byte].decode(\n\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t)\n\t\t\timported_names.append(f\"{module_path}.default\")\n\n\t\t# 2. Named imports: \"import { foo, bar as baz } from 'module'\"\n\t\tnamed_imports = node.child_by_field_name(\"named_imports\")\n\t\tif named_imports:\n\t\t\tfor child in named_imports.children:\n\t\t\t\tif child.type == \"import_specifier\":\n\t\t\t\t\timported_name = child.child_by_field_name(\"name\")\n\t\t\t\t\tif imported_name:\n\t\t\t\t\t\tname = content_bytes[imported_name.start_byte : imported_name.end_byte].decode(\n\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\timported_names.append(f\"{module_path}.{name}\")\n\n\t\t# 3. Namespace import: \"import * as Name from 'module'\"\n\t\tnamespace_import = node.child_by_field_name(\"namespace_import\")\n\t\tif namespace_import:\n\t\t\timported_names.append(f\"{module_path}.*\")\n\n\t\t# If no specific imports found but we have a module, add the whole module\n\t\tif not imported_names and module_path:\n\t\t\timported_names.append(module_path)\n\n\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\tlogger.warning(\"Failed to decode JavaScript imports: %s\", e)\n\n\treturn imported_names\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_calls","title":"extract_calls","text":"<pre><code>extract_calls(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract names of functions/methods called within a JS node's scope.</p> <p>Recursively searches for 'call_expression' nodes and extracts the function identifier.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (e.g., function/method body)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of called function/method names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract names of functions/methods called within a JS node's scope.\n\n\tRecursively searches for 'call_expression' nodes and extracts the function identifier.\n\n\tArgs:\n\t    node: The tree-sitter node (e.g., function/method body)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of called function/method names\n\n\t\"\"\"\n\tcalls = []\n\tfor child in node.children:\n\t\tif child.type == \"call_expression\":\n\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\tif function_node:\n\t\t\t\t# Extract the identifier (e.g., funcName, obj.methodName)\n\t\t\t\ttry:\n\t\t\t\t\tcall_name = content_bytes[function_node.start_byte : function_node.end_byte].decode(\n\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t)\n\t\t\t\t\tcalls.append(call_name)\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tpass  # Ignore decoding errors\n\t\t# Recursively search deeper within non-call children\n\t\telse:\n\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\treturn list(set(calls))  # Return unique calls\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/","title":"Python","text":"<p>Python-specific configuration for syntax chunking.</p>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig","title":"PythonConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Python-specific syntax chunking configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>class PythonConfig(LanguageConfig):\n\t\"\"\"Python-specific syntax chunking configuration.\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]] = [\"module\"]\n\tnamespace: ClassVar[list[str]] = []  # Python doesn't have explicit namespaces\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]] = [\"class_definition\"]\n\tinterface: ClassVar[list[str]] = [\"class_definition\"]  # Python uses ABC classes\n\tprotocol: ClassVar[list[str]] = [\"class_definition\"]  # Protocol classes\n\tstruct: ClassVar[list[str]] = [\"class_definition\"]  # Python uses regular classes\n\tenum: ClassVar[list[str]] = [\"class_definition\"]  # Enum classes\n\ttype_alias: ClassVar[list[str]] = [\"assignment\"]  # Type assignments\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]] = [\"function_definition\"]\n\tmethod: ClassVar[list[str]] = [\"function_definition\"]  # Inside class\n\tproperty_def: ClassVar[list[str]] = [\"decorated_definition\"]  # @property decorated functions\n\ttest_case: ClassVar[list[str]] = [\"function_definition\"]  # test_* functions\n\ttest_suite: ClassVar[list[str]] = [\"class_definition\"]  # Test* classes\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]] = [\"assignment\"]\n\tconstant: ClassVar[list[str]] = [\"assignment\"]  # Uppercase assignments\n\tclass_field: ClassVar[list[str]] = [\"class_variable_definition\"]\n\n\t# Code organization\n\timport_: ClassVar[list[str]] = [\"import_statement\", \"import_from_statement\"]\n\tdecorator: ClassVar[list[str]] = [\"decorator\"]\n\n\t# Documentation\n\tcomment: ClassVar[list[str]] = [\"comment\"]\n\tdocstring: ClassVar[list[str]] = [\"string\"]  # First string in module/class/function\n\n\tfile_extensions: ClassVar[list[str]] = [\".py\", \".pyi\"]\n\ttree_sitter_name: ClassVar[str] = \"python\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['module']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str] = ['assignment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = ['decorated_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str] = ['assignment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str] = ['assignment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str] = ['class_variable_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str] = [\n\t\"import_statement\",\n\t\"import_from_statement\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str] = ['decorator']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['string']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.py', '.pyi']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = 'python'\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PYTHON_CONFIG","title":"PYTHON_CONFIG  <code>module-attribute</code>","text":"<pre><code>PYTHON_CONFIG = PythonConfig()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler","title":"PythonSyntaxHandler","text":"<p>               Bases: <code>LanguageSyntaxHandler</code></p> <p>Python-specific syntax handling logic.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>class PythonSyntaxHandler(LanguageSyntaxHandler):\n\t\"\"\"Python-specific syntax handling logic.\"\"\"\n\n\t@staticmethod\n\tdef _get_node_text(node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"Helper to get node text safely.\"\"\"\n\t\ttry:\n\t\t\treturn content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept IndexError:\n\t\t\treturn \"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize with Python configuration.\"\"\"\n\t\tsuper().__init__(PYTHON_CONFIG)\n\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a Python node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\t\tnode_type = node.type\n\t\tlogger.debug(\n\t\t\t\"Getting entity type for Python node: type=%s, parent_type=%s\", node_type, parent.type if parent else None\n\t\t)\n\n\t\t# Print node content for debugging\n\t\ttry:\n\t\t\tnode_content = self._get_node_text(node, content_bytes)\n\t\t\tlogger.debug(\"Node content: %s\", node_content)\n\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\tlogger.debug(\"Failed to decode node content: %s\", str(e))\n\n\t\t# Special case: if this is an expression statement containing an assignment\n\t\t# We do NOT want to classify the expression_statement itself as a constant/variable\n\t\t# Instead, let the child assignment node be classified properly to avoid duplicates\n\t\tif node_type == \"expression_statement\":\n\t\t\t# Check if it contains an assignment - if so, return UNKNOWN for the expression_statement\n\t\t\t# The assignment child will be processed separately and classified properly\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"assignment\":\n\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\"Expression statement contains assignment - skipping classification to avoid duplicates\"\n\t\t\t\t\t)\n\t\t\t\t\treturn EntityType.UNKNOWN\n\n\t\t# Module-level\n\t\tif node_type in self.config.module:\n\t\t\treturn EntityType.MODULE\n\t\tif node_type in self.config.namespace:\n\t\t\treturn EntityType.NAMESPACE\n\n\t\t# Documentation\n\t\tif node_type in self.config.docstring:\n\t\t\t# Check if this is a docstring (first string in a container)\n\t\t\tif self._is_docstring(node, parent):\n\t\t\t\treturn EntityType.DOCSTRING\n\t\t\treturn EntityType.UNKNOWN  # Regular string literals\n\t\tif node_type in self.config.comment:\n\t\t\treturn EntityType.COMMENT\n\n\t\t# Type definitions\n\t\tif node_type in self.config.class_:\n\t\t\treturn EntityType.CLASS\n\t\tif node_type in self.config.interface:\n\t\t\t# Would need to check for ABC inheritance to be precise\n\t\t\treturn EntityType.INTERFACE\n\t\tif node_type in self.config.protocol:\n\t\t\t# Would need to check for Protocol inheritance to be precise\n\t\t\treturn EntityType.PROTOCOL\n\t\tif node_type in self.config.type_alias and node_type == \"assignment\":\n\t\t\t# For assignments, check if it's a constant (all uppercase) first\n\t\t\tname_node = node.child_by_field_name(\"left\")\n\t\t\tif name_node:\n\t\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\t\tlogger.debug(\"Checking potential constant in type_alias: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\t\treturn EntityType.CONSTANT\n\n\t\t\t# Check if this is actually a type alias by examining the right-hand side\n\t\t\tvalue_node = node.child_by_field_name(\"right\")\n\t\t\tif value_node:\n\t\t\t\ttry:\n\t\t\t\t\tvalue_text = self._get_node_text(value_node, content_bytes)\n\t\t\t\t\t# Check for type alias indicators - TypeVar or typing module types\n\t\t\t\t\tif \"TypeVar\" in value_text or any(\n\t\t\t\t\t\ttyping_type in value_text\n\t\t\t\t\t\tfor typing_type in [\n\t\t\t\t\t\t\t\"Dict\",\n\t\t\t\t\t\t\t\"List\",\n\t\t\t\t\t\t\t\"Tuple\",\n\t\t\t\t\t\t\t\"Set\",\n\t\t\t\t\t\t\t\"Union\",\n\t\t\t\t\t\t\t\"Optional\",\n\t\t\t\t\t\t\t\"Callable\",\n\t\t\t\t\t\t\t\"Any\",\n\t\t\t\t\t\t\t\"Type[\",\n\t\t\t\t\t\t\t\"ClassVar\",\n\t\t\t\t\t\t\t\"Final\",\n\t\t\t\t\t\t\t\"Literal\",\n\t\t\t\t\t\t\t\"Protocol\",\n\t\t\t\t\t\t\t\"Generic\",\n\t\t\t\t\t\t]\n\t\t\t\t\t):\n\t\t\t\t\t\tlogger.debug(\"Identified as TYPE_ALIAS: %s\", name if name_node else \"unknown\")\n\t\t\t\t\t\treturn EntityType.TYPE_ALIAS\n\t\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\t\tlogger.debug(\"Failed to decode type value: %s\", str(e))\n\n\t\t# If it's an assignment but not a constant or type alias, fall through to variable check\n\t\t# Don't return TYPE_ALIAS by default for all assignments\n\n\t\t# Functions and methods\n\t\tif node_type in self.config.function:\n\t\t\t# Check if this is a test function\n\t\t\tname = self.extract_name(node, content_bytes)\n\t\t\tif name.startswith(\"test_\"):\n\t\t\t\treturn EntityType.TEST_CASE\n\n\t\t\t# Check if this is a method by looking for class ancestry\n\t\t\tif self._is_within_class_context(node):\n\t\t\t\treturn EntityType.METHOD\n\t\t\treturn EntityType.FUNCTION\n\n\t\t# Check for properties - decorated definitions\n\t\tif node_type in self.config.property_def:\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"decorator\":\n\t\t\t\t\tdecorator_text = self._get_node_text(child, content_bytes)\n\t\t\t\t\tif \"@property\" in decorator_text:\n\t\t\t\t\t\treturn EntityType.PROPERTY\n\t\t\t# If no @property decorator, treat as method if in class, otherwise function\n\t\t\tif self._is_within_class_context(node):\n\t\t\t\treturn EntityType.METHOD\n\t\t\treturn EntityType.FUNCTION\n\n\t\t# Variables and constants\n\t\tif node_type in self.config.variable:\n\t\t\t# Check if it looks like a constant (uppercase name)\n\t\t\tname_node = node.child_by_field_name(\"left\")\n\t\t\tif name_node:\n\t\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\t\tlogger.debug(\"Checking potential constant: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\t\treturn EntityType.CONSTANT\n\t\t\tlogger.debug(\"Identified as VARIABLE: node_type=%s\", node_type)\n\t\t\treturn EntityType.VARIABLE\n\n\t\t# Class fields\n\t\tif node_type in self.config.class_field:\n\t\t\treturn EntityType.CLASS_FIELD\n\n\t\t# Code organization\n\t\tif node_type in self.config.import_:\n\t\t\treturn EntityType.IMPORT\n\t\tif node_type in self.config.decorator:\n\t\t\treturn EntityType.DECORATOR\n\n\t\treturn EntityType.UNKNOWN\n\n\tdef _is_within_class_context(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if the node is defined within a class definition.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node is within a class context\n\n\t\t\"\"\"\n\t\tancestor = node.parent\n\t\twhile ancestor:\n\t\t\tif ancestor.type in self.config.class_:\n\t\t\t\treturn True\n\t\t\t# Stop search if we hit module or another function definition\n\t\t\tif ancestor.type in self.config.module or ancestor.type in self.config.function:\n\t\t\t\tbreak\n\t\t\tancestor = ancestor.parent\n\t\treturn False\n\n\tdef _is_docstring(self, node: Node, parent: Node | None) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a string node is a docstring.\n\n\t\tArgs:\n\t\t    node: The string node\n\t\t    parent: The parent node\n\n\t\tReturns:\n\t\t    True if the node is a docstring\n\n\t\t\"\"\"\n\t\tif not parent:\n\t\t\treturn False\n\n\t\t# For module docstrings, check if it's the first string in the module\n\t\tif parent.type == \"module\":\n\t\t\tnon_comment_children = [c for c in parent.children if c.type not in [\"comment\"]]\n\t\t\treturn bool(non_comment_children and node == non_comment_children[0])\n\n\t\t# For expression statements containing string literals\n\t\tif parent.type == \"expression_statement\":\n\t\t\t# Check if this is the first child of a function or class body\n\t\t\tgrandparent = parent.parent\n\t\t\tif grandparent and grandparent.type == \"block\":\n\t\t\t\tgreat_grandparent = grandparent.parent\n\t\t\t\tif great_grandparent and great_grandparent.type in (self.config.function + self.config.class_):\n\t\t\t\t\t# Check if it's the first item in the block\n\t\t\t\t\tnon_comment_children = [c for c in grandparent.children if c.type not in [\"comment\"]]\n\t\t\t\t\treturn bool(non_comment_children and parent == non_comment_children[0])\n\t\t\treturn False\n\n\t\treturn False\n\n\tdef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\t\"\"\"\n\t\tFind the docstring associated with a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t    - The extracted docstring text (or None).\n\t\t    - The specific AST node representing the docstring (or None).\n\n\t\t\"\"\"\n\t\tbody_node = self.get_body_node(node)\n\t\tif not body_node:\n\t\t\t# Handle module docstring case (no explicit body node)\n\t\t\tif node.type == \"module\":\n\t\t\t\tbody_node = node  # Treat module itself as the body context\n\t\t\telse:\n\t\t\t\treturn None, None\n\n\t\tif body_node.named_child_count == 0:\n\t\t\treturn None, None\n\n\t\t# Look for the first child that might be a docstring\n\t\tfirst_body_child = None\n\t\tfor child in body_node.children:\n\t\t\tif child.is_named:\n\t\t\t\tfirst_body_child = child\n\t\t\t\tbreak\n\n\t\tif not first_body_child:\n\t\t\treturn None, None\n\n\t\tactual_string_node = None\n\t\tdocstring_container_node = None  # The node to skip during processing\n\n\t\tif first_body_child.type == \"expression_statement\":\n\t\t\t# For expression statements containing string literals\n\t\t\tfor child in first_body_child.children:\n\t\t\t\tif child.type in self.config.docstring:\n\t\t\t\t\tactual_string_node = child\n\t\t\t\t\tdocstring_container_node = first_body_child\n\t\t\t\t\tbreak\n\t\telif first_body_child.type in self.config.docstring:\n\t\t\t# Direct string literal\n\t\t\tactual_string_node = first_body_child\n\t\t\tdocstring_container_node = first_body_child\n\n\t\tif actual_string_node:\n\t\t\ttry:\n\t\t\t\tdocstring_text = self._get_node_text(actual_string_node, content_bytes).strip(\"\\\"' \\n\")\n\t\t\t\treturn docstring_text, docstring_container_node\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode/extract Python docstring: %s\", e)\n\n\t\treturn None, None\n\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\t\t# Try to find the name field\n\t\tname_node = node.child_by_field_name(\"name\")\n\n\t\t# Handle assignments\n\t\tif not name_node and node.type == \"assignment\":\n\t\t\tname_node = node.child_by_field_name(\"left\")\n\n\t\t# Handle expression statements with assignments\n\t\tif not name_node and node.type == \"expression_statement\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"assignment\":\n\t\t\t\t\tname_node = child.child_by_field_name(\"left\")\n\t\t\t\t\tif name_node:\n\t\t\t\t\t\tbreak\n\n\t\t# Handle decorated definitions\n\t\tif not name_node and node.type == \"decorated_definition\":\n\t\t\tfunc_def = node.child_by_field_name(\"definition\")\n\t\t\tif func_def:\n\t\t\t\tname_node = func_def.child_by_field_name(\"name\")\n\n\t\tif name_node:\n\t\t\ttry:\n\t\t\t\treturn self._get_node_text(name_node, content_bytes)\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode Python name: %s\", e)\n\t\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t\treturn f\"&lt;anonymous-{node.type}&gt;\"\n\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"Get the block node for function/class definition body.\"\"\"\n\t\tif node.type in (\"function_definition\", \"class_definition\", \"decorated_definition\"):\n\t\t\t# Handle decorated definitions properly\n\t\t\tactual_def_node = node\n\t\t\tif node.type == \"decorated_definition\":\n\t\t\t\t# Find the actual function/class definition node within the decoration\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\t\tactual_def_node = child\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\treturn None  # Could not find definition within decorator\n\n\t\t\t# Find the 'block' node which contains the body statements\n\t\t\tfor child in actual_def_node.children:\n\t\t\t\tif child.type == \"block\":\n\t\t\t\t\treturn child\n\t\treturn None  # Not a function/class definition or no block found\n\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\t\t# Process children of the body node if it exists, otherwise process direct children\n\t\treturn list(body_node.children) if body_node else list(node.children)\n\n\tdef should_skip_node(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tDetermine if a node should be skipped entirely during processing.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node should be skipped\n\n\t\t\"\"\"\n\t\t# Skip non-named nodes (like punctuation, operators)\n\t\tif not node.is_named:\n\t\t\treturn True\n\n\t\t# Skip syntax nodes that don't contribute to code structure\n\t\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\"]\n\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported module names from a Python import statement.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node representing an import statement\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported module names as strings\n\n\t\t\"\"\"\n\t\tif node.type not in self.config.import_:\n\t\t\treturn []\n\n\t\timported_names = []\n\n\t\ttry:\n\t\t\t# Handle regular import statements: \"import foo, bar\"\n\t\t\tif node.type == \"import_statement\":\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\t\tmodule_name = self._get_node_text(child, content_bytes)\n\t\t\t\t\t\timported_names.append(module_name)\n\n\t\t\t# Handle import from statements: \"from foo.bar import baz, qux\"\n\t\t\telif node.type == \"import_from_statement\":\n\t\t\t\t# Get the module being imported from\n\t\t\t\tmodule_node = None\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\t\tmodule_node = child\n\t\t\t\t\t\tbreak\n\n\t\t\t\tif module_node:\n\t\t\t\t\tmodule_name = self._get_node_text(module_node, content_bytes)\n\n\t\t\t\t\t# Get the imported names\n\t\t\t\t\timport_node = node.child_by_field_name(\"import\")\n\t\t\t\t\tif import_node:\n\t\t\t\t\t\t# Check for the wildcard import case: \"from foo import *\"\n\t\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\t\tif child.type == \"wildcard_import\":\n\t\t\t\t\t\t\t\timported_names.append(f\"{module_name}.*\")\n\t\t\t\t\t\t\t\treturn imported_names\n\n\t\t\t\t\t\t# Regular named imports\n\t\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\t\tif child.type == \"import_list\":\n\t\t\t\t\t\t\t\tfor item in child.children:\n\t\t\t\t\t\t\t\t\tif item.type in {\"dotted_name\", \"identifier\"}:\n\t\t\t\t\t\t\t\t\t\tname = self._get_node_text(item, content_bytes)\n\t\t\t\t\t\t\t\t\t\timported_names.append(f\"{module_name}.{name}\")\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode Python imports: %s\", e)\n\n\t\treturn imported_names\n\n\tdef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract names of functions/methods called within a Python node's scope.\n\n\t\tRecursively searches for 'call' nodes and extracts the function identifier.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (e.g., function/method body)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of called function/method names\n\n\t\t\"\"\"\n\t\tcalls = []\n\t\tfor child in node.children:\n\t\t\tif child.type == \"call\":\n\t\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\t\tif function_node:\n\t\t\t\t\t# Extract the identifier (could be simple name or attribute access like obj.method)\n\t\t\t\t\t# For simplicity, we take the full text of the function node\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcall_name = self._get_node_text(function_node, content_bytes)\n\t\t\t\t\t\tcalls.append(call_name)\n\t\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\t\tpass  # Ignore decoding errors\n\t\t\t# Recursively search within the arguments or children of the call if needed, but often not necessary\n\t\t\t# for call details, just the name.\n\t\t\t# Else, recursively search deeper within non-call children\n\t\t\telse:\n\t\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\t\treturn list(set(calls))  # Return unique calls\n\n\tdef extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\t\"\"\"Extract the signature up to the colon ':' for Python functions/classes.\"\"\"\n\t\tsignature_node = node\n\t\t# If it's a decorated definition, find the actual definition node for the signature start\n\t\tif node.type == \"decorated_definition\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\tsignature_node = child\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]  # Fallback to first line of decorator\n\n\t\t# Find the colon that ends the signature part\n\t\tcolon_node = None\n\t\tfor child in signature_node.children:\n\t\t\tif child.type == \":\":\n\t\t\t\tcolon_node = child\n\t\t\t\tbreak\n\t\t\t# Handle async functions where 'def' is preceded by 'async'\n\t\t\tif child.type == \"async\":\n\t\t\t\tcontinue  # skip 'async' keyword itself\n\t\t\tif child.type in {\"def\", \"class\"}:\n\t\t\t\tcontinue  # skip 'def'/'class' keywords\n\t\t\t# Stop if we hit the body block before finding a colon (shouldn't happen in valid code)\n\t\t\tif child.type == \"block\":\n\t\t\t\tbreak\n\n\t\tif colon_node:\n\t\t\t# Extract text from the start of the definition node up to the end of the colon\n\t\t\tstart_byte = signature_node.start_byte\n\t\t\tend_byte = colon_node.end_byte\n\t\t\ttry:\n\t\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\t\texcept IndexError:\n\t\t\t\treturn None\n\t\telse:\n\t\t\t# Fallback: if no colon found (e.g., malformed code?), return the first line\n\t\t\treturn self._get_node_text(signature_node, content_bytes).splitlines()[0]\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"Find the first ancestor node matching the target Python entity type.\"\"\"\n\t\ttarget_node_types = []\n\t\tif target_type == EntityType.CLASS:\n\t\t\ttarget_node_types = [\"class_definition\", \"decorated_definition\"]  # Include decorated\n\t\telif target_type == EntityType.FUNCTION:\n\t\t\ttarget_node_types = [\"function_definition\", \"decorated_definition\"]  # Include decorated\n\t\telif target_type == EntityType.MODULE:\n\t\t\t# Module is typically the root node or identified by file, not easily findable as ancestor type\n\t\t\treturn None  # Or return root node? Depends on desired behavior.\n\t\t# Add other types if needed\n\n\t\tif not target_node_types:\n\t\t\treturn None\n\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\t# Check if the current node is the target type or a decorator containing it\n\t\t\tnode_to_check = current\n\t\t\tactual_node_type = current.type\n\n\t\t\tif current.type == \"decorated_definition\":\n\t\t\t\t# Check the *content* of the decorated definition\n\t\t\t\tfound_target_in_decorator = False\n\t\t\t\tfor child in current.children:\n\t\t\t\t\tif child.type in target_node_types and child.type != \"decorated_definition\":\n\t\t\t\t\t\t# We found the actual class/func def inside the decorator\n\t\t\t\t\t\tnode_to_check = child\n\t\t\t\t\t\tactual_node_type = child.type\n\t\t\t\t\t\tfound_target_in_decorator = True\n\t\t\t\t\t\tbreak\n\t\t\t\tif not found_target_in_decorator:\n\t\t\t\t\tactual_node_type = \"decorated_definition\"  # Treat decorator itself if no target found within\n\n\t\t\t# Now check if the node (or the one found inside decorator) matches\n\t\t\tif actual_node_type in target_node_types and actual_node_type != \"decorated_definition\":\n\t\t\t\treturn node_to_check  # Return the actual definition node\n\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef _find_decorated_definition(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"Helper to get the actual definition node from a decorated_definition.\"\"\"\n\t\tif node.type == \"decorated_definition\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\treturn child\n\t\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize with Python configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize with Python configuration.\"\"\"\n\tsuper().__init__(PYTHON_CONFIG)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_entity_type","title":"get_entity_type","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a Python node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a Python node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n\tnode_type = node.type\n\tlogger.debug(\n\t\t\"Getting entity type for Python node: type=%s, parent_type=%s\", node_type, parent.type if parent else None\n\t)\n\n\t# Print node content for debugging\n\ttry:\n\t\tnode_content = self._get_node_text(node, content_bytes)\n\t\tlogger.debug(\"Node content: %s\", node_content)\n\texcept (UnicodeDecodeError, IndexError) as e:\n\t\tlogger.debug(\"Failed to decode node content: %s\", str(e))\n\n\t# Special case: if this is an expression statement containing an assignment\n\t# We do NOT want to classify the expression_statement itself as a constant/variable\n\t# Instead, let the child assignment node be classified properly to avoid duplicates\n\tif node_type == \"expression_statement\":\n\t\t# Check if it contains an assignment - if so, return UNKNOWN for the expression_statement\n\t\t# The assignment child will be processed separately and classified properly\n\t\tfor child in node.children:\n\t\t\tif child.type == \"assignment\":\n\t\t\t\tlogger.debug(\n\t\t\t\t\t\"Expression statement contains assignment - skipping classification to avoid duplicates\"\n\t\t\t\t)\n\t\t\t\treturn EntityType.UNKNOWN\n\n\t# Module-level\n\tif node_type in self.config.module:\n\t\treturn EntityType.MODULE\n\tif node_type in self.config.namespace:\n\t\treturn EntityType.NAMESPACE\n\n\t# Documentation\n\tif node_type in self.config.docstring:\n\t\t# Check if this is a docstring (first string in a container)\n\t\tif self._is_docstring(node, parent):\n\t\t\treturn EntityType.DOCSTRING\n\t\treturn EntityType.UNKNOWN  # Regular string literals\n\tif node_type in self.config.comment:\n\t\treturn EntityType.COMMENT\n\n\t# Type definitions\n\tif node_type in self.config.class_:\n\t\treturn EntityType.CLASS\n\tif node_type in self.config.interface:\n\t\t# Would need to check for ABC inheritance to be precise\n\t\treturn EntityType.INTERFACE\n\tif node_type in self.config.protocol:\n\t\t# Would need to check for Protocol inheritance to be precise\n\t\treturn EntityType.PROTOCOL\n\tif node_type in self.config.type_alias and node_type == \"assignment\":\n\t\t# For assignments, check if it's a constant (all uppercase) first\n\t\tname_node = node.child_by_field_name(\"left\")\n\t\tif name_node:\n\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\tlogger.debug(\"Checking potential constant in type_alias: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\treturn EntityType.CONSTANT\n\n\t\t# Check if this is actually a type alias by examining the right-hand side\n\t\tvalue_node = node.child_by_field_name(\"right\")\n\t\tif value_node:\n\t\t\ttry:\n\t\t\t\tvalue_text = self._get_node_text(value_node, content_bytes)\n\t\t\t\t# Check for type alias indicators - TypeVar or typing module types\n\t\t\t\tif \"TypeVar\" in value_text or any(\n\t\t\t\t\ttyping_type in value_text\n\t\t\t\t\tfor typing_type in [\n\t\t\t\t\t\t\"Dict\",\n\t\t\t\t\t\t\"List\",\n\t\t\t\t\t\t\"Tuple\",\n\t\t\t\t\t\t\"Set\",\n\t\t\t\t\t\t\"Union\",\n\t\t\t\t\t\t\"Optional\",\n\t\t\t\t\t\t\"Callable\",\n\t\t\t\t\t\t\"Any\",\n\t\t\t\t\t\t\"Type[\",\n\t\t\t\t\t\t\"ClassVar\",\n\t\t\t\t\t\t\"Final\",\n\t\t\t\t\t\t\"Literal\",\n\t\t\t\t\t\t\"Protocol\",\n\t\t\t\t\t\t\"Generic\",\n\t\t\t\t\t]\n\t\t\t\t):\n\t\t\t\t\tlogger.debug(\"Identified as TYPE_ALIAS: %s\", name if name_node else \"unknown\")\n\t\t\t\t\treturn EntityType.TYPE_ALIAS\n\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\tlogger.debug(\"Failed to decode type value: %s\", str(e))\n\n\t# If it's an assignment but not a constant or type alias, fall through to variable check\n\t# Don't return TYPE_ALIAS by default for all assignments\n\n\t# Functions and methods\n\tif node_type in self.config.function:\n\t\t# Check if this is a test function\n\t\tname = self.extract_name(node, content_bytes)\n\t\tif name.startswith(\"test_\"):\n\t\t\treturn EntityType.TEST_CASE\n\n\t\t# Check if this is a method by looking for class ancestry\n\t\tif self._is_within_class_context(node):\n\t\t\treturn EntityType.METHOD\n\t\treturn EntityType.FUNCTION\n\n\t# Check for properties - decorated definitions\n\tif node_type in self.config.property_def:\n\t\tfor child in node.children:\n\t\t\tif child.type == \"decorator\":\n\t\t\t\tdecorator_text = self._get_node_text(child, content_bytes)\n\t\t\t\tif \"@property\" in decorator_text:\n\t\t\t\t\treturn EntityType.PROPERTY\n\t\t# If no @property decorator, treat as method if in class, otherwise function\n\t\tif self._is_within_class_context(node):\n\t\t\treturn EntityType.METHOD\n\t\treturn EntityType.FUNCTION\n\n\t# Variables and constants\n\tif node_type in self.config.variable:\n\t\t# Check if it looks like a constant (uppercase name)\n\t\tname_node = node.child_by_field_name(\"left\")\n\t\tif name_node:\n\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\tlogger.debug(\"Checking potential constant: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\treturn EntityType.CONSTANT\n\t\tlogger.debug(\"Identified as VARIABLE: node_type=%s\", node_type)\n\t\treturn EntityType.VARIABLE\n\n\t# Class fields\n\tif node_type in self.config.class_field:\n\t\treturn EntityType.CLASS_FIELD\n\n\t# Code organization\n\tif node_type in self.config.import_:\n\t\treturn EntityType.IMPORT\n\tif node_type in self.config.decorator:\n\t\treturn EntityType.DECORATOR\n\n\treturn EntityType.UNKNOWN\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.find_docstring","title":"find_docstring","text":"<pre><code>find_docstring(\n\tnode: Node, content_bytes: bytes\n) -&gt; tuple[str | None, Node | None]\n</code></pre> <p>Find the docstring associated with a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A tuple containing:</p> <code>Node | None</code> <ul> <li>The extracted docstring text (or None).</li> </ul> <code>tuple[str | None, Node | None]</code> <ul> <li>The specific AST node representing the docstring (or None).</li> </ul> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\"\"\"\n\tFind the docstring associated with a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    A tuple containing:\n\t    - The extracted docstring text (or None).\n\t    - The specific AST node representing the docstring (or None).\n\n\t\"\"\"\n\tbody_node = self.get_body_node(node)\n\tif not body_node:\n\t\t# Handle module docstring case (no explicit body node)\n\t\tif node.type == \"module\":\n\t\t\tbody_node = node  # Treat module itself as the body context\n\t\telse:\n\t\t\treturn None, None\n\n\tif body_node.named_child_count == 0:\n\t\treturn None, None\n\n\t# Look for the first child that might be a docstring\n\tfirst_body_child = None\n\tfor child in body_node.children:\n\t\tif child.is_named:\n\t\t\tfirst_body_child = child\n\t\t\tbreak\n\n\tif not first_body_child:\n\t\treturn None, None\n\n\tactual_string_node = None\n\tdocstring_container_node = None  # The node to skip during processing\n\n\tif first_body_child.type == \"expression_statement\":\n\t\t# For expression statements containing string literals\n\t\tfor child in first_body_child.children:\n\t\t\tif child.type in self.config.docstring:\n\t\t\t\tactual_string_node = child\n\t\t\t\tdocstring_container_node = first_body_child\n\t\t\t\tbreak\n\telif first_body_child.type in self.config.docstring:\n\t\t# Direct string literal\n\t\tactual_string_node = first_body_child\n\t\tdocstring_container_node = first_body_child\n\n\tif actual_string_node:\n\t\ttry:\n\t\t\tdocstring_text = self._get_node_text(actual_string_node, content_bytes).strip(\"\\\"' \\n\")\n\t\t\treturn docstring_text, docstring_container_node\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode/extract Python docstring: %s\", e)\n\n\treturn None, None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_name","title":"extract_name","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n\t# Try to find the name field\n\tname_node = node.child_by_field_name(\"name\")\n\n\t# Handle assignments\n\tif not name_node and node.type == \"assignment\":\n\t\tname_node = node.child_by_field_name(\"left\")\n\n\t# Handle expression statements with assignments\n\tif not name_node and node.type == \"expression_statement\":\n\t\tfor child in node.children:\n\t\t\tif child.type == \"assignment\":\n\t\t\t\tname_node = child.child_by_field_name(\"left\")\n\t\t\t\tif name_node:\n\t\t\t\t\tbreak\n\n\t# Handle decorated definitions\n\tif not name_node and node.type == \"decorated_definition\":\n\t\tfunc_def = node.child_by_field_name(\"definition\")\n\t\tif func_def:\n\t\t\tname_node = func_def.child_by_field_name(\"name\")\n\n\tif name_node:\n\t\ttry:\n\t\t\treturn self._get_node_text(name_node, content_bytes)\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode Python name: %s\", e)\n\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\treturn f\"&lt;anonymous-{node.type}&gt;\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_body_node","title":"get_body_node","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the block node for function/class definition body.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"Get the block node for function/class definition body.\"\"\"\n\tif node.type in (\"function_definition\", \"class_definition\", \"decorated_definition\"):\n\t\t# Handle decorated definitions properly\n\t\tactual_def_node = node\n\t\tif node.type == \"decorated_definition\":\n\t\t\t# Find the actual function/class definition node within the decoration\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\tactual_def_node = child\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treturn None  # Could not find definition within decorator\n\n\t\t# Find the 'block' node which contains the body statements\n\t\tfor child in actual_def_node.children:\n\t\t\tif child.type == \"block\":\n\t\t\t\treturn child\n\treturn None  # Not a function/class definition or no block found\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_children_to_process","title":"get_children_to_process","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n\t# Process children of the body node if it exists, otherwise process direct children\n\treturn list(body_node.children) if body_node else list(node.children)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.should_skip_node","title":"should_skip_node","text":"<pre><code>should_skip_node(node: Node) -&gt; bool\n</code></pre> <p>Determine if a node should be skipped entirely during processing.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be skipped</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def should_skip_node(self, node: Node) -&gt; bool:\n\t\"\"\"\n\tDetermine if a node should be skipped entirely during processing.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    True if the node should be skipped\n\n\t\"\"\"\n\t# Skip non-named nodes (like punctuation, operators)\n\tif not node.is_named:\n\t\treturn True\n\n\t# Skip syntax nodes that don't contribute to code structure\n\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_imports","title":"extract_imports","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported module names from a Python import statement.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node representing an import statement</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported module names as strings</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported module names from a Python import statement.\n\n\tArgs:\n\t    node: The tree-sitter node representing an import statement\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported module names as strings\n\n\t\"\"\"\n\tif node.type not in self.config.import_:\n\t\treturn []\n\n\timported_names = []\n\n\ttry:\n\t\t# Handle regular import statements: \"import foo, bar\"\n\t\tif node.type == \"import_statement\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\tmodule_name = self._get_node_text(child, content_bytes)\n\t\t\t\t\timported_names.append(module_name)\n\n\t\t# Handle import from statements: \"from foo.bar import baz, qux\"\n\t\telif node.type == \"import_from_statement\":\n\t\t\t# Get the module being imported from\n\t\t\tmodule_node = None\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\tmodule_node = child\n\t\t\t\t\tbreak\n\n\t\t\tif module_node:\n\t\t\t\tmodule_name = self._get_node_text(module_node, content_bytes)\n\n\t\t\t\t# Get the imported names\n\t\t\t\timport_node = node.child_by_field_name(\"import\")\n\t\t\t\tif import_node:\n\t\t\t\t\t# Check for the wildcard import case: \"from foo import *\"\n\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\tif child.type == \"wildcard_import\":\n\t\t\t\t\t\t\timported_names.append(f\"{module_name}.*\")\n\t\t\t\t\t\t\treturn imported_names\n\n\t\t\t\t\t# Regular named imports\n\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\tif child.type == \"import_list\":\n\t\t\t\t\t\t\tfor item in child.children:\n\t\t\t\t\t\t\t\tif item.type in {\"dotted_name\", \"identifier\"}:\n\t\t\t\t\t\t\t\t\tname = self._get_node_text(item, content_bytes)\n\t\t\t\t\t\t\t\t\timported_names.append(f\"{module_name}.{name}\")\n\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\tlogger.warning(\"Failed to decode Python imports: %s\", e)\n\n\treturn imported_names\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_calls","title":"extract_calls","text":"<pre><code>extract_calls(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract names of functions/methods called within a Python node's scope.</p> <p>Recursively searches for 'call' nodes and extracts the function identifier.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (e.g., function/method body)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of called function/method names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract names of functions/methods called within a Python node's scope.\n\n\tRecursively searches for 'call' nodes and extracts the function identifier.\n\n\tArgs:\n\t    node: The tree-sitter node (e.g., function/method body)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of called function/method names\n\n\t\"\"\"\n\tcalls = []\n\tfor child in node.children:\n\t\tif child.type == \"call\":\n\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\tif function_node:\n\t\t\t\t# Extract the identifier (could be simple name or attribute access like obj.method)\n\t\t\t\t# For simplicity, we take the full text of the function node\n\t\t\t\ttry:\n\t\t\t\t\tcall_name = self._get_node_text(function_node, content_bytes)\n\t\t\t\t\tcalls.append(call_name)\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tpass  # Ignore decoding errors\n\t\t# Recursively search within the arguments or children of the call if needed, but often not necessary\n\t\t# for call details, just the name.\n\t\t# Else, recursively search deeper within non-call children\n\t\telse:\n\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\treturn list(set(calls))  # Return unique calls\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_signature","title":"extract_signature","text":"<pre><code>extract_signature(\n\tnode: Node, content_bytes: bytes\n) -&gt; str | None\n</code></pre> <p>Extract the signature up to the colon ':' for Python functions/classes.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\"\"\"Extract the signature up to the colon ':' for Python functions/classes.\"\"\"\n\tsignature_node = node\n\t# If it's a decorated definition, find the actual definition node for the signature start\n\tif node.type == \"decorated_definition\":\n\t\tfor child in node.children:\n\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\tsignature_node = child\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]  # Fallback to first line of decorator\n\n\t# Find the colon that ends the signature part\n\tcolon_node = None\n\tfor child in signature_node.children:\n\t\tif child.type == \":\":\n\t\t\tcolon_node = child\n\t\t\tbreak\n\t\t# Handle async functions where 'def' is preceded by 'async'\n\t\tif child.type == \"async\":\n\t\t\tcontinue  # skip 'async' keyword itself\n\t\tif child.type in {\"def\", \"class\"}:\n\t\t\tcontinue  # skip 'def'/'class' keywords\n\t\t# Stop if we hit the body block before finding a colon (shouldn't happen in valid code)\n\t\tif child.type == \"block\":\n\t\t\tbreak\n\n\tif colon_node:\n\t\t# Extract text from the start of the definition node up to the end of the colon\n\t\tstart_byte = signature_node.start_byte\n\t\tend_byte = colon_node.end_byte\n\t\ttry:\n\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\texcept IndexError:\n\t\t\treturn None\n\telse:\n\t\t# Fallback: if no colon found (e.g., malformed code?), return the first line\n\t\treturn self._get_node_text(signature_node, content_bytes).splitlines()[0]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node matching the target Python entity type.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"Find the first ancestor node matching the target Python entity type.\"\"\"\n\ttarget_node_types = []\n\tif target_type == EntityType.CLASS:\n\t\ttarget_node_types = [\"class_definition\", \"decorated_definition\"]  # Include decorated\n\telif target_type == EntityType.FUNCTION:\n\t\ttarget_node_types = [\"function_definition\", \"decorated_definition\"]  # Include decorated\n\telif target_type == EntityType.MODULE:\n\t\t# Module is typically the root node or identified by file, not easily findable as ancestor type\n\t\treturn None  # Or return root node? Depends on desired behavior.\n\t# Add other types if needed\n\n\tif not target_node_types:\n\t\treturn None\n\n\tcurrent = node.parent\n\twhile current:\n\t\t# Check if the current node is the target type or a decorator containing it\n\t\tnode_to_check = current\n\t\tactual_node_type = current.type\n\n\t\tif current.type == \"decorated_definition\":\n\t\t\t# Check the *content* of the decorated definition\n\t\t\tfound_target_in_decorator = False\n\t\t\tfor child in current.children:\n\t\t\t\tif child.type in target_node_types and child.type != \"decorated_definition\":\n\t\t\t\t\t# We found the actual class/func def inside the decorator\n\t\t\t\t\tnode_to_check = child\n\t\t\t\t\tactual_node_type = child.type\n\t\t\t\t\tfound_target_in_decorator = True\n\t\t\t\t\tbreak\n\t\t\tif not found_target_in_decorator:\n\t\t\t\tactual_node_type = \"decorated_definition\"  # Treat decorator itself if no target found within\n\n\t\t# Now check if the node (or the one found inside decorator) matches\n\t\tif actual_node_type in target_node_types and actual_node_type != \"decorated_definition\":\n\t\t\treturn node_to_check  # Return the actual definition node\n\n\t\tcurrent = current.parent\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/","title":"Typescript","text":"<p>TypeScript-specific configuration for syntax chunking.</p>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig","title":"TypeScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>TypeScript-specific syntax chunking configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>class TypeScriptConfig(LanguageConfig):\n\t\"\"\"TypeScript-specific syntax chunking configuration.\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tnamespace: ClassVar[list[str]] = [\"export_statement\", \"namespace_declaration\"]\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tinterface: ClassVar[list[str]] = [\"interface_declaration\"]\n\tprotocol: ClassVar[list[str]] = []  # TypeScript doesn't have protocols\n\tstruct: ClassVar[list[str]] = []  # TypeScript doesn't have structs\n\tenum: ClassVar[list[str]] = [\"enum_declaration\"]\n\ttype_alias: ClassVar[list[str]] = [\"type_alias_declaration\"]\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.function, \"function_signature\"]\n\tmethod: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.method, \"method_signature\"]\n\tproperty_def: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.property_def, \"public_field_definition\"]\n\ttest_case: ClassVar[list[str]] = JAVASCRIPT_CONFIG.test_case\n\ttest_suite: ClassVar[list[str]] = JAVASCRIPT_CONFIG.test_suite\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]] = JAVASCRIPT_CONFIG.variable\n\tconstant: ClassVar[list[str]] = JAVASCRIPT_CONFIG.constant\n\tclass_field: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.class_field, \"public_field_definition\"]\n\n\t# Code organization\n\timport_: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.import_, \"import_alias\"]\n\tdecorator: ClassVar[list[str]] = JAVASCRIPT_CONFIG.decorator\n\n\t# Documentation\n\tcomment: ClassVar[list[str]] = JAVASCRIPT_CONFIG.comment\n\tdocstring: ClassVar[list[str]] = JAVASCRIPT_CONFIG.docstring\n\n\tfile_extensions: ClassVar[list[str]] = [\".ts\", \".tsx\"]\n\ttree_sitter_name: ClassVar[str] = \"typescript\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str] = [\n\t\"export_statement\",\n\t\"namespace_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str] = ['interface_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str] = ['enum_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str] = ['type_alias_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [*function, 'function_signature']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str] = [*method, 'method_signature']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t*property_def,\n\t\"public_field_definition\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str] = test_case\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str] = test_suite\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str] = variable\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str] = constant\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str] = [\n\t*class_field,\n\t\"public_field_definition\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str] = [*import_, 'import_alias']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str] = decorator\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str] = comment\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = docstring\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.ts', '.tsx']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = 'typescript'\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TYPESCRIPT_CONFIG","title":"TYPESCRIPT_CONFIG  <code>module-attribute</code>","text":"<pre><code>TYPESCRIPT_CONFIG = TypeScriptConfig()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler","title":"TypeScriptSyntaxHandler","text":"<p>               Bases: <code>JavaScriptSyntaxHandler</code></p> <p>TypeScript-specific syntax handling logic.</p> <p>Inherits from JavaScript handler to reuse common logic.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>class TypeScriptSyntaxHandler(JavaScriptSyntaxHandler):\n\t\"\"\"\n\tTypeScript-specific syntax handling logic.\n\n\tInherits from JavaScript handler to reuse common logic.\n\n\t\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize with TypeScript configuration.\"\"\"\n\t\t# Revert to super() and ignore potential linter false positive\n\t\tsuper().__init__(TYPESCRIPT_CONFIG)  # type: ignore[call-arg] # pylint: disable=too-many-function-args\n\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a TypeScript node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\t\tnode_type = node.type\n\t\tlogger.debug(\n\t\t\t\"Getting entity type for TypeScript node: type=%s, parent_type=%s\",\n\t\t\tnode_type,\n\t\t\tparent.type if parent else None,\n\t\t)\n\n\t\t# Check for TypeScript specific types first\n\t\tif node_type in self.config.interface:\n\t\t\treturn EntityType.INTERFACE\n\t\tif node_type in self.config.type_alias:\n\t\t\treturn EntityType.TYPE_ALIAS\n\t\tif node_type == \"enum_declaration\":\n\t\t\treturn EntityType.ENUM\n\t\tif node_type == \"module\":  # TS internal modules/namespaces\n\t\t\treturn EntityType.NAMESPACE\n\t\tif node_type == \"namespace_declaration\":\n\t\t\treturn EntityType.NAMESPACE\n\t\tif node_type == \"method_signature\":\n\t\t\treturn EntityType.METHOD\n\t\tif node_type == \"property_signature\":\n\t\t\treturn EntityType.PROPERTY\n\n\t\t# Use the JavaScript logic for common types\n\t\treturn super().get_entity_type(node, parent, content_bytes)\n\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\t\t# Handle TypeScript-specific node types first\n\t\tname_node = None\n\n\t\tif node.type in [\n\t\t\t\"interface_declaration\",\n\t\t\t\"enum_declaration\",\n\t\t\t\"type_alias_declaration\",\n\t\t\t\"namespace_declaration\",\n\t\t] or node.type in [\"method_signature\", \"property_signature\"]:\n\t\t\tname_node = node.child_by_field_name(\"name\")\n\n\t\tif name_node:\n\t\t\ttry:\n\t\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode TypeScript name: %s\", e)\n\t\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t\t# Fall back to JavaScript name extraction\n\t\treturn super().extract_name(node, content_bytes)\n\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"\n\t\tGet the node representing the 'body' of a definition.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    The body node if available, None otherwise\n\n\t\t\"\"\"\n\t\tif node.type in (\"interface_declaration\", \"function_signature\", \"method_signature\"):\n\t\t\treturn None  # Interfaces and signatures have no body block\n\t\treturn super().get_body_node(node)\n\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\t\t# TypeScript-specific handling\n\t\tif node.type == \"type_alias_declaration\":\n\t\t\t# Type aliases don't have children to process\n\t\t\treturn []\n\n\t\t# Fall back to JavaScript children processing\n\t\treturn super().get_children_to_process(node, body_node)\n\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported module names from a TypeScript import statement.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node representing an import statement\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported module names as strings\n\n\t\t\"\"\"\n\t\t# TypeScript import statements are the same as JavaScript\n\t\treturn super().extract_imports(node, content_bytes)\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"\n\t\tFind the first ancestor node matching the target TypeScript entity type.\n\n\t\tHandles INTERFACE specifically and falls back to the JavaScript handler\n\t\tfor other types (CLASS, FUNCTION, METHOD, MODULE).\n\n\t\tArgs:\n\t\t    node: The starting node.\n\t\t    target_type: The EntityType to search for in ancestors.\n\n\t\tReturns:\n\t\t    The ancestor node if found, otherwise None.\n\n\t\t\"\"\"\n\t\tif target_type == EntityType.INTERFACE:\n\t\t\ttarget_node_types = [\"interface_declaration\"]\n\t\t\tcurrent = node.parent\n\t\t\twhile current:\n\t\t\t\tif current.type in target_node_types:\n\t\t\t\t\treturn current\n\t\t\t\tcurrent = current.parent\n\t\t\treturn None\n\t\t# Fall back to JS handler for other types (CLASS, FUNCTION, METHOD, MODULE)\n\t\treturn super().get_enclosing_node_of_type(node, target_type)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize with TypeScript configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize with TypeScript configuration.\"\"\"\n\t# Revert to super() and ignore potential linter false positive\n\tsuper().__init__(TYPESCRIPT_CONFIG)  # type: ignore[call-arg] # pylint: disable=too-many-function-args\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_entity_type","title":"get_entity_type","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a TypeScript node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a TypeScript node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n\tnode_type = node.type\n\tlogger.debug(\n\t\t\"Getting entity type for TypeScript node: type=%s, parent_type=%s\",\n\t\tnode_type,\n\t\tparent.type if parent else None,\n\t)\n\n\t# Check for TypeScript specific types first\n\tif node_type in self.config.interface:\n\t\treturn EntityType.INTERFACE\n\tif node_type in self.config.type_alias:\n\t\treturn EntityType.TYPE_ALIAS\n\tif node_type == \"enum_declaration\":\n\t\treturn EntityType.ENUM\n\tif node_type == \"module\":  # TS internal modules/namespaces\n\t\treturn EntityType.NAMESPACE\n\tif node_type == \"namespace_declaration\":\n\t\treturn EntityType.NAMESPACE\n\tif node_type == \"method_signature\":\n\t\treturn EntityType.METHOD\n\tif node_type == \"property_signature\":\n\t\treturn EntityType.PROPERTY\n\n\t# Use the JavaScript logic for common types\n\treturn super().get_entity_type(node, parent, content_bytes)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.extract_name","title":"extract_name","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n\t# Handle TypeScript-specific node types first\n\tname_node = None\n\n\tif node.type in [\n\t\t\"interface_declaration\",\n\t\t\"enum_declaration\",\n\t\t\"type_alias_declaration\",\n\t\t\"namespace_declaration\",\n\t] or node.type in [\"method_signature\", \"property_signature\"]:\n\t\tname_node = node.child_by_field_name(\"name\")\n\n\tif name_node:\n\t\ttry:\n\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode TypeScript name: %s\", e)\n\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t# Fall back to JavaScript name extraction\n\treturn super().extract_name(node, content_bytes)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_body_node","title":"get_body_node","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the node representing the 'body' of a definition.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The body node if available, None otherwise</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"\n\tGet the node representing the 'body' of a definition.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    The body node if available, None otherwise\n\n\t\"\"\"\n\tif node.type in (\"interface_declaration\", \"function_signature\", \"method_signature\"):\n\t\treturn None  # Interfaces and signatures have no body block\n\treturn super().get_body_node(node)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_children_to_process","title":"get_children_to_process","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n\t# TypeScript-specific handling\n\tif node.type == \"type_alias_declaration\":\n\t\t# Type aliases don't have children to process\n\t\treturn []\n\n\t# Fall back to JavaScript children processing\n\treturn super().get_children_to_process(node, body_node)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.extract_imports","title":"extract_imports","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported module names from a TypeScript import statement.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node representing an import statement</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported module names as strings</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported module names from a TypeScript import statement.\n\n\tArgs:\n\t    node: The tree-sitter node representing an import statement\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported module names as strings\n\n\t\"\"\"\n\t# TypeScript import statements are the same as JavaScript\n\treturn super().extract_imports(node, content_bytes)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node matching the target TypeScript entity type.</p> <p>Handles INTERFACE specifically and falls back to the JavaScript handler for other types (CLASS, FUNCTION, METHOD, MODULE).</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The starting node.</p> required <code>target_type</code> <code>EntityType</code> <p>The EntityType to search for in ancestors.</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The ancestor node if found, otherwise None.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"\n\tFind the first ancestor node matching the target TypeScript entity type.\n\n\tHandles INTERFACE specifically and falls back to the JavaScript handler\n\tfor other types (CLASS, FUNCTION, METHOD, MODULE).\n\n\tArgs:\n\t    node: The starting node.\n\t    target_type: The EntityType to search for in ancestors.\n\n\tReturns:\n\t    The ancestor node if found, otherwise None.\n\n\t\"\"\"\n\tif target_type == EntityType.INTERFACE:\n\t\ttarget_node_types = [\"interface_declaration\"]\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type in target_node_types:\n\t\t\t\treturn current\n\t\t\tcurrent = current.parent\n\t\treturn None\n\t# Fall back to JS handler for other types (CLASS, FUNCTION, METHOD, MODULE)\n\treturn super().get_enclosing_node_of_type(node, target_type)\n</code></pre>"},{"location":"api/processor/utils/","title":"Utils Overview","text":"<p>Processor Utilities Package.</p> <ul> <li>Embedding Utils - Utilities for generating text embeddings.</li> <li>Sync Utils - Utilities for synchronization logic.</li> </ul>"},{"location":"api/processor/utils/embedding_utils/","title":"Embedding Utils","text":"<p>Utilities for generating text embeddings.</p>"},{"location":"api/processor/utils/embedding_utils/#codemap.processor.utils.embedding_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/utils/embedding_utils/#codemap.processor.utils.embedding_utils.generate_embedding","title":"generate_embedding","text":"<pre><code>generate_embedding(\n\ttexts: list[str], config_loader: ConfigLoader\n) -&gt; list[list[float]]\n</code></pre> <p>Generate embeddings for a list of texts using model2vec.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str]</code> <p>List of text strings to embed.</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance used to load embedding model configuration.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>List of embeddings (each embedding is a list of floats)</p> Source code in <code>src/codemap/processor/utils/embedding_utils.py</code> <pre><code>def generate_embedding(texts: list[str], config_loader: \"ConfigLoader\") -&gt; list[list[float]]:\n\t\"\"\"\n\tGenerate embeddings for a list of texts using model2vec.\n\n\tArgs:\n\t\ttexts: List of text strings to embed.\n\t\tconfig_loader: ConfigLoader instance used to load embedding model configuration.\n\n\tReturns:\n\t\tList of embeddings (each embedding is a list of floats)\n\n\t\"\"\"\n\twith progress_indicator(\"Loading model...\"):\n\t\tfrom model2vec import StaticModel\n\n\t\tmodel_name = config_loader.get.embedding.model_name\n\t\tmodel = StaticModel.from_pretrained(model_name)\n\n\twith progress_indicator(\"Generating embeddings...\"):\n\t\ttry:\n\t\t\tembeddings = model.encode(texts)\n\t\t\treturn embeddings.tolist()  # Convert np.ndarray to list of lists\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error generating embeddings\")\n\t\t\traise\n</code></pre>"},{"location":"api/processor/utils/sync_utils/","title":"Sync Utils","text":"<p>Utilities for synchronization logic.</p>"},{"location":"api/processor/utils/sync_utils/#codemap.processor.utils.sync_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/utils/sync_utils/#codemap.processor.utils.sync_utils.compare_states","title":"compare_states","text":"<pre><code>compare_states(\n\tcurrent_files: Mapping[str, str],\n\tdb_files: Mapping[str, str | set[str]],\n) -&gt; tuple[set[str], set[str], set[str]]\n</code></pre> <p>Compare current file state with database state to find differences.</p> <p>Handles cases where the database might store multiple hashes per file path (e.g., if different chunks of the same file have different source hashes, although typically it should be one hash per file path in the DB state dict).</p> <p>Parameters:</p> Name Type Description Default <code>current_files</code> <code>Mapping[str, str]</code> <p>Dictionary mapping file paths to their                              current hash (e.g., from Git).</p> required <code>db_files</code> <code>Mapping[str, str | set[str]]</code> <p>Dictionary mapping file paths to their                                    hash(es) stored in the database.                                    Values can be single hashes (str) or                                    sets of hashes (set[str]).</p> required <p>Returns:</p> Type Description <code>tuple[set[str], set[str], set[str]]</code> <p>tuple[set[str], set[str], set[str]]: A tuple containing: - files_to_add: Set of file paths present in current_files but not db_files. - files_to_update: Set of file paths present in both, but with different hashes. - files_to_delete: Set of file paths present in db_files but not current_files.</p> Source code in <code>src/codemap/processor/utils/sync_utils.py</code> <pre><code>def compare_states(\n\tcurrent_files: Mapping[str, str],\n\tdb_files: Mapping[str, str | set[str]],\n) -&gt; tuple[set[str], set[str], set[str]]:\n\t\"\"\"\n\tCompare current file state with database state to find differences.\n\n\tHandles cases where the database might store multiple hashes per file path\n\t(e.g., if different chunks of the same file have different source hashes,\n\talthough typically it should be one hash per file path in the DB state dict).\n\n\tArgs:\n\t    current_files (Mapping[str, str]): Dictionary mapping file paths to their\n\t                                     current hash (e.g., from Git).\n\t    db_files (Mapping[str, str | set[str]]): Dictionary mapping file paths to their\n\t                                           hash(es) stored in the database.\n\t                                           Values can be single hashes (str) or\n\t                                           sets of hashes (set[str]).\n\n\tReturns:\n\t    tuple[set[str], set[str], set[str]]: A tuple containing:\n\t        - files_to_add: Set of file paths present in current_files but not db_files.\n\t        - files_to_update: Set of file paths present in both, but with different hashes.\n\t        - files_to_delete: Set of file paths present in db_files but not current_files.\n\n\t\"\"\"\n\tcurrent_paths = set(current_files.keys())\n\tdb_paths = set(db_files.keys())\n\n\t# Files in current state but not in DB -&gt; Add\n\tfiles_to_add = current_paths - db_paths\n\n\t# Files in DB but not in current state -&gt; Delete\n\tfiles_to_delete = db_paths - current_paths\n\n\t# Files in both -&gt; Check hash for updates\n\tfiles_to_update: set[str] = set()\n\tcommon_paths = current_paths.intersection(db_paths)\n\n\tfor path in common_paths:\n\t\tcurrent_hash = current_files[path]\n\t\tdb_hash_or_hashes = db_files[path]\n\n\t\tneeds_update = False\n\t\tif isinstance(db_hash_or_hashes, str):\n\t\t\t# DB stores a single hash for the file\n\t\t\tif current_hash != db_hash_or_hashes:\n\t\t\t\tneeds_update = True\n\t\telif isinstance(db_hash_or_hashes, set):\n\t\t\t# DB stores multiple hashes (e.g., different versions/chunks)\n\t\t\t# Update if the current hash is not among the DB hashes\n\t\t\tif current_hash not in db_hash_or_hashes:\n\t\t\t\tneeds_update = True\n\t\t\t# Optional: Consider updating if the set doesn't *exactly* match\n\t\t\t# (e.g., if DB has extra hashes not in current state -&gt; cleanup?)\n\t\t\t# For now, just check if the current hash exists.\n\t\telse:\n\t\t\tlogger.warning(f\"Unexpected hash type in db_files for path '{path}': {type(db_hash_or_hashes)}\")\n\t\t\t# Treat as needing update to be safe\n\t\t\tneeds_update = True\n\n\t\tif needs_update:\n\t\t\tfiles_to_update.add(path)\n\n\tlogger.debug(\n\t\tf\"State comparison results: Add: {len(files_to_add)}, \"\n\t\tf\"Update: {len(files_to_update)}, Delete: {len(files_to_delete)}\"\n\t)\n\treturn files_to_add, files_to_update, files_to_delete\n</code></pre>"},{"location":"api/processor/vector/","title":"Vector Overview","text":"<p>Vector processing package for CodeMap.</p> <ul> <li>Chunking - Module for chunking source code files using LODGenerator.</li> <li>Qdrant Manager - Module for managing Qdrant vector database collections.</li> <li>Schema - Schema for the vector database.</li> <li>Synchronizer - Module for synchronizing HNSW index with Git state.</li> <li>Utils - Utility functions for the vector processor.</li> </ul>"},{"location":"api/processor/vector/chunking/","title":"Chunking","text":"<p>Module for chunking source code files using LODGenerator.</p>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker","title":"TreeSitterChunker","text":"<p>Chunks code files based on LODEntity structure generated by LODGenerator.</p> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>class TreeSitterChunker:\n\t\"\"\"Chunks code files based on LODEntity structure generated by LODGenerator.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tlod_generator: LODGenerator | None = None,\n\t\tconfig_loader: \"ConfigLoader | None\" = None,\n\t\tgit_context: GitRepoContext | None = None,\n\t\trepo_checksum_calculator: \"RepoChecksumCalculator | None\" = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the chunker.\n\n\t\tArgs:\n\t\t    lod_generator: An instance of LODGenerator. If None, creates a new one.\n\t\t    config_loader: Configuration loader instance.\n\t\t    git_context: Git repository context instance.\n\t\t    repo_checksum_calculator: Optional RepoChecksumCalculator instance.\n\n\t\t\"\"\"\n\t\tself.lod_generator = lod_generator or LODGenerator()\n\t\tif config_loader:\n\t\t\tself.config_loader = config_loader\n\t\telse:\n\t\t\tfrom codemap.config import ConfigLoader\n\n\t\t\tself.config_loader = ConfigLoader.get_instance()\n\n\t\t# Load configuration values\n\t\tembedding_config = self.config_loader.get.embedding\n\t\tchunking_config = embedding_config.chunking\n\n\t\t# Set constants from config with fallbacks\n\t\tself.max_hierarchy_depth = chunking_config.max_hierarchy_depth\n\t\tself.max_file_lines = chunking_config.max_file_lines\n\n\t\tself.git_context = git_context\n\t\tself.repo_checksum_calculator = repo_checksum_calculator\n\n\tdef _get_entity_code_content(self, entity: LODEntity, file_lines: list[str]) -&gt; str | None:\n\t\t\"\"\"Extract the raw code content for an entity using its line numbers.\"\"\"\n\t\tif entity.start_line is None or entity.end_line is None:\n\t\t\treturn None\n\n\t\tstart_idx = entity.start_line - 1\n\t\tend_idx = entity.end_line\n\t\tif 0 &lt;= start_idx &lt; end_idx &lt;= len(file_lines):\n\t\t\treturn \"\\n\".join(file_lines[start_idx:end_idx])\n\t\tlogger.warning(\n\t\t\t\"Invalid line numbers for entity %s in %s: start=%s, end=%s, total_lines=%d\",\n\t\t\tentity.name,\n\t\t\tentity.metadata.get(\"file_path\"),\n\t\t\tentity.start_line,\n\t\t\tentity.end_line,\n\t\t\tlen(file_lines),\n\t\t)\n\t\treturn None\n\n\tdef _build_hierarchy_path(self, entity: LODEntity, parent_path: str = \"\") -&gt; str:\n\t\t\"\"\"\n\t\tBuild a hierarchical path string representing the entity's position in the code.\n\n\t\tArgs:\n\t\t        entity: The current entity\n\t\t        parent_path: Path of parent entities\n\n\t\tReturns:\n\t\t        String representation of the hierarchy path\n\n\t\t\"\"\"\n\t\tentity_name = entity.name or f\"&lt;{entity.entity_type.name.lower()}&gt;\"\n\t\tif not parent_path:\n\t\t\treturn entity_name\n\t\treturn f\"{parent_path}.{entity_name}\"\n\n\tdef _extract_nested_entities(self, entity: LODEntity) -&gt; list[dict[str, Any]]:\n\t\t\"\"\"\n\t\tExtract information about nested entities to enhance chunk context.\n\n\t\tArgs:\n\t\t        entity: The current entity\n\n\t\tReturns:\n\t\t        List of dictionaries containing info about nested entities\n\n\t\t\"\"\"\n\t\tnested_info = []\n\n\t\tdef process_nested(nested_entity: LODEntity, depth: int = 1) -&gt; None:\n\t\t\t\"\"\"Process a nested entity and its children recursively to extract information.\n\n\t\t\tArgs:\n\t\t\t\tnested_entity: The nested entity to process\n\t\t\t\tdepth: Current depth in the hierarchy (default: 1)\n\n\t\t\tReturns:\n\t\t\t\tNone: Modifies nested_info in place by appending entity information\n\t\t\t\"\"\"\n\t\t\t# Skip UNKNOWN entities\n\t\t\tif nested_entity.entity_type == EntityType.UNKNOWN:\n\t\t\t\treturn\n\n\t\t\tentity_info = {\n\t\t\t\t\"type\": nested_entity.entity_type.name,\n\t\t\t\t\"name\": nested_entity.name or f\"&lt;{str(nested_entity.entity_type.name).lower()}&gt;\",\n\t\t\t\t\"signature\": nested_entity.signature or \"\",\n\t\t\t\t\"depth\": depth,\n\t\t\t\t\"line_range\": f\"{nested_entity.start_line}-{nested_entity.end_line}\"\n\t\t\t\tif nested_entity.start_line and nested_entity.end_line\n\t\t\t\telse \"\",\n\t\t\t}\n\t\t\tnested_info.append(entity_info)\n\n\t\t\t# Process children (limited by configured max hierarchy depth)\n\t\t\tif depth &lt; self.max_hierarchy_depth:\n\t\t\t\tfor child in nested_entity.children:\n\t\t\t\t\tprocess_nested(child, depth + 1)\n\n\t\t# Process all direct children\n\t\tfor child in entity.children:\n\t\t\tprocess_nested(child)\n\n\t\treturn nested_info\n\n\tdef _chunk_entity_recursive(\n\t\tself,\n\t\tentity: LODEntity,\n\t\tabsolute_file_path: Path,\n\t\tfile_lines: list[str],\n\t\tgit_hash: str | None,\n\t\tfile_content_hash: str,\n\t\tlanguage: str,\n\t\tlast_modified_time: float,\n\t\tparent_hierarchy: str = \"\",\n\t\tfile_entity: LODEntity | None = None,\n\t) -&gt; Generator[ChunkSchema, None, None]:\n\t\t\"\"\"Recursive helper to generate chunks from the LODEntity tree with hierarchy context.\"\"\"\n\t\t# Decide which entity types are significant enough to become their own chunk\n\t\tprimary_chunkable_types = (\n\t\t\tEntityType.MODULE,\n\t\t\tEntityType.CLASS,\n\t\t\tEntityType.INTERFACE,\n\t\t\tEntityType.STRUCT,\n\t\t)\n\n\t\tsecondary_chunkable_types = (\n\t\t\tEntityType.FUNCTION,\n\t\t\tEntityType.METHOD,\n\t\t)\n\n\t\t# Skip UNKNOWN entities entirely\n\t\tif entity.entity_type == EntityType.UNKNOWN:\n\t\t\treturn\n\n\t\t# Build hierarchy path for this entity\n\t\tentity_hierarchy = self._build_hierarchy_path(entity, parent_hierarchy)\n\n\t\t# For primary entities (modules, classes), create full chunks with all their content\n\t\tif (\n\t\t\tentity.entity_type in primary_chunkable_types\n\t\t\tand entity.start_line is not None\n\t\t\tand entity.end_line is not None\n\t\t):\n\t\t\ttry:\n\t\t\t\t# Get full content including all nested entities\n\t\t\t\tcode_content = self._get_entity_code_content(entity, file_lines)\n\t\t\t\tif code_content:\n\t\t\t\t\t# Extract information about nested entities to enhance context\n\t\t\t\t\tnested_entities = self._extract_nested_entities(entity)\n\n\t\t\t\t\t# Construct rich chunk content with nested entity information\n\t\t\t\t\tcontent_parts = []\n\t\t\t\t\tcontent_parts.append(f\"Type: {entity.entity_type.name}\")\n\t\t\t\t\tcontent_parts.append(f\"Path: {entity_hierarchy}\")\n\t\t\t\t\tif entity.name:\n\t\t\t\t\t\tcontent_parts.append(f\"Name: {entity.name}\")\n\t\t\t\t\tif entity.signature:\n\t\t\t\t\t\tcontent_parts.append(f\"Signature: {entity.signature}\")\n\t\t\t\t\tif entity.docstring:\n\t\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{entity.docstring}\")\n\n\t\t\t\t\t# Add structure overview\n\t\t\t\t\tif nested_entities:\n\t\t\t\t\t\tcontent_parts.append(\"Contains:\")\n\t\t\t\t\t\tfor ne in nested_entities:\n\t\t\t\t\t\t\tindent = \"  \" * ne[\"depth\"]\n\t\t\t\t\t\t\tcontent_parts.append(\n\t\t\t\t\t\t\t\tf\"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})\"\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t# Add the full code\n\t\t\t\t\tcontent_parts.append(f\"Code:\\n```{language}\\n{code_content}\\n```\")\n\n\t\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\t\tcontent_parts.append(f\"Raw:\\n{code_content}\")\n\n\t\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t\t# Generate content_hash from the final chunk_content\n\t\t\t\t\tcontent_hasher = xxhash.xxh3_64()\n\t\t\t\t\tcontent_hasher.update(chunk_content.encode(\"utf-8\"))\n\t\t\t\t\tchunk_content_hash = content_hasher.hexdigest()\n\n\t\t\t\t\t# Reverted path logic: use original file_path\n\t\t\t\t\t# Removed relative path calculation\n\t\t\t\t\tmetadata = self._make_chunk_metadata(\n\t\t\t\t\t\tchunk_content_hash,\n\t\t\t\t\t\tfile_content_hash,\n\t\t\t\t\t\tabsolute_file_path,\n\t\t\t\t\t\tentity.start_line,\n\t\t\t\t\t\tentity.end_line,\n\t\t\t\t\t\tstr(entity.entity_type.name),\n\t\t\t\t\t\tentity.name or \"\",\n\t\t\t\t\t\tlanguage,\n\t\t\t\t\t\tentity_hierarchy,\n\t\t\t\t\t\tlast_modified_time,\n\t\t\t\t\t)\n\t\t\t\t\tyield ChunkSchema(content=chunk_content, metadata=metadata)\n\n\t\t\texcept (ValueError, TypeError, KeyError, AttributeError):\n\t\t\t\tlogger.exception(\"Error processing LOD entity %s in %s\", entity.name, absolute_file_path)\n\n\t\t# For secondary entities (functions, methods), create individual chunks\n\t\telif (\n\t\t\tentity.entity_type in secondary_chunkable_types\n\t\t\tand entity.start_line is not None\n\t\t\tand entity.end_line is not None\n\t\t):\n\t\t\ttry:\n\t\t\t\tcode_content = self._get_entity_code_content(entity, file_lines)\n\t\t\t\tif code_content:\n\t\t\t\t\t# Use file entity if available (for better context)\n\t\t\t\t\tfile_context = \"\"\n\t\t\t\t\tif file_entity and file_entity.entity_type == EntityType.MODULE:\n\t\t\t\t\t\tfile_context = f\"File: {file_entity.name or absolute_file_path.name}\\n\"\n\n\t\t\t\t\t# Construct rich chunk content\n\t\t\t\t\tcontent_parts = []\n\t\t\t\t\tcontent_parts.append(f\"{file_context}Type: {entity.entity_type.name}\")\n\t\t\t\t\tcontent_parts.append(f\"Path: {entity_hierarchy}\")\n\t\t\t\t\tif entity.name:\n\t\t\t\t\t\tcontent_parts.append(f\"Name: {entity.name}\")\n\t\t\t\t\tif entity.signature:\n\t\t\t\t\t\tcontent_parts.append(f\"Signature: {entity.signature}\")\n\t\t\t\t\tif entity.docstring:\n\t\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{entity.docstring}\")\n\n\t\t\t\t\t# Add code with any dependencies visible in comments\n\t\t\t\t\tcontent_parts.append(f\"Code:\\n```{language}\\n{code_content}\\n```\")\n\n\t\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\t\tcontent_parts.append(f\"Raw:\\n{code_content}\")\n\n\t\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t\t# Generate content_hash from the final chunk_content\n\t\t\t\t\tcontent_hasher = xxhash.xxh3_64()\n\t\t\t\t\tcontent_hasher.update(chunk_content.encode(\"utf-8\"))\n\t\t\t\t\tchunk_content_hash = content_hasher.hexdigest()\n\n\t\t\t\t\t# Use default chunk_id generation from schema for now\n\t\t\t\t\tmetadata = self._make_chunk_metadata(\n\t\t\t\t\t\tchunk_content_hash,\n\t\t\t\t\t\tfile_content_hash,\n\t\t\t\t\t\tabsolute_file_path,\n\t\t\t\t\t\tentity.start_line,\n\t\t\t\t\t\tentity.end_line,\n\t\t\t\t\t\tstr(entity.entity_type.name),\n\t\t\t\t\t\tentity.name or \"\",\n\t\t\t\t\t\tlanguage,\n\t\t\t\t\t\tentity_hierarchy,\n\t\t\t\t\t\tlast_modified_time,\n\t\t\t\t\t)\n\t\t\t\t\tyield ChunkSchema(content=chunk_content, metadata=metadata)\n\n\t\t\texcept (ValueError, TypeError, KeyError, AttributeError):\n\t\t\t\tlogger.exception(\"Error processing LOD entity %s in %s\", entity.name, absolute_file_path)\n\n\t\t# Recursively process children, remove repo_path pass\n\t\tfor child in entity.children:\n\t\t\tyield from self._chunk_entity_recursive(\n\t\t\t\tchild,\n\t\t\t\tabsolute_file_path,\n\t\t\t\tfile_lines,\n\t\t\t\tgit_hash,\n\t\t\t\tfile_content_hash,\n\t\t\t\tlanguage,\n\t\t\t\tlast_modified_time,\n\t\t\t\tentity_hierarchy,\n\t\t\t\tfile_entity=file_entity,\n\t\t\t)\n\n\tdef chunk_file(\n\t\tself,\n\t\tabsolute_file_path: Path,\n\t\tgit_hash: str | None = None,\n\t\tlod_level: LODLevel = LODLevel.FULL,  # Use FULL for max info, not DETAIL\n\t) -&gt; Generator[ChunkSchema, None, None]:\n\t\t\"\"\"\n\t\tGenerates code chunks for a given file using LODGenerator.\n\n\t\tArgs:\n\t\t    absolute_file_path: The absolute path to the file to chunk.\n\t\t    git_hash: Optional Git hash of the file content (blob hash).\n\t\t    lod_level: The level of detail to request from LODGenerator.\n\n\t\tYields:\n\t\t    CodeChunk dictionaries, each representing a semantically rich code chunk.\n\n\t\t\"\"\"\n\t\tif not absolute_file_path.is_absolute():\n\t\t\tlogger.warning(f\"chunk_file received relative path: {absolute_file_path}. Resolving.\")\n\t\t\tabsolute_file_path = absolute_file_path.resolve()\n\n\t\ttry:\n\t\t\tlast_modified_time = absolute_file_path.stat().st_mtime\n\n\t\t\t# Generate the LODEntity tree for the file using the specified level of detail\n\t\t\troot_entity = self.lod_generator.generate_lod(absolute_file_path, lod_level)\n\n\t\t\tif not root_entity:\n\t\t\t\tlogger.debug(\"LODGenerator returned no entity for %s, skipping chunking\", absolute_file_path)\n\t\t\t\treturn\n\n\t\t\t# Try to get full_content_str from root_entity metadata (set by LODGenerator)\n\t\t\tcontent = root_entity.metadata.get(\"full_content_str\")\n\n\t\t\tif content is None:  # Fallback if not provided by LODGenerator\n\t\t\t\tlogger.debug(\n\t\t\t\t\t\"full_content_str not in root_entity metadata for %s. Reading file directly.\", absolute_file_path\n\t\t\t\t)\n\t\t\t\tcontent = read_file_content(absolute_file_path)\n\t\t\t\tif content is None:\n\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\"Skipping file %s - could not obtain content via LOD or direct read\", absolute_file_path\n\t\t\t\t\t)\n\t\t\t\t\treturn\n\n\t\t\t# Language should be available in the root entity metadata now\n\t\t\tresolved_language = root_entity.metadata.get(\"language\", \"unknown\")\n\t\t\tfile_lines = content.splitlines()\n\n\t\t\t# Generate file_hash from the entire file content\n\t\t\tfile_content_hasher = xxhash.xxh3_128()\n\t\t\tfile_content_hasher.update(content.encode(\"utf-8\"))\n\t\t\tentire_file_content_hash = file_content_hasher.hexdigest()\n\n\t\t\t# First, create a chunk for the entire file if it's small enough\n\t\t\tif len(file_lines) &lt; self.max_file_lines:\n\t\t\t\t# Create a chunk for the entire file\n\t\t\t\ttry:\n\t\t\t\t\twhole_file_content = \"\\n\".join(file_lines)\n\n\t\t\t\t\t# Information about the file as a whole\n\t\t\t\t\tcontent_parts = []\n\t\t\t\t\tcontent_parts.append(\"Type: FILE\")\n\t\t\t\t\tfile_name = absolute_file_path.name\n\t\t\t\t\tcontent_parts.append(f\"Path: {file_name}\")\n\t\t\t\t\tcontent_parts.append(f\"Name: {file_name}\")\n\n\t\t\t\t\t# Add docstring if the file has one (module docstring)\n\t\t\t\t\tif root_entity.docstring:\n\t\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{root_entity.docstring}\")\n\n\t\t\t\t\t# Get structure overview\n\t\t\t\t\tnested_entities = self._extract_nested_entities(root_entity)\n\t\t\t\t\tif nested_entities:\n\t\t\t\t\t\tcontent_parts.append(\"Contains:\")\n\t\t\t\t\t\tfor ne in nested_entities:\n\t\t\t\t\t\t\tindent = \"  \" * ne[\"depth\"]\n\t\t\t\t\t\t\tcontent_parts.append(\n\t\t\t\t\t\t\t\tf\"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})\"\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t# Add the full code\n\t\t\t\t\tcontent_parts.append(f\"Code:\\n```{resolved_language}\\n{whole_file_content}\\n```\")\n\n\t\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\t\tcontent_parts.append(f\"Raw:\\n{whole_file_content}\")\n\n\t\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t\t# Generate content_hash from the final chunk_content\n\t\t\t\t\tcontent_hasher = xxhash.xxh3_64()\n\t\t\t\t\tcontent_hasher.update(chunk_content.encode(\"utf-8\"))\n\t\t\t\t\tchunk_content_hash = content_hasher.hexdigest()\n\n\t\t\t\t\tmetadata = self._make_chunk_metadata(\n\t\t\t\t\t\tchunk_content_hash,\n\t\t\t\t\t\tentire_file_content_hash,\n\t\t\t\t\t\tabsolute_file_path,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tlen(file_lines),\n\t\t\t\t\t\t\"FILE\",\n\t\t\t\t\t\tfile_name,\n\t\t\t\t\t\tresolved_language,\n\t\t\t\t\t\tfile_name,\n\t\t\t\t\t\tlast_modified_time,\n\t\t\t\t\t)\n\t\t\t\t\tyield ChunkSchema(content=chunk_content, metadata=metadata)\n\t\t\t\texcept (ValueError, TypeError, KeyError, AttributeError) as e:\n\t\t\t\t\tlogger.warning(\"Error creating whole-file chunk for %s: %s\", absolute_file_path, e)\n\n\t\t\t# Then create more specific chunks for the individual entities\n\t\t\tyield from self._chunk_entity_recursive(\n\t\t\t\troot_entity,\n\t\t\t\tabsolute_file_path,\n\t\t\t\tfile_lines,\n\t\t\t\tgit_hash,\n\t\t\t\tentire_file_content_hash,\n\t\t\t\tresolved_language,\n\t\t\t\tlast_modified_time,\n\t\t\t\tfile_entity=root_entity,\n\t\t\t)\n\n\t\texcept (OSError, ValueError, TypeError, KeyError, AttributeError) as e:\n\t\t\tlogger.debug(\"Failed to chunk file %s: %s\", absolute_file_path, str(e))\n\t\t\treturn\n\n\tdef _make_git_metadata(self, relative_file_path_for_git: str, start_line: int, end_line: int) -&gt; GitMetadataSchema:\n\t\t\"\"\"Get git metadata for a file.\n\n\t\tArgs:\n\t\t\trelative_file_path_for_git (str): Path relative to repo root or filename\n\t\t\tstart_line (int): Start line of the chunk\n\t\t\tend_line (int): End line of the chunk\n\n\t\tReturns:\n\t\t\tGitMetadataSchema: Git metadata for the file\n\t\t\"\"\"\n\t\tif not self.git_context:\n\t\t\t# fallback: return empty/default metadata\n\t\t\treturn GitMetadataSchema(\n\t\t\t\tgit_hash=\"\",\n\t\t\t\ttracked=False,\n\t\t\t\tbranch=\"\",\n\t\t\t\tblame=[],\n\t\t\t)\n\n\t\t# Ensure we have a proper path that can be found in the repository\n\t\tif (\n\t\t\t\"/\" not in relative_file_path_for_git\n\t\t\tand \"\\\\\" not in relative_file_path_for_git\n\t\t\tand self.git_context.tracked_files\n\t\t):\n\t\t\t# We have just a filename - try to find it in tracked files\n\t\t\tmatching_paths = [p for p in self.git_context.tracked_files if p.endswith(\"/\" + relative_file_path_for_git)]\n\t\t\tif len(matching_paths) == 1:\n\t\t\t\trelative_file_path_for_git = matching_paths[0]\n\t\t\t\tlogger.debug(f\"Updated path to use tracked file path: {relative_file_path_for_git}\")\n\n\t\t# Get metadata from git context\n\t\treturn self.git_context.get_metadata_schema(relative_file_path_for_git, start_line, end_line)\n\n\tdef _make_chunk_metadata(\n\t\tself,\n\t\tchunk_content_hash: str,\n\t\tfile_content_hash: str,\n\t\tabsolute_file_path: Path,\n\t\tstart_line: int,\n\t\tend_line: int,\n\t\tentity_type: str,\n\t\tentity_name: str,\n\t\tlanguage: str,\n\t\thierarchy_path: str,\n\t\tlast_modified_time: float,\n\t) -&gt; ChunkMetadataSchema:\n\t\t\"\"\"\n\t\tCreate a ChunkMetadataSchema for the chunk.\n\n\t\tArgs:\n\t\t\tchunk_content_hash (str): The content hash.\n\t\t\tfile_content_hash (str): The hash of the entire file content.\n\t\t\tabsolute_file_path (Path): The absolute path to the file.\n\t\t\tstart_line (int): Start line.\n\t\t\tend_line (int): End line.\n\t\t\tentity_type (str): Entity type.\n\t\t\tentity_name (str): Entity name.\n\t\t\tlanguage (str): Language.\n\t\t\thierarchy_path (str): Hierarchy path.\n\t\t\tlast_modified_time (float): File's last modification timestamp.\n\n\t\tReturns:\n\t\t\tChunkMetadataSchema: The chunk metadata.\n\t\t\"\"\"\n\t\t# Default to file name only if we can't determine the relative path\n\t\trelative_path_for_git_and_schema = str(absolute_file_path.name)\n\n\t\tif self.git_context and self.git_context.repo_root:\n\t\t\ttry:\n\t\t\t\t# Try to make the path relative to the git repo root\n\t\t\t\trelative_path = absolute_file_path.relative_to(self.git_context.repo_root)\n\t\t\t\trelative_path_for_git_and_schema = str(relative_path.as_posix())\n\t\t\texcept ValueError:\n\t\t\t\t# If that fails, check if the file is tracked in the repository\n\t\t\t\tif self.git_context.tracked_files and absolute_file_path.name in [\n\t\t\t\t\tPath(p).name for p in self.git_context.tracked_files\n\t\t\t\t]:\n\t\t\t\t\t# Try to find the actual relative path from tracked files\n\t\t\t\t\tmatching_paths = [\n\t\t\t\t\t\tp for p in self.git_context.tracked_files if Path(p).name == absolute_file_path.name\n\t\t\t\t\t]\n\t\t\t\t\tif len(matching_paths) == 1:\n\t\t\t\t\t\t# Found a unique match in tracked files\n\t\t\t\t\t\trelative_path_for_git_and_schema = matching_paths[0]\n\t\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\tf\"Found tracked file path for {absolute_file_path.name}: {relative_path_for_git_and_schema}\"\n\t\t\t\t\t\t)\n\t\t\t\t\telif len(matching_paths) &gt; 1:\n\t\t\t\t\t\t# Multiple matches - use the one with path most similar to absolute_file_path\n\t\t\t\t\t\tbest_match = max(\n\t\t\t\t\t\t\tmatching_paths,\n\t\t\t\t\t\t\tkey=lambda p: sum(\n\t\t\t\t\t\t\t\t1 for a, b in zip(str(p), str(absolute_file_path), strict=False) if a == b\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t)\n\t\t\t\t\t\trelative_path_for_git_and_schema = best_match\n\t\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\tf\"Multiple tracked paths for {absolute_file_path.name}, \"\n\t\t\t\t\t\t\tf\"using best match: {relative_path_for_git_and_schema}\"\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\tf\"File path {absolute_file_path} could not be made relative to \"\n\t\t\t\t\t\t\tf\"git repo root {self.git_context.repo_root}. Using filename as fallback.\"\n\t\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"File path {absolute_file_path} could not be made relative to \"\n\t\t\t\t\t\tf\"git repo root {self.git_context.repo_root}. Using filename as fallback.\"\n\t\t\t\t\t)\n\t\telse:\n\t\t\t# If no git_context or repo_root, we can't reliably determine the relative path\n\t\t\tlogger.debug(\n\t\t\t\t\"No Git context or repo root available. Using file name \"\n\t\t\t\tf\"'{absolute_file_path.name}' as file_path in metadata.\"\n\t\t\t)\n\n\t\tgenerated_chunk_id = str(uuid.uuid4())\n\n\t\treturn ChunkMetadataSchema(\n\t\t\tchunk_id=generated_chunk_id,\n\t\t\tcontent_hash=chunk_content_hash,\n\t\t\tstart_line=start_line,\n\t\t\tend_line=end_line,\n\t\t\tentity_type=entity_type,\n\t\t\tentity_name=entity_name or \"\",\n\t\t\thierarchy_path=hierarchy_path,\n\t\t\tgit_metadata=self._make_git_metadata(relative_path_for_git_and_schema, start_line, end_line),\n\t\t\tfile_metadata=FileMetadataSchema(\n\t\t\t\tfile_path=relative_path_for_git_and_schema,\n\t\t\t\tlanguage=language,\n\t\t\t\tlast_modified_time=last_modified_time,\n\t\t\t\tfile_content_hash=file_content_hash,\n\t\t\t),\n\t\t)\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.__init__","title":"__init__","text":"<pre><code>__init__(\n\tlod_generator: LODGenerator | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n\tgit_context: GitRepoContext | None = None,\n\trepo_checksum_calculator: RepoChecksumCalculator\n\t| None = None,\n) -&gt; None\n</code></pre> <p>Initialize the chunker.</p> <p>Parameters:</p> Name Type Description Default <code>lod_generator</code> <code>LODGenerator | None</code> <p>An instance of LODGenerator. If None, creates a new one.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance.</p> <code>None</code> <code>git_context</code> <code>GitRepoContext | None</code> <p>Git repository context instance.</p> <code>None</code> <code>repo_checksum_calculator</code> <code>RepoChecksumCalculator | None</code> <p>Optional RepoChecksumCalculator instance.</p> <code>None</code> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>def __init__(\n\tself,\n\tlod_generator: LODGenerator | None = None,\n\tconfig_loader: \"ConfigLoader | None\" = None,\n\tgit_context: GitRepoContext | None = None,\n\trepo_checksum_calculator: \"RepoChecksumCalculator | None\" = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the chunker.\n\n\tArgs:\n\t    lod_generator: An instance of LODGenerator. If None, creates a new one.\n\t    config_loader: Configuration loader instance.\n\t    git_context: Git repository context instance.\n\t    repo_checksum_calculator: Optional RepoChecksumCalculator instance.\n\n\t\"\"\"\n\tself.lod_generator = lod_generator or LODGenerator()\n\tif config_loader:\n\t\tself.config_loader = config_loader\n\telse:\n\t\tfrom codemap.config import ConfigLoader\n\n\t\tself.config_loader = ConfigLoader.get_instance()\n\n\t# Load configuration values\n\tembedding_config = self.config_loader.get.embedding\n\tchunking_config = embedding_config.chunking\n\n\t# Set constants from config with fallbacks\n\tself.max_hierarchy_depth = chunking_config.max_hierarchy_depth\n\tself.max_file_lines = chunking_config.max_file_lines\n\n\tself.git_context = git_context\n\tself.repo_checksum_calculator = repo_checksum_calculator\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.lod_generator","title":"lod_generator  <code>instance-attribute</code>","text":"<pre><code>lod_generator = lod_generator or LODGenerator()\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.max_hierarchy_depth","title":"max_hierarchy_depth  <code>instance-attribute</code>","text":"<pre><code>max_hierarchy_depth = max_hierarchy_depth\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.max_file_lines","title":"max_file_lines  <code>instance-attribute</code>","text":"<pre><code>max_file_lines = max_file_lines\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.git_context","title":"git_context  <code>instance-attribute</code>","text":"<pre><code>git_context = git_context\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.repo_checksum_calculator","title":"repo_checksum_calculator  <code>instance-attribute</code>","text":"<pre><code>repo_checksum_calculator = repo_checksum_calculator\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.chunk_file","title":"chunk_file","text":"<pre><code>chunk_file(\n\tabsolute_file_path: Path,\n\tgit_hash: str | None = None,\n\tlod_level: LODLevel = FULL,\n) -&gt; Generator[ChunkSchema, None, None]\n</code></pre> <p>Generates code chunks for a given file using LODGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>absolute_file_path</code> <code>Path</code> <p>The absolute path to the file to chunk.</p> required <code>git_hash</code> <code>str | None</code> <p>Optional Git hash of the file content (blob hash).</p> <code>None</code> <code>lod_level</code> <code>LODLevel</code> <p>The level of detail to request from LODGenerator.</p> <code>FULL</code> <p>Yields:</p> Type Description <code>ChunkSchema</code> <p>CodeChunk dictionaries, each representing a semantically rich code chunk.</p> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>def chunk_file(\n\tself,\n\tabsolute_file_path: Path,\n\tgit_hash: str | None = None,\n\tlod_level: LODLevel = LODLevel.FULL,  # Use FULL for max info, not DETAIL\n) -&gt; Generator[ChunkSchema, None, None]:\n\t\"\"\"\n\tGenerates code chunks for a given file using LODGenerator.\n\n\tArgs:\n\t    absolute_file_path: The absolute path to the file to chunk.\n\t    git_hash: Optional Git hash of the file content (blob hash).\n\t    lod_level: The level of detail to request from LODGenerator.\n\n\tYields:\n\t    CodeChunk dictionaries, each representing a semantically rich code chunk.\n\n\t\"\"\"\n\tif not absolute_file_path.is_absolute():\n\t\tlogger.warning(f\"chunk_file received relative path: {absolute_file_path}. Resolving.\")\n\t\tabsolute_file_path = absolute_file_path.resolve()\n\n\ttry:\n\t\tlast_modified_time = absolute_file_path.stat().st_mtime\n\n\t\t# Generate the LODEntity tree for the file using the specified level of detail\n\t\troot_entity = self.lod_generator.generate_lod(absolute_file_path, lod_level)\n\n\t\tif not root_entity:\n\t\t\tlogger.debug(\"LODGenerator returned no entity for %s, skipping chunking\", absolute_file_path)\n\t\t\treturn\n\n\t\t# Try to get full_content_str from root_entity metadata (set by LODGenerator)\n\t\tcontent = root_entity.metadata.get(\"full_content_str\")\n\n\t\tif content is None:  # Fallback if not provided by LODGenerator\n\t\t\tlogger.debug(\n\t\t\t\t\"full_content_str not in root_entity metadata for %s. Reading file directly.\", absolute_file_path\n\t\t\t)\n\t\t\tcontent = read_file_content(absolute_file_path)\n\t\t\tif content is None:\n\t\t\t\tlogger.debug(\n\t\t\t\t\t\"Skipping file %s - could not obtain content via LOD or direct read\", absolute_file_path\n\t\t\t\t)\n\t\t\t\treturn\n\n\t\t# Language should be available in the root entity metadata now\n\t\tresolved_language = root_entity.metadata.get(\"language\", \"unknown\")\n\t\tfile_lines = content.splitlines()\n\n\t\t# Generate file_hash from the entire file content\n\t\tfile_content_hasher = xxhash.xxh3_128()\n\t\tfile_content_hasher.update(content.encode(\"utf-8\"))\n\t\tentire_file_content_hash = file_content_hasher.hexdigest()\n\n\t\t# First, create a chunk for the entire file if it's small enough\n\t\tif len(file_lines) &lt; self.max_file_lines:\n\t\t\t# Create a chunk for the entire file\n\t\t\ttry:\n\t\t\t\twhole_file_content = \"\\n\".join(file_lines)\n\n\t\t\t\t# Information about the file as a whole\n\t\t\t\tcontent_parts = []\n\t\t\t\tcontent_parts.append(\"Type: FILE\")\n\t\t\t\tfile_name = absolute_file_path.name\n\t\t\t\tcontent_parts.append(f\"Path: {file_name}\")\n\t\t\t\tcontent_parts.append(f\"Name: {file_name}\")\n\n\t\t\t\t# Add docstring if the file has one (module docstring)\n\t\t\t\tif root_entity.docstring:\n\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{root_entity.docstring}\")\n\n\t\t\t\t# Get structure overview\n\t\t\t\tnested_entities = self._extract_nested_entities(root_entity)\n\t\t\t\tif nested_entities:\n\t\t\t\t\tcontent_parts.append(\"Contains:\")\n\t\t\t\t\tfor ne in nested_entities:\n\t\t\t\t\t\tindent = \"  \" * ne[\"depth\"]\n\t\t\t\t\t\tcontent_parts.append(\n\t\t\t\t\t\t\tf\"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})\"\n\t\t\t\t\t\t)\n\n\t\t\t\t# Add the full code\n\t\t\t\tcontent_parts.append(f\"Code:\\n```{resolved_language}\\n{whole_file_content}\\n```\")\n\n\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\tcontent_parts.append(f\"Raw:\\n{whole_file_content}\")\n\n\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t# Generate content_hash from the final chunk_content\n\t\t\t\tcontent_hasher = xxhash.xxh3_64()\n\t\t\t\tcontent_hasher.update(chunk_content.encode(\"utf-8\"))\n\t\t\t\tchunk_content_hash = content_hasher.hexdigest()\n\n\t\t\t\tmetadata = self._make_chunk_metadata(\n\t\t\t\t\tchunk_content_hash,\n\t\t\t\t\tentire_file_content_hash,\n\t\t\t\t\tabsolute_file_path,\n\t\t\t\t\t1,\n\t\t\t\t\tlen(file_lines),\n\t\t\t\t\t\"FILE\",\n\t\t\t\t\tfile_name,\n\t\t\t\t\tresolved_language,\n\t\t\t\t\tfile_name,\n\t\t\t\t\tlast_modified_time,\n\t\t\t\t)\n\t\t\t\tyield ChunkSchema(content=chunk_content, metadata=metadata)\n\t\t\texcept (ValueError, TypeError, KeyError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Error creating whole-file chunk for %s: %s\", absolute_file_path, e)\n\n\t\t# Then create more specific chunks for the individual entities\n\t\tyield from self._chunk_entity_recursive(\n\t\t\troot_entity,\n\t\t\tabsolute_file_path,\n\t\t\tfile_lines,\n\t\t\tgit_hash,\n\t\t\tentire_file_content_hash,\n\t\t\tresolved_language,\n\t\t\tlast_modified_time,\n\t\t\tfile_entity=root_entity,\n\t\t)\n\n\texcept (OSError, ValueError, TypeError, KeyError, AttributeError) as e:\n\t\tlogger.debug(\"Failed to chunk file %s: %s\", absolute_file_path, str(e))\n\t\treturn\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/","title":"Qdrant Manager","text":"<p>Module for managing Qdrant vector database collections.</p>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.HTTP_NOT_FOUND","title":"HTTP_NOT_FOUND  <code>module-attribute</code>","text":"<pre><code>HTTP_NOT_FOUND = 404\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager","title":"QdrantManager","text":"<p>Manages interactions with a Qdrant vector database collection.</p> <p>Handles initialization, adding/upserting points (vectors + payload), searching, and retrieving points based on IDs or filters. Uses an AsyncQdrantClient for non-blocking operations.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>class QdrantManager:\n\t\"\"\"\n\tManages interactions with a Qdrant vector database collection.\n\n\tHandles initialization, adding/upserting points (vectors + payload),\n\tsearching, and retrieving points based on IDs or filters. Uses an\n\tAsyncQdrantClient for non-blocking operations.\n\n\t\"\"\"\n\n\tvalue_error_msg = \"Filter condition cannot be None if point_ids are not provided\"\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: \"ConfigLoader\",\n\t\tcollection_name: str | None = None,\n\t\tdim: int | None = None,\n\t\tdistance: Distance | None = None,\n\t\tapi_key: str | None = None,  # For Qdrant Cloud\n\t\turl: str | None = None,  # For self-hosted or Cloud URL\n\t\tprefer_grpc: bool | None = None,\n\t\ttimeout: float | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the QdrantManager.\n\n\t\tArgs:\n\t\t    config_loader: Configuration loader instance.\n\t\t    location: Path for local storage or \":memory:\". Ignored if url is provided.\n\t\t    collection_name: Name of the Qdrant collection to use.\n\t\t    dim: Dimension of the vectors.\n\t\t    distance: Distance metric for vector comparison.\n\t\t    api_key: API key for Qdrant Cloud authentication.\n\t\t    url: URL for connecting to a remote Qdrant instance (overrides location).\n\t\t    prefer_grpc: Whether to prefer gRPC over REST.\n\t\t    timeout: Connection timeout in seconds.\n\n\t\t\"\"\"\n\t\t# Get embedding configuration\n\t\tself.config_loader = config_loader\n\t\tembedding_config = self.config_loader.get.embedding\n\n\t\t# Get distance metric from config or parameter\n\t\tdistance_metric_str = embedding_config.dimension_metric.upper()\n\t\tdefault_distance = (\n\t\t\tgetattr(Distance, distance_metric_str) if hasattr(Distance, distance_metric_str) else Distance.COSINE\n\t\t)\n\n\t\t# Load values from parameters or fall back to config\n\t\tself.collection_name = collection_name or \"codemap_vectors\"\n\t\tself.dim = dim or embedding_config.dimension\n\t\tself.distance = distance or default_distance\n\n\t\t# Build client args\n\t\tself.client_args = {\n\t\t\t\"api_key\": api_key or embedding_config.api_key,\n\t\t\t\"url\": url or embedding_config.url,\n\t\t\t\"prefer_grpc\": prefer_grpc if prefer_grpc is not None else embedding_config.prefer_grpc,\n\t\t\t\"timeout\": timeout or embedding_config.timeout,\n\t\t}\n\t\t# Remove None values from args\n\t\tself.client_args = {k: v for k, v in self.client_args.items() if v is not None}\n\n\t\t# Initialize client later in an async context\n\t\tself.client: AsyncQdrantClient | None = None\n\t\tself.is_initialized = False\n\n\tasync def initialize(self) -&gt; None:\n\t\t\"\"\"Asynchronously initialize the Qdrant client and ensure the collection exists.\"\"\"\n\t\tif self.is_initialized:\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(\"Initializing Qdrant client with args: %s\", self.client_args)\n\t\t\t# Create client with explicit type casting to avoid type issues\n\t\t\tclient_kwargs: dict[str, Any] = {}\n\t\t\tif \"api_key\" in self.client_args and self.client_args[\"api_key\"] is not None:\n\t\t\t\tclient_kwargs[\"api_key\"] = str(self.client_args[\"api_key\"])\n\t\t\tif \"url\" in self.client_args and self.client_args[\"url\"] is not None:\n\t\t\t\tclient_kwargs[\"url\"] = str(self.client_args[\"url\"])\n\t\t\tif \"prefer_grpc\" in self.client_args and self.client_args[\"prefer_grpc\"] is not None:\n\t\t\t\tclient_kwargs[\"prefer_grpc\"] = bool(self.client_args[\"prefer_grpc\"])\n\t\t\tif \"timeout\" in self.client_args and self.client_args[\"timeout\"] is not None:\n\t\t\t\tclient_kwargs[\"timeout\"] = int(self.client_args[\"timeout\"])\n\n\t\t\tself.client = AsyncQdrantClient(**client_kwargs)\n\n\t\t\t# Check if collection exists\n\t\t\tcollections_response = await self.client.get_collections()\n\t\t\tcollection_names = {col.name for col in collections_response.collections}\n\n\t\t\tif self.collection_name not in collection_names:\n\t\t\t\tlogger.info(f\"Collection '{self.collection_name}' not found. Creating...\")\n\t\t\t\tawait self.client.create_collection(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tvectors_config=VectorParams(size=self.dim, distance=self.distance),\n\t\t\t\t)\n\t\t\t\tlogger.info(f\"Collection '{self.collection_name}' created successfully.\")\n\n\t\t\t\t# Create payload indexes for commonly filtered fields\n\t\t\t\tlogger.info(f\"Creating payload indexes for collection '{self.collection_name}'\")\n\t\t\t\t# Common fields used in filters from ChunkMetadata\n\t\t\t\tawait self._create_payload_indexes()\n\t\t\telse:\n\t\t\t\tlogger.info(f\"Using existing Qdrant collection: '{self.collection_name}'\")\n\t\t\t\t# Check if indexes exist, create if missing\n\t\t\t\tawait self._ensure_payload_indexes()\n\n\t\t\tself.is_initialized = True\n\t\t\tlogger.info(\"QdrantManager initialized successfully.\")\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to initialize QdrantManager or collection\")\n\t\t\tself.client = None  # Ensure client is None if init fails\n\t\t\traise  # Re-raise the exception to signal failure\n\n\tasync def _create_payload_indexes(self) -&gt; None:\n\t\t\"\"\"Create payload indexes for all fields in ChunkMetadataSchema and GitMetadataSchema.\"\"\"\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized before creating indexes\"\n\t\t\traise RuntimeError(msg)\n\t\tclient: AsyncQdrantClient = self.client\n\n\t\ttry:\n\t\t\t# Index fields for ChunkMetadataSchema\n\t\t\tindex_fields = [\n\t\t\t\t(\"chunk_id\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"start_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t\t(\"end_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t\t(\"entity_type\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"entity_name\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"hierarchy_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t# FileMetadataSchema subfields (nested under file_metadata)\n\t\t\t\t(\"file_metadata.file_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"file_metadata.language\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"file_metadata.last_modified_time\", models.PayloadSchemaType.FLOAT),\n\t\t\t\t(\"file_metadata.file_content_hash\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t# GitMetadataSchema subfields (nested under git_metadata)\n\t\t\t\t(\"git_metadata.git_hash\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"git_metadata.tracked\", models.PayloadSchemaType.BOOL),\n\t\t\t\t(\"git_metadata.branch\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t# (\"git_metadata.blame\", models.PayloadSchemaType.KEYWORD),  # Blame is a list of objects, not indexed\n\t\t\t]\n\t\t\t# Add indexes for all fields\n\t\t\tfor field_name, field_type in index_fields:\n\t\t\t\tlogger.info(f\"Creating index for field: {field_name} ({field_type})\")\n\t\t\t\tawait client.create_payload_index(\n\t\t\t\t\tcollection_name=self.collection_name, field_name=field_name, field_schema=field_type\n\t\t\t\t)\n\t\t\tlogger.info(f\"Created {len(index_fields)} payload indexes successfully\")\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tlogger.warning(f\"Error creating payload indexes: {e}\")\n\t\t\t# Continue even if index creation fails - collection will still work\n\n\tasync def _ensure_payload_indexes(self) -&gt; None:\n\t\t\"\"\"Check if payload indexes exist and create any missing ones to match the schema.\"\"\"\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized before checking indexes\"\n\t\t\traise RuntimeError(msg)\n\t\tclient: AsyncQdrantClient = self.client\n\n\t\ttry:\n\t\t\t# Get existing collection info\n\t\t\tcollection_info = await client.get_collection(collection_name=self.collection_name)\n\t\t\texisting_schema = collection_info.payload_schema\n\n\t\t\t# List of fields that should be indexed (same as in _create_payload_indexes)\n\t\t\tindex_fields = [\n\t\t\t\t(\"chunk_id\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"file_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"start_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t\t(\"end_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t\t(\"entity_type\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"entity_name\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"language\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"hierarchy_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t# FileMetadataSchema subfields\n\t\t\t\t(\"file_metadata.file_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"file_metadata.language\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"file_metadata.last_modified_time\", models.PayloadSchemaType.FLOAT),\n\t\t\t\t(\"file_metadata.file_content_hash\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t# GitMetadataSchema subfields\n\t\t\t\t(\"git_metadata.git_hash\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"git_metadata.tracked\", models.PayloadSchemaType.BOOL),\n\t\t\t\t(\"git_metadata.branch\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t# (\"git_metadata.blame\", models.PayloadSchemaType.KEYWORD),  # Not indexed\n\t\t\t]\n\t\t\t# Create any missing indexes\n\t\t\tfor field_name, field_type in index_fields:\n\t\t\t\tif field_name not in existing_schema:\n\t\t\t\t\tlogger.info(f\"Creating missing index for field: {field_name} ({field_type})\")\n\t\t\t\t\tawait client.create_payload_index(\n\t\t\t\t\t\tcollection_name=self.collection_name, field_name=field_name, field_schema=field_type\n\t\t\t\t\t)\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tlogger.warning(f\"Error checking or creating payload indexes: {e}\")\n\t\t\t# Continue even if index check fails\n\n\tasync def _ensure_initialized(self) -&gt; None:\n\t\t\"\"\"Ensure the client is initialized before performing operations.\"\"\"\n\t\tif not self.is_initialized or self.client is None:\n\t\t\tawait self.initialize()\n\t\tif not self.client:\n\t\t\t# Should not happen if initialize didn't raise, but check anyway\n\t\t\tmsg = \"Qdrant client is not available after initialization attempt.\"\n\t\t\traise RuntimeError(msg)\n\n\tasync def upsert_points(self, points: list[PointStruct]) -&gt; None:\n\t\t\"\"\"\n\t\tAdd or update points (vectors and payloads) in the collection.\n\n\t\tArgs:\n\t\t    points: A list of Qdrant PointStruct objects. Each payload should be a dict matching ChunkMetadataSchema.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tclient: AsyncQdrantClient = self.client\n\t\tif not points:\n\t\t\tlogger.warning(\"upsert_points called with an empty list.\")\n\t\t\treturn\n\t\t# Ensure all payloads are dicts (convert from Pydantic BaseModel if needed)\n\t\tfor point in points:\n\t\t\tif hasattr(point, \"payload\") and isinstance(point.payload, BaseModel):\n\t\t\t\tpoint.payload = point.payload.model_dump()\n\t\ttry:\n\t\t\tlogger.info(f\"Upserting {len(points)} points into '{self.collection_name}'\")\n\t\t\tawait client.upsert(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints=points,\n\t\t\t\twait=True,  # Wait for operation to complete\n\t\t\t)\n\t\t\tlogger.debug(f\"Successfully upserted {len(points)} points.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error upserting points into Qdrant\")\n\t\t\t# Decide if partial failure needs specific handling or re-raising\n\n\tasync def delete_points(self, point_ids: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tDelete points from the collection by their IDs.\n\n\t\tArgs:\n\t\t    point_ids: A list of point IDs to delete.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tif not point_ids:\n\t\t\tlogger.warning(\"delete_points called with an empty list.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Deleting {len(point_ids)} points from '{self.collection_name}'\")\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\tawait self.client.delete(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints_selector=models.PointIdsList(points=qdrant_ids),\n\t\t\t\twait=True,\n\t\t\t)\n\t\t\tlogger.debug(f\"Successfully deleted {len(point_ids)} points.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error deleting points from Qdrant\")\n\t\t\t# Consider error handling strategy\n\n\tasync def delete_points_by_filter(self, qdrant_filter: models.Filter) -&gt; None:\n\t\t\"\"\"\n\t\tDelete points from the collection based on a filter condition.\n\n\t\tArgs:\n\t\t\tqdrant_filter: A Qdrant Filter object specifying the points to delete.\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\ttry:\n\t\t\tlogger.info(f\"Deleting points from '{self.collection_name}' based on filter\")\n\t\t\tawait self.client.delete(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints_selector=models.FilterSelector(filter=qdrant_filter),\n\t\t\t\twait=True,\n\t\t\t)\n\t\t\tlogger.debug(\"Successfully deleted points.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error deleting points from Qdrant\")\n\n\tasync def search(\n\t\tself,\n\t\tquery_vector: list[float],\n\t\tk: int = 5,\n\t\tquery_filter: models.Filter | None = None,\n\t) -&gt; list[models.ScoredPoint]:\n\t\t\"\"\"\n\t\tPerform a vector search with optional filtering.\n\n\t\tArgs:\n\t\t    query_vector: The vector to search for.\n\t\t    k: The number of nearest neighbors to return.\n\t\t    query_filter: Optional Qdrant filter conditions.\n\n\t\tReturns:\n\t\t    A list of ScoredPoint objects, including ID, score, and payload.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tif not query_vector:\n\t\t\tlogger.error(\"Search called with empty query vector.\")\n\t\t\treturn []\n\n\t\ttry:\n\t\t\tsearch_result = await self.client.search(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tquery_vector=query_vector,\n\t\t\t\tquery_filter=query_filter,\n\t\t\t\tlimit=k,\n\t\t\t\twith_payload=True,  # Always include payload\n\t\t\t\twith_vectors=False,  # Usually not needed in results\n\t\t\t)\n\t\t\tlogger.debug(f\"Search returned {len(search_result)} results.\")\n\t\t\treturn search_result\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error during Qdrant search\")\n\t\t\treturn []\n\n\tasync def get_all_point_ids_with_filter(\n\t\tself, query_filter: models.Filter | None = None\n\t) -&gt; list[str | int | uuid.UUID]:\n\t\t\"\"\"\n\t\tRetrieves all point IDs currently in the collection, optionally filtered.\n\n\t\tUses scrolling API to handle potentially large collections.\n\n\t\tArgs:\n\t\t    query_filter: Optional Qdrant filter to apply.\n\n\t\tReturns:\n\t\t    A list of all point IDs matching the filter.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tclient: AsyncQdrantClient = self.client\n\t\tall_ids: list[str | int | uuid.UUID] = []\n\t\t# Use Any for offset type hint due to persistent linter issues\n\t\tnext_offset: Any | None = None\n\t\tlimit_per_scroll = 1000\n\n\t\t# Add logging for parameters\n\t\tlogger.info(\n\t\t\tf\"[QdrantManager Get IDs] Fetching all point IDs from collection '{self.collection_name}'%s...\",\n\t\t\tf\" with filter: {query_filter}\" if query_filter else \"\",\n\t\t)\n\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scrolling with offset: {next_offset}\")\n\t\t\t\tscroll_response, next_offset_id = await client.scroll(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tscroll_filter=query_filter,\n\t\t\t\t\tlimit=limit_per_scroll,\n\t\t\t\t\toffset=next_offset,\n\t\t\t\t\twith_payload=False,\n\t\t\t\t\twith_vectors=False,\n\t\t\t\t)\n\t\t\t\tbatch_ids: list[ExtendedPointId] = [point.id for point in scroll_response]\n\t\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scroll returned {len(batch_ids)} IDs in this batch.\")\n\t\t\t\tif not batch_ids:\n\t\t\t\t\tlogger.debug(\"[QdrantManager Get IDs] No more IDs returned by scroll. Stopping.\")\n\t\t\t\t\tbreak\n\t\t\t\tall_ids.extend([cast(\"str | int | uuid.UUID\", point_id) for point_id in batch_ids])\n\t\t\t\t# Assign the returned offset ID - type is likely PointId but linter struggles\n\t\t\t\tnext_offset = next_offset_id  # No ignore needed if next_offset is Any\n\t\t\t\tif next_offset is None:\n\t\t\t\t\tbreak\n\n\t\t\texcept UnexpectedResponse as e:\n\t\t\t\t# Qdrant might return 404 if offset points to non-existent ID after deletions\n\t\t\t\tif e.status_code == HTTP_NOT_FOUND and \"Point with id\" in str(e.content):\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"Scroll encountered potentially deleted point ID at offset: {next_offset}. Stopping scroll.\"\n\t\t\t\t\t)\n\t\t\t\t\tbreak\n\t\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\t\traise  # Reraise other unexpected errors\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\t\traise\n\n\t\tlogger.info(f\"Retrieved {len(all_ids)} point IDs from collection '{self.collection_name}'.\")\n\t\treturn all_ids\n\n\tasync def get_payloads_by_ids(self, point_ids: list[str | int | uuid.UUID]) -&gt; dict[str, dict[str, Any]]:\n\t\t\"\"\"\n\t\tRetrieves payloads for specific point IDs.\n\n\t\tArgs:\n\t\t    point_ids: List of point IDs to fetch payloads for.\n\n\t\tReturns:\n\t\t    A dictionary mapping point IDs (as strings) to their payloads (as dicts matching ChunkMetadataSchema).\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tif not point_ids:\n\t\t\treturn {}\n\n\t\ttry:\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\t\t\tpoints_data = await self.client.retrieve(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t# Ignore linter complaint about list type compatibility\n\t\t\t\tids=qdrant_ids,  # type: ignore[arg-type]\n\t\t\t\twith_payload=True,\n\t\t\t\twith_vectors=False,\n\t\t\t)\n\t\t\tpayloads = {}\n\t\t\tfor point in points_data:\n\t\t\t\tif point.payload:\n\t\t\t\t\t# Try to parse as ChunkMetadataSchema, fallback to dict\n\t\t\t\t\ttry:\n\t\t\t\t\t\tpayloads[str(point.id)] = ChunkMetadataSchema.model_validate(point.payload).model_dump()\n\t\t\t\t\texcept (ValidationError, TypeError, ValueError):\n\t\t\t\t\t\tpayloads[str(point.id)] = point.payload\n\t\t\tlogger.debug(f\"Retrieved payloads for {len(payloads)} points.\")\n\t\t\treturn payloads\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error retrieving payloads from Qdrant\")\n\t\t\treturn {}\n\n\tasync def close(self) -&gt; None:\n\t\t\"\"\"Close the AsyncQdrantClient connection.\"\"\"\n\t\tif self.client:\n\t\t\tlogger.info(\"Closing Qdrant client connection.\")\n\t\t\ttry:\n\t\t\t\tawait self.client.close()\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error closing Qdrant client\")\n\t\t\tfinally:\n\t\t\t\tself.client = None\n\t\t\t\tself.is_initialized = False\n\n\tasync def __aenter__(self) -&gt; \"QdrantManager\":\n\t\t\"\"\"Enter the async context manager, initializing the Qdrant client.\"\"\"\n\t\tawait self.initialize()\n\t\treturn self\n\n\tasync def __aexit__(\n\t\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n\t) -&gt; None:\n\t\t\"\"\"Exit the async context manager, closing the Qdrant client.\"\"\"\n\t\tawait self.close()\n\n\tasync def set_payload(\n\t\tself,\n\t\tfilter_condition: models.Filter,\n\t\tpayload: dict[str, Any],\n\t\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\t\tkey: str | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tSet specific payload fields for points without overwriting existing fields.\n\n\t\tArgs:\n\t\t    payload: Dictionary of payload fields to set\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\t\t    key: Optional specific payload key path to modify\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not payload:\n\t\t\tlogger.warning(\"set_payload called with empty payload.\")\n\t\t\treturn\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"set_payload called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Setting payload fields: {list(payload.keys())} for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\t\tawait self.client.set_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\tkey=key,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here because of the initial check\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.set_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\tkey=key,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(f\"Successfully set payload fields: {list(payload.keys())}\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error setting payload in Qdrant\")\n\n\tasync def overwrite_payload(\n\t\tself,\n\t\tpayload: dict[str, Any],\n\t\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\t\tfilter_condition: models.Filter | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tCompletely replace the payload for points with the new payload.\n\n\t\tArgs:\n\t\t    payload: Dictionary of payload fields to set\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not payload:\n\t\t\tlogger.warning(\"overwrite_payload called with empty payload.\")\n\t\t\treturn\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"overwrite_payload called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Overwriting payload for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\t\tawait self.client.overwrite_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here\n\t\t\t\tif filter_condition is None:\n\t\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.overwrite_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(\"Successfully overwrote payload\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error overwriting payload in Qdrant\")\n\n\tasync def clear_payload(\n\t\tself, point_ids: list[str | int | uuid.UUID] | None = None, filter_condition: models.Filter | None = None\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tRemove all payload fields from points.\n\n\t\tArgs:\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"clear_payload called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Clearing payload for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)  # type: ignore[arg-type]\n\n\t\t\t\tawait self.client.clear_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpoints_selector=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here\n\t\t\t\tif filter_condition is None:\n\t\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.clear_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpoints_selector=points_selector,  # type: ignore[arg-type]\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(\"Successfully cleared payload\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error clearing payload in Qdrant\")\n\n\tasync def delete_payload_keys(\n\t\tself,\n\t\tkeys: list[str],\n\t\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\t\tfilter_condition: models.Filter | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tDelete specific payload fields from points.\n\n\t\tArgs:\n\t\t    keys: List of payload field keys to delete\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not keys:\n\t\t\tlogger.warning(\"delete_payload_keys called with empty keys list.\")\n\t\t\treturn\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"delete_payload_keys called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Deleting payload keys {keys} for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\t\tawait self.client.delete_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tkeys=keys,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here\n\t\t\t\tif filter_condition is None:\n\t\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.delete_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tkeys=keys,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(f\"Successfully deleted payload keys: {keys}\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error deleting payload keys in Qdrant\")\n\n\tasync def get_all_chunks_content_hashes(self) -&gt; set[str]:\n\t\t\"\"\"\n\t\tRetrieves all content hashes of chunks in the collection.\n\n\t\tUsed for deduplication when syncing.\n\n\t\tReturns:\n\t\t\tA set of combined content_hash:file_content_hash strings for all chunks in the collection.\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tcontent_hashes = set()\n\n\t\ttry:\n\t\t\t# Get all payloads for deduplication check\n\t\t\tall_ids = await self.get_all_point_ids_with_filter()\n\n\t\t\tif not all_ids:\n\t\t\t\treturn content_hashes\n\n\t\t\t# Process in batches to avoid memory issues\n\t\t\tfor i in range(0, len(all_ids), 100):\n\t\t\t\tbatch_ids = all_ids[i : i + 100]\n\t\t\t\tpayloads = await self.get_payloads_by_ids(batch_ids)\n\n\t\t\t\tfor payload in payloads.values():\n\t\t\t\t\tif not payload:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tcontent_hash = payload.get(\"content_hash\", \"\")\n\t\t\t\t\tfile_metadata = payload.get(\"file_metadata\", {})\n\n\t\t\t\t\tif isinstance(file_metadata, dict):\n\t\t\t\t\t\tfile_content_hash = file_metadata.get(\"file_content_hash\", \"\")\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile_content_hash = \"\"\n\n\t\t\t\t\tif content_hash and file_content_hash:\n\t\t\t\t\t\t# Create a composite key for both the chunk content and file content\n\t\t\t\t\t\tdedup_key = f\"{content_hash}:{file_content_hash}\"\n\t\t\t\t\t\tcontent_hashes.add(dedup_key)\n\n\t\t\tlogger.info(f\"Retrieved {len(content_hashes)} unique content hash combinations from Qdrant.\")\n\t\t\treturn content_hashes\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error retrieving content hashes from Qdrant\")\n\t\t\treturn set()\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.value_error_msg","title":"value_error_msg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value_error_msg = \"Filter condition cannot be None if point_ids are not provided\"\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig_loader: ConfigLoader,\n\tcollection_name: str | None = None,\n\tdim: int | None = None,\n\tdistance: Distance | None = None,\n\tapi_key: str | None = None,\n\turl: str | None = None,\n\tprefer_grpc: bool | None = None,\n\ttimeout: float | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the QdrantManager.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>Configuration loader instance.</p> required <code>location</code> <p>Path for local storage or \":memory:\". Ignored if url is provided.</p> required <code>collection_name</code> <code>str | None</code> <p>Name of the Qdrant collection to use.</p> <code>None</code> <code>dim</code> <code>int | None</code> <p>Dimension of the vectors.</p> <code>None</code> <code>distance</code> <code>Distance | None</code> <p>Distance metric for vector comparison.</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for Qdrant Cloud authentication.</p> <code>None</code> <code>url</code> <code>str | None</code> <p>URL for connecting to a remote Qdrant instance (overrides location).</p> <code>None</code> <code>prefer_grpc</code> <code>bool | None</code> <p>Whether to prefer gRPC over REST.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>Connection timeout in seconds.</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: \"ConfigLoader\",\n\tcollection_name: str | None = None,\n\tdim: int | None = None,\n\tdistance: Distance | None = None,\n\tapi_key: str | None = None,  # For Qdrant Cloud\n\turl: str | None = None,  # For self-hosted or Cloud URL\n\tprefer_grpc: bool | None = None,\n\ttimeout: float | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the QdrantManager.\n\n\tArgs:\n\t    config_loader: Configuration loader instance.\n\t    location: Path for local storage or \":memory:\". Ignored if url is provided.\n\t    collection_name: Name of the Qdrant collection to use.\n\t    dim: Dimension of the vectors.\n\t    distance: Distance metric for vector comparison.\n\t    api_key: API key for Qdrant Cloud authentication.\n\t    url: URL for connecting to a remote Qdrant instance (overrides location).\n\t    prefer_grpc: Whether to prefer gRPC over REST.\n\t    timeout: Connection timeout in seconds.\n\n\t\"\"\"\n\t# Get embedding configuration\n\tself.config_loader = config_loader\n\tembedding_config = self.config_loader.get.embedding\n\n\t# Get distance metric from config or parameter\n\tdistance_metric_str = embedding_config.dimension_metric.upper()\n\tdefault_distance = (\n\t\tgetattr(Distance, distance_metric_str) if hasattr(Distance, distance_metric_str) else Distance.COSINE\n\t)\n\n\t# Load values from parameters or fall back to config\n\tself.collection_name = collection_name or \"codemap_vectors\"\n\tself.dim = dim or embedding_config.dimension\n\tself.distance = distance or default_distance\n\n\t# Build client args\n\tself.client_args = {\n\t\t\"api_key\": api_key or embedding_config.api_key,\n\t\t\"url\": url or embedding_config.url,\n\t\t\"prefer_grpc\": prefer_grpc if prefer_grpc is not None else embedding_config.prefer_grpc,\n\t\t\"timeout\": timeout or embedding_config.timeout,\n\t}\n\t# Remove None values from args\n\tself.client_args = {k: v for k, v in self.client_args.items() if v is not None}\n\n\t# Initialize client later in an async context\n\tself.client: AsyncQdrantClient | None = None\n\tself.is_initialized = False\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.collection_name","title":"collection_name  <code>instance-attribute</code>","text":"<pre><code>collection_name = collection_name or 'codemap_vectors'\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.dim","title":"dim  <code>instance-attribute</code>","text":"<pre><code>dim = dim or dimension\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.distance","title":"distance  <code>instance-attribute</code>","text":"<pre><code>distance = distance or default_distance\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.client_args","title":"client_args  <code>instance-attribute</code>","text":"<pre><code>client_args = {k: _wfor (k, v) in items() if v is not None}\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client: AsyncQdrantClient | None = None\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.is_initialized","title":"is_initialized  <code>instance-attribute</code>","text":"<pre><code>is_initialized = False\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize() -&gt; None\n</code></pre> <p>Asynchronously initialize the Qdrant client and ensure the collection exists.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def initialize(self) -&gt; None:\n\t\"\"\"Asynchronously initialize the Qdrant client and ensure the collection exists.\"\"\"\n\tif self.is_initialized:\n\t\treturn\n\n\ttry:\n\t\tlogger.info(\"Initializing Qdrant client with args: %s\", self.client_args)\n\t\t# Create client with explicit type casting to avoid type issues\n\t\tclient_kwargs: dict[str, Any] = {}\n\t\tif \"api_key\" in self.client_args and self.client_args[\"api_key\"] is not None:\n\t\t\tclient_kwargs[\"api_key\"] = str(self.client_args[\"api_key\"])\n\t\tif \"url\" in self.client_args and self.client_args[\"url\"] is not None:\n\t\t\tclient_kwargs[\"url\"] = str(self.client_args[\"url\"])\n\t\tif \"prefer_grpc\" in self.client_args and self.client_args[\"prefer_grpc\"] is not None:\n\t\t\tclient_kwargs[\"prefer_grpc\"] = bool(self.client_args[\"prefer_grpc\"])\n\t\tif \"timeout\" in self.client_args and self.client_args[\"timeout\"] is not None:\n\t\t\tclient_kwargs[\"timeout\"] = int(self.client_args[\"timeout\"])\n\n\t\tself.client = AsyncQdrantClient(**client_kwargs)\n\n\t\t# Check if collection exists\n\t\tcollections_response = await self.client.get_collections()\n\t\tcollection_names = {col.name for col in collections_response.collections}\n\n\t\tif self.collection_name not in collection_names:\n\t\t\tlogger.info(f\"Collection '{self.collection_name}' not found. Creating...\")\n\t\t\tawait self.client.create_collection(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tvectors_config=VectorParams(size=self.dim, distance=self.distance),\n\t\t\t)\n\t\t\tlogger.info(f\"Collection '{self.collection_name}' created successfully.\")\n\n\t\t\t# Create payload indexes for commonly filtered fields\n\t\t\tlogger.info(f\"Creating payload indexes for collection '{self.collection_name}'\")\n\t\t\t# Common fields used in filters from ChunkMetadata\n\t\t\tawait self._create_payload_indexes()\n\t\telse:\n\t\t\tlogger.info(f\"Using existing Qdrant collection: '{self.collection_name}'\")\n\t\t\t# Check if indexes exist, create if missing\n\t\t\tawait self._ensure_payload_indexes()\n\n\t\tself.is_initialized = True\n\t\tlogger.info(\"QdrantManager initialized successfully.\")\n\n\texcept Exception:\n\t\tlogger.exception(\"Failed to initialize QdrantManager or collection\")\n\t\tself.client = None  # Ensure client is None if init fails\n\t\traise  # Re-raise the exception to signal failure\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.upsert_points","title":"upsert_points  <code>async</code>","text":"<pre><code>upsert_points(points: list[PointStruct]) -&gt; None\n</code></pre> <p>Add or update points (vectors and payloads) in the collection.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list[PointStruct]</code> <p>A list of Qdrant PointStruct objects. Each payload should be a dict matching ChunkMetadataSchema.</p> required Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def upsert_points(self, points: list[PointStruct]) -&gt; None:\n\t\"\"\"\n\tAdd or update points (vectors and payloads) in the collection.\n\n\tArgs:\n\t    points: A list of Qdrant PointStruct objects. Each payload should be a dict matching ChunkMetadataSchema.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tclient: AsyncQdrantClient = self.client\n\tif not points:\n\t\tlogger.warning(\"upsert_points called with an empty list.\")\n\t\treturn\n\t# Ensure all payloads are dicts (convert from Pydantic BaseModel if needed)\n\tfor point in points:\n\t\tif hasattr(point, \"payload\") and isinstance(point.payload, BaseModel):\n\t\t\tpoint.payload = point.payload.model_dump()\n\ttry:\n\t\tlogger.info(f\"Upserting {len(points)} points into '{self.collection_name}'\")\n\t\tawait client.upsert(\n\t\t\tcollection_name=self.collection_name,\n\t\t\tpoints=points,\n\t\t\twait=True,  # Wait for operation to complete\n\t\t)\n\t\tlogger.debug(f\"Successfully upserted {len(points)} points.\")\n\texcept Exception:\n\t\tlogger.exception(\"Error upserting points into Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.delete_points","title":"delete_points  <code>async</code>","text":"<pre><code>delete_points(point_ids: list[str]) -&gt; None\n</code></pre> <p>Delete points from the collection by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>point_ids</code> <code>list[str]</code> <p>A list of point IDs to delete.</p> required Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def delete_points(self, point_ids: list[str]) -&gt; None:\n\t\"\"\"\n\tDelete points from the collection by their IDs.\n\n\tArgs:\n\t    point_ids: A list of point IDs to delete.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tif not point_ids:\n\t\tlogger.warning(\"delete_points called with an empty list.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Deleting {len(point_ids)} points from '{self.collection_name}'\")\n\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\tawait self.client.delete(\n\t\t\tcollection_name=self.collection_name,\n\t\t\tpoints_selector=models.PointIdsList(points=qdrant_ids),\n\t\t\twait=True,\n\t\t)\n\t\tlogger.debug(f\"Successfully deleted {len(point_ids)} points.\")\n\texcept Exception:\n\t\tlogger.exception(\"Error deleting points from Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.delete_points_by_filter","title":"delete_points_by_filter  <code>async</code>","text":"<pre><code>delete_points_by_filter(qdrant_filter: Filter) -&gt; None\n</code></pre> <p>Delete points from the collection based on a filter condition.</p> <p>Parameters:</p> Name Type Description Default <code>qdrant_filter</code> <code>Filter</code> <p>A Qdrant Filter object specifying the points to delete.</p> required Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def delete_points_by_filter(self, qdrant_filter: models.Filter) -&gt; None:\n\t\"\"\"\n\tDelete points from the collection based on a filter condition.\n\n\tArgs:\n\t\tqdrant_filter: A Qdrant Filter object specifying the points to delete.\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\ttry:\n\t\tlogger.info(f\"Deleting points from '{self.collection_name}' based on filter\")\n\t\tawait self.client.delete(\n\t\t\tcollection_name=self.collection_name,\n\t\t\tpoints_selector=models.FilterSelector(filter=qdrant_filter),\n\t\t\twait=True,\n\t\t)\n\t\tlogger.debug(\"Successfully deleted points.\")\n\texcept Exception:\n\t\tlogger.exception(\"Error deleting points from Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.search","title":"search  <code>async</code>","text":"<pre><code>search(\n\tquery_vector: list[float],\n\tk: int = 5,\n\tquery_filter: Filter | None = None,\n) -&gt; list[ScoredPoint]\n</code></pre> <p>Perform a vector search with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>query_vector</code> <code>list[float]</code> <p>The vector to search for.</p> required <code>k</code> <code>int</code> <p>The number of nearest neighbors to return.</p> <code>5</code> <code>query_filter</code> <code>Filter | None</code> <p>Optional Qdrant filter conditions.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ScoredPoint]</code> <p>A list of ScoredPoint objects, including ID, score, and payload.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def search(\n\tself,\n\tquery_vector: list[float],\n\tk: int = 5,\n\tquery_filter: models.Filter | None = None,\n) -&gt; list[models.ScoredPoint]:\n\t\"\"\"\n\tPerform a vector search with optional filtering.\n\n\tArgs:\n\t    query_vector: The vector to search for.\n\t    k: The number of nearest neighbors to return.\n\t    query_filter: Optional Qdrant filter conditions.\n\n\tReturns:\n\t    A list of ScoredPoint objects, including ID, score, and payload.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tif not query_vector:\n\t\tlogger.error(\"Search called with empty query vector.\")\n\t\treturn []\n\n\ttry:\n\t\tsearch_result = await self.client.search(\n\t\t\tcollection_name=self.collection_name,\n\t\t\tquery_vector=query_vector,\n\t\t\tquery_filter=query_filter,\n\t\t\tlimit=k,\n\t\t\twith_payload=True,  # Always include payload\n\t\t\twith_vectors=False,  # Usually not needed in results\n\t\t)\n\t\tlogger.debug(f\"Search returned {len(search_result)} results.\")\n\t\treturn search_result\n\texcept Exception:\n\t\tlogger.exception(\"Error during Qdrant search\")\n\t\treturn []\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.get_all_point_ids_with_filter","title":"get_all_point_ids_with_filter  <code>async</code>","text":"<pre><code>get_all_point_ids_with_filter(\n\tquery_filter: Filter | None = None,\n) -&gt; list[str | int | UUID]\n</code></pre> <p>Retrieves all point IDs currently in the collection, optionally filtered.</p> <p>Uses scrolling API to handle potentially large collections.</p> <p>Parameters:</p> Name Type Description Default <code>query_filter</code> <code>Filter | None</code> <p>Optional Qdrant filter to apply.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str | int | UUID]</code> <p>A list of all point IDs matching the filter.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def get_all_point_ids_with_filter(\n\tself, query_filter: models.Filter | None = None\n) -&gt; list[str | int | uuid.UUID]:\n\t\"\"\"\n\tRetrieves all point IDs currently in the collection, optionally filtered.\n\n\tUses scrolling API to handle potentially large collections.\n\n\tArgs:\n\t    query_filter: Optional Qdrant filter to apply.\n\n\tReturns:\n\t    A list of all point IDs matching the filter.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tclient: AsyncQdrantClient = self.client\n\tall_ids: list[str | int | uuid.UUID] = []\n\t# Use Any for offset type hint due to persistent linter issues\n\tnext_offset: Any | None = None\n\tlimit_per_scroll = 1000\n\n\t# Add logging for parameters\n\tlogger.info(\n\t\tf\"[QdrantManager Get IDs] Fetching all point IDs from collection '{self.collection_name}'%s...\",\n\t\tf\" with filter: {query_filter}\" if query_filter else \"\",\n\t)\n\n\twhile True:\n\t\ttry:\n\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scrolling with offset: {next_offset}\")\n\t\t\tscroll_response, next_offset_id = await client.scroll(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tscroll_filter=query_filter,\n\t\t\t\tlimit=limit_per_scroll,\n\t\t\t\toffset=next_offset,\n\t\t\t\twith_payload=False,\n\t\t\t\twith_vectors=False,\n\t\t\t)\n\t\t\tbatch_ids: list[ExtendedPointId] = [point.id for point in scroll_response]\n\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scroll returned {len(batch_ids)} IDs in this batch.\")\n\t\t\tif not batch_ids:\n\t\t\t\tlogger.debug(\"[QdrantManager Get IDs] No more IDs returned by scroll. Stopping.\")\n\t\t\t\tbreak\n\t\t\tall_ids.extend([cast(\"str | int | uuid.UUID\", point_id) for point_id in batch_ids])\n\t\t\t# Assign the returned offset ID - type is likely PointId but linter struggles\n\t\t\tnext_offset = next_offset_id  # No ignore needed if next_offset is Any\n\t\t\tif next_offset is None:\n\t\t\t\tbreak\n\n\t\texcept UnexpectedResponse as e:\n\t\t\t# Qdrant might return 404 if offset points to non-existent ID after deletions\n\t\t\tif e.status_code == HTTP_NOT_FOUND and \"Point with id\" in str(e.content):\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"Scroll encountered potentially deleted point ID at offset: {next_offset}. Stopping scroll.\"\n\t\t\t\t)\n\t\t\t\tbreak\n\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\traise  # Reraise other unexpected errors\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\traise\n\n\tlogger.info(f\"Retrieved {len(all_ids)} point IDs from collection '{self.collection_name}'.\")\n\treturn all_ids\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.get_payloads_by_ids","title":"get_payloads_by_ids  <code>async</code>","text":"<pre><code>get_payloads_by_ids(\n\tpoint_ids: list[str | int | UUID],\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Retrieves payloads for specific point IDs.</p> <p>Parameters:</p> Name Type Description Default <code>point_ids</code> <code>list[str | int | UUID]</code> <p>List of point IDs to fetch payloads for.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping point IDs (as strings) to their payloads (as dicts matching ChunkMetadataSchema).</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def get_payloads_by_ids(self, point_ids: list[str | int | uuid.UUID]) -&gt; dict[str, dict[str, Any]]:\n\t\"\"\"\n\tRetrieves payloads for specific point IDs.\n\n\tArgs:\n\t    point_ids: List of point IDs to fetch payloads for.\n\n\tReturns:\n\t    A dictionary mapping point IDs (as strings) to their payloads (as dicts matching ChunkMetadataSchema).\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tif not point_ids:\n\t\treturn {}\n\n\ttry:\n\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\t\tpoints_data = await self.client.retrieve(\n\t\t\tcollection_name=self.collection_name,\n\t\t\t# Ignore linter complaint about list type compatibility\n\t\t\tids=qdrant_ids,  # type: ignore[arg-type]\n\t\t\twith_payload=True,\n\t\t\twith_vectors=False,\n\t\t)\n\t\tpayloads = {}\n\t\tfor point in points_data:\n\t\t\tif point.payload:\n\t\t\t\t# Try to parse as ChunkMetadataSchema, fallback to dict\n\t\t\t\ttry:\n\t\t\t\t\tpayloads[str(point.id)] = ChunkMetadataSchema.model_validate(point.payload).model_dump()\n\t\t\t\texcept (ValidationError, TypeError, ValueError):\n\t\t\t\t\tpayloads[str(point.id)] = point.payload\n\t\tlogger.debug(f\"Retrieved payloads for {len(payloads)} points.\")\n\t\treturn payloads\n\texcept Exception:\n\t\tlogger.exception(\"Error retrieving payloads from Qdrant\")\n\t\treturn {}\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the AsyncQdrantClient connection.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def close(self) -&gt; None:\n\t\"\"\"Close the AsyncQdrantClient connection.\"\"\"\n\tif self.client:\n\t\tlogger.info(\"Closing Qdrant client connection.\")\n\t\ttry:\n\t\t\tawait self.client.close()\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error closing Qdrant client\")\n\t\tfinally:\n\t\t\tself.client = None\n\t\t\tself.is_initialized = False\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; QdrantManager\n</code></pre> <p>Enter the async context manager, initializing the Qdrant client.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def __aenter__(self) -&gt; \"QdrantManager\":\n\t\"\"\"Enter the async context manager, initializing the Qdrant client.\"\"\"\n\tawait self.initialize()\n\treturn self\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n\texc_type: type[BaseException] | None,\n\texc_val: BaseException | None,\n\texc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit the async context manager, closing the Qdrant client.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def __aexit__(\n\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n) -&gt; None:\n\t\"\"\"Exit the async context manager, closing the Qdrant client.\"\"\"\n\tawait self.close()\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.set_payload","title":"set_payload  <code>async</code>","text":"<pre><code>set_payload(\n\tfilter_condition: Filter,\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tkey: str | None = None,\n) -&gt; None\n</code></pre> <p>Set specific payload fields for points without overwriting existing fields.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict[str, Any]</code> <p>Dictionary of payload fields to set</p> required <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter</code> <p>Optional filter to select points</p> required <code>key</code> <code>str | None</code> <p>Optional specific payload key path to modify</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def set_payload(\n\tself,\n\tfilter_condition: models.Filter,\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\tkey: str | None = None,\n) -&gt; None:\n\t\"\"\"\n\tSet specific payload fields for points without overwriting existing fields.\n\n\tArgs:\n\t    payload: Dictionary of payload fields to set\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\t    key: Optional specific payload key path to modify\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not payload:\n\t\tlogger.warning(\"set_payload called with empty payload.\")\n\t\treturn\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"set_payload called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Setting payload fields: {list(payload.keys())} for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\tawait self.client.set_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\tkey=key,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here because of the initial check\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.set_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\tkey=key,\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(f\"Successfully set payload fields: {list(payload.keys())}\")\n\texcept Exception:\n\t\tlogger.exception(\"Error setting payload in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.overwrite_payload","title":"overwrite_payload  <code>async</code>","text":"<pre><code>overwrite_payload(\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tfilter_condition: Filter | None = None,\n) -&gt; None\n</code></pre> <p>Completely replace the payload for points with the new payload.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict[str, Any]</code> <p>Dictionary of payload fields to set</p> required <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter | None</code> <p>Optional filter to select points</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def overwrite_payload(\n\tself,\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\tfilter_condition: models.Filter | None = None,\n) -&gt; None:\n\t\"\"\"\n\tCompletely replace the payload for points with the new payload.\n\n\tArgs:\n\t    payload: Dictionary of payload fields to set\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not payload:\n\t\tlogger.warning(\"overwrite_payload called with empty payload.\")\n\t\treturn\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"overwrite_payload called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Overwriting payload for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\tawait self.client.overwrite_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here\n\t\t\tif filter_condition is None:\n\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.overwrite_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(\"Successfully overwrote payload\")\n\texcept Exception:\n\t\tlogger.exception(\"Error overwriting payload in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.clear_payload","title":"clear_payload  <code>async</code>","text":"<pre><code>clear_payload(\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tfilter_condition: Filter | None = None,\n) -&gt; None\n</code></pre> <p>Remove all payload fields from points.</p> <p>Parameters:</p> Name Type Description Default <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter | None</code> <p>Optional filter to select points</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def clear_payload(\n\tself, point_ids: list[str | int | uuid.UUID] | None = None, filter_condition: models.Filter | None = None\n) -&gt; None:\n\t\"\"\"\n\tRemove all payload fields from points.\n\n\tArgs:\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"clear_payload called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Clearing payload for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)  # type: ignore[arg-type]\n\n\t\t\tawait self.client.clear_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints_selector=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here\n\t\t\tif filter_condition is None:\n\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.clear_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints_selector=points_selector,  # type: ignore[arg-type]\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(\"Successfully cleared payload\")\n\texcept Exception:\n\t\tlogger.exception(\"Error clearing payload in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.delete_payload_keys","title":"delete_payload_keys  <code>async</code>","text":"<pre><code>delete_payload_keys(\n\tkeys: list[str],\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tfilter_condition: Filter | None = None,\n) -&gt; None\n</code></pre> <p>Delete specific payload fields from points.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>List of payload field keys to delete</p> required <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter | None</code> <p>Optional filter to select points</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def delete_payload_keys(\n\tself,\n\tkeys: list[str],\n\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\tfilter_condition: models.Filter | None = None,\n) -&gt; None:\n\t\"\"\"\n\tDelete specific payload fields from points.\n\n\tArgs:\n\t    keys: List of payload field keys to delete\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not keys:\n\t\tlogger.warning(\"delete_payload_keys called with empty keys list.\")\n\t\treturn\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"delete_payload_keys called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Deleting payload keys {keys} for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\tawait self.client.delete_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tkeys=keys,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here\n\t\t\tif filter_condition is None:\n\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.delete_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tkeys=keys,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(f\"Successfully deleted payload keys: {keys}\")\n\texcept Exception:\n\t\tlogger.exception(\"Error deleting payload keys in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.get_all_chunks_content_hashes","title":"get_all_chunks_content_hashes  <code>async</code>","text":"<pre><code>get_all_chunks_content_hashes() -&gt; set[str]\n</code></pre> <p>Retrieves all content hashes of chunks in the collection.</p> <p>Used for deduplication when syncing.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>A set of combined content_hash:file_content_hash strings for all chunks in the collection.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def get_all_chunks_content_hashes(self) -&gt; set[str]:\n\t\"\"\"\n\tRetrieves all content hashes of chunks in the collection.\n\n\tUsed for deduplication when syncing.\n\n\tReturns:\n\t\tA set of combined content_hash:file_content_hash strings for all chunks in the collection.\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tcontent_hashes = set()\n\n\ttry:\n\t\t# Get all payloads for deduplication check\n\t\tall_ids = await self.get_all_point_ids_with_filter()\n\n\t\tif not all_ids:\n\t\t\treturn content_hashes\n\n\t\t# Process in batches to avoid memory issues\n\t\tfor i in range(0, len(all_ids), 100):\n\t\t\tbatch_ids = all_ids[i : i + 100]\n\t\t\tpayloads = await self.get_payloads_by_ids(batch_ids)\n\n\t\t\tfor payload in payloads.values():\n\t\t\t\tif not payload:\n\t\t\t\t\tcontinue\n\n\t\t\t\tcontent_hash = payload.get(\"content_hash\", \"\")\n\t\t\t\tfile_metadata = payload.get(\"file_metadata\", {})\n\n\t\t\t\tif isinstance(file_metadata, dict):\n\t\t\t\t\tfile_content_hash = file_metadata.get(\"file_content_hash\", \"\")\n\t\t\t\telse:\n\t\t\t\t\tfile_content_hash = \"\"\n\n\t\t\t\tif content_hash and file_content_hash:\n\t\t\t\t\t# Create a composite key for both the chunk content and file content\n\t\t\t\t\tdedup_key = f\"{content_hash}:{file_content_hash}\"\n\t\t\t\t\tcontent_hashes.add(dedup_key)\n\n\t\tlogger.info(f\"Retrieved {len(content_hashes)} unique content hash combinations from Qdrant.\")\n\t\treturn content_hashes\n\n\texcept Exception:\n\t\tlogger.exception(\"Error retrieving content hashes from Qdrant\")\n\t\treturn set()\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.create_qdrant_point","title":"create_qdrant_point","text":"<pre><code>create_qdrant_point(\n\tchunk_id: str,\n\tvector: list[float],\n\tpayload: dict[str, Any],\n) -&gt; PointStruct\n</code></pre> <p>Helper function to create a Qdrant PointStruct.</p> <p>Ensures the ID is treated appropriately (UUIDs if possible).</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>def create_qdrant_point(chunk_id: str, vector: list[float], payload: dict[str, Any]) -&gt; PointStruct:\n\t\"\"\"\n\tHelper function to create a Qdrant PointStruct.\n\n\tEnsures the ID is treated appropriately (UUIDs if possible).\n\n\t\"\"\"\n\tpoint_id: ExtendedPointId\n\ttry:\n\t\t# Validate it's a UUID but use string representation for ExtendedPointId\n\t\tuuid_obj = uuid.UUID(chunk_id)\n\t\tpoint_id = str(uuid_obj)\n\texcept ValueError:\n\t\tpoint_id = chunk_id\n\n\treturn PointStruct(id=point_id, vector=vector, payload=payload)\n</code></pre>"},{"location":"api/processor/vector/schema/","title":"Schema","text":"<p>Schema for the vector database.</p>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitBlameSchema","title":"GitBlameSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a git blame.</p> Source code in <code>src/codemap/processor/vector/schema.py</code> <pre><code>class GitBlameSchema(BaseModel):\n\t\"\"\"Metadata for a git blame.\"\"\"\n\n\tcommit_id: str\n\tdate: str\n\tauthor_name: str\n\tstart_line: int\n\tend_line: int\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitBlameSchema.commit_id","title":"commit_id  <code>instance-attribute</code>","text":"<pre><code>commit_id: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitBlameSchema.date","title":"date  <code>instance-attribute</code>","text":"<pre><code>date: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitBlameSchema.author_name","title":"author_name  <code>instance-attribute</code>","text":"<pre><code>author_name: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitBlameSchema.start_line","title":"start_line  <code>instance-attribute</code>","text":"<pre><code>start_line: int\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitBlameSchema.end_line","title":"end_line  <code>instance-attribute</code>","text":"<pre><code>end_line: int\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitMetadataSchema","title":"GitMetadataSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a git repository.</p> Source code in <code>src/codemap/processor/vector/schema.py</code> <pre><code>class GitMetadataSchema(BaseModel):\n\t\"\"\"Metadata for a git repository.\"\"\"\n\n\tgit_hash: str\n\ttracked: bool\n\tbranch: str\n\tblame: list[GitBlameSchema] = []\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitMetadataSchema.git_hash","title":"git_hash  <code>instance-attribute</code>","text":"<pre><code>git_hash: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitMetadataSchema.tracked","title":"tracked  <code>instance-attribute</code>","text":"<pre><code>tracked: bool\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitMetadataSchema.branch","title":"branch  <code>instance-attribute</code>","text":"<pre><code>branch: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.GitMetadataSchema.blame","title":"blame  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>blame: list[GitBlameSchema] = []\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.FileMetadataSchema","title":"FileMetadataSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a file.</p> Source code in <code>src/codemap/processor/vector/schema.py</code> <pre><code>class FileMetadataSchema(BaseModel):\n\t\"\"\"Metadata for a file.\"\"\"\n\n\tfile_path: str\n\tlanguage: str\n\tlast_modified_time: float\n\tfile_content_hash: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.FileMetadataSchema.file_path","title":"file_path  <code>instance-attribute</code>","text":"<pre><code>file_path: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.FileMetadataSchema.language","title":"language  <code>instance-attribute</code>","text":"<pre><code>language: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.FileMetadataSchema.last_modified_time","title":"last_modified_time  <code>instance-attribute</code>","text":"<pre><code>last_modified_time: float\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.FileMetadataSchema.file_content_hash","title":"file_content_hash  <code>instance-attribute</code>","text":"<pre><code>file_content_hash: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema","title":"ChunkMetadataSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a chunk of code.</p> Source code in <code>src/codemap/processor/vector/schema.py</code> <pre><code>class ChunkMetadataSchema(BaseModel):\n\t\"\"\"Metadata for a chunk of code.\"\"\"\n\n\tchunk_id: str\n\tcontent_hash: str\n\tstart_line: int\n\tend_line: int\n\tentity_type: str\n\tentity_name: str\n\thierarchy_path: str\n\tgit_metadata: GitMetadataSchema\n\tfile_metadata: FileMetadataSchema\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.chunk_id","title":"chunk_id  <code>instance-attribute</code>","text":"<pre><code>chunk_id: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.content_hash","title":"content_hash  <code>instance-attribute</code>","text":"<pre><code>content_hash: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.start_line","title":"start_line  <code>instance-attribute</code>","text":"<pre><code>start_line: int\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.end_line","title":"end_line  <code>instance-attribute</code>","text":"<pre><code>end_line: int\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.entity_type","title":"entity_type  <code>instance-attribute</code>","text":"<pre><code>entity_type: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.entity_name","title":"entity_name  <code>instance-attribute</code>","text":"<pre><code>entity_name: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.hierarchy_path","title":"hierarchy_path  <code>instance-attribute</code>","text":"<pre><code>hierarchy_path: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.git_metadata","title":"git_metadata  <code>instance-attribute</code>","text":"<pre><code>git_metadata: GitMetadataSchema\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkMetadataSchema.file_metadata","title":"file_metadata  <code>instance-attribute</code>","text":"<pre><code>file_metadata: FileMetadataSchema\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkSchema","title":"ChunkSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for a chunk of code.</p> Source code in <code>src/codemap/processor/vector/schema.py</code> <pre><code>class ChunkSchema(BaseModel):\n\t\"\"\"Schema for a chunk of code.\"\"\"\n\n\tcontent: str\n\tmetadata: ChunkMetadataSchema\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkSchema.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/processor/vector/schema/#codemap.processor.vector.schema.ChunkSchema.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: ChunkMetadataSchema\n</code></pre>"},{"location":"api/processor/vector/synchronizer/","title":"Synchronizer","text":"<p>Module for synchronizing HNSW index with Git state.</p>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer","title":"VectorSynchronizer","text":"<p>Handles asynchronous synchronization between Git repository and Qdrant vector index.</p> Source code in <code>src/codemap/processor/vector/synchronizer.py</code> <pre><code>class VectorSynchronizer:\n\t\"\"\"Handles asynchronous synchronization between Git repository and Qdrant vector index.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_path: Path,\n\t\tqdrant_manager: QdrantManager,\n\t\tchunker: TreeSitterChunker | None,\n\t\tembedding_model_name: str,\n\t\tanalyzer: TreeSitterAnalyzer | None = None,\n\t\tconfig_loader: \"ConfigLoader | None\" = None,\n\t\trepo_checksum_calculator: RepoChecksumCalculator | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the vector synchronizer.\n\n\t\tArgs:\n\t\t    repo_path: Path to the git repository root.\n\t\t    qdrant_manager: Instance of QdrantManager to handle vector storage.\n\t\t    chunker: Instance of chunker used to create code chunks.\n\t\t    embedding_model_name: Name of the embedding model to use.\n\t\t    analyzer: Optional TreeSitterAnalyzer instance.\n\t\t    config_loader: Configuration loader instance.\n\t\t    repo_checksum_calculator: Optional RepoChecksumCalculator instance.\n\n\t\t\"\"\"\n\t\tself.repo_path = repo_path\n\t\tself.qdrant_manager = qdrant_manager\n\t\tself.git_context = ExtendedGitRepoContext.get_instance()\n\t\tself.embedding_model_name = embedding_model_name\n\t\tself.analyzer = analyzer or TreeSitterAnalyzer()\n\n\t\t# Ensure RepoChecksumCalculator is instantiated with git_context\n\t\tif repo_checksum_calculator is None:\n\t\t\tself.repo_checksum_calculator = RepoChecksumCalculator.get_instance(\n\t\t\t\trepo_path=self.repo_path, git_context=self.git_context, config_loader=config_loader\n\t\t\t)\n\t\telse:\n\t\t\tself.repo_checksum_calculator = repo_checksum_calculator\n\t\t\t# Ensure existing calculator also has git_context, as it might be crucial for branch logic\n\t\t\tif self.repo_checksum_calculator.git_context is None and self.git_context:\n\t\t\t\tself.repo_checksum_calculator.git_context = self.git_context\n\n\t\tif config_loader:\n\t\t\tself.config_loader = config_loader\n\t\telse:\n\t\t\tfrom codemap.config import ConfigLoader\n\n\t\t\tself.config_loader = ConfigLoader.get_instance()\n\n\t\tembedding_config = self.config_loader.get.embedding\n\t\tself.qdrant_batch_size = embedding_config.qdrant_batch_size\n\n\t\tif chunker is None:\n\t\t\tself.chunker = TreeSitterChunker(\n\t\t\t\tgit_context=self.git_context,\n\t\t\t\tconfig_loader=self.config_loader,\n\t\t\t\trepo_checksum_calculator=self.repo_checksum_calculator,\n\t\t\t)\n\t\telse:\n\t\t\tif getattr(chunker, \"git_context\", None) is None:\n\t\t\t\tchunker.git_context = self.git_context\n\t\t\tif (\n\t\t\t\thasattr(chunker, \"repo_checksum_calculator\")\n\t\t\t\tand getattr(chunker, \"repo_checksum_calculator\", None) is None\n\t\t\t\tand self.repo_checksum_calculator\n\t\t\t):\n\t\t\t\tchunker.repo_checksum_calculator = self.repo_checksum_calculator\n\t\t\tself.chunker = chunker\n\n\t\tlogger.info(\n\t\t\tf\"VectorSynchronizer initialized for repo: {repo_path} \"\n\t\t\tf\"using Qdrant collection: '{qdrant_manager.collection_name}' \"\n\t\t\tf\"and embedding model: {embedding_model_name}\"\n\t\t)\n\t\tif not self.repo_checksum_calculator:\n\t\t\tlogger.warning(\"RepoChecksumCalculator could not be initialized. Checksum-based sync will be skipped.\")\n\n\t\t# Initialize checksum map attribute\n\t\tself.all_nodes_map_from_checksum: dict[str, dict[str, str]] = {}\n\n\tdef _get_checksum_cache_path(self) -&gt; Path:\n\t\t\"\"\"Gets the path to the checksum cache file within .codemap_cache.\"\"\"\n\t\t# Ensure .codemap_cache directory is at the root of the repo_path passed to VectorSynchronizer\n\t\tcache_dir = self.repo_path / \".codemap_cache\"\n\t\treturn cache_dir / \"last_sync_checksum.txt\"\n\n\tdef _read_last_sync_checksum(self) -&gt; str | None:\n\t\t\"\"\"Reads the last successful sync checksum from the cache file.\"\"\"\n\t\tcache_file = self._get_checksum_cache_path()\n\t\ttry:\n\t\t\tif cache_file.exists():\n\t\t\t\treturn cache_file.read_text().strip()\n\t\texcept OSError as e:\n\t\t\tlogger.warning(f\"Error reading checksum cache file {cache_file}: {e}\")\n\t\treturn None\n\n\tdef _write_last_sync_checksum(self, checksum: str) -&gt; None:\n\t\t\"\"\"Writes the given checksum to the cache file.\"\"\"\n\t\tcache_file = self._get_checksum_cache_path()\n\t\ttry:\n\t\t\tcache_file.parent.mkdir(parents=True, exist_ok=True)  # Ensure .codemap_cache exists\n\t\t\tcache_file.write_text(checksum)\n\t\t\tlogger.info(f\"Updated checksum cache file {cache_file} with checksum {checksum[:8]}...\")\n\t\texcept OSError:\n\t\t\tlogger.exception(f\"Error writing checksum cache file {cache_file}\")\n\n\tasync def _generate_chunks_for_file(self, file_path_str: str, file_hash: str) -&gt; list[dict[str, Any]]:\n\t\t\"\"\"Helper coroutine to generate chunks for a single file.\n\n\t\tFile hash is passed for context (e.g. git hash or content hash).\n\t\t\"\"\"\n\t\tchunks_for_file = []\n\t\tabsolute_path = self.repo_path / file_path_str\n\t\ttry:\n\t\t\t# Pass the file_hash (which could be git_hash for tracked or content_hash for untracked) to chunker\n\t\t\tfile_chunks_generator = self.chunker.chunk_file(absolute_path, git_hash=file_hash)\n\t\t\tif file_chunks_generator:\n\t\t\t\tchunks_for_file.extend([chunk.model_dump() for chunk in file_chunks_generator])\n\t\t\t\tlogger.debug(f\"Generated {len(chunks_for_file)} chunks for {file_path_str}\")\n\t\t\telse:\n\t\t\t\tlogger.debug(f\"No chunks generated for file: {file_path_str}\")\n\t\texcept Exception:\n\t\t\tlogger.exception(f\"Error processing file {file_path_str} during chunk generation\")\n\t\treturn chunks_for_file\n\n\tasync def _get_qdrant_state(self) -&gt; dict[str, set[tuple[str, str]]]:\n\t\tr\"\"\"\n\t\tRetrieves the current state from Qdrant.\n\n\t\tMaps file paths to sets of (chunk_id, git_or_content_hash).\n\t\tThe hash stored in Qdrant should be the file\\'s content hash for\n\t\tuntracked or git_hash for tracked files.\n\t\t\"\"\"\n\t\tawait self.qdrant_manager.initialize()\n\t\tlogger.info(\"Retrieving current state from Qdrant collection...\")\n\t\tqdrant_state: dict[str, set[tuple[str, str]]] = defaultdict(set)\n\t\tall_ids = await self.qdrant_manager.get_all_point_ids_with_filter()\n\t\tlogger.info(f\"[State Check] Retrieved {len(all_ids)} point IDs from Qdrant.\")\n\n\t\tpayloads = {}\n\t\tif all_ids:\n\t\t\tfor i in range(0, len(all_ids), self.qdrant_batch_size):\n\t\t\t\tbatch_ids = all_ids[i : i + self.qdrant_batch_size]\n\t\t\t\tbatch_payloads = await self.qdrant_manager.get_payloads_by_ids(batch_ids)\n\t\t\t\tpayloads.update(batch_payloads)\n\t\tlogger.info(f\"[State Check] Retrieved {len(payloads)} payloads from Qdrant.\")\n\n\t\tprocessed_count = 0\n\t\tfor point_id, payload_dict in payloads.items():\n\t\t\tif payload_dict:\n\t\t\t\tfile_metadata = payload_dict.get(\"file_metadata\")\n\t\t\t\tfile_path_val: str | None = None\n\t\t\t\tcomparison_hash_for_state: str | None = None\n\n\t\t\t\tif isinstance(file_metadata, dict):\n\t\t\t\t\tfile_path_val = file_metadata.get(\"file_path\")\n\t\t\t\t\tfile_content_hash_from_meta = file_metadata.get(\"file_content_hash\")\n\n\t\t\t\t\tgit_metadata = payload_dict.get(\"git_metadata\")\n\t\t\t\t\tif isinstance(git_metadata, dict) and git_metadata.get(\"tracked\") is True:\n\t\t\t\t\t\tgit_hash_from_meta = git_metadata.get(\"git_hash\")\n\t\t\t\t\t\tif isinstance(git_hash_from_meta, str):\n\t\t\t\t\t\t\tcomparison_hash_for_state = git_hash_from_meta\n\t\t\t\t\telif isinstance(file_content_hash_from_meta, str):\n\t\t\t\t\t\tcomparison_hash_for_state = file_content_hash_from_meta\n\n\t\t\t\tif (\n\t\t\t\t\tisinstance(file_path_val, str)\n\t\t\t\t\tand file_path_val.strip()\n\t\t\t\t\tand isinstance(comparison_hash_for_state, str)\n\t\t\t\t):\n\t\t\t\t\tqdrant_state[file_path_val].add((str(point_id), comparison_hash_for_state))\n\t\t\t\t\tprocessed_count += 1\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"Point {point_id} in Qdrant is missing file_path or comparison_hash. \"\n\t\t\t\t\t\tf\"Payload components: file_path_val={file_path_val}, \"\n\t\t\t\t\t\tf\"comparison_hash_for_state={comparison_hash_for_state}\"\n\t\t\t\t\t)\n\t\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tlogger.warning(f\"Point ID {point_id} has an empty or None payload. Skipping.\")\n\n\t\tlogger.info(f\"Retrieved state for {len(qdrant_state)} files ({processed_count} chunks) from Qdrant.\")\n\t\treturn qdrant_state\n\n\tasync def _compare_states(\n\t\tself,\n\t\tcurrent_file_hashes: dict[str, str],  # relative_path -&gt; content_hash (from all_repo_files)\n\t\tprevious_nodes_map: dict[str, dict[str, str]] | None,  # path -&gt; {\"type\": \"file\"|\"dir\", \"hash\": hash_val}\n\t\tqdrant_state: dict[str, set[tuple[str, str]]],  # file_path -&gt; set of (chunk_id, git_or_content_hash_in_db)\n\t) -&gt; tuple[set[str], set[str]]:\n\t\t\"\"\"\n\t\tCompare current file hashes with previous checksum map and Qdrant state.\n\n\t\tArgs:\n\t\t    current_file_hashes: Current state of files in repo (path -&gt; content/git hash).\n\t\t    previous_nodes_map: Previously stored checksum map (path -&gt; {type, hash}).\n\t\t    qdrant_state: Current state of chunks in Qdrant.\n\n\t\tReturns:\n\t\t    tuple[set[str], set[str]]:\n\t\t        - files_to_process: Relative paths of files that are new or changed.\n\t\t        - chunks_to_delete_from_qdrant: Specific Qdrant chunk_ids to delete.\n\t\t\"\"\"\n\t\tfiles_to_process: set[str] = set()\n\t\tchunks_to_delete_from_qdrant: set[str] = set()\n\t\tprocessed_qdrant_paths_this_cycle: set[str] = set()\n\n\t\t# 1. Determine files to process based on current vs. previous checksums\n\t\tfor file_path, current_hash in current_file_hashes.items():\n\t\t\tprevious_file_hash: str | None = None\n\t\t\tif previous_nodes_map and file_path in previous_nodes_map:\n\t\t\t\tnode_info = previous_nodes_map[file_path]\n\t\t\t\tif node_info.get(\"type\") == \"file\":\n\t\t\t\t\tprevious_file_hash = node_info.get(\"hash\")\n\n\t\t\tif previous_file_hash is None or previous_file_hash != current_hash:\n\t\t\t\tlogger.info(\n\t\t\t\t\tf\"[Compare] File '{file_path}' is new or changed. \"\n\t\t\t\t\tf\"Old hash: {previous_file_hash}, New hash: {current_hash}\"\n\t\t\t\t)\n\t\t\t\tfiles_to_process.add(file_path)\n\t\t\t\t# If the file was present and its hash matches, it can be skipped\n\t\t\t\tif file_path in qdrant_state:\n\t\t\t\t\tlogger.info(\n\t\t\t\t\t\tf\"[Compare] Marking existing Qdrant chunks for changed file '{file_path}' for deletion.\"\n\t\t\t\t\t)\n\t\t\t\t\tchunks_to_delete_from_qdrant.update(cid for cid, _ in qdrant_state[file_path])\n\t\t\tprocessed_qdrant_paths_this_cycle.add(file_path)  # Mark as seen from current repo state\n\n\t\t# 2. Determine files/chunks to delete based on previous checksums vs. current\n\t\tif previous_nodes_map:\n\t\t\tfor old_path, node_info in previous_nodes_map.items():\n\t\t\t\tif node_info.get(\"type\") == \"file\" and old_path not in current_file_hashes:\n\t\t\t\t\tlogger.info(\n\t\t\t\t\t\tf\"[Compare] File '{old_path}' was in previous checksum but not current. Deleting from Qdrant.\"\n\t\t\t\t\t)\n\t\t\t\t\tif old_path in qdrant_state:\n\t\t\t\t\t\tfor chunk_id, _ in qdrant_state[old_path]:\n\t\t\t\t\t\t\tchunks_to_delete_from_qdrant.add(chunk_id)\n\t\t\t\t\tprocessed_qdrant_paths_this_cycle.add(old_path)  # Mark as seen from previous repo state\n\n\t\t# 3. Clean up any orphaned Qdrant entries not covered by current or previous valid states\n\t\t# These might be from very old states or errors.\n\t\tall_known_valid_paths = set(current_file_hashes.keys())\n\t\tif previous_nodes_map:\n\t\t\tall_known_valid_paths.update(p for p, info in previous_nodes_map.items() if info.get(\"type\") == \"file\")\n\n\t\tfor qdrant_file_path, qdrant_chunks_set in qdrant_state.items():\n\t\t\tif qdrant_file_path not in all_known_valid_paths:\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"Orphaned file_path '{qdrant_file_path}' in Qdrant not found in current \"\n\t\t\t\t\t\"or previous valid checksums. Deleting its chunks.\"\n\t\t\t\t)\n\t\t\t\tfor chunk_id, _ in qdrant_chunks_set:\n\t\t\t\t\tchunks_to_delete_from_qdrant.add(chunk_id)\n\n\t\tlogger.info(\n\t\t\tf\"[Compare States] Result: {len(files_to_process)} files to process/reprocess, \"  # noqa: S608\n\t\t\tf\"{len(chunks_to_delete_from_qdrant)} specific chunks to delete from Qdrant.\"\n\t\t)\n\t\t# The second element of the tuple (files_whose_chunks_are_all_deleted) is no longer\n\t\t# explicitly needed with this logic.\n\t\treturn files_to_process, chunks_to_delete_from_qdrant\n\n\tasync def _process_and_upsert_batch(self, chunk_batch: list[dict[str, Any]]) -&gt; int:\n\t\t\"\"\"Process a batch of chunks by generating embeddings and upserting to Qdrant.\"\"\"\n\t\tif not chunk_batch:\n\t\t\treturn 0\n\n\t\tdeduplicated_batch = []\n\t\tseen_content_hashes = set()\n\t\tfor chunk in chunk_batch:\n\t\t\tcontent_hash = chunk[\"metadata\"].get(\"content_hash\", \"\")\n\t\t\tfile_metadata_dict = chunk[\"metadata\"].get(\"file_metadata\", {})\n\t\t\tfile_content_hash = (\n\t\t\t\tfile_metadata_dict.get(\"file_content_hash\", \"\") if isinstance(file_metadata_dict, dict) else \"\"\n\t\t\t)\n\t\t\tdedup_key = f\"{content_hash}:{file_content_hash}\"\n\t\t\tif dedup_key not in seen_content_hashes:\n\t\t\t\tseen_content_hashes.add(dedup_key)\n\t\t\t\tdeduplicated_batch.append(chunk)\n\n\t\tif len(deduplicated_batch) &lt; len(chunk_batch):\n\t\t\tlogger.info(\n\t\t\t\tf\"Removed {len(chunk_batch) - len(deduplicated_batch)} \"\n\t\t\t\tf\"duplicate chunks, processing {len(deduplicated_batch)} unique chunks\"\n\t\t\t)\n\n\t\tif not deduplicated_batch:  # If all chunks were duplicates\n\t\t\tlogger.info(\"All chunks in the batch were duplicates. Nothing to process.\")\n\t\t\treturn 0\n\n\t\tlogger.info(f\"Processing batch of {len(deduplicated_batch)} unique chunks for embedding and upsert.\")\n\t\ttexts_to_embed = [chunk[\"content\"] for chunk in deduplicated_batch]\n\t\tembeddings = generate_embedding(texts_to_embed, self.config_loader)\n\n\t\tif embeddings is None or len(embeddings) != len(deduplicated_batch):\n\t\t\tlogger.error(\n\t\t\t\t\"Embed batch failed: \"\n\t\t\t\tf\"got {len(embeddings) if embeddings else 0}, expected {len(deduplicated_batch)}. Skipping.\"\n\t\t\t)\n\t\t\tfailed_files = {chunk[\"metadata\"].get(\"file_path\", \"unknown\") for chunk in deduplicated_batch}\n\t\t\tlogger.error(f\"Failed batch involved files: {failed_files}\")\n\t\t\treturn 0\n\n\t\tpoints_to_upsert = []\n\t\tfor chunk, embedding in zip(deduplicated_batch, embeddings, strict=True):\n\t\t\toriginal_file_path_str = chunk[\"metadata\"].get(\"file_path\", \"unknown\")\n\t\t\tchunk_id = str(uuid.uuid4())\n\t\t\tchunk[\"metadata\"][\"chunk_id\"] = chunk_id\n\t\t\tchunk[\"metadata\"][\"file_path\"] = original_file_path_str\n\t\t\tpayload: dict[str, Any] = cast(\"dict[str, Any]\", chunk[\"metadata\"])\n\t\t\tpoint = create_qdrant_point(chunk_id, embedding, payload)\n\t\t\tpoints_to_upsert.append(point)\n\n\t\tif points_to_upsert:\n\t\t\tawait self.qdrant_manager.upsert_points(points_to_upsert)\n\t\t\tlogger.info(f\"Successfully upserted {len(points_to_upsert)} points from batch.\")\n\t\t\treturn len(points_to_upsert)\n\t\tlogger.warning(\"No points generated from batch to upsert.\")\n\t\treturn 0\n\n\tasync def sync_index(self) -&gt; bool:\n\t\t\"\"\"\n\t\tAsynchronously synchronize the Qdrant index with the current repository state.\n\n\t\tReturns True if synchronization completed successfully, False otherwise.\n\t\t\"\"\"\n\t\tsync_success = False\n\t\tcurrent_repo_root_checksum: str | None = None\n\t\t# This local variable will hold the map for the current sync operation.\n\t\tcurrent_nodes_map: dict[str, dict[str, str]] = {}  # Initialize as empty\n\n\t\tprevious_root_hash: str | None = None\n\t\tprevious_nodes_map: dict[str, dict[str, str]] | None = None\n\n\t\tif self.repo_checksum_calculator:\n\t\t\t# Attempt to read the checksum from the last successful sync for the current branch\n\t\t\tlogger.info(\"Attempting to read latest checksum data for current branch...\")\n\t\t\tprev_hash, prev_map = self.repo_checksum_calculator.read_latest_checksum_data_for_current_branch()\n\t\t\tif prev_hash:\n\t\t\t\tprevious_root_hash = prev_hash\n\t\t\tif prev_map:\n\t\t\t\tprevious_nodes_map = prev_map\n\n\t\t\ttry:\n\t\t\t\t# calculate_repo_checksum returns: tuple[str, dict[str, dict[str, str]]]\n\t\t\t\t# Renamed local var to avoid confusion if self.all_nodes_map_from_checksum is used elsewhere\n\t\t\t\t(\n\t\t\t\t\tcalculated_root_hash,\n\t\t\t\t\tcalculated_nodes_map,\n\t\t\t\t) = await self.repo_checksum_calculator.calculate_repo_checksum()\n\t\t\t\tcurrent_repo_root_checksum = calculated_root_hash\n\t\t\t\tself.all_nodes_map_from_checksum = calculated_nodes_map  # Store the fresh map on self\n\t\t\t\tcurrent_nodes_map = self.all_nodes_map_from_checksum  # Use this fresh map for the current sync\n\n\t\t\t\t# Quick sync: If root checksums match, assume no changes and skip detailed comparison.\n\t\t\t\tif previous_root_hash and current_repo_root_checksum == previous_root_hash:\n\t\t\t\t\tbranch_name = (\n\t\t\t\t\t\tself.repo_checksum_calculator.git_context.get_current_branch()\n\t\t\t\t\t\tif self.repo_checksum_calculator.git_context\n\t\t\t\t\t\telse \"unknown\"\n\t\t\t\t\t)\n\t\t\t\t\tlogger.info(\n\t\t\t\t\t\tf\"Root checksum ({current_repo_root_checksum}) matches \"\n\t\t\t\t\t\tf\"previous state for branch '{branch_name}'. \"\n\t\t\t\t\t\t\"Quick sync indicates no changes needed.\"\n\t\t\t\t\t)\n\t\t\t\t\t# Consider updating a 'last_synced_timestamp' or similar marker here if needed.\n\t\t\t\t\treturn True  # Successfully synced (no changes)\n\t\t\t\tlogger.info(\n\t\t\t\t\t\"Root checksum mismatch or no previous checksum. Proceeding with detailed comparison and sync.\"\n\t\t\t\t)\n\t\t\texcept Exception:  # pylint: disable=broad-except\n\t\t\t\tlogger.exception(\n\t\t\t\t\t\"Error calculating repository checksum. \"\n\t\t\t\t\t\"Proceeding with full comparison using potentially stale or no current checksum data.\"\n\t\t\t\t)\n\t\t\t\t# current_nodes_map remains {}, signifying we couldn't get a fresh current state.\n\t\t\t\t# This will be handled by the check below.\n\t\telse:\n\t\t\tlogger.warning(\n\t\t\t\t\"RepoChecksumCalculator not available. Cannot perform checksum-based \"\n\t\t\t\t\"quick sync, read previous checksum, or get fresh current state. \"\n\t\t\t\t\"Proceeding with comparison based on Qdrant state only if necessary, \"\n\t\t\t\t\"but sync will likely be incomplete.\"\n\t\t\t)\n\t\t\t# previous_root_hash and previous_nodes_map remain None.\n\t\t\t# current_nodes_map remains {}, signifying we couldn't get a fresh current state.\n\t\t\t# This will be handled by the check below.\n\n\t\t# Populate current_file_hashes from the local current_nodes_map.\n\t\t# current_nodes_map will be populated if checksum calculation succeeded, otherwise it's {}.\n\t\tcurrent_file_hashes: dict[str, str] = {}\n\t\tif not current_nodes_map:  # Checks if the map is empty\n\t\t\t# This means checksum calculation failed or RepoChecksumCalculator was not available.\n\t\t\t# We cannot reliably determine the current state of files.\n\t\t\tlogger.error(\n\t\t\t\t\"Current repository file map is empty (failed to calculate checksums \"\n\t\t\t\t\"or RepoChecksumCalculator missing). Cannot proceed with accurate sync \"\n\t\t\t\t\"as current file states are unknown.\"\n\t\t\t)\n\t\t\treturn False  # Cannot sync without knowing current file states.\n\n\t\t# If current_nodes_map is not empty, proceed to populate current_file_hashes\n\t\tfor path, node_info in current_nodes_map.items():\n\t\t\tif node_info.get(\"type\") == \"file\" and \"hash\" in node_info:  # Ensure hash key exists\n\t\t\t\tcurrent_file_hashes[path] = node_info[\"hash\"]\n\n\t\t# If current_nodes_map was valid (not empty) but contained no files (e.g. empty repo),\n\t\t# current_file_hashes will be empty. This is a valid state for _compare_states.\n\n\t\t# Get the current state from Qdrant\n\t\tqdrant_state = await self._get_qdrant_state()\n\n\t\twith progress_indicator(\"Comparing repository state with vector state...\"):\n\t\t\tfiles_to_process, chunks_to_delete = await self._compare_states(\n\t\t\t\tcurrent_file_hashes, previous_nodes_map, qdrant_state\n\t\t\t)\n\n\t\twith progress_indicator(f\"Deleting {len(chunks_to_delete)} outdated vectors...\"):\n\t\t\tif chunks_to_delete:\n\t\t\t\tdelete_ids_list = list(chunks_to_delete)\n\t\t\t\tfor i in range(0, len(delete_ids_list), self.qdrant_batch_size):\n\t\t\t\t\tbatch_ids_to_delete = delete_ids_list[i : i + self.qdrant_batch_size]\n\t\t\t\t\tawait self.qdrant_manager.delete_points(batch_ids_to_delete)\n\t\t\t\t\tlogger.info(f\"Deleted batch of {len(batch_ids_to_delete)} vectors.\")\n\t\t\t\tlogger.info(f\"Finished deleting {len(chunks_to_delete)} vectors.\")\n\t\t\telse:\n\t\t\t\tlogger.info(\"No vectors to delete.\")\n\n\t\t# Step: Update git_metadata for files whose content hasn't changed but Git status might have\n\t\tlogger.info(\"Checking for Git metadata updates for unchanged files...\")\n\n\t\t# Candidate files: in current repo, content hash same as previous, so not in files_to_process\n\t\tfiles_to_check_for_git_metadata_update = set(current_file_hashes.keys()) - files_to_process\n\n\t\tchunk_ids_to_fetch_payloads_for_meta_check: list[str] = []\n\t\t# Maps file_path_str to list of its chunk_ids that are candidates for metadata update\n\t\tcandidate_file_to_chunks_map: dict[str, list[str]] = defaultdict(list)\n\n\t\tfor file_path_str in files_to_check_for_git_metadata_update:\n\t\t\tif file_path_str not in qdrant_state:  # No existing chunks for this file in Qdrant\n\t\t\t\tcontinue\n\n\t\t\t# Consider only chunks that are not already marked for deletion\n\t\t\tchunks_for_this_file = [cid for cid, _ in qdrant_state[file_path_str] if cid not in chunks_to_delete]\n\t\t\tif chunks_for_this_file:\n\t\t\t\t# qdrant_state stores chunk_ids as strings (derived from ExtendedPointId)\n\t\t\t\tchunk_ids_to_fetch_payloads_for_meta_check.extend(chunks_for_this_file)\n\t\t\t\tcandidate_file_to_chunks_map[file_path_str] = chunks_for_this_file\n\n\t\t# Batch fetch payloads for all potentially affected chunks\n\t\tfetched_payloads_for_meta_check: dict[str, dict[str, Any]] = {}\n\t\tif chunk_ids_to_fetch_payloads_for_meta_check:\n\t\t\tlogger.info(\n\t\t\t\tf\"Fetching payloads for {len(chunk_ids_to_fetch_payloads_for_meta_check)} chunks to check Git metadata.\"\n\t\t\t)\n\t\t\tfor i in range(0, len(chunk_ids_to_fetch_payloads_for_meta_check), self.qdrant_batch_size):\n\t\t\t\tbatch_ids = chunk_ids_to_fetch_payloads_for_meta_check[i : i + self.qdrant_batch_size]\n\t\t\t\t# Cast to satisfy linter for QdrantManager's expected type\n\t\t\t\ttyped_batch_ids = cast(\"list[str | int | uuid.UUID]\", batch_ids)\n\t\t\t\tbatch_payloads = await self.qdrant_manager.get_payloads_by_ids(typed_batch_ids)\n\t\t\t\tfetched_payloads_for_meta_check.update(batch_payloads)\n\n\t\t# Dictionary to group chunk_ids by the required new git_metadata\n\t\t# Key: frozenset of new_git_metadata.items() to make it hashable\n\t\t# Value: list of chunk_ids (strings)\n\t\tgit_metadata_update_groups: dict[frozenset, list[str]] = defaultdict(list)\n\n\t\tfor file_path_str, chunk_ids_in_file in candidate_file_to_chunks_map.items():\n\t\t\tcurrent_is_tracked = self.git_context.is_file_tracked(file_path_str)\n\t\t\tcurrent_branch = self.git_context.get_current_branch()\n\t\t\tcurrent_git_hash_for_file: str | None = None\n\t\t\tif current_is_tracked:\n\t\t\t\ttry:\n\t\t\t\t\t# This should be the blob OID for the file\n\t\t\t\t\tcurrent_git_hash_for_file = self.git_context.get_file_git_hash(file_path_str)\n\t\t\t\texcept Exception:  # noqa: BLE001\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"Could not get git hash for tracked file {file_path_str} during metadata update check.\",\n\t\t\t\t\t\texc_info=True,\n\t\t\t\t\t)\n\n\t\t\trequired_new_git_metadata = {\n\t\t\t\t\"tracked\": current_is_tracked,\n\t\t\t\t\"branch\": current_branch,\n\t\t\t\t\"git_hash\": current_git_hash_for_file,  # Will be None if untracked or error getting hash\n\t\t\t}\n\n\t\t\tfor chunk_id in chunk_ids_in_file:  # chunk_id is already a string\n\t\t\t\tchunk_payload = fetched_payloads_for_meta_check.get(chunk_id)\n\t\t\t\tif not chunk_payload:\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"Payload not found for chunk {chunk_id} of file {file_path_str} \"\n\t\t\t\t\t\t\"during metadata check. Skipping this chunk.\"\n\t\t\t\t\t)\n\t\t\t\t\tcontinue\n\n\t\t\t\told_git_metadata = chunk_payload.get(\"git_metadata\")\n\t\t\t\tupdate_needed = False\n\t\t\t\tif not isinstance(old_git_metadata, dict):\n\t\t\t\t\tupdate_needed = True  # If no proper old metadata, or it's missing, update to current\n\t\t\t\telif (\n\t\t\t\t\told_git_metadata.get(\"tracked\") != required_new_git_metadata[\"tracked\"]\n\t\t\t\t\tor old_git_metadata.get(\"branch\") != required_new_git_metadata[\"branch\"]\n\t\t\t\t\tor old_git_metadata.get(\"git_hash\") != required_new_git_metadata[\"git_hash\"]\n\t\t\t\t):\n\t\t\t\t\tupdate_needed = True\n\n\t\t\t\tif update_needed:\n\t\t\t\t\tkey = frozenset(required_new_git_metadata.items())\n\t\t\t\t\tgit_metadata_update_groups[key].append(chunk_id)\n\n\t\tif git_metadata_update_groups:\n\t\t\tnum_chunks_to_update = sum(len(ids) for ids in git_metadata_update_groups.values())\n\t\t\tlogger.info(\n\t\t\t\tf\"Found {num_chunks_to_update} chunks requiring Git metadata updates, \"\n\t\t\t\tf\"grouped into {len(git_metadata_update_groups)} unique metadata sets.\"\n\t\t\t)\n\n\t\t\ttotal_update_batches = sum(\n\t\t\t\t(len(chunk_ids_group) + self.qdrant_batch_size - 1) // self.qdrant_batch_size\n\t\t\t\tfor chunk_ids_group in git_metadata_update_groups.values()\n\t\t\t)\n\t\t\t# Ensure total is at least 1 if there are groups, for progress bar logic\n\t\t\tprogress_total = (\n\t\t\t\ttotal_update_batches if total_update_batches &gt; 0 else (1 if git_metadata_update_groups else 0)\n\t\t\t)\n\n\t\t\tif progress_total &gt; 0:  # Only show progress if there's something to do\n\t\t\t\twith progress_indicator(\n\t\t\t\t\t\"Applying Git metadata updates to chunks...\",\n\t\t\t\t\ttotal=progress_total,\n\t\t\t\t\tstyle=\"progress\",\n\t\t\t\t\ttransient=True,\n\t\t\t\t) as update_meta_progress_bar:\n\t\t\t\t\tapplied_batches_count = 0\n\t\t\t\t\tfor new_meta_fset, chunk_ids_to_update_with_this_meta in git_metadata_update_groups.items():\n\t\t\t\t\t\tnew_meta_dict = dict(new_meta_fset)\n\t\t\t\t\t\tpayload_to_set = {\"git_metadata\": new_meta_dict}\n\n\t\t\t\t\t\tfor i in range(0, len(chunk_ids_to_update_with_this_meta), self.qdrant_batch_size):\n\t\t\t\t\t\t\tbatch_chunk_ids = chunk_ids_to_update_with_this_meta[i : i + self.qdrant_batch_size]\n\t\t\t\t\t\t\tif batch_chunk_ids:  # Ensure batch is not empty\n\t\t\t\t\t\t\t\t# Cast to satisfy linter for QdrantManager's expected type\n\t\t\t\t\t\t\t\ttyped_point_ids = cast(\"list[str | int | uuid.UUID]\", batch_chunk_ids)\n\t\t\t\t\t\t\t\tawait self.qdrant_manager.set_payload(\n\t\t\t\t\t\t\t\t\tpayload=payload_to_set, point_ids=typed_point_ids, filter_condition=models.Filter()\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\t\tf\"Updated git_metadata for {len(batch_chunk_ids)} chunks with: {new_meta_dict}\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\tapplied_batches_count += 1\n\t\t\t\t\t\t\tupdate_meta_progress_bar(None, applied_batches_count, None)  # Update progress\n\n\t\t\t\t\t# Ensure progress bar completes if all batches were empty but groups existed\n\t\t\t\t\tif applied_batches_count == 0 and git_metadata_update_groups:\n\t\t\t\t\t\tupdate_meta_progress_bar(None, progress_total, None)  # Force completion\n\t\t\telif num_chunks_to_update &gt; 0:  # Log if groups existed but somehow total_progress was 0\n\t\t\t\tlogger.info(f\"Updating {num_chunks_to_update} chunks' Git metadata without progress bar (zero batches)\")\n\t\t\t\tfor new_meta_fset, chunk_ids_to_update_with_this_meta in git_metadata_update_groups.items():\n\t\t\t\t\tnew_meta_dict = dict(new_meta_fset)\n\t\t\t\t\tpayload_to_set = {\"git_metadata\": new_meta_dict}\n\t\t\t\t\tif chunk_ids_to_update_with_this_meta:  # Check if list is not empty\n\t\t\t\t\t\t# Cast to satisfy linter for QdrantManager's expected type\n\t\t\t\t\t\ttyped_point_ids_single_batch = cast(\n\t\t\t\t\t\t\t\"list[str | int | uuid.UUID]\", chunk_ids_to_update_with_this_meta\n\t\t\t\t\t\t)\n\t\t\t\t\t\tawait self.qdrant_manager.set_payload(\n\t\t\t\t\t\t\tpayload=payload_to_set,\n\t\t\t\t\t\t\tpoint_ids=typed_point_ids_single_batch,\n\t\t\t\t\t\t\tfilter_condition=models.Filter(),\n\t\t\t\t\t\t)\n\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\tf\"Updated git_metadata for {len(chunk_ids_to_update_with_this_meta)} \"\n\t\t\t\t\t\t\tf\"chunks (in a single batch) with: {new_meta_dict}\"\n\t\t\t\t\t\t)\n\n\t\telse:\n\t\t\tlogger.info(\"No Git metadata updates required for existing chunks of unchanged files.\")\n\n\t\tnum_files_to_process = len(files_to_process)\n\t\tall_chunks: list[dict[str, Any]] = []  # Ensure all_chunks is initialized\n\t\tmsg = \"Processing new/updated files...\"\n\n\t\twith progress_indicator(\n\t\t\tmsg,\n\t\t\tstyle=\"progress\",\n\t\t\ttotal=num_files_to_process if num_files_to_process &gt; 0 else 1,  # total must be &gt; 0\n\t\t\ttransient=True,\n\t\t) as update_file_progress:\n\t\t\tif num_files_to_process &gt; 0:\n\t\t\t\tprocessed_files_counter = [0]  # Use list to make it mutable from inner function\n\n\t\t\t\t# Wrapper coroutine to update progress as tasks complete\n\t\t\t\tasync def wrapped_generate_chunks(file_path: str, f_hash: str) -&gt; list[dict[str, Any]]:\n\t\t\t\t\tresult: list[dict[str, Any]] = []\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresult = await self._generate_chunks_for_file(file_path, f_hash)\n\t\t\t\t\tfinally:\n\t\t\t\t\t\tprocessed_files_counter[0] += 1\n\t\t\t\t\t\tupdate_file_progress(None, processed_files_counter[0], None)\n\t\t\t\t\treturn result\n\n\t\t\t\ttasks_to_gather = []\n\t\t\t\tfor file_path_to_proc in files_to_process:\n\t\t\t\t\tfile_current_hash = current_file_hashes.get(file_path_to_proc)\n\t\t\t\t\tif file_current_hash:\n\t\t\t\t\t\ttasks_to_gather.append(wrapped_generate_chunks(file_path_to_proc, file_current_hash))\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\tf\"File '{file_path_to_proc}' marked to process but its current hash not found. Skipping.\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t# If a file is skipped, increment progress here as it won't be wrapped\n\t\t\t\t\t\tprocessed_files_counter[0] += 1\n\t\t\t\t\t\tupdate_file_progress(None, processed_files_counter[0], None)\n\n\t\t\t\tif tasks_to_gather:\n\t\t\t\t\tlogger.info(f\"Concurrently generating chunks for {len(tasks_to_gather)} files...\")\n\t\t\t\t\tlist_of_chunk_lists = await asyncio.gather(*tasks_to_gather)\n\t\t\t\t\tall_chunks = [chunk for sublist in list_of_chunk_lists for chunk in sublist]\n\t\t\t\t\tlogger.info(f\"Total chunks generated: {len(all_chunks)}.\")\n\t\t\t\telse:\n\t\t\t\t\tlogger.info(\"No files eligible for concurrent chunk generation.\")\n\n\t\t\t\t# Final update to ensure the progress bar completes to 100% if some files were skipped\n\t\t\t\t# and caused processed_files_count to not reach num_files_to_process via the finally blocks alone.\n\t\t\t\tif processed_files_counter[0] &lt; num_files_to_process:\n\t\t\t\t\tupdate_file_progress(None, num_files_to_process, None)\n\n\t\t\telse:  # num_files_to_process == 0\n\t\t\t\tlogger.info(\"No new/updated files to process.\")\n\t\t\t\tupdate_file_progress(None, 1, None)  # Complete the dummy task if total was 1\n\n\t\twith progress_indicator(\"Processing chunks...\"):\n\t\t\tawait self._process_and_upsert_batch(all_chunks)\n\n\t\tsync_success = True\n\t\tlogger.info(\"Vector index synchronization completed successfully.\")\n\n\t\treturn sync_success\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_path: Path,\n\tqdrant_manager: QdrantManager,\n\tchunker: TreeSitterChunker | None,\n\tembedding_model_name: str,\n\tanalyzer: TreeSitterAnalyzer | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n\trepo_checksum_calculator: RepoChecksumCalculator\n\t| None = None,\n) -&gt; None\n</code></pre> <p>Initialize the vector synchronizer.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to the git repository root.</p> required <code>qdrant_manager</code> <code>QdrantManager</code> <p>Instance of QdrantManager to handle vector storage.</p> required <code>chunker</code> <code>TreeSitterChunker | None</code> <p>Instance of chunker used to create code chunks.</p> required <code>embedding_model_name</code> <code>str</code> <p>Name of the embedding model to use.</p> required <code>analyzer</code> <code>TreeSitterAnalyzer | None</code> <p>Optional TreeSitterAnalyzer instance.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance.</p> <code>None</code> <code>repo_checksum_calculator</code> <code>RepoChecksumCalculator | None</code> <p>Optional RepoChecksumCalculator instance.</p> <code>None</code> Source code in <code>src/codemap/processor/vector/synchronizer.py</code> <pre><code>def __init__(\n\tself,\n\trepo_path: Path,\n\tqdrant_manager: QdrantManager,\n\tchunker: TreeSitterChunker | None,\n\tembedding_model_name: str,\n\tanalyzer: TreeSitterAnalyzer | None = None,\n\tconfig_loader: \"ConfigLoader | None\" = None,\n\trepo_checksum_calculator: RepoChecksumCalculator | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the vector synchronizer.\n\n\tArgs:\n\t    repo_path: Path to the git repository root.\n\t    qdrant_manager: Instance of QdrantManager to handle vector storage.\n\t    chunker: Instance of chunker used to create code chunks.\n\t    embedding_model_name: Name of the embedding model to use.\n\t    analyzer: Optional TreeSitterAnalyzer instance.\n\t    config_loader: Configuration loader instance.\n\t    repo_checksum_calculator: Optional RepoChecksumCalculator instance.\n\n\t\"\"\"\n\tself.repo_path = repo_path\n\tself.qdrant_manager = qdrant_manager\n\tself.git_context = ExtendedGitRepoContext.get_instance()\n\tself.embedding_model_name = embedding_model_name\n\tself.analyzer = analyzer or TreeSitterAnalyzer()\n\n\t# Ensure RepoChecksumCalculator is instantiated with git_context\n\tif repo_checksum_calculator is None:\n\t\tself.repo_checksum_calculator = RepoChecksumCalculator.get_instance(\n\t\t\trepo_path=self.repo_path, git_context=self.git_context, config_loader=config_loader\n\t\t)\n\telse:\n\t\tself.repo_checksum_calculator = repo_checksum_calculator\n\t\t# Ensure existing calculator also has git_context, as it might be crucial for branch logic\n\t\tif self.repo_checksum_calculator.git_context is None and self.git_context:\n\t\t\tself.repo_checksum_calculator.git_context = self.git_context\n\n\tif config_loader:\n\t\tself.config_loader = config_loader\n\telse:\n\t\tfrom codemap.config import ConfigLoader\n\n\t\tself.config_loader = ConfigLoader.get_instance()\n\n\tembedding_config = self.config_loader.get.embedding\n\tself.qdrant_batch_size = embedding_config.qdrant_batch_size\n\n\tif chunker is None:\n\t\tself.chunker = TreeSitterChunker(\n\t\t\tgit_context=self.git_context,\n\t\t\tconfig_loader=self.config_loader,\n\t\t\trepo_checksum_calculator=self.repo_checksum_calculator,\n\t\t)\n\telse:\n\t\tif getattr(chunker, \"git_context\", None) is None:\n\t\t\tchunker.git_context = self.git_context\n\t\tif (\n\t\t\thasattr(chunker, \"repo_checksum_calculator\")\n\t\t\tand getattr(chunker, \"repo_checksum_calculator\", None) is None\n\t\t\tand self.repo_checksum_calculator\n\t\t):\n\t\t\tchunker.repo_checksum_calculator = self.repo_checksum_calculator\n\t\tself.chunker = chunker\n\n\tlogger.info(\n\t\tf\"VectorSynchronizer initialized for repo: {repo_path} \"\n\t\tf\"using Qdrant collection: '{qdrant_manager.collection_name}' \"\n\t\tf\"and embedding model: {embedding_model_name}\"\n\t)\n\tif not self.repo_checksum_calculator:\n\t\tlogger.warning(\"RepoChecksumCalculator could not be initialized. Checksum-based sync will be skipped.\")\n\n\t# Initialize checksum map attribute\n\tself.all_nodes_map_from_checksum: dict[str, dict[str, str]] = {}\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.qdrant_manager","title":"qdrant_manager  <code>instance-attribute</code>","text":"<pre><code>qdrant_manager = qdrant_manager\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.git_context","title":"git_context  <code>instance-attribute</code>","text":"<pre><code>git_context = get_instance()\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.embedding_model_name","title":"embedding_model_name  <code>instance-attribute</code>","text":"<pre><code>embedding_model_name = embedding_model_name\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.analyzer","title":"analyzer  <code>instance-attribute</code>","text":"<pre><code>analyzer = analyzer or TreeSitterAnalyzer()\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.repo_checksum_calculator","title":"repo_checksum_calculator  <code>instance-attribute</code>","text":"<pre><code>repo_checksum_calculator = get_instance(\n\trepo_path=repo_path,\n\tgit_context=git_context,\n\tconfig_loader=config_loader,\n)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.qdrant_batch_size","title":"qdrant_batch_size  <code>instance-attribute</code>","text":"<pre><code>qdrant_batch_size = qdrant_batch_size\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.chunker","title":"chunker  <code>instance-attribute</code>","text":"<pre><code>chunker = TreeSitterChunker(\n\tgit_context=git_context,\n\tconfig_loader=config_loader,\n\trepo_checksum_calculator=repo_checksum_calculator,\n)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.all_nodes_map_from_checksum","title":"all_nodes_map_from_checksum  <code>instance-attribute</code>","text":"<pre><code>all_nodes_map_from_checksum: dict[str, dict[str, str]] = {}\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.sync_index","title":"sync_index  <code>async</code>","text":"<pre><code>sync_index() -&gt; bool\n</code></pre> <p>Asynchronously synchronize the Qdrant index with the current repository state.</p> <p>Returns True if synchronization completed successfully, False otherwise.</p> Source code in <code>src/codemap/processor/vector/synchronizer.py</code> <pre><code>async def sync_index(self) -&gt; bool:\n\t\"\"\"\n\tAsynchronously synchronize the Qdrant index with the current repository state.\n\n\tReturns True if synchronization completed successfully, False otherwise.\n\t\"\"\"\n\tsync_success = False\n\tcurrent_repo_root_checksum: str | None = None\n\t# This local variable will hold the map for the current sync operation.\n\tcurrent_nodes_map: dict[str, dict[str, str]] = {}  # Initialize as empty\n\n\tprevious_root_hash: str | None = None\n\tprevious_nodes_map: dict[str, dict[str, str]] | None = None\n\n\tif self.repo_checksum_calculator:\n\t\t# Attempt to read the checksum from the last successful sync for the current branch\n\t\tlogger.info(\"Attempting to read latest checksum data for current branch...\")\n\t\tprev_hash, prev_map = self.repo_checksum_calculator.read_latest_checksum_data_for_current_branch()\n\t\tif prev_hash:\n\t\t\tprevious_root_hash = prev_hash\n\t\tif prev_map:\n\t\t\tprevious_nodes_map = prev_map\n\n\t\ttry:\n\t\t\t# calculate_repo_checksum returns: tuple[str, dict[str, dict[str, str]]]\n\t\t\t# Renamed local var to avoid confusion if self.all_nodes_map_from_checksum is used elsewhere\n\t\t\t(\n\t\t\t\tcalculated_root_hash,\n\t\t\t\tcalculated_nodes_map,\n\t\t\t) = await self.repo_checksum_calculator.calculate_repo_checksum()\n\t\t\tcurrent_repo_root_checksum = calculated_root_hash\n\t\t\tself.all_nodes_map_from_checksum = calculated_nodes_map  # Store the fresh map on self\n\t\t\tcurrent_nodes_map = self.all_nodes_map_from_checksum  # Use this fresh map for the current sync\n\n\t\t\t# Quick sync: If root checksums match, assume no changes and skip detailed comparison.\n\t\t\tif previous_root_hash and current_repo_root_checksum == previous_root_hash:\n\t\t\t\tbranch_name = (\n\t\t\t\t\tself.repo_checksum_calculator.git_context.get_current_branch()\n\t\t\t\t\tif self.repo_checksum_calculator.git_context\n\t\t\t\t\telse \"unknown\"\n\t\t\t\t)\n\t\t\t\tlogger.info(\n\t\t\t\t\tf\"Root checksum ({current_repo_root_checksum}) matches \"\n\t\t\t\t\tf\"previous state for branch '{branch_name}'. \"\n\t\t\t\t\t\"Quick sync indicates no changes needed.\"\n\t\t\t\t)\n\t\t\t\t# Consider updating a 'last_synced_timestamp' or similar marker here if needed.\n\t\t\t\treturn True  # Successfully synced (no changes)\n\t\t\tlogger.info(\n\t\t\t\t\"Root checksum mismatch or no previous checksum. Proceeding with detailed comparison and sync.\"\n\t\t\t)\n\t\texcept Exception:  # pylint: disable=broad-except\n\t\t\tlogger.exception(\n\t\t\t\t\"Error calculating repository checksum. \"\n\t\t\t\t\"Proceeding with full comparison using potentially stale or no current checksum data.\"\n\t\t\t)\n\t\t\t# current_nodes_map remains {}, signifying we couldn't get a fresh current state.\n\t\t\t# This will be handled by the check below.\n\telse:\n\t\tlogger.warning(\n\t\t\t\"RepoChecksumCalculator not available. Cannot perform checksum-based \"\n\t\t\t\"quick sync, read previous checksum, or get fresh current state. \"\n\t\t\t\"Proceeding with comparison based on Qdrant state only if necessary, \"\n\t\t\t\"but sync will likely be incomplete.\"\n\t\t)\n\t\t# previous_root_hash and previous_nodes_map remain None.\n\t\t# current_nodes_map remains {}, signifying we couldn't get a fresh current state.\n\t\t# This will be handled by the check below.\n\n\t# Populate current_file_hashes from the local current_nodes_map.\n\t# current_nodes_map will be populated if checksum calculation succeeded, otherwise it's {}.\n\tcurrent_file_hashes: dict[str, str] = {}\n\tif not current_nodes_map:  # Checks if the map is empty\n\t\t# This means checksum calculation failed or RepoChecksumCalculator was not available.\n\t\t# We cannot reliably determine the current state of files.\n\t\tlogger.error(\n\t\t\t\"Current repository file map is empty (failed to calculate checksums \"\n\t\t\t\"or RepoChecksumCalculator missing). Cannot proceed with accurate sync \"\n\t\t\t\"as current file states are unknown.\"\n\t\t)\n\t\treturn False  # Cannot sync without knowing current file states.\n\n\t# If current_nodes_map is not empty, proceed to populate current_file_hashes\n\tfor path, node_info in current_nodes_map.items():\n\t\tif node_info.get(\"type\") == \"file\" and \"hash\" in node_info:  # Ensure hash key exists\n\t\t\tcurrent_file_hashes[path] = node_info[\"hash\"]\n\n\t# If current_nodes_map was valid (not empty) but contained no files (e.g. empty repo),\n\t# current_file_hashes will be empty. This is a valid state for _compare_states.\n\n\t# Get the current state from Qdrant\n\tqdrant_state = await self._get_qdrant_state()\n\n\twith progress_indicator(\"Comparing repository state with vector state...\"):\n\t\tfiles_to_process, chunks_to_delete = await self._compare_states(\n\t\t\tcurrent_file_hashes, previous_nodes_map, qdrant_state\n\t\t)\n\n\twith progress_indicator(f\"Deleting {len(chunks_to_delete)} outdated vectors...\"):\n\t\tif chunks_to_delete:\n\t\t\tdelete_ids_list = list(chunks_to_delete)\n\t\t\tfor i in range(0, len(delete_ids_list), self.qdrant_batch_size):\n\t\t\t\tbatch_ids_to_delete = delete_ids_list[i : i + self.qdrant_batch_size]\n\t\t\t\tawait self.qdrant_manager.delete_points(batch_ids_to_delete)\n\t\t\t\tlogger.info(f\"Deleted batch of {len(batch_ids_to_delete)} vectors.\")\n\t\t\tlogger.info(f\"Finished deleting {len(chunks_to_delete)} vectors.\")\n\t\telse:\n\t\t\tlogger.info(\"No vectors to delete.\")\n\n\t# Step: Update git_metadata for files whose content hasn't changed but Git status might have\n\tlogger.info(\"Checking for Git metadata updates for unchanged files...\")\n\n\t# Candidate files: in current repo, content hash same as previous, so not in files_to_process\n\tfiles_to_check_for_git_metadata_update = set(current_file_hashes.keys()) - files_to_process\n\n\tchunk_ids_to_fetch_payloads_for_meta_check: list[str] = []\n\t# Maps file_path_str to list of its chunk_ids that are candidates for metadata update\n\tcandidate_file_to_chunks_map: dict[str, list[str]] = defaultdict(list)\n\n\tfor file_path_str in files_to_check_for_git_metadata_update:\n\t\tif file_path_str not in qdrant_state:  # No existing chunks for this file in Qdrant\n\t\t\tcontinue\n\n\t\t# Consider only chunks that are not already marked for deletion\n\t\tchunks_for_this_file = [cid for cid, _ in qdrant_state[file_path_str] if cid not in chunks_to_delete]\n\t\tif chunks_for_this_file:\n\t\t\t# qdrant_state stores chunk_ids as strings (derived from ExtendedPointId)\n\t\t\tchunk_ids_to_fetch_payloads_for_meta_check.extend(chunks_for_this_file)\n\t\t\tcandidate_file_to_chunks_map[file_path_str] = chunks_for_this_file\n\n\t# Batch fetch payloads for all potentially affected chunks\n\tfetched_payloads_for_meta_check: dict[str, dict[str, Any]] = {}\n\tif chunk_ids_to_fetch_payloads_for_meta_check:\n\t\tlogger.info(\n\t\t\tf\"Fetching payloads for {len(chunk_ids_to_fetch_payloads_for_meta_check)} chunks to check Git metadata.\"\n\t\t)\n\t\tfor i in range(0, len(chunk_ids_to_fetch_payloads_for_meta_check), self.qdrant_batch_size):\n\t\t\tbatch_ids = chunk_ids_to_fetch_payloads_for_meta_check[i : i + self.qdrant_batch_size]\n\t\t\t# Cast to satisfy linter for QdrantManager's expected type\n\t\t\ttyped_batch_ids = cast(\"list[str | int | uuid.UUID]\", batch_ids)\n\t\t\tbatch_payloads = await self.qdrant_manager.get_payloads_by_ids(typed_batch_ids)\n\t\t\tfetched_payloads_for_meta_check.update(batch_payloads)\n\n\t# Dictionary to group chunk_ids by the required new git_metadata\n\t# Key: frozenset of new_git_metadata.items() to make it hashable\n\t# Value: list of chunk_ids (strings)\n\tgit_metadata_update_groups: dict[frozenset, list[str]] = defaultdict(list)\n\n\tfor file_path_str, chunk_ids_in_file in candidate_file_to_chunks_map.items():\n\t\tcurrent_is_tracked = self.git_context.is_file_tracked(file_path_str)\n\t\tcurrent_branch = self.git_context.get_current_branch()\n\t\tcurrent_git_hash_for_file: str | None = None\n\t\tif current_is_tracked:\n\t\t\ttry:\n\t\t\t\t# This should be the blob OID for the file\n\t\t\t\tcurrent_git_hash_for_file = self.git_context.get_file_git_hash(file_path_str)\n\t\t\texcept Exception:  # noqa: BLE001\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"Could not get git hash for tracked file {file_path_str} during metadata update check.\",\n\t\t\t\t\texc_info=True,\n\t\t\t\t)\n\n\t\trequired_new_git_metadata = {\n\t\t\t\"tracked\": current_is_tracked,\n\t\t\t\"branch\": current_branch,\n\t\t\t\"git_hash\": current_git_hash_for_file,  # Will be None if untracked or error getting hash\n\t\t}\n\n\t\tfor chunk_id in chunk_ids_in_file:  # chunk_id is already a string\n\t\t\tchunk_payload = fetched_payloads_for_meta_check.get(chunk_id)\n\t\t\tif not chunk_payload:\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"Payload not found for chunk {chunk_id} of file {file_path_str} \"\n\t\t\t\t\t\"during metadata check. Skipping this chunk.\"\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\told_git_metadata = chunk_payload.get(\"git_metadata\")\n\t\t\tupdate_needed = False\n\t\t\tif not isinstance(old_git_metadata, dict):\n\t\t\t\tupdate_needed = True  # If no proper old metadata, or it's missing, update to current\n\t\t\telif (\n\t\t\t\told_git_metadata.get(\"tracked\") != required_new_git_metadata[\"tracked\"]\n\t\t\t\tor old_git_metadata.get(\"branch\") != required_new_git_metadata[\"branch\"]\n\t\t\t\tor old_git_metadata.get(\"git_hash\") != required_new_git_metadata[\"git_hash\"]\n\t\t\t):\n\t\t\t\tupdate_needed = True\n\n\t\t\tif update_needed:\n\t\t\t\tkey = frozenset(required_new_git_metadata.items())\n\t\t\t\tgit_metadata_update_groups[key].append(chunk_id)\n\n\tif git_metadata_update_groups:\n\t\tnum_chunks_to_update = sum(len(ids) for ids in git_metadata_update_groups.values())\n\t\tlogger.info(\n\t\t\tf\"Found {num_chunks_to_update} chunks requiring Git metadata updates, \"\n\t\t\tf\"grouped into {len(git_metadata_update_groups)} unique metadata sets.\"\n\t\t)\n\n\t\ttotal_update_batches = sum(\n\t\t\t(len(chunk_ids_group) + self.qdrant_batch_size - 1) // self.qdrant_batch_size\n\t\t\tfor chunk_ids_group in git_metadata_update_groups.values()\n\t\t)\n\t\t# Ensure total is at least 1 if there are groups, for progress bar logic\n\t\tprogress_total = (\n\t\t\ttotal_update_batches if total_update_batches &gt; 0 else (1 if git_metadata_update_groups else 0)\n\t\t)\n\n\t\tif progress_total &gt; 0:  # Only show progress if there's something to do\n\t\t\twith progress_indicator(\n\t\t\t\t\"Applying Git metadata updates to chunks...\",\n\t\t\t\ttotal=progress_total,\n\t\t\t\tstyle=\"progress\",\n\t\t\t\ttransient=True,\n\t\t\t) as update_meta_progress_bar:\n\t\t\t\tapplied_batches_count = 0\n\t\t\t\tfor new_meta_fset, chunk_ids_to_update_with_this_meta in git_metadata_update_groups.items():\n\t\t\t\t\tnew_meta_dict = dict(new_meta_fset)\n\t\t\t\t\tpayload_to_set = {\"git_metadata\": new_meta_dict}\n\n\t\t\t\t\tfor i in range(0, len(chunk_ids_to_update_with_this_meta), self.qdrant_batch_size):\n\t\t\t\t\t\tbatch_chunk_ids = chunk_ids_to_update_with_this_meta[i : i + self.qdrant_batch_size]\n\t\t\t\t\t\tif batch_chunk_ids:  # Ensure batch is not empty\n\t\t\t\t\t\t\t# Cast to satisfy linter for QdrantManager's expected type\n\t\t\t\t\t\t\ttyped_point_ids = cast(\"list[str | int | uuid.UUID]\", batch_chunk_ids)\n\t\t\t\t\t\t\tawait self.qdrant_manager.set_payload(\n\t\t\t\t\t\t\t\tpayload=payload_to_set, point_ids=typed_point_ids, filter_condition=models.Filter()\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\tf\"Updated git_metadata for {len(batch_chunk_ids)} chunks with: {new_meta_dict}\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\tapplied_batches_count += 1\n\t\t\t\t\t\tupdate_meta_progress_bar(None, applied_batches_count, None)  # Update progress\n\n\t\t\t\t# Ensure progress bar completes if all batches were empty but groups existed\n\t\t\t\tif applied_batches_count == 0 and git_metadata_update_groups:\n\t\t\t\t\tupdate_meta_progress_bar(None, progress_total, None)  # Force completion\n\t\telif num_chunks_to_update &gt; 0:  # Log if groups existed but somehow total_progress was 0\n\t\t\tlogger.info(f\"Updating {num_chunks_to_update} chunks' Git metadata without progress bar (zero batches)\")\n\t\t\tfor new_meta_fset, chunk_ids_to_update_with_this_meta in git_metadata_update_groups.items():\n\t\t\t\tnew_meta_dict = dict(new_meta_fset)\n\t\t\t\tpayload_to_set = {\"git_metadata\": new_meta_dict}\n\t\t\t\tif chunk_ids_to_update_with_this_meta:  # Check if list is not empty\n\t\t\t\t\t# Cast to satisfy linter for QdrantManager's expected type\n\t\t\t\t\ttyped_point_ids_single_batch = cast(\n\t\t\t\t\t\t\"list[str | int | uuid.UUID]\", chunk_ids_to_update_with_this_meta\n\t\t\t\t\t)\n\t\t\t\t\tawait self.qdrant_manager.set_payload(\n\t\t\t\t\t\tpayload=payload_to_set,\n\t\t\t\t\t\tpoint_ids=typed_point_ids_single_batch,\n\t\t\t\t\t\tfilter_condition=models.Filter(),\n\t\t\t\t\t)\n\t\t\t\t\tlogger.info(\n\t\t\t\t\t\tf\"Updated git_metadata for {len(chunk_ids_to_update_with_this_meta)} \"\n\t\t\t\t\t\tf\"chunks (in a single batch) with: {new_meta_dict}\"\n\t\t\t\t\t)\n\n\telse:\n\t\tlogger.info(\"No Git metadata updates required for existing chunks of unchanged files.\")\n\n\tnum_files_to_process = len(files_to_process)\n\tall_chunks: list[dict[str, Any]] = []  # Ensure all_chunks is initialized\n\tmsg = \"Processing new/updated files...\"\n\n\twith progress_indicator(\n\t\tmsg,\n\t\tstyle=\"progress\",\n\t\ttotal=num_files_to_process if num_files_to_process &gt; 0 else 1,  # total must be &gt; 0\n\t\ttransient=True,\n\t) as update_file_progress:\n\t\tif num_files_to_process &gt; 0:\n\t\t\tprocessed_files_counter = [0]  # Use list to make it mutable from inner function\n\n\t\t\t# Wrapper coroutine to update progress as tasks complete\n\t\t\tasync def wrapped_generate_chunks(file_path: str, f_hash: str) -&gt; list[dict[str, Any]]:\n\t\t\t\tresult: list[dict[str, Any]] = []\n\t\t\t\ttry:\n\t\t\t\t\tresult = await self._generate_chunks_for_file(file_path, f_hash)\n\t\t\t\tfinally:\n\t\t\t\t\tprocessed_files_counter[0] += 1\n\t\t\t\t\tupdate_file_progress(None, processed_files_counter[0], None)\n\t\t\t\treturn result\n\n\t\t\ttasks_to_gather = []\n\t\t\tfor file_path_to_proc in files_to_process:\n\t\t\t\tfile_current_hash = current_file_hashes.get(file_path_to_proc)\n\t\t\t\tif file_current_hash:\n\t\t\t\t\ttasks_to_gather.append(wrapped_generate_chunks(file_path_to_proc, file_current_hash))\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"File '{file_path_to_proc}' marked to process but its current hash not found. Skipping.\"\n\t\t\t\t\t)\n\t\t\t\t\t# If a file is skipped, increment progress here as it won't be wrapped\n\t\t\t\t\tprocessed_files_counter[0] += 1\n\t\t\t\t\tupdate_file_progress(None, processed_files_counter[0], None)\n\n\t\t\tif tasks_to_gather:\n\t\t\t\tlogger.info(f\"Concurrently generating chunks for {len(tasks_to_gather)} files...\")\n\t\t\t\tlist_of_chunk_lists = await asyncio.gather(*tasks_to_gather)\n\t\t\t\tall_chunks = [chunk for sublist in list_of_chunk_lists for chunk in sublist]\n\t\t\t\tlogger.info(f\"Total chunks generated: {len(all_chunks)}.\")\n\t\t\telse:\n\t\t\t\tlogger.info(\"No files eligible for concurrent chunk generation.\")\n\n\t\t\t# Final update to ensure the progress bar completes to 100% if some files were skipped\n\t\t\t# and caused processed_files_count to not reach num_files_to_process via the finally blocks alone.\n\t\t\tif processed_files_counter[0] &lt; num_files_to_process:\n\t\t\t\tupdate_file_progress(None, num_files_to_process, None)\n\n\t\telse:  # num_files_to_process == 0\n\t\t\tlogger.info(\"No new/updated files to process.\")\n\t\t\tupdate_file_progress(None, 1, None)  # Complete the dummy task if total was 1\n\n\twith progress_indicator(\"Processing chunks...\"):\n\t\tawait self._process_and_upsert_batch(all_chunks)\n\n\tsync_success = True\n\tlogger.info(\"Vector index synchronization completed successfully.\")\n\n\treturn sync_success\n</code></pre>"},{"location":"api/processor/vector/utils/","title":"Utils","text":"<p>Utility functions for the vector processor.</p>"},{"location":"api/utils/","title":"Utils Overview","text":"<p>Utility module for CodeMap package.</p> <ul> <li>Cli Utils - Utility functions for CLI operations in CodeMap.</li> <li>Docker Utils - Utilities for working with Docker containers directly via Python.</li> <li>File Utils - Utility functions for file operations in CodeMap.</li> <li>Git Hooks - Utilities for managing and running git hooks directly.</li> <li>Git Utils - Utilities for interacting with Git.</li> <li>Log Setup - Logging setup for CodeMap.</li> <li>Path Utils - Utilities for handling paths and file system operations.</li> </ul>"},{"location":"api/utils/cli_utils/","title":"Cli Utils","text":"<p>Utility functions for CLI operations in CodeMap.</p>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.ProgressUpdater","title":"ProgressUpdater  <code>module-attribute</code>","text":"<pre><code>ProgressUpdater = Callable[\n\t[str | None, int | None, int | None], None\n]\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState","title":"SpinnerState","text":"<p>Singleton class to manage the stack and display of active spinners.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>class SpinnerState:\n\t\"\"\"Singleton class to manage the stack and display of active spinners.\"\"\"\n\n\t_instance: Self | None = None\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the spinner state attributes.\"\"\"\n\t\tif not hasattr(self, \"spinner_message_stack\"):\n\t\t\tself.spinner_message_stack: list[str] = []\n\t\t\tself.active_rich_status_cm: Status | None = None\n\t\t\tself.tree_display_active: bool = False\n\n\tdef __new__(cls) -&gt; Self:\n\t\t\"\"\"Create or return the singleton instance.\n\n\t\tReturns:\n\t\t    Self: The singleton instance of SpinnerState\n\t\t\"\"\"\n\t\tif cls._instance is None:\n\t\t\tcls._instance = super().__new__(cls)\n\t\treturn cls._instance\n\n\tdef _stop_active_status_cm(self) -&gt; None:\n\t\t\"\"\"Safely stops (exits) the currently active Rich Status context manager.\"\"\"\n\t\tif self.active_rich_status_cm:\n\t\t\ttry:\n\t\t\t\tself.active_rich_status_cm.__exit__(None, None, None)\n\t\t\texcept (RuntimeError, TypeError, ValueError) as exc:  # pragma: no cover\n\t\t\t\tlogger.debug(\"Error stopping previous status context manager\", exc_info=exc)\n\t\t\tself.active_rich_status_cm = None\n\t\t\tself.tree_display_active = False\n\n\tdef _format_tree_display(self) -&gt; str:\n\t\t\"\"\"Format the spinner messages as a tree structure.\n\n\t\tReturns:\n\t\t    str: A formatted tree representation of all spinners in the stack\n\t\t\"\"\"\n\t\tif not self.spinner_message_stack:\n\t\t\treturn \"\"\n\n\t\tresult = []\n\t\tstatic_spinner_char = \"\u25b8\"  # Static indicator for child lines\n\n\t\tfor i, message in enumerate(self.spinner_message_stack):\n\t\t\tline_parts = []\n\t\t\tif i == 0:  # Root message\n\t\t\t\tline_parts.append(message)\n\t\t\telse:  # Child messages\n\t\t\t\t# Add indentation and vertical bars for ancestors\n\t\t\t\tline_parts.extend([\"   \"] * (i - 1))\n\n\t\t\t\t# Add connector from direct parent\n\t\t\t\tline_parts.append(\"[green]\u2514\u2500 [/green]\")\n\t\t\t\tline_parts.append(f\"{static_spinner_char} \")\n\t\t\t\tline_parts.append(message)\n\n\t\t\tresult.append(\"\".join(line_parts))\n\n\t\treturn \"\\n\".join(result)\n\n\tdef _start_status_cm_for_tree(self) -&gt; None:\n\t\t\"\"\"Creates or updates a status display showing all spinners in a tree structure.\"\"\"\n\t\tif not self.spinner_message_stack:\n\t\t\tself._stop_active_status_cm()  # Ensure spinner stops if stack is empty\n\t\t\treturn\n\n\t\ttree_display_text = self._format_tree_display()\n\n\t\tif self.active_rich_status_cm and self.tree_display_active:\n\t\t\t# If a tree display is already active, just update its content\n\t\t\ttry:\n\t\t\t\tself.active_rich_status_cm.update(tree_display_text)\n\t\t\texcept (RuntimeError, TypeError, ValueError) as e:  # pragma: no cover\n\t\t\t\tlogger.debug(f\"Error updating tree status: {e}\", exc_info=True)\n\t\t\t\t# If update fails, fall back to recreating (e.g., if status was manually stopped)\n\t\t\t\tself._stop_active_status_cm()  # Clean up before recreating\n\t\t\t\tself._create_new_tree_status(tree_display_text)\n\t\telse:\n\t\t\t# Otherwise, stop any existing (non-tree) status and create a new tree status\n\t\t\tself._stop_active_status_cm()\n\t\t\tself._create_new_tree_status(tree_display_text)\n\n\tdef _create_new_tree_status(self, tree_display_text: str) -&gt; None:\n\t\t\"\"\"Helper to create and start a new status cm for the tree display.\"\"\"\n\t\tnew_status_cm = console.status(tree_display_text)  # Using default Rich spinner\n\t\ttry:\n\t\t\tnew_status_cm.__enter__()\n\t\t\tself.active_rich_status_cm = new_status_cm\n\t\t\tself.tree_display_active = True\n\t\texcept (RuntimeError, TypeError, ValueError) as exc:  # pragma: no cover\n\t\t\tlogger.debug(\"Error starting new tree status context manager\", exc_info=exc)\n\n\tdef start_new_spinner(self, message: str) -&gt; None:\n\t\t\"\"\"Handles the start of a new spinner.\n\n\t\tAdds the new spinner message to the stack and updates the tree display.\n\t\t\"\"\"\n\t\tself.spinner_message_stack.append(message)\n\t\tself._start_status_cm_for_tree()\n\n\tdef stop_current_spinner_and_resume_parent(self) -&gt; None:\n\t\t\"\"\"Handles the end of the current spinner.\n\n\t\tPops the top spinner from the stack and updates the tree display.\n\t\t\"\"\"\n\t\tif self.spinner_message_stack:\n\t\t\tself.spinner_message_stack.pop()  # Remove the spinner that just ended\n\n\t\tif self.spinner_message_stack:  # If any spinners remain\n\t\t\tself._start_status_cm_for_tree()\n\t\telse:  # No spinners left on stack\n\t\t\tself._stop_active_status_cm()\n\n\tdef temporarily_halt_visual_spinner(self) -&gt; bool:\n\t\t\"\"\"Stops the current visual spinner if one is active.\n\n\t\tUsed when a progress bar is about to take over.\n\t\tReturns True if a visual spinner was halted, False otherwise.\n\t\t\"\"\"\n\t\tif self.active_rich_status_cm:\n\t\t\tself._stop_active_status_cm()  # This already sets tree_display_active to False\n\t\t\treturn True\n\t\treturn False\n\n\tdef resume_visual_spinner_if_needed(self) -&gt; None:\n\t\t\"\"\"Resumes a visual spinner for the top message on the stack.\n\n\t\tIf the stack is not empty and no visual spinner is currently active.\n\t\tUsed after a progress bar (that might have halted a spinner) finishes.\n\t\t\"\"\"\n\t\tif self.spinner_message_stack and not self.active_rich_status_cm:\n\t\t\tself._start_status_cm_for_tree()\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the spinner state attributes.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the spinner state attributes.\"\"\"\n\tif not hasattr(self, \"spinner_message_stack\"):\n\t\tself.spinner_message_stack: list[str] = []\n\t\tself.active_rich_status_cm: Status | None = None\n\t\tself.tree_display_active: bool = False\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.spinner_message_stack","title":"spinner_message_stack  <code>instance-attribute</code>","text":"<pre><code>spinner_message_stack: list[str] = []\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.active_rich_status_cm","title":"active_rich_status_cm  <code>instance-attribute</code>","text":"<pre><code>active_rich_status_cm: Status | None = None\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.tree_display_active","title":"tree_display_active  <code>instance-attribute</code>","text":"<pre><code>tree_display_active: bool = False\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.__new__","title":"__new__","text":"<pre><code>__new__() -&gt; Self\n</code></pre> <p>Create or return the singleton instance.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The singleton instance of SpinnerState</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def __new__(cls) -&gt; Self:\n\t\"\"\"Create or return the singleton instance.\n\n\tReturns:\n\t    Self: The singleton instance of SpinnerState\n\t\"\"\"\n\tif cls._instance is None:\n\t\tcls._instance = super().__new__(cls)\n\treturn cls._instance\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.start_new_spinner","title":"start_new_spinner","text":"<pre><code>start_new_spinner(message: str) -&gt; None\n</code></pre> <p>Handles the start of a new spinner.</p> <p>Adds the new spinner message to the stack and updates the tree display.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def start_new_spinner(self, message: str) -&gt; None:\n\t\"\"\"Handles the start of a new spinner.\n\n\tAdds the new spinner message to the stack and updates the tree display.\n\t\"\"\"\n\tself.spinner_message_stack.append(message)\n\tself._start_status_cm_for_tree()\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.stop_current_spinner_and_resume_parent","title":"stop_current_spinner_and_resume_parent","text":"<pre><code>stop_current_spinner_and_resume_parent() -&gt; None\n</code></pre> <p>Handles the end of the current spinner.</p> <p>Pops the top spinner from the stack and updates the tree display.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def stop_current_spinner_and_resume_parent(self) -&gt; None:\n\t\"\"\"Handles the end of the current spinner.\n\n\tPops the top spinner from the stack and updates the tree display.\n\t\"\"\"\n\tif self.spinner_message_stack:\n\t\tself.spinner_message_stack.pop()  # Remove the spinner that just ended\n\n\tif self.spinner_message_stack:  # If any spinners remain\n\t\tself._start_status_cm_for_tree()\n\telse:  # No spinners left on stack\n\t\tself._stop_active_status_cm()\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.temporarily_halt_visual_spinner","title":"temporarily_halt_visual_spinner","text":"<pre><code>temporarily_halt_visual_spinner() -&gt; bool\n</code></pre> <p>Stops the current visual spinner if one is active.</p> <p>Used when a progress bar is about to take over. Returns True if a visual spinner was halted, False otherwise.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def temporarily_halt_visual_spinner(self) -&gt; bool:\n\t\"\"\"Stops the current visual spinner if one is active.\n\n\tUsed when a progress bar is about to take over.\n\tReturns True if a visual spinner was halted, False otherwise.\n\t\"\"\"\n\tif self.active_rich_status_cm:\n\t\tself._stop_active_status_cm()  # This already sets tree_display_active to False\n\t\treturn True\n\treturn False\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.resume_visual_spinner_if_needed","title":"resume_visual_spinner_if_needed","text":"<pre><code>resume_visual_spinner_if_needed() -&gt; None\n</code></pre> <p>Resumes a visual spinner for the top message on the stack.</p> <p>If the stack is not empty and no visual spinner is currently active. Used after a progress bar (that might have halted a spinner) finishes.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def resume_visual_spinner_if_needed(self) -&gt; None:\n\t\"\"\"Resumes a visual spinner for the top message on the stack.\n\n\tIf the stack is not empty and no visual spinner is currently active.\n\tUsed after a progress bar (that might have halted a spinner) finishes.\n\t\"\"\"\n\tif self.spinner_message_stack and not self.active_rich_status_cm:\n\t\tself._start_status_cm_for_tree()\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.progress_indicator","title":"progress_indicator","text":"<pre><code>progress_indicator(\n\tmessage: str,\n\tstyle: Literal[\"spinner\", \"progress\"] = \"spinner\",\n\ttotal: int | None = None,\n\ttransient: bool = False,\n) -&gt; Iterator[ProgressUpdater]\n</code></pre> <p>Standardized progress indicator that supports different styles uniformly.</p> <p>Manages nested spinners and interaction between spinners and progress bars to prevent UI flickering and ensure a clear display.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to display with the progress indicator.</p> required <code>style</code> <code>Literal['spinner', 'progress']</code> <p>The style of progress indicator ('spinner' or 'progress').</p> <code>'spinner'</code> <code>total</code> <code>int | None</code> <p>For determinate progress, the total units of work.</p> <code>None</code> <code>transient</code> <code>bool</code> <p>Whether the progress indicator should disappear after completion.</p> <code>False</code> <p>Yields:</p> Type Description <code>ProgressUpdater</code> <p>A callable (ProgressUpdater) for updating the progress/spinner.</p> <code>ProgressUpdater</code> <p>For spinners, the callable is a no-op accepting three ignored arguments.</p> <code>ProgressUpdater</code> <p>For progress bars, it accepts description, completed, and total (all optional).</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>@contextlib.contextmanager\ndef progress_indicator(\n\tmessage: str,\n\tstyle: Literal[\"spinner\", \"progress\"] = \"spinner\",\n\ttotal: int | None = None,\n\ttransient: bool = False,\n) -&gt; Iterator[ProgressUpdater]:\n\t\"\"\"Standardized progress indicator that supports different styles uniformly.\n\n\tManages nested spinners and interaction between spinners and progress bars\n\tto prevent UI flickering and ensure a clear display.\n\n\tArgs:\n\t    message: The message to display with the progress indicator.\n\t    style: The style of progress indicator ('spinner' or 'progress').\n\t    total: For determinate progress, the total units of work.\n\t    transient: Whether the progress indicator should disappear after completion.\n\n\tYields:\n\t    A callable (ProgressUpdater) for updating the progress/spinner.\n\t    For spinners, the callable is a no-op accepting three ignored arguments.\n\t    For progress bars, it accepts description, completed, and total (all optional).\n\t\"\"\"\n\tif os.environ.get(\"PYTEST_CURRENT_TEST\") or os.environ.get(\"CI\"):\n\t\tyield lambda _d=None, _c=None, _t=None: None\n\t\treturn\n\n\tspinner_state = SpinnerState()\n\n\tif style == \"spinner\":\n\t\tspinner_state.start_new_spinner(message)\n\t\ttry:\n\t\t\tyield lambda _d=None, _c=None, _t=None: None  # No-op for spinner\n\t\tfinally:\n\t\t\tspinner_state.stop_current_spinner_and_resume_parent()\n\telif style == \"progress\":\n\t\twas_spinner_visually_active = spinner_state.temporarily_halt_visual_spinner()\n\t\ttry:\n\t\t\tprogress = Progress(\n\t\t\t\tSpinnerColumn(),\n\t\t\t\tTextColumn(\"[progress.description]{task.description}\"),\n\t\t\t\tBarColumn(),\n\t\t\t\tTextColumn(\"{task.completed}/{task.total}\"),\n\t\t\t\tTimeElapsedColumn(),\n\t\t\t\ttransient=transient,\n\t\t\t\tconsole=console,  # Ensure it uses the same console\n\t\t\t)\n\t\t\twith progress:  # Progress context manager handles its own start/stop\n\t\t\t\ttask_id = progress.add_task(message, total=total)  # total can be None for indeterminate\n\t\t\t\tyield lambda description=None, completed=None, new_total=None: progress.update(\n\t\t\t\t\ttask_id,\n\t\t\t\t\tdescription=description,\n\t\t\t\t\tcompleted=completed,\n\t\t\t\t\ttotal=new_total if new_total is not None else total,  # Use new_total if provided\n\t\t\t\t)\n\t\tfinally:\n\t\t\t# Progress bar's 'with' context has exited.\n\t\t\t# If a spinner was active before this progress bar, try to resume it.\n\t\t\tif was_spinner_visually_active:\n\t\t\t\tspinner_state.resume_visual_spinner_if_needed()\n\telse:\n\t\t# Should not happen due to Literal type hint, but as a fallback\n\t\tlogger.warning(f\"Unknown progress_indicator style: {style}\")\n\t\tyield lambda _d=None, _c=None, _t=None: None\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.exit_with_error","title":"exit_with_error","text":"<pre><code>exit_with_error(\n\tmessage: str,\n\texit_code: int = 1,\n\texception: BaseException | None = None,\n) -&gt; None\n</code></pre> <p>Display an error message and exit.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message to display</p> required <code>exit_code</code> <code>int</code> <p>Exit code to use</p> <code>1</code> <code>exception</code> <code>BaseException | None</code> <p>Optional exception that caused the error</p> <code>None</code> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def exit_with_error(message: str, exit_code: int = 1, exception: BaseException | None = None) -&gt; None:\n\t\"\"\"\n\tDisplay an error message and exit.\n\n\tArgs:\n\t        message: Error message to display\n\t        exit_code: Exit code to use\n\t        exception: Optional exception that caused the error\n\n\t\"\"\"\n\tlogger.error(message, exc_info=exception)\n\tif exception is not None:\n\t\traise typer.Exit(exit_code) from exception\n\traise typer.Exit(exit_code)\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.handle_keyboard_interrupt","title":"handle_keyboard_interrupt","text":"<pre><code>handle_keyboard_interrupt() -&gt; None\n</code></pre> <p>Handles KeyboardInterrupt by printing a message and exiting cleanly.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def handle_keyboard_interrupt() -&gt; None:\n\t\"\"\"Handles KeyboardInterrupt by printing a message and exiting cleanly.\"\"\"\n\tconsole.print(\"\\n[yellow]Operation cancelled by user.[/yellow]\")\n\traise typer.Exit(130)  # Standard exit code for SIGINT\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.check_for_updates","title":"check_for_updates","text":"<pre><code>check_for_updates(is_verbose_param: bool) -&gt; None\n</code></pre> <p>Check PyPI for a new version of CodeMap and warn if available.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def check_for_updates(is_verbose_param: bool) -&gt; None:\n\t\"\"\"Check PyPI for a new version of CodeMap and warn if available.\"\"\"\n\ttry:\n\t\tpackage_name = \"codemap\"\n\t\tlogger.debug(f\"Checking for updates for package: {package_name}\")\n\n\t\tcurrent_v = parse_version(__version__)\n\t\tis_current_prerelease = current_v.is_prerelease\n\t\tlogger.debug(f\"Current version: {current_v} (Is pre-release: {is_current_prerelease})\")\n\n\t\treq = urllib.request.Request(\n\t\t\tf\"https://pypi.org/pypi/{package_name}/json\",\n\t\t\theaders={\"User-Agent\": f\"CodeMap-CLI-Update-Check/{__version__}\"},\n\t\t)\n\t\twith urllib.request.urlopen(req, timeout=5) as response:  # noqa: S310\n\t\t\tif response.status == HTTPStatus.OK:\n\t\t\t\tdata = json.load(response)\n\t\t\t\tpypi_releases = data.get(\"releases\", {})\n\t\t\t\tif not pypi_releases:\n\t\t\t\t\tlogger.debug(\"No releases found in PyPI response.\")\n\t\t\t\t\treturn\n\n\t\t\t\tvalid_pypi_versions_str = []\n\t\t\t\tfor version_str, release_files_list in pypi_releases.items():\n\t\t\t\t\tif not release_files_list:  # Skip if no files for this version\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t# Consider version yanked if all its files are yanked\n\t\t\t\t\tversion_is_yanked = all(file_info.get(\"yanked\", False) for file_info in release_files_list)\n\t\t\t\t\tif not version_is_yanked:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t# Ensure the version string can be parsed and has a release segment\n\t\t\t\t\t\t\tif parse_version(version_str).release is not None:\n\t\t\t\t\t\t\t\tvalid_pypi_versions_str.append(version_str)\n\t\t\t\t\t\texcept InvalidVersion:  # Catch specific exception\n\t\t\t\t\t\t\tlogger.debug(f\"Could not parse version string from PyPI: {version_str}\")\n\n\t\t\t\tif not valid_pypi_versions_str:\n\t\t\t\t\tlogger.debug(\"No valid, non-yanked releases found on PyPI after filtering.\")\n\t\t\t\t\treturn\n\n\t\t\t\tall_pypi_versions = sorted(\n\t\t\t\t\t[parse_version(v) for v in valid_pypi_versions_str],\n\t\t\t\t\treverse=True,\n\t\t\t\t)\n\n\t\t\t\tif not all_pypi_versions:\n\t\t\t\t\tlogger.debug(\"No valid parseable releases found on PyPI after filtering.\")\n\t\t\t\t\treturn\n\n\t\t\t\tlatest_candidate_v = None\n\t\t\t\tif is_current_prerelease:\n\t\t\t\t\t# If current is pre-release, consider the absolute latest version from PyPI\n\t\t\t\t\tlatest_candidate_v = all_pypi_versions[0]\n\t\t\t\telse:\n\t\t\t\t\t# If current is stable, consider the latest stable version from PyPI\n\t\t\t\t\tstable_pypi_versions = [v for v in all_pypi_versions if not v.is_prerelease]\n\t\t\t\t\tif stable_pypi_versions:\n\t\t\t\t\t\tlatest_candidate_v = stable_pypi_versions[0]\n\n\t\t\t\tif latest_candidate_v:\n\t\t\t\t\tlogger.debug(f\"Latest candidate version for comparison: {latest_candidate_v}\")\n\t\t\t\t\tif latest_candidate_v &gt; current_v:\n\t\t\t\t\t\ttyper.secho(\n\t\t\t\t\t\t\tf\"\\n[!] A new version of CodeMap is available: {latest_candidate_v} (You have {current_v})\",\n\t\t\t\t\t\t\tfg=typer.colors.YELLOW,\n\t\t\t\t\t\t)\n\t\t\t\t\t\ttyper.secho(\n\t\t\t\t\t\t\tf\"[!] To update, run: pip install --upgrade {package_name}\",\n\t\t\t\t\t\t\tfg=typer.colors.YELLOW,\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.debug(\"No newer version found on PyPI for current version type (stable/prerelease).\")\n\t\t\telse:\n\t\t\t\tlogger.debug(f\"Failed to fetch update info from PyPI. Status: {response.status}\")\n\n\texcept urllib.error.URLError as e:\n\t\tlogger.debug(f\"Could not connect to PyPI to check for updates (URLError): {e.reason}\")\n\texcept json.JSONDecodeError:\n\t\tlogger.debug(\"Could not parse PyPI response as JSON.\")\n\texcept TimeoutError:\n\t\tlogger.debug(\"Timeout while checking for updates on PyPI.\")\n\texcept Exception as e:  # noqa: BLE001\n\t\tlogger.debug(f\"An unexpected error occurred during update check: {e}\", exc_info=is_verbose_param)\n</code></pre>"},{"location":"api/utils/docker_utils/","title":"Docker Utils","text":"<p>Utilities for working with Docker containers directly via Python.</p>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.DEFAULT_TIMEOUT","title":"DEFAULT_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TIMEOUT = 60\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.HTTP_OK","title":"HTTP_OK  <code>module-attribute</code>","text":"<pre><code>HTTP_OK = 200\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_IMAGE","title":"QDRANT_IMAGE  <code>module-attribute</code>","text":"<pre><code>QDRANT_IMAGE = 'qdrant/qdrant:latest'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_CONTAINER_NAME","title":"QDRANT_CONTAINER_NAME  <code>module-attribute</code>","text":"<pre><code>QDRANT_CONTAINER_NAME = 'codemap-qdrant'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_HOST_PORT","title":"QDRANT_HOST_PORT  <code>module-attribute</code>","text":"<pre><code>QDRANT_HOST_PORT = 6333\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_HTTP_PORT","title":"QDRANT_HTTP_PORT  <code>module-attribute</code>","text":"<pre><code>QDRANT_HTTP_PORT = 6333\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_GRPC_PORT","title":"QDRANT_GRPC_PORT  <code>module-attribute</code>","text":"<pre><code>QDRANT_GRPC_PORT = 6334\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_STORAGE_PATH","title":"QDRANT_STORAGE_PATH  <code>module-attribute</code>","text":"<pre><code>QDRANT_STORAGE_PATH = '.codemap_cache/qdrant'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_IMAGE","title":"POSTGRES_IMAGE  <code>module-attribute</code>","text":"<pre><code>POSTGRES_IMAGE = 'postgres:latest'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_CONTAINER_NAME","title":"POSTGRES_CONTAINER_NAME  <code>module-attribute</code>","text":"<pre><code>POSTGRES_CONTAINER_NAME = 'codemap-postgres'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_HOST_PORT","title":"POSTGRES_HOST_PORT  <code>module-attribute</code>","text":"<pre><code>POSTGRES_HOST_PORT = 5432\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_ENV","title":"POSTGRES_ENV  <code>module-attribute</code>","text":"<pre><code>POSTGRES_ENV = {\n\t\"POSTGRES_PASSWORD\": \"postgres\",\n\t\"POSTGRES_USER\": \"postgres\",\n\t\"POSTGRES_DB\": \"codemap\",\n}\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_STORAGE_PATH","title":"POSTGRES_STORAGE_PATH  <code>module-attribute</code>","text":"<pre><code>POSTGRES_STORAGE_PATH = '.codemap_cache/postgres_data'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.is_docker_running","title":"is_docker_running  <code>async</code>","text":"<pre><code>is_docker_running() -&gt; bool\n</code></pre> <p>Check if the Docker daemon is running.</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def is_docker_running() -&gt; bool:\n\t\"\"\"Check if the Docker daemon is running.\"\"\"\n\n\tdef _check_docker_sync() -&gt; bool:\n\t\tclient = None\n\t\ttry:\n\t\t\tclient = docker.from_env()\n\t\t\tclient.ping()\n\t\t\treturn True\n\t\texcept DockerException:\n\t\t\treturn False\n\t\tfinally:\n\t\t\tif client is not None:\n\t\t\t\tclient.close()\n\t\treturn False\n\n\treturn await asyncio.to_thread(_check_docker_sync)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.is_container_running","title":"is_container_running  <code>async</code>","text":"<pre><code>is_container_running(container_name: str) -&gt; bool\n</code></pre> <p>Check if a specific Docker container is running.</p> <p>Parameters:</p> Name Type Description Default <code>container_name</code> <code>str</code> <p>Name of the container to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the container is running, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def is_container_running(container_name: str) -&gt; bool:\n\t\"\"\"\n\tCheck if a specific Docker container is running.\n\n\tArgs:\n\t    container_name: Name of the container to check\n\n\tReturns:\n\t    True if the container is running, False otherwise\n\n\t\"\"\"\n\n\tdef _check_container_sync(name: str) -&gt; bool:\n\t\tclient = None\n\t\ttry:\n\t\t\tclient = docker.from_env()\n\t\t\ttry:\n\t\t\t\tcontainer = cast(\"Container\", client.containers.get(name))\n\t\t\t\treturn container.status == \"running\"\n\t\t\texcept NotFound:\n\t\t\t\treturn False\n\t\texcept DockerException:\n\t\t\tlogger.exception(\"Docker error while checking container status for %s\", name)\n\t\t\treturn False\n\t\tfinally:\n\t\t\tif client is not None:\n\t\t\t\tclient.close()\n\t\treturn False\n\n\treturn await asyncio.to_thread(_check_container_sync, container_name)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.pull_image_if_needed","title":"pull_image_if_needed  <code>async</code>","text":"<pre><code>pull_image_if_needed(image_name: str) -&gt; bool\n</code></pre> <p>Pull a Docker image if it's not already available locally.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of the image to pull</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def pull_image_if_needed(image_name: str) -&gt; bool:\n\t\"\"\"\n\tPull a Docker image if it's not already available locally.\n\n\tArgs:\n\t    image_name: Name of the image to pull\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\n\tdef _pull_image_sync(name: str) -&gt; bool:\n\t\tclient = None\n\t\ttry:\n\t\t\tclient = docker.from_env()\n\t\t\ttry:\n\t\t\t\tclient.images.get(name)\n\t\t\t\tlogger.info(f\"Image {name} already exists locally\")\n\t\t\t\treturn True\n\t\t\texcept ImageNotFound:\n\t\t\t\tlogger.info(f\"Pulling image {name}...\")\n\t\t\t\ttry:\n\t\t\t\t\tclient.images.pull(name)\n\t\t\t\t\tlogger.info(f\"Successfully pulled image {name}\")\n\t\t\t\t\treturn True\n\t\t\t\texcept APIError:\n\t\t\t\t\tlogger.exception(f\"Failed to pull image {name}\")\n\t\t\t\t\treturn False\n\t\texcept DockerException:  # Catch potential errors from images.get()\n\t\t\tlogger.exception(f\"Docker error while checking image {name}\")\n\t\t\treturn False\n\t\tfinally:\n\t\t\tif client is not None:\n\t\t\t\tclient.close()\n\t\treturn False\n\n\treturn await asyncio.to_thread(_pull_image_sync, image_name)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.ensure_volume_path_exists","title":"ensure_volume_path_exists  <code>async</code>","text":"<pre><code>ensure_volume_path_exists(path: str) -&gt; None\n</code></pre> <p>Ensure the host path for a volume exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to ensure exists</p> required Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def ensure_volume_path_exists(path: str) -&gt; None:\n\t\"\"\"\n\tEnsure the host path for a volume exists.\n\n\tArgs:\n\t    path: Path to ensure exists\n\n\t\"\"\"\n\n\tdef _ensure_path_sync(p: str) -&gt; None:\n\t\tPath(p).mkdir(parents=True, exist_ok=True)\n\n\tawait asyncio.to_thread(_ensure_path_sync, path)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.start_qdrant_container","title":"start_qdrant_container  <code>async</code>","text":"<pre><code>start_qdrant_container() -&gt; bool\n</code></pre> <p>Start the Qdrant container.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def start_qdrant_container() -&gt; bool:\n\t\"\"\"\n\tStart the Qdrant container.\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\n\tdef _start_qdrant_sync() -&gt; bool:\n\t\tclient = None\n\t\ttry:\n\t\t\tclient = docker.from_env()\n\n\t\t\t# Ensure image is available (This function is already async, called separately)\n\t\t\t# if not await pull_image_if_needed(QDRANT_IMAGE):\n\t\t\t# \treturn False\n\n\t\t\t# Ensure storage directory exists (This function is already async, called separately)\n\t\t\t# await ensure_volume_path_exists(QDRANT_STORAGE_PATH)\n\n\t\t\t# Check if container already exists\n\t\t\ttry:\n\t\t\t\tcontainer = cast(\"Container\", client.containers.get(QDRANT_CONTAINER_NAME))\n\t\t\t\tif container.status == \"running\":\n\t\t\t\t\tlogger.info(f\"Container {QDRANT_CONTAINER_NAME} is already running\")\n\t\t\t\t\treturn True\n\n\t\t\t\t# If container exists but is not running, start it\n\t\t\t\tlogger.info(f\"Starting existing container {QDRANT_CONTAINER_NAME}\")\n\t\t\t\tcontainer.start()\n\t\t\t\tlogger.info(f\"Started container {QDRANT_CONTAINER_NAME}\")\n\t\t\t\treturn True\n\n\t\t\texcept NotFound:\n\t\t\t\t# Container doesn't exist, create and start it\n\t\t\t\tabs_storage_path = str(Path(QDRANT_STORAGE_PATH).absolute())\n\n\t\t\t\tlogger.info(f\"Creating and starting container {QDRANT_CONTAINER_NAME}\")\n\n\t\t\t\t# Define volume binding in Docker SDK format\n\t\t\t\tvolumes: list[str] = [f\"{abs_storage_path}:/qdrant/storage:rw\"]\n\n\t\t\t\t# Define port mapping\n\t\t\t\tports: dict[str, int | list[int] | tuple[str, int] | None] = {\n\t\t\t\t\tf\"{QDRANT_HTTP_PORT}/tcp\": QDRANT_HOST_PORT,\n\t\t\t\t\tf\"{QDRANT_GRPC_PORT}/tcp\": QDRANT_GRPC_PORT,\n\t\t\t\t}\n\n\t\t\t\trestart_policy = {\"Name\": \"always\"}\n\n\t\t\t\tclient.containers.run(\n\t\t\t\t\timage=QDRANT_IMAGE,\n\t\t\t\t\tname=QDRANT_CONTAINER_NAME,\n\t\t\t\t\tports=ports,\n\t\t\t\t\tvolumes=volumes,\n\t\t\t\t\tdetach=True,\n\t\t\t\t\trestart_policy=restart_policy,  # type: ignore[arg-type]\n\t\t\t\t)\n\t\t\t\tlogger.info(f\"Created and started container {QDRANT_CONTAINER_NAME}\")\n\t\t\t\treturn True\n\n\t\texcept DockerException:\n\t\t\tlogger.exception(\"Docker error while starting Qdrant container\")\n\t\t\treturn False\n\t\tfinally:\n\t\t\tif client is not None:\n\t\t\t\tclient.close()\n\t\treturn False\n\n\t# Ensure image is available\n\tif not await pull_image_if_needed(QDRANT_IMAGE):\n\t\treturn False\n\n\t# Ensure storage directory exists\n\tawait ensure_volume_path_exists(QDRANT_STORAGE_PATH)\n\n\treturn await asyncio.to_thread(_start_qdrant_sync)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.start_postgres_container","title":"start_postgres_container  <code>async</code>","text":"<pre><code>start_postgres_container() -&gt; bool\n</code></pre> <p>Start the PostgreSQL container.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def start_postgres_container() -&gt; bool:\n\t\"\"\"\n\tStart the PostgreSQL container.\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\n\tdef _start_postgres_sync() -&gt; bool:\n\t\tclient = None\n\t\ttry:\n\t\t\tclient = docker.from_env()\n\n\t\t\t# Ensure image is available (This function is already async, called separately)\n\t\t\t# if not await pull_image_if_needed(POSTGRES_IMAGE):\n\t\t\t# \treturn False\n\n\t\t\t# Ensure storage directory exists (This function is already async, called separately)\n\t\t\t# await ensure_volume_path_exists(POSTGRES_STORAGE_PATH)\n\n\t\t\t# Check if container already exists\n\t\t\ttry:\n\t\t\t\tcontainer = cast(\"Container\", client.containers.get(POSTGRES_CONTAINER_NAME))\n\t\t\t\tif container.status == \"running\":\n\t\t\t\t\tlogger.info(f\"Container {POSTGRES_CONTAINER_NAME} is already running\")\n\t\t\t\t\treturn True\n\n\t\t\t\t# If container exists but is not running, start it\n\t\t\t\tlogger.info(f\"Starting existing container {POSTGRES_CONTAINER_NAME}\")\n\t\t\t\tcontainer.start()\n\t\t\t\tlogger.info(f\"Started container {POSTGRES_CONTAINER_NAME}\")\n\t\t\t\treturn True\n\n\t\t\texcept NotFound:\n\t\t\t\t# Container doesn't exist, create and start it\n\t\t\t\tabs_storage_path = str(Path(POSTGRES_STORAGE_PATH).absolute())\n\n\t\t\t\tlogger.info(f\"Creating and starting container {POSTGRES_CONTAINER_NAME}\")\n\n\t\t\t\t# Define volume binding in Docker SDK format\n\t\t\t\tvolumes: list[str] = [f\"{abs_storage_path}:/var/lib/postgresql/data:rw\"]\n\n\t\t\t\t# Define port mapping\n\t\t\t\tports: dict[str, int | list[int] | tuple[str, int] | None] = {\"5432/tcp\": POSTGRES_HOST_PORT}\n\n\t\t\t\trestart_policy = {\"Name\": \"always\"}\n\n\t\t\t\tclient.containers.run(\n\t\t\t\t\timage=POSTGRES_IMAGE,\n\t\t\t\t\tname=POSTGRES_CONTAINER_NAME,\n\t\t\t\t\tports=ports,\n\t\t\t\t\tvolumes=volumes,\n\t\t\t\t\tenvironment=POSTGRES_ENV,\n\t\t\t\t\tdetach=True,\n\t\t\t\t\trestart_policy=restart_policy,  # type: ignore[arg-type]\n\t\t\t\t)\n\t\t\t\tlogger.info(f\"Created and started container {POSTGRES_CONTAINER_NAME}\")\n\t\t\t\treturn True\n\n\t\texcept DockerException:\n\t\t\tlogger.exception(\"Docker error while starting PostgreSQL container\")\n\t\t\treturn False\n\t\tfinally:\n\t\t\tif client is not None:\n\t\t\t\tclient.close()\n\t\treturn False\n\n\t# Ensure image is available\n\tif not await pull_image_if_needed(POSTGRES_IMAGE):\n\t\treturn False\n\n\t# Ensure storage directory exists\n\tawait ensure_volume_path_exists(POSTGRES_STORAGE_PATH)\n\n\treturn await asyncio.to_thread(_start_postgres_sync)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.check_qdrant_health","title":"check_qdrant_health  <code>async</code>","text":"<pre><code>check_qdrant_health(\n\turl: str = f\"http://localhost:{QDRANT_HOST_PORT}\",\n) -&gt; bool\n</code></pre> <p>Check if Qdrant service is healthy and ready to accept connections.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Base URL of the Qdrant service</p> <code>f'http://localhost:{QDRANT_HOST_PORT}'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if Qdrant is healthy, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def check_qdrant_health(url: str = f\"http://localhost:{QDRANT_HOST_PORT}\") -&gt; bool:\n\t\"\"\"\n\tCheck if Qdrant service is healthy and ready to accept connections.\n\n\tArgs:\n\t    url: Base URL of the Qdrant service\n\n\tReturns:\n\t    True if Qdrant is healthy, False otherwise\n\n\t\"\"\"\n\tfrom httpx import AsyncClient, RequestError\n\n\thealth_url = f\"{url}/healthz\"\n\tstart_time = time.time()\n\n\tasync with AsyncClient() as client:\n\t\ttry:\n\t\t\tasync with asyncio.timeout(DEFAULT_TIMEOUT):\n\t\t\t\twhile True:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresponse = await client.get(health_url)\n\t\t\t\t\t\tif response.status_code == HTTP_OK:\n\t\t\t\t\t\t\tlogger.info(\"Qdrant service is healthy (responded 200 OK)\")\n\t\t\t\t\t\t\treturn True\n\t\t\t\t\texcept RequestError:\n\t\t\t\t\t\tpass\n\n\t\t\t\t\tif time.time() - start_time &gt;= DEFAULT_TIMEOUT:\n\t\t\t\t\t\tbreak\n\n\t\t\t\t\t# Wait before trying again\n\t\t\t\t\tawait asyncio.sleep(1)\n\t\texcept TimeoutError:\n\t\t\tpass\n\n\tlogger.error(f\"Qdrant service did not become healthy within {DEFAULT_TIMEOUT} seconds\")\n\treturn False\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.ensure_qdrant_running","title":"ensure_qdrant_running  <code>async</code>","text":"<pre><code>ensure_qdrant_running(\n\twait_for_health: bool = True,\n\tqdrant_url: str = f\"http://localhost:{QDRANT_HOST_PORT}\",\n) -&gt; tuple[bool, str]\n</code></pre> <p>Ensure the Qdrant container is running, starting it if needed.</p> <p>Parameters:</p> Name Type Description Default <code>wait_for_health</code> <code>bool</code> <p>Whether to wait for Qdrant to be healthy</p> <code>True</code> <code>qdrant_url</code> <code>str</code> <p>URL of the Qdrant service</p> <code>f'http://localhost:{QDRANT_HOST_PORT}'</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple of (success, message)</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def ensure_qdrant_running(\n\twait_for_health: bool = True, qdrant_url: str = f\"http://localhost:{QDRANT_HOST_PORT}\"\n) -&gt; tuple[bool, str]:\n\t\"\"\"\n\tEnsure the Qdrant container is running, starting it if needed.\n\n\tArgs:\n\t    wait_for_health: Whether to wait for Qdrant to be healthy\n\t    qdrant_url: URL of the Qdrant service\n\n\tReturns:\n\t    Tuple of (success, message)\n\n\t\"\"\"\n\tif not await is_docker_running():\n\t\treturn False, \"Docker daemon is not running\"\n\n\t# Check if Qdrant service is already running\n\tqdrant_running = False\n\n\tfrom httpx import AsyncClient, HTTPError, RequestError\n\n\ttry:\n\t\t# Try a direct HTTP request first to see if Qdrant is up\n\t\tasync with AsyncClient(timeout=3.0) as client:\n\t\t\ttry:\n\t\t\t\tresponse = await client.get(f\"{qdrant_url}/health\")\n\t\t\t\tif response.status_code == HTTP_OK:\n\t\t\t\t\tlogger.info(\"Qdrant is already available via HTTP\")\n\t\t\t\t\tqdrant_running = True\n\t\t\texcept RequestError:\n\t\t\t\t# HTTP request failed, now check if it's running in Docker\n\t\t\t\tqdrant_running = await is_container_running(QDRANT_CONTAINER_NAME)\n\texcept (HTTPError, ConnectionError, OSError) as e:\n\t\tlogger.warning(f\"Error checking Qdrant service: {e}\")\n\n\t# Start services if needed\n\tif not qdrant_running:\n\t\tlogger.info(\"Qdrant service is not running, starting container...\")\n\t\tstarted = await start_qdrant_container()\n\t\tif not started:\n\t\t\treturn False, \"Failed to start Qdrant container\"\n\n\t\tif wait_for_health:\n\t\t\t# Wait for Qdrant to be healthy\n\t\t\tlogger.info(f\"Waiting for Qdrant service to be healthy (timeout: {DEFAULT_TIMEOUT}s)...\")\n\t\t\thealthy = await check_qdrant_health(qdrant_url)\n\t\t\tif not healthy:\n\t\t\t\treturn False, \"Qdrant service failed to become healthy within the timeout period\"\n\n\treturn True, \"Qdrant container is running\"\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.ensure_postgres_running","title":"ensure_postgres_running  <code>async</code>","text":"<pre><code>ensure_postgres_running() -&gt; tuple[bool, str]\n</code></pre> <p>Ensure the PostgreSQL container is running, starting it if needed.</p> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple of (success, message)</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def ensure_postgres_running() -&gt; tuple[bool, str]:\n\t\"\"\"\n\tEnsure the PostgreSQL container is running, starting it if needed.\n\n\tReturns:\n\t    Tuple of (success, message)\n\n\t\"\"\"\n\tif not await is_docker_running():\n\t\treturn False, \"Docker daemon is not running\"\n\n\t# Check if PostgreSQL container is already running\n\tpostgres_running = await is_container_running(POSTGRES_CONTAINER_NAME)\n\n\t# Start container if needed\n\tif not postgres_running:\n\t\tlogger.info(\"PostgreSQL service is not running, starting container...\")\n\t\tstarted = await start_postgres_container()\n\t\tif not started:\n\t\t\treturn False, \"Failed to start PostgreSQL container\"\n\n\treturn True, \"PostgreSQL container is running\"\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.stop_container","title":"stop_container  <code>async</code>","text":"<pre><code>stop_container(container_name: str) -&gt; bool\n</code></pre> <p>Stop a Docker container.</p> <p>Parameters:</p> Name Type Description Default <code>container_name</code> <code>str</code> <p>Name of the container to stop</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def stop_container(container_name: str) -&gt; bool:\n\t\"\"\"\n\tStop a Docker container.\n\n\tArgs:\n\t    container_name: Name of the container to stop\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\n\tdef _stop_sync(name: str) -&gt; bool:\n\t\tclient = None\n\t\ttry:\n\t\t\tclient = docker.from_env()\n\t\t\ttry:\n\t\t\t\tcontainer = cast(\"Container\", client.containers.get(name))\n\t\t\t\tif container.status == \"running\":\n\t\t\t\t\tlogger.info(f\"Stopping container {name}\")\n\t\t\t\t\tcontainer.stop(timeout=10)  # Wait up to 10 seconds for clean shutdown\n\t\t\t\t\tlogger.info(f\"Stopped container {name}\")\n\t\t\t\treturn True\n\t\t\texcept NotFound:\n\t\t\t\tlogger.info(f\"Container {name} does not exist\")\n\t\t\t\treturn True\n\t\texcept DockerException:  # Catch DockerException from client.containers.get or container.stop\n\t\t\tlogger.exception(f\"Docker error while stopping container {name}\")\n\t\t\treturn False\n\t\tfinally:\n\t\t\tif client is not None:\n\t\t\t\tclient.close()\n\t\treturn False\n\n\treturn await asyncio.to_thread(_stop_sync, container_name)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.stop_all_codemap_containers","title":"stop_all_codemap_containers  <code>async</code>","text":"<pre><code>stop_all_codemap_containers() -&gt; tuple[bool, str]\n</code></pre> <p>Stop all CodeMap containers.</p> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple of (success, message)</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def stop_all_codemap_containers() -&gt; tuple[bool, str]:\n\t\"\"\"\n\tStop all CodeMap containers.\n\n\tReturns:\n\t    Tuple of (success, message)\n\n\t\"\"\"\n\tcontainers = [QDRANT_CONTAINER_NAME, POSTGRES_CONTAINER_NAME]\n\tsuccess = True\n\n\tfor container_name in containers:\n\t\tif not await stop_container(container_name):\n\t\t\tsuccess = False\n\n\tif success:\n\t\treturn True, \"All CodeMap containers stopped successfully\"\n\treturn False, \"Failed to stop some CodeMap containers\"\n</code></pre>"},{"location":"api/utils/file_utils/","title":"File Utils","text":"<p>Utility functions for file operations in CodeMap.</p>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.count_tokens","title":"count_tokens","text":"<pre><code>count_tokens(file_path: Path) -&gt; int\n</code></pre> <p>Rough estimation of tokens in a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to count tokens in.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens in the file.</p> Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def count_tokens(file_path: Path) -&gt; int:\n\t\"\"\"\n\tRough estimation of tokens in a file.\n\n\tArgs:\n\t    file_path: Path to the file to count tokens in.\n\n\tReturns:\n\t    Estimated number of tokens in the file.\n\n\t\"\"\"\n\ttry:\n\t\twith file_path.open(encoding=\"utf-8\") as f:\n\t\t\tcontent = f.read()\n\t\t\t# Simple tokenization by whitespace\n\t\t\treturn len(content.split())\n\texcept (OSError, UnicodeDecodeError):\n\t\treturn 0\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.read_file_content","title":"read_file_content","text":"<pre><code>read_file_content(file_path: Path | str) -&gt; str | None\n</code></pre> <p>Read content from a file with proper error handling.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Content of the file as string, or None if the file cannot be read</p> Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def read_file_content(file_path: Path | str) -&gt; str | None:\n\t\"\"\"\n\tRead content from a file with proper error handling.\n\n\tArgs:\n\t    file_path: Path to the file to read\n\n\tReturns:\n\t    Content of the file as string, or None if the file cannot be read\n\n\t\"\"\"\n\tpath_obj = Path(file_path)\n\ttry:\n\t\twith path_obj.open(\"r\", encoding=\"utf-8\") as f:\n\t\t\treturn f.read()\n\texcept FileNotFoundError:\n\t\t# Handle case where file was tracked but has been deleted\n\t\tlogger.debug(f\"File not found: {path_obj} - possibly deleted since last tracked\")\n\t\treturn None\n\texcept UnicodeDecodeError:\n\t\t# Try to read as binary and then decode with error handling\n\t\tlogger.warning(\"File %s contains non-UTF-8 characters, attempting to decode with errors='replace'\", path_obj)\n\t\ttry:\n\t\t\twith path_obj.open(\"rb\") as f:\n\t\t\t\tcontent = f.read()\n\t\t\t\treturn content.decode(\"utf-8\", errors=\"replace\")\n\t\texcept (OSError, FileNotFoundError):\n\t\t\tlogger.debug(f\"Unable to read file as binary: {path_obj}\")\n\t\t\treturn None\n\texcept OSError as e:\n\t\t# Handle other file access errors\n\t\tlogger.debug(f\"Error reading file {path_obj}: {e}\")\n\t\treturn None\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.ensure_directory_exists","title":"ensure_directory_exists","text":"<pre><code>ensure_directory_exists(dir_path: Path) -&gt; None\n</code></pre> <p>Ensure that a directory exists, creating it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>The path to the directory.</p> required Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def ensure_directory_exists(dir_path: Path) -&gt; None:\n\t\"\"\"\n\tEnsure that a directory exists, creating it if necessary.\n\n\tArgs:\n\t    dir_path (Path): The path to the directory.\n\n\t\"\"\"\n\tif not dir_path.exists():\n\t\tlogger.info(f\"Creating directory: {dir_path}\")\n\t\ttry:\n\t\t\tdir_path.mkdir(parents=True, exist_ok=True)\n\t\texcept OSError:\n\t\t\tlogger.exception(f\"Failed to create directory {dir_path}\")\n\t\t\traise\n\telif not dir_path.is_dir():\n\t\tlogger.error(f\"Path exists but is not a directory: {dir_path}\")\n\t\tmsg = f\"Path exists but is not a directory: {dir_path}\"\n\t\traise NotADirectoryError(msg)\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.is_binary_file","title":"is_binary_file","text":"<pre><code>is_binary_file(file_path: Path) -&gt; bool\n</code></pre> <p>Check if a file is binary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file is binary, False otherwise</p> Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def is_binary_file(file_path: Path) -&gt; bool:\n\t\"\"\"\n\tCheck if a file is binary.\n\n\tArgs:\n\t        file_path: Path to the file\n\n\tReturns:\n\t        True if the file is binary, False otherwise\n\n\t\"\"\"\n\t# Skip files larger than 10 MB\n\ttry:\n\t\tif file_path.stat().st_size &gt; 10 * 1024 * 1024:\n\t\t\treturn True\n\n\t\t# Try to read as text\n\t\twith file_path.open(encoding=\"utf-8\") as f:\n\t\t\tchunk = f.read(1024)\n\t\t\treturn \"\\0\" in chunk\n\texcept UnicodeDecodeError:\n\t\treturn True\n\texcept (OSError, PermissionError):\n\t\treturn True\n</code></pre>"},{"location":"api/utils/git_hooks/","title":"Git Hooks","text":"<p>Utilities for managing and running git hooks directly.</p>"},{"location":"api/utils/git_hooks/#codemap.utils.git_hooks.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/git_hooks/#codemap.utils.git_hooks.HOOK_TYPES","title":"HOOK_TYPES  <code>module-attribute</code>","text":"<pre><code>HOOK_TYPES = Literal[\n\t\"pre-commit\",\n\t\"post-commit\",\n\t\"commit-msg\",\n\t\"pre-push\",\n\t\"post-checkout\",\n]\n</code></pre>"},{"location":"api/utils/git_hooks/#codemap.utils.git_hooks.get_git_hooks_dir","title":"get_git_hooks_dir","text":"<pre><code>get_git_hooks_dir(repo_root: Path | None = None) -&gt; Path\n</code></pre> <p>Get the .git/hooks directory for the current or given repo.</p> <p>Parameters:</p> Name Type Description Default <code>repo_root</code> <code>Path | None</code> <p>Path to the repo root. Defaults to cwd.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the .git/hooks directory.</p> Source code in <code>src/codemap/utils/git_hooks.py</code> <pre><code>def get_git_hooks_dir(repo_root: Path | None = None) -&gt; Path:\n\t\"\"\"\n\tGet the .git/hooks directory for the current or given repo.\n\n\tArgs:\n\t    repo_root: Path to the repo root. Defaults to cwd.\n\n\tReturns:\n\t    Path to the .git/hooks directory.\n\t\"\"\"\n\troot = repo_root or Path.cwd()\n\t# Find .git directory\n\tgit_dir = root / \".git\"\n\tif not git_dir.exists():\n\t\tmsg = f\".git directory not found at {git_dir}\"\n\t\traise FileNotFoundError(msg)\n\thooks_dir = git_dir / \"hooks\"\n\tif not hooks_dir.exists():\n\t\tmsg = f\".git/hooks directory not found at {hooks_dir}\"\n\t\traise FileNotFoundError(msg)\n\treturn hooks_dir\n</code></pre>"},{"location":"api/utils/git_hooks/#codemap.utils.git_hooks.hook_exists","title":"hook_exists","text":"<pre><code>hook_exists(\n\thook_type: HOOK_TYPES, repo_root: Path | None = None\n) -&gt; bool\n</code></pre> <p>Check if a given hook exists and is executable.</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>HOOK_TYPES</code> <p>The hook type (\"pre-commit\" or \"pre-push\").</p> required <code>repo_root</code> <code>Path | None</code> <p>Path to the repo root. Defaults to cwd.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the hook exists and is executable, False otherwise.</p> Source code in <code>src/codemap/utils/git_hooks.py</code> <pre><code>def hook_exists(hook_type: HOOK_TYPES, repo_root: Path | None = None) -&gt; bool:\n\t\"\"\"\n\tCheck if a given hook exists and is executable.\n\n\tArgs:\n\t    hook_type: The hook type (\"pre-commit\" or \"pre-push\").\n\t    repo_root: Path to the repo root. Defaults to cwd.\n\n\tReturns:\n\t    True if the hook exists and is executable, False otherwise.\n\t\"\"\"\n\thooks_dir = get_git_hooks_dir(repo_root)\n\thook_path = hooks_dir / hook_type\n\treturn bool(hook_path.exists() and hook_path.is_file() and (hook_path.stat().st_mode &amp; 0o111))\n</code></pre>"},{"location":"api/utils/git_hooks/#codemap.utils.git_hooks.run_hook","title":"run_hook","text":"<pre><code>run_hook(\n\thook_type: HOOK_TYPES, repo_root: Path | None = None\n) -&gt; int\n</code></pre> <p>Run the specified git hook directly using bash.</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>HOOK_TYPES</code> <p>The hook type (\"pre-commit\" or \"pre-push\").</p> required <code>repo_root</code> <code>Path | None</code> <p>Path to the repo root. Defaults to cwd.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The exit code of the hook process (0 if hook doesn't exist).</p> Source code in <code>src/codemap/utils/git_hooks.py</code> <pre><code>def run_hook(hook_type: HOOK_TYPES, repo_root: Path | None = None) -&gt; int:\n\t\"\"\"\n\tRun the specified git hook directly using bash.\n\n\tArgs:\n\t    hook_type: The hook type (\"pre-commit\" or \"pre-push\").\n\t    repo_root: Path to the repo root. Defaults to cwd.\n\n\tReturns:\n\t    The exit code of the hook process (0 if hook doesn't exist).\n\t\"\"\"\n\thooks_dir = get_git_hooks_dir(repo_root)\n\thook_path = hooks_dir / hook_type\n\tif not hook_path.exists():\n\t\tlogger.debug(f\"{hook_type} hook not found at {hook_path}\")\n\t\treturn 0\n\tif not (hook_path.stat().st_mode &amp; 0o111):\n\t\tlogger.warning(f\"{hook_type} hook at {hook_path} is not executable\")\n\t\treturn 1\n\ttry:\n\t\tlogger.info(f\"Running git hook: {hook_path}\")\n\t\tresult = subprocess.run([\"bash\", str(hook_path)], capture_output=True, text=True, check=False)  # noqa: S603, S607\n\t\tif result.stdout:\n\t\t\tlogger.info(f\"{hook_type} hook output:\\n{result.stdout}\")\n\t\tif result.stderr:\n\t\t\tlogger.warning(f\"{hook_type} hook error:\\n{result.stderr}\")\n\t\treturn result.returncode\n\texcept Exception:\n\t\tlogger.exception(f\"Failed to run {hook_type} hook\")\n\t\treturn 1\n</code></pre>"},{"location":"api/utils/git_hooks/#codemap.utils.git_hooks.run_all_hooks","title":"run_all_hooks","text":"<pre><code>run_all_hooks(\n\trepo_root: Path | None = None,\n) -&gt; dict[str, int]\n</code></pre> <p>Run all supported hooks (pre-commit, pre-push) if they exist.</p> <p>Parameters:</p> Name Type Description Default <code>repo_root</code> <code>Path | None</code> <p>Path to the repo root. Defaults to cwd.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict mapping hook type to exit code.</p> Source code in <code>src/codemap/utils/git_hooks.py</code> <pre><code>def run_all_hooks(repo_root: Path | None = None) -&gt; dict[str, int]:\n\t\"\"\"\n\tRun all supported hooks (pre-commit, pre-push) if they exist.\n\n\tArgs:\n\t    repo_root: Path to the repo root. Defaults to cwd.\n\n\tReturns:\n\t    Dict mapping hook type to exit code.\n\t\"\"\"\n\tresults = {}\n\thooks: list[HOOK_TYPES] = [\"pre-commit\", \"post-commit\", \"commit-msg\", \"pre-push\", \"post-checkout\"]\n\tfor hook in hooks:\n\t\tif hook_exists(hook, repo_root):\n\t\t\tresults[hook] = run_hook(hook, repo_root)\n\treturn results\n</code></pre>"},{"location":"api/utils/git_utils/","title":"Git Utils","text":"<p>Utilities for interacting with Git.</p>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitError","title":"GitError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for Git-related errors.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>class GitError(Exception):\n\t\"\"\"Custom exception for Git-related errors.\"\"\"\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext","title":"GitRepoContext","text":"<p>Context manager for efficient Git operations using pygit2.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>class GitRepoContext:\n\t\"\"\"Context manager for efficient Git operations using pygit2.\"\"\"\n\n\tlogger = logging.getLogger(__name__)\n\n\trepo_root: Path | None = None\n\t_instance: \"GitRepoContext | None\" = None\n\n\t@classmethod\n\tdef get_instance(cls) -&gt; \"GitRepoContext\":\n\t\t\"\"\"Get a cached instance of GitRepoContext for a given repo_path.\"\"\"\n\t\tif cls._instance is None:\n\t\t\tcls._instance = cls()\n\t\treturn cls._instance\n\n\t@classmethod\n\tdef get_repo_root(cls, path: Path | None = None) -&gt; Path:\n\t\t\"\"\"Get the root directory of the Git repository.\"\"\"\n\t\tgit_dir = discover_repository(str(path or Path.cwd()))\n\t\tif git_dir is None:\n\t\t\tmsg = \"Not a git repository\"\n\t\t\tlogger.error(msg)\n\t\t\traise GitError(msg)\n\t\treturn Path(git_dir)\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the GitRepoContext with the given repository path.\"\"\"\n\t\tif self.repo_root is None:\n\t\t\tself.repo_root = self.get_repo_root()\n\t\tself.repo = Repository(str(self.repo_root))\n\t\tself.branch = self.get_current_branch()\n\t\tself.exclude_patterns = self._get_exclude_patterns()\n\t\tself.tracked_files = self._get_tracked_files()\n\t\tself._blame_cache: dict[str, Blame] = {}  # Cache for blame objects\n\n\t@staticmethod\n\tdef _get_exclude_patterns() -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet the list of path patterns to exclude from processing.\n\n\t\tReturns:\n\t\t\tList of regex patterns for paths to exclude\n\t\t\"\"\"\n\t\tfrom codemap.config import ConfigLoader  # Local import to avoid cycles\n\n\t\tconfig_loader = ConfigLoader.get_instance()\n\t\tconfig_patterns = config_loader.get.sync.exclude_patterns\n\t\tdefault_patterns = [\n\t\t\tr\"^node_modules/\",\n\t\t\tr\"^\\.venv/\",\n\t\t\tr\"^venv/\",\n\t\t\tr\"^env/\",\n\t\t\tr\"^__pycache__/\",\n\t\t\tr\"^\\.mypy_cache/\",\n\t\t\tr\"^\\.pytest_cache/\",\n\t\t\tr\"^\\.ruff_cache/\",\n\t\t\tr\"^dist/\",\n\t\t\tr\"^build/\",\n\t\t\tr\"^\\.git/\",\n\t\t\tr\"^typings/\",\n\t\t\tr\"\\.pyc$\",\n\t\t\tr\"\\.pyo$\",\n\t\t\tr\"\\.so$\",\n\t\t\tr\"\\.dll$\",\n\t\t\tr\"\\.lib$\",\n\t\t\tr\"\\.a$\",\n\t\t\tr\"\\.o$\",\n\t\t\tr\"\\.class$\",\n\t\t\tr\"\\.jar$\",\n\t\t]\n\t\tpatterns = list(config_patterns)\n\t\tfor pattern in default_patterns:\n\t\t\tif pattern not in patterns:\n\t\t\t\tpatterns.append(pattern)\n\t\treturn patterns\n\n\tdef _should_exclude_path(self, file_path: str) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a file path should be excluded from processing based on patterns.\n\n\t\tArgs:\n\t\t\tfile_path: The file path to check\n\n\t\tReturns:\n\t\t\tTrue if the path should be excluded, False otherwise\n\t\t\"\"\"\n\t\tfor pattern in self.exclude_patterns:\n\t\t\tif re.search(pattern, file_path):\n\t\t\t\tself.logger.debug(f\"Excluding file from processing due to pattern '{pattern}': {file_path}\")\n\t\t\t\treturn True\n\t\treturn False\n\n\tdef _get_tracked_files(self) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tGet all tracked files in the Git repository with their blob hashes.\n\n\t\tReturns:\n\t\t\tdict[str, str]: A dictionary of tracked files with their blob hashes.\n\t\t\"\"\"\n\t\ttracked_files: dict[str, str] = {}\n\t\tfor entry in self.repo.index:\n\t\t\tif not self._should_exclude_path(entry.path):\n\t\t\t\ttracked_files[entry.path] = str(entry.id)\n\t\tself.logger.info(f\"Found {len(tracked_files)} tracked files in Git repository: {self.repo.path}\")\n\t\treturn tracked_files\n\n\tdef get_current_branch(self) -&gt; str:\n\t\t\"\"\"\n\t\tGet the current branch name of the Git repository.\n\n\t\tReturns:\n\t\t\tstr: The current branch name, or empty string if detached.\n\t\t\"\"\"\n\t\tif self.repo.head_is_detached:\n\t\t\treturn \"\"\n\t\treturn self.repo.head.shorthand or \"\"\n\n\tdef get_file_git_hash(self, file_path: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the Git hash (blob ID) for a specific tracked file.\n\n\t\tArgs:\n\t\t\tfile_path (str): The path to the file relative to the repository root.\n\n\t\tReturns:\n\t\t\tstr: The Git blob hash of the file, or empty string if not found.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tcommit = self.repo.head.peel(Commit)\n\t\t\tif commit is None:\n\t\t\t\tself.logger.warning(f\"HEAD does not point to a commit in repo {self.repo.path}\")\n\t\t\t\treturn \"\"\n\t\t\ttree = commit.tree\n\t\t\tentry = tree[file_path]\n\t\t\treturn str(entry.id)\n\t\texcept KeyError:\n\t\t\tself.logger.warning(f\"File {file_path} not found in HEAD tree of repo {self.repo.path}\")\n\t\t\treturn \"\"\n\t\texcept Exception:\n\t\t\tself.logger.exception(f\"Failed to get git hash for {file_path}\")\n\t\t\treturn \"\"\n\n\tdef get_git_blame(self, file_path: str, start_line: int, end_line: int) -&gt; list[GitBlameSchema]:\n\t\t\"\"\"\n\t\tGet the Git blame for a specific range of lines in a file.\n\n\t\tArgs:\n\t\t\tfile_path (str): The path to the file relative to the repository root.\n\t\t\tstart_line (int): The starting line number of the range.\n\t\t\tend_line (int): The ending line number of the range.\n\n\t\tReturns:\n\t\t\tlist[GitBlameSchema]: A list of Git blame results.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Check if the file is actually tracked by git\n\t\t\tfile_is_tracked = file_path in self.tracked_files\n\t\t\tif not file_is_tracked:\n\t\t\t\t# Skip blame lookup for untracked files\n\t\t\t\tlogger.debug(f\"File '{file_path}' is not tracked in git - skipping blame lookup\")\n\t\t\t\treturn []\n\n\t\t\t# Handle ambiguous file paths without full directory path\n\t\t\t# Only attempt path resolution if we have:\n\t\t\t# 1. A valid repo_root\n\t\t\t# 2. A simple filename without path separators\n\t\t\tif self.repo_root is not None and \"/\" not in file_path and \"\\\\\" not in file_path and self.tracked_files:\n\t\t\t\t# We have just a filename - try to find it in tracked files\n\t\t\t\tmatching_paths = [\n\t\t\t\t\ttracked_path\n\t\t\t\t\tfor tracked_path in self.tracked_files\n\t\t\t\t\tif tracked_path.endswith(f\"/{file_path}\") or tracked_path == file_path\n\t\t\t\t]\n\n\t\t\t\tif len(matching_paths) == 1:\n\t\t\t\t\t# Found exactly one match, use it\n\t\t\t\t\tfile_path = matching_paths[0]\n\t\t\t\t\tself.logger.debug(f\"Found unique match for '{file_path}': {matching_paths[0]}\")\n\t\t\t\telif len(matching_paths) &gt; 1:\n\t\t\t\t\t# Multiple matches, use the most specific one or log warning\n\t\t\t\t\tself.logger.warning(f\"Ambiguous file '{file_path}' has multiple matches: {matching_paths}\")\n\t\t\t\t\t# For now, pick the first candidate but this could be improved\n\t\t\t\t\tfile_path = matching_paths[0]\n\t\t\t\t\tself.logger.debug(f\"Using first match: {file_path}\")\n\t\t\t\telse:\n\t\t\t\t\tself.logger.warning(f\"No matching file found for '{file_path}' in tracked files\")\n\t\t\t\t\treturn []  # Return empty blame list if no match found\n\n\t\t\ttry:\n\t\t\t\tblame_obj: Blame\n\t\t\t\tif file_path not in self._blame_cache:\n\t\t\t\t\t# Ensure file_path is relative to repo root for pygit2.blame\n\t\t\t\t\tself._blame_cache[file_path] = self.repo.blame(file_path)\n\t\t\t\tblame_obj = self._blame_cache[file_path]\n\t\t\texcept KeyError:\n\t\t\t\t# File not found in git repository\n\t\t\t\tlogger.debug(f\"File '{file_path}' not found in git repository - skipping blame lookup\")\n\t\t\t\treturn []\n\n\t\t\t# Using a dictionary to collect unique commit information for the given line range\n\t\t\t# Key: commit_id_str, Value: GitBlameSchema\n\t\t\tprocessed_commits: dict[str, GitBlameSchema] = {}\n\n\t\t\tfor line_num_1_indexed in range(start_line, end_line + 1):\n\t\t\t\tif line_num_1_indexed &lt;= 0:\n\t\t\t\t\tcontinue  # Line numbers are 1-indexed\n\t\t\t\ttry:\n\t\t\t\t\t# pygit2.Blame.__getitem__ expects 0-indexed line number\n\t\t\t\t\thunk = blame_obj[line_num_1_indexed - 1]\n\t\t\t\texcept IndexError:\n\t\t\t\t\t# This can happen if start_line/end_line are outside the file's actual line count\n\t\t\t\t\tself.logger.warning(\n\t\t\t\t\t\tf\"Line {line_num_1_indexed} is out of range for \"\n\t\t\t\t\t\tf\"blame in file {file_path}. Total lines in \"\n\t\t\t\t\t\tf\"blame: {len(blame_obj)}.\"\n\t\t\t\t\t)\n\t\t\t\t\tcontinue\n\n\t\t\t\tcommit_id_str = str(hunk.final_commit_id) if hunk.final_commit_id else \"Unknown\"\n\n\t\t\t\t# If this commit_id is already in processed_commits, it means we've started\n\t\t\t\t# processing this commit's hunk. We update its end_line.\n\t\t\t\tif commit_id_str in processed_commits:\n\t\t\t\t\tcurrent = processed_commits[commit_id_str]\n\t\t\t\t\tcurrent.end_line = max(current.end_line, line_num_1_indexed)\n\t\t\t\t\t# Continue to the next line, as we only need one entry per commit within the chunk,\n\t\t\t\t\t# but covering the full range of lines it affected in this chunk.\n\t\t\t\t\tcontinue\n\n\t\t\t\t# New commit_id encountered for this chunk, gather its details\n\t\t\t\tauthor_name = \"Unknown\"\n\t\t\t\tcommit_date_str = \"Unknown\"\n\n\t\t\t\tif hunk.final_commit_id:\n\t\t\t\t\tcommit_details_obj = None\n\t\t\t\t\ttry:\n\t\t\t\t\t\tgit_object = self.repo.get(hunk.final_commit_id)\n\t\t\t\t\t\tif isinstance(git_object, Commit):\n\t\t\t\t\t\t\tcommit_details_obj = git_object\n\t\t\t\t\t\t\tif commit_details_obj.author:\n\t\t\t\t\t\t\t\tauthor_name = commit_details_obj.author.name\n\t\t\t\t\t\t\t\tcommit_date_str = str(commit_details_obj.author.time)\n\t\t\t\t\t\t\telif commit_details_obj.committer:\n\t\t\t\t\t\t\t\tauthor_name = commit_details_obj.committer.name\n\t\t\t\t\t\t\t\tcommit_date_str = str(commit_details_obj.committer.time)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tself.logger.warning(f\"Commit {hunk.final_commit_id} has no author or committer.\")\n\t\t\t\t\t\telif git_object is not None:\n\t\t\t\t\t\t\tself.logger.warning(f\"Object {hunk.final_commit_id} is not a Commit: {type(git_object)}\")\n\t\t\t\t\texcept (KeyError, TypeError, GitError) as e:\n\t\t\t\t\t\tself.logger.warning(f\"Error retrieving details for commit {hunk.final_commit_id}: {e}\")\n\n\t\t\t\tprocessed_commits[commit_id_str] = GitBlameSchema(\n\t\t\t\t\tcommit_id=commit_id_str,\n\t\t\t\t\tdate=commit_date_str,\n\t\t\t\t\tauthor_name=author_name,\n\t\t\t\t\tstart_line=line_num_1_indexed,  # First line this commit is seen for in the current range\n\t\t\t\t\tend_line=line_num_1_indexed,  # Last line this commit is seen for (so far)\n\t\t\t\t)\n\n\t\t\treturn list(processed_commits.values())\n\n\t\texcept Exception:  # Catch broader exceptions like repo.blame() failing or other issues\n\t\t\tself.logger.exception(f\"Failed to get git blame for {file_path}\")\n\t\t\treturn []\n\n\tdef get_metadata_schema(self, file_path: str, start_line: int, end_line: int) -&gt; GitMetadataSchema:\n\t\t\"\"\"\n\t\tDerive the complete GitMetadataSchema for a given file.\n\n\t\tArgs:\n\t\t\tfile_path (str): The path to the file relative to the repository root.\n\t\t\tstart_line (int): The starting line number of the range.\n\t\t\tend_line (int): The ending line number of the range.\n\n\t\tReturns:\n\t\t\tGitMetadataSchema: The metadata for the file in the git repository.\n\t\t\"\"\"\n\t\tgit_hash = self.get_file_git_hash(file_path)\n\t\tblame = self.get_git_blame(file_path, start_line, end_line)\n\t\ttracked = file_path in self.tracked_files\n\t\treturn GitMetadataSchema(\n\t\t\tgit_hash=git_hash,\n\t\t\ttracked=tracked,\n\t\t\tbranch=self.branch,\n\t\t\tblame=blame,\n\t\t)\n\n\tdef get_untracked_files(self) -&gt; list[str]:\n\t\t\"\"\"Get a list of untracked files in the repository.\"\"\"\n\t\tstatus = self.repo.status()\n\t\treturn [path for path, flags in status.items() if flags &amp; FileStatus.WT_NEW]\n\n\tdef is_git_ignored(self, file_path: str) -&gt; bool:\n\t\t\"\"\"Check if a file is ignored by Git.\"\"\"\n\t\treturn self.repo.path_is_ignored(file_path)\n\n\tdef is_file_tracked(self, file_path: str) -&gt; bool:\n\t\t\"\"\"Check if a file is tracked in the Git repository.\"\"\"\n\t\treturn file_path in self.tracked_files\n\n\tdef unstage_files(self, files: list[str]) -&gt; None:\n\t\t\"\"\"Unstage the specified files.\"\"\"\n\t\tfor file in files:\n\t\t\tself.repo.index.remove(file)\n\t\tself.repo.index.write()\n\n\tdef switch_branch(self, branch_name: str) -&gt; None:\n\t\t\"\"\"Switch the current Git branch to the specified branch name.\"\"\"\n\t\tref = f\"refs/heads/{branch_name}\"\n\t\tself.repo.checkout(ref)\n\n\tdef stage_files(self, files: list[str]) -&gt; None:\n\t\t\"\"\"Stage the specified files.\"\"\"\n\t\tfor file in files:\n\t\t\tself.repo.index.add(file)\n\t\tself.repo.index.write()\n\n\tdef commit(self, message: str) -&gt; None:\n\t\t\"\"\"Create a commit with the given message.\"\"\"\n\t\tauthor = self.repo.default_signature\n\t\tcommitter = self.repo.default_signature\n\t\ttree = self.repo.index.write_tree()\n\t\tparents = [self.repo.head.target] if self.repo.head_is_unborn is False else []\n\t\tself.repo.create_commit(\"HEAD\", author, committer, message, tree, parents)\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.logger","title":"logger  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.repo_root","title":"repo_root  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repo_root: Path | None = None\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.get_instance","title":"get_instance  <code>classmethod</code>","text":"<pre><code>get_instance() -&gt; GitRepoContext\n</code></pre> <p>Get a cached instance of GitRepoContext for a given repo_path.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>@classmethod\ndef get_instance(cls) -&gt; \"GitRepoContext\":\n\t\"\"\"Get a cached instance of GitRepoContext for a given repo_path.\"\"\"\n\tif cls._instance is None:\n\t\tcls._instance = cls()\n\treturn cls._instance\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.get_repo_root","title":"get_repo_root  <code>classmethod</code>","text":"<pre><code>get_repo_root(path: Path | None = None) -&gt; Path\n</code></pre> <p>Get the root directory of the Git repository.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>@classmethod\ndef get_repo_root(cls, path: Path | None = None) -&gt; Path:\n\t\"\"\"Get the root directory of the Git repository.\"\"\"\n\tgit_dir = discover_repository(str(path or Path.cwd()))\n\tif git_dir is None:\n\t\tmsg = \"Not a git repository\"\n\t\tlogger.error(msg)\n\t\traise GitError(msg)\n\treturn Path(git_dir)\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the GitRepoContext with the given repository path.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the GitRepoContext with the given repository path.\"\"\"\n\tif self.repo_root is None:\n\t\tself.repo_root = self.get_repo_root()\n\tself.repo = Repository(str(self.repo_root))\n\tself.branch = self.get_current_branch()\n\tself.exclude_patterns = self._get_exclude_patterns()\n\tself.tracked_files = self._get_tracked_files()\n\tself._blame_cache: dict[str, Blame] = {}  # Cache for blame objects\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.repo","title":"repo  <code>instance-attribute</code>","text":"<pre><code>repo = Repository(str(repo_root))\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.branch","title":"branch  <code>instance-attribute</code>","text":"<pre><code>branch = get_current_branch()\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.exclude_patterns","title":"exclude_patterns  <code>instance-attribute</code>","text":"<pre><code>exclude_patterns = _get_exclude_patterns()\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.tracked_files","title":"tracked_files  <code>instance-attribute</code>","text":"<pre><code>tracked_files = _get_tracked_files()\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.get_current_branch","title":"get_current_branch","text":"<pre><code>get_current_branch() -&gt; str\n</code></pre> <p>Get the current branch name of the Git repository.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The current branch name, or empty string if detached.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def get_current_branch(self) -&gt; str:\n\t\"\"\"\n\tGet the current branch name of the Git repository.\n\n\tReturns:\n\t\tstr: The current branch name, or empty string if detached.\n\t\"\"\"\n\tif self.repo.head_is_detached:\n\t\treturn \"\"\n\treturn self.repo.head.shorthand or \"\"\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.get_file_git_hash","title":"get_file_git_hash","text":"<pre><code>get_file_git_hash(file_path: str) -&gt; str\n</code></pre> <p>Get the Git hash (blob ID) for a specific tracked file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file relative to the repository root.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Git blob hash of the file, or empty string if not found.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def get_file_git_hash(self, file_path: str) -&gt; str:\n\t\"\"\"\n\tGet the Git hash (blob ID) for a specific tracked file.\n\n\tArgs:\n\t\tfile_path (str): The path to the file relative to the repository root.\n\n\tReturns:\n\t\tstr: The Git blob hash of the file, or empty string if not found.\n\t\"\"\"\n\ttry:\n\t\tcommit = self.repo.head.peel(Commit)\n\t\tif commit is None:\n\t\t\tself.logger.warning(f\"HEAD does not point to a commit in repo {self.repo.path}\")\n\t\t\treturn \"\"\n\t\ttree = commit.tree\n\t\tentry = tree[file_path]\n\t\treturn str(entry.id)\n\texcept KeyError:\n\t\tself.logger.warning(f\"File {file_path} not found in HEAD tree of repo {self.repo.path}\")\n\t\treturn \"\"\n\texcept Exception:\n\t\tself.logger.exception(f\"Failed to get git hash for {file_path}\")\n\t\treturn \"\"\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.get_git_blame","title":"get_git_blame","text":"<pre><code>get_git_blame(\n\tfile_path: str, start_line: int, end_line: int\n) -&gt; list[GitBlameSchema]\n</code></pre> <p>Get the Git blame for a specific range of lines in a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file relative to the repository root.</p> required <code>start_line</code> <code>int</code> <p>The starting line number of the range.</p> required <code>end_line</code> <code>int</code> <p>The ending line number of the range.</p> required <p>Returns:</p> Type Description <code>list[GitBlameSchema]</code> <p>list[GitBlameSchema]: A list of Git blame results.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def get_git_blame(self, file_path: str, start_line: int, end_line: int) -&gt; list[GitBlameSchema]:\n\t\"\"\"\n\tGet the Git blame for a specific range of lines in a file.\n\n\tArgs:\n\t\tfile_path (str): The path to the file relative to the repository root.\n\t\tstart_line (int): The starting line number of the range.\n\t\tend_line (int): The ending line number of the range.\n\n\tReturns:\n\t\tlist[GitBlameSchema]: A list of Git blame results.\n\t\"\"\"\n\ttry:\n\t\t# Check if the file is actually tracked by git\n\t\tfile_is_tracked = file_path in self.tracked_files\n\t\tif not file_is_tracked:\n\t\t\t# Skip blame lookup for untracked files\n\t\t\tlogger.debug(f\"File '{file_path}' is not tracked in git - skipping blame lookup\")\n\t\t\treturn []\n\n\t\t# Handle ambiguous file paths without full directory path\n\t\t# Only attempt path resolution if we have:\n\t\t# 1. A valid repo_root\n\t\t# 2. A simple filename without path separators\n\t\tif self.repo_root is not None and \"/\" not in file_path and \"\\\\\" not in file_path and self.tracked_files:\n\t\t\t# We have just a filename - try to find it in tracked files\n\t\t\tmatching_paths = [\n\t\t\t\ttracked_path\n\t\t\t\tfor tracked_path in self.tracked_files\n\t\t\t\tif tracked_path.endswith(f\"/{file_path}\") or tracked_path == file_path\n\t\t\t]\n\n\t\t\tif len(matching_paths) == 1:\n\t\t\t\t# Found exactly one match, use it\n\t\t\t\tfile_path = matching_paths[0]\n\t\t\t\tself.logger.debug(f\"Found unique match for '{file_path}': {matching_paths[0]}\")\n\t\t\telif len(matching_paths) &gt; 1:\n\t\t\t\t# Multiple matches, use the most specific one or log warning\n\t\t\t\tself.logger.warning(f\"Ambiguous file '{file_path}' has multiple matches: {matching_paths}\")\n\t\t\t\t# For now, pick the first candidate but this could be improved\n\t\t\t\tfile_path = matching_paths[0]\n\t\t\t\tself.logger.debug(f\"Using first match: {file_path}\")\n\t\t\telse:\n\t\t\t\tself.logger.warning(f\"No matching file found for '{file_path}' in tracked files\")\n\t\t\t\treturn []  # Return empty blame list if no match found\n\n\t\ttry:\n\t\t\tblame_obj: Blame\n\t\t\tif file_path not in self._blame_cache:\n\t\t\t\t# Ensure file_path is relative to repo root for pygit2.blame\n\t\t\t\tself._blame_cache[file_path] = self.repo.blame(file_path)\n\t\t\tblame_obj = self._blame_cache[file_path]\n\t\texcept KeyError:\n\t\t\t# File not found in git repository\n\t\t\tlogger.debug(f\"File '{file_path}' not found in git repository - skipping blame lookup\")\n\t\t\treturn []\n\n\t\t# Using a dictionary to collect unique commit information for the given line range\n\t\t# Key: commit_id_str, Value: GitBlameSchema\n\t\tprocessed_commits: dict[str, GitBlameSchema] = {}\n\n\t\tfor line_num_1_indexed in range(start_line, end_line + 1):\n\t\t\tif line_num_1_indexed &lt;= 0:\n\t\t\t\tcontinue  # Line numbers are 1-indexed\n\t\t\ttry:\n\t\t\t\t# pygit2.Blame.__getitem__ expects 0-indexed line number\n\t\t\t\thunk = blame_obj[line_num_1_indexed - 1]\n\t\t\texcept IndexError:\n\t\t\t\t# This can happen if start_line/end_line are outside the file's actual line count\n\t\t\t\tself.logger.warning(\n\t\t\t\t\tf\"Line {line_num_1_indexed} is out of range for \"\n\t\t\t\t\tf\"blame in file {file_path}. Total lines in \"\n\t\t\t\t\tf\"blame: {len(blame_obj)}.\"\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\tcommit_id_str = str(hunk.final_commit_id) if hunk.final_commit_id else \"Unknown\"\n\n\t\t\t# If this commit_id is already in processed_commits, it means we've started\n\t\t\t# processing this commit's hunk. We update its end_line.\n\t\t\tif commit_id_str in processed_commits:\n\t\t\t\tcurrent = processed_commits[commit_id_str]\n\t\t\t\tcurrent.end_line = max(current.end_line, line_num_1_indexed)\n\t\t\t\t# Continue to the next line, as we only need one entry per commit within the chunk,\n\t\t\t\t# but covering the full range of lines it affected in this chunk.\n\t\t\t\tcontinue\n\n\t\t\t# New commit_id encountered for this chunk, gather its details\n\t\t\tauthor_name = \"Unknown\"\n\t\t\tcommit_date_str = \"Unknown\"\n\n\t\t\tif hunk.final_commit_id:\n\t\t\t\tcommit_details_obj = None\n\t\t\t\ttry:\n\t\t\t\t\tgit_object = self.repo.get(hunk.final_commit_id)\n\t\t\t\t\tif isinstance(git_object, Commit):\n\t\t\t\t\t\tcommit_details_obj = git_object\n\t\t\t\t\t\tif commit_details_obj.author:\n\t\t\t\t\t\t\tauthor_name = commit_details_obj.author.name\n\t\t\t\t\t\t\tcommit_date_str = str(commit_details_obj.author.time)\n\t\t\t\t\t\telif commit_details_obj.committer:\n\t\t\t\t\t\t\tauthor_name = commit_details_obj.committer.name\n\t\t\t\t\t\t\tcommit_date_str = str(commit_details_obj.committer.time)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.logger.warning(f\"Commit {hunk.final_commit_id} has no author or committer.\")\n\t\t\t\t\telif git_object is not None:\n\t\t\t\t\t\tself.logger.warning(f\"Object {hunk.final_commit_id} is not a Commit: {type(git_object)}\")\n\t\t\t\texcept (KeyError, TypeError, GitError) as e:\n\t\t\t\t\tself.logger.warning(f\"Error retrieving details for commit {hunk.final_commit_id}: {e}\")\n\n\t\t\tprocessed_commits[commit_id_str] = GitBlameSchema(\n\t\t\t\tcommit_id=commit_id_str,\n\t\t\t\tdate=commit_date_str,\n\t\t\t\tauthor_name=author_name,\n\t\t\t\tstart_line=line_num_1_indexed,  # First line this commit is seen for in the current range\n\t\t\t\tend_line=line_num_1_indexed,  # Last line this commit is seen for (so far)\n\t\t\t)\n\n\t\treturn list(processed_commits.values())\n\n\texcept Exception:  # Catch broader exceptions like repo.blame() failing or other issues\n\t\tself.logger.exception(f\"Failed to get git blame for {file_path}\")\n\t\treturn []\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.get_metadata_schema","title":"get_metadata_schema","text":"<pre><code>get_metadata_schema(\n\tfile_path: str, start_line: int, end_line: int\n) -&gt; GitMetadataSchema\n</code></pre> <p>Derive the complete GitMetadataSchema for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file relative to the repository root.</p> required <code>start_line</code> <code>int</code> <p>The starting line number of the range.</p> required <code>end_line</code> <code>int</code> <p>The ending line number of the range.</p> required <p>Returns:</p> Name Type Description <code>GitMetadataSchema</code> <code>GitMetadataSchema</code> <p>The metadata for the file in the git repository.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def get_metadata_schema(self, file_path: str, start_line: int, end_line: int) -&gt; GitMetadataSchema:\n\t\"\"\"\n\tDerive the complete GitMetadataSchema for a given file.\n\n\tArgs:\n\t\tfile_path (str): The path to the file relative to the repository root.\n\t\tstart_line (int): The starting line number of the range.\n\t\tend_line (int): The ending line number of the range.\n\n\tReturns:\n\t\tGitMetadataSchema: The metadata for the file in the git repository.\n\t\"\"\"\n\tgit_hash = self.get_file_git_hash(file_path)\n\tblame = self.get_git_blame(file_path, start_line, end_line)\n\ttracked = file_path in self.tracked_files\n\treturn GitMetadataSchema(\n\t\tgit_hash=git_hash,\n\t\ttracked=tracked,\n\t\tbranch=self.branch,\n\t\tblame=blame,\n\t)\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.get_untracked_files","title":"get_untracked_files","text":"<pre><code>get_untracked_files() -&gt; list[str]\n</code></pre> <p>Get a list of untracked files in the repository.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def get_untracked_files(self) -&gt; list[str]:\n\t\"\"\"Get a list of untracked files in the repository.\"\"\"\n\tstatus = self.repo.status()\n\treturn [path for path, flags in status.items() if flags &amp; FileStatus.WT_NEW]\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.is_git_ignored","title":"is_git_ignored","text":"<pre><code>is_git_ignored(file_path: str) -&gt; bool\n</code></pre> <p>Check if a file is ignored by Git.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def is_git_ignored(self, file_path: str) -&gt; bool:\n\t\"\"\"Check if a file is ignored by Git.\"\"\"\n\treturn self.repo.path_is_ignored(file_path)\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.is_file_tracked","title":"is_file_tracked","text":"<pre><code>is_file_tracked(file_path: str) -&gt; bool\n</code></pre> <p>Check if a file is tracked in the Git repository.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def is_file_tracked(self, file_path: str) -&gt; bool:\n\t\"\"\"Check if a file is tracked in the Git repository.\"\"\"\n\treturn file_path in self.tracked_files\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.unstage_files","title":"unstage_files","text":"<pre><code>unstage_files(files: list[str]) -&gt; None\n</code></pre> <p>Unstage the specified files.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def unstage_files(self, files: list[str]) -&gt; None:\n\t\"\"\"Unstage the specified files.\"\"\"\n\tfor file in files:\n\t\tself.repo.index.remove(file)\n\tself.repo.index.write()\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.switch_branch","title":"switch_branch","text":"<pre><code>switch_branch(branch_name: str) -&gt; None\n</code></pre> <p>Switch the current Git branch to the specified branch name.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def switch_branch(self, branch_name: str) -&gt; None:\n\t\"\"\"Switch the current Git branch to the specified branch name.\"\"\"\n\tref = f\"refs/heads/{branch_name}\"\n\tself.repo.checkout(ref)\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.stage_files","title":"stage_files","text":"<pre><code>stage_files(files: list[str]) -&gt; None\n</code></pre> <p>Stage the specified files.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def stage_files(self, files: list[str]) -&gt; None:\n\t\"\"\"Stage the specified files.\"\"\"\n\tfor file in files:\n\t\tself.repo.index.add(file)\n\tself.repo.index.write()\n</code></pre>"},{"location":"api/utils/git_utils/#codemap.utils.git_utils.GitRepoContext.commit","title":"commit","text":"<pre><code>commit(message: str) -&gt; None\n</code></pre> <p>Create a commit with the given message.</p> Source code in <code>src/codemap/utils/git_utils.py</code> <pre><code>def commit(self, message: str) -&gt; None:\n\t\"\"\"Create a commit with the given message.\"\"\"\n\tauthor = self.repo.default_signature\n\tcommitter = self.repo.default_signature\n\ttree = self.repo.index.write_tree()\n\tparents = [self.repo.head.target] if self.repo.head_is_unborn is False else []\n\tself.repo.create_commit(\"HEAD\", author, committer, message, tree, parents)\n</code></pre>"},{"location":"api/utils/log_setup/","title":"Log Setup","text":"<p>Logging setup for CodeMap.</p> <p>This module configures logging for different parts of the CodeMap application, ensuring logs are stored in the appropriate directories.</p>"},{"location":"api/utils/log_setup/#codemap.utils.log_setup.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/utils/log_setup/#codemap.utils.log_setup.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(\n\tis_verbose: bool = False,\n\tlog_to_console: bool = True,\n\tlog_file_path: Path | str | None = None,\n\tlog_level_default: int = ERROR,\n\tlog_level_verbose: int = DEBUG,\n) -&gt; None\n</code></pre> <p>Set up logging configuration.</p> <p>Parameters:</p> Name Type Description Default <code>is_verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>log_to_console</code> <code>bool</code> <p>Whether to log to the console</p> <code>True</code> <code>log_file_path</code> <code>Path | str | None</code> <p>Optional path to a file for logging. If None, no file logging.</p> <code>None</code> <code>log_level_default</code> <code>int</code> <p>The log level to use for default logging.</p> <code>ERROR</code> <code>log_level_verbose</code> <code>int</code> <p>The log level to use for verbose logging.</p> <code>DEBUG</code> Source code in <code>src/codemap/utils/log_setup.py</code> <pre><code>def setup_logging(\n\tis_verbose: bool = False,\n\tlog_to_console: bool = True,\n\tlog_file_path: Path | str | None = None,\n\tlog_level_default: int = logging.ERROR,\n\tlog_level_verbose: int = logging.DEBUG,\n) -&gt; None:\n\t\"\"\"\n\tSet up logging configuration.\n\n\tArgs:\n\t    is_verbose: Enable verbose logging\n\t    log_to_console: Whether to log to the console\n\t    log_file_path: Optional path to a file for logging. If None, no file logging.\n\t    log_level_default: The log level to use for default logging.\n\t    log_level_verbose: The log level to use for verbose logging.\n\t\"\"\"\n\t# Determine log level\n\tlog_level = log_level_default if not is_verbose else log_level_verbose\n\n\t# Root logger configuration\n\troot_logger = logging.getLogger()\n\troot_logger.setLevel(log_level)\n\n\t# Clear existing handlers to avoid duplicate logs if called multiple times\n\tfor handler in root_logger.handlers[:]:\n\t\troot_logger.removeHandler(handler)\n\n\t# Setup console logging if requested\n\tif log_to_console:\n\t\tconsole_handler = RichHandler(\n\t\t\tlevel=log_level,\n\t\t\trich_tracebacks=True,\n\t\t\tshow_time=True,\n\t\t\tshow_path=is_verbose,\n\t\t)\n\t\troot_logger.addHandler(console_handler)\n\n\t# Setup file logging if a path is provided\n\tif log_file_path:\n\t\ttry:\n\t\t\tfile_handler_path = Path(log_file_path)\n\t\t\t# Ensure the directory for the log file exists\n\t\t\tfile_handler_path.parent.mkdir(parents=True, exist_ok=True)\n\n\t\t\tfile_handler = logging.FileHandler(file_handler_path, mode=\"a\", encoding=\"utf-8\")\n\t\t\tfile_handler.setLevel(logging.DEBUG)  # Log all debug messages and above to file\n\t\t\tfile_formatter = logging.Formatter(\n\t\t\t\t\"%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s\"\n\t\t\t)\n\t\t\tfile_handler.setFormatter(file_formatter)\n\t\t\troot_logger.addHandler(file_handler)\n\t\t\troot_logger.debug(f\"Logging to file: {file_handler_path}\")  # Log where file logs are going\n\t\texcept (OSError, PermissionError, TypeError) as e:\n\t\t\t# If file logging setup fails, log error to console (if available) and continue\n\t\t\tconsole_error_msg = f\"[CODEMAP CLI CRITICAL] Failed to set up file logging to {log_file_path}: {e}\"\n\t\t\t# Use root logger to print critical setup error to console if possible\n\t\t\t# This avoids dependency on whether RichHandler was successfully added.\n\t\t\tcrit_logger = logging.getLogger(\"codemap.cli.critical_setup\")\n\t\t\tcrit_logger.handlers.clear()  # Ensure it only goes to basic stderr if no console handler\n\t\t\tconsole_err_handler = logging.StreamHandler()\n\t\t\tconsole_err_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n\t\t\tcrit_logger.addHandler(console_err_handler)\n\t\t\tcrit_logger.propagate = False\n\t\t\tcrit_logger.critical(console_error_msg)\n</code></pre>"},{"location":"api/utils/path_utils/","title":"Path Utils","text":"<p>Utilities for handling paths and file system operations.</p>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.filter_paths_by_gitignore","title":"filter_paths_by_gitignore","text":"<pre><code>filter_paths_by_gitignore(\n\tpaths: Sequence[Path], repo_root: Path\n) -&gt; list[Path]\n</code></pre> <p>Filter paths based on .gitignore patterns.</p> <p>This function filters a list of paths to exclude those that match patterns in a .gitignore file, while preserving the directory structure.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Sequence[Path]</code> <p>Sequence of paths to filter</p> required <code>repo_root</code> <code>Path</code> <p>Root directory of the repository</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of paths that don't match any gitignore patterns</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def filter_paths_by_gitignore(paths: Sequence[Path], repo_root: Path) -&gt; list[Path]:\n\t\"\"\"\n\tFilter paths based on .gitignore patterns.\n\n\tThis function filters a list of paths to exclude those that match\n\tpatterns in a .gitignore file, while preserving the directory structure.\n\n\tArgs:\n\t    paths: Sequence of paths to filter\n\t    repo_root: Root directory of the repository\n\n\tReturns:\n\t    List of paths that don't match any gitignore patterns\n\n\t\"\"\"\n\ttry:\n\t\timport pathspec\n\texcept ImportError:\n\t\tlogger.warning(\"pathspec package not installed, gitignore filtering disabled\")\n\t\treturn list(paths)\n\n\t# Read .gitignore if it exists\n\tgitignore_path = repo_root / \".gitignore\"\n\tgitignore_patterns = []\n\n\tif gitignore_path.exists():\n\t\t# Parse gitignore patterns\n\t\twith gitignore_path.open(\"r\", encoding=\"utf-8\") as f:\n\t\t\tgitignore_content = f.read()\n\t\tgitignore_patterns = gitignore_content.splitlines()\n\n\t# Add default patterns for common directories that should be ignored\n\tdefault_ignore_patterns = [\n\t\t\"__pycache__/\",\n\t\t\"*.py[cod]\",\n\t\t\"*$py.class\",\n\t\t\".git/\",\n\t\t\".pytest_cache/\",\n\t\t\".coverage\",\n\t\t\"htmlcov/\",\n\t\t\".tox/\",\n\t\t\".nox/\",\n\t\t\".hypothesis/\",\n\t\t\".mypy_cache/\",\n\t\t\".ruff_cache/\",\n\t\t\"dist/\",\n\t\t\"build/\",\n\t\t\"*.so\",\n\t\t\"*.egg\",\n\t\t\"*.egg-info/\",\n\t\t\".env/\",\n\t\t\"venv/\",\n\t\t\".venv/\",\n\t\t\"env/\",\n\t\t\"ENV/\",\n\t\t\"node_modules/\",\n\t]\n\n\t# Combine patterns with existing ones, avoiding duplicates\n\tall_patterns = gitignore_patterns + [p for p in default_ignore_patterns if p not in gitignore_patterns]\n\n\t# Create path spec with direct import\n\tspec = pathspec.PathSpec.from_lines(\"gitwildmatch\", all_patterns)\n\n\t# Filter paths\n\tfiltered_paths = []\n\n\t# Process files first\n\tfile_paths = [p for p in paths if p.is_file()]\n\tfor path in file_paths:\n\t\ttry:\n\t\t\trel_path = path.relative_to(repo_root)\n\t\t\tif not spec.match_file(str(rel_path)):\n\t\t\t\tfiltered_paths.append(path)\n\t\texcept ValueError:\n\t\t\t# Path is not relative to repo_root\n\t\t\tfiltered_paths.append(path)\n\n\t# Process directories\n\tdir_paths = [p for p in paths if p.is_dir()]\n\n\t# First check which directories are included according to gitignore patterns\n\tincluded_dirs = []\n\tfor dir_path in dir_paths:\n\t\ttry:\n\t\t\trel_path = dir_path.relative_to(repo_root)\n\t\t\trel_path_str = str(rel_path) + \"/\"  # Add trailing slash for directory patterns\n\n\t\t\t# Skip the directory if it matches a gitignore pattern\n\t\t\tif spec.match_file(rel_path_str):\n\t\t\t\tlogger.debug(f\"Skipping ignored directory: {rel_path}\")\n\t\t\t\tcontinue\n\n\t\t\t# Check if any parent directory is already ignored\n\t\t\tparent_ignored = False\n\t\t\tfor parent in rel_path.parents:\n\t\t\t\tparent_str = str(parent) + \"/\"\n\t\t\t\tif spec.match_file(parent_str):\n\t\t\t\t\tparent_ignored = True\n\t\t\t\t\tlogger.debug(f\"Skipping directory with ignored parent: {parent}\")\n\t\t\t\t\tbreak\n\n\t\t\tif not parent_ignored:\n\t\t\t\tincluded_dirs.append(dir_path)\n\n\t\texcept ValueError:\n\t\t\t# Path is not relative to repo_root\n\t\t\tincluded_dirs.append(dir_path)\n\n\t# Include all directories at all levels to preserve hierarchy\n\t# Directories with no content might still be needed for the tree visualization\n\tfiltered_paths.extend(included_dirs)\n\n\tlogger.debug(f\"Filtered {len(paths)} paths down to {len(filtered_paths)} after applying gitignore patterns\")\n\treturn filtered_paths\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.normalize_path","title":"normalize_path","text":"<pre><code>normalize_path(path: str | Path) -&gt; Path\n</code></pre> <p>Normalize a path to an absolute Path object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path string or object</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Normalized absolute Path</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def normalize_path(path: str | Path) -&gt; Path:\n\t\"\"\"\n\tNormalize a path to an absolute Path object.\n\n\tArgs:\n\t    path: Path string or object\n\n\tReturns:\n\t    Normalized absolute Path\n\n\t\"\"\"\n\tif isinstance(path, str):\n\t\tpath = Path(path)\n\treturn path.expanduser().resolve()\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.get_relative_path","title":"get_relative_path","text":"<pre><code>get_relative_path(path: Path, base_path: Path) -&gt; Path\n</code></pre> <p>Get path relative to base_path if possible, otherwise return absolute path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to make relative</p> required <code>base_path</code> <code>Path</code> <p>The base path to make it relative to</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Relative path if possible, otherwise absolute path</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def get_relative_path(path: Path, base_path: Path) -&gt; Path:\n\t\"\"\"\n\tGet path relative to base_path if possible, otherwise return absolute path.\n\n\tArgs:\n\t    path: The path to make relative\n\t    base_path: The base path to make it relative to\n\n\tReturns:\n\t    Relative path if possible, otherwise absolute path\n\n\t\"\"\"\n\ttry:\n\t\treturn path.relative_to(base_path)\n\texcept ValueError:\n\t\treturn path.absolute()\n</code></pre>"},{"location":"api/watcher/","title":"Watcher Overview","text":"<p>Watcher module for CodeMap.</p> <ul> <li>File Watcher - File watcher module for CodeMap.</li> </ul>"},{"location":"api/watcher/file_watcher/","title":"File Watcher","text":"<p>File watcher module for CodeMap.</p>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler","title":"FileChangeHandler","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Handles file system events and triggers a callback.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>class FileChangeHandler(FileSystemEventHandler):\n\t\"\"\"Handles file system events and triggers a callback.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tcallback: Callable[[], Coroutine[None, None, None]],\n\t\tdebounce_delay: float = 1.0,\n\t\tevent_loop: asyncio.AbstractEventLoop | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the handler.\n\n\t\tArgs:\n\t\t    callback: An async function to call when changes are detected.\n\t\t    debounce_delay: Minimum time (seconds) between callback triggers.\n\t\t    event_loop: The asyncio event loop to use, or None to use the current one.\n\n\t\t\"\"\"\n\t\tself.callback = callback\n\t\tself.debounce_delay = debounce_delay\n\t\tself._last_event_time: float = 0\n\t\tself._debounce_task: asyncio.Task | None = None\n\t\t# Set up a thread-safe way to communicate with the event loop\n\t\tself._event_queue = queue.Queue()\n\t\tself._event_processed = threading.Event()\n\t\t# Store or get the event loop\n\t\tself._event_loop = event_loop or asyncio.get_event_loop()\n\t\t# Flag to track if we're in the process of handling an event\n\t\tself._processing = False\n\n\tdef _schedule_callback(self) -&gt; None:\n\t\t\"\"\"Schedule the callback execution from a thread-safe context.\"\"\"\n\t\t# If processing is already in progress, just return\n\t\tif self._processing:\n\t\t\treturn\n\n\t\t# Put the event in the queue for the event loop to process\n\t\tself._event_processed.clear()\n\t\tself._event_queue.put_nowait(\"file_change\")\n\n\t\t# Schedule the task in the event loop\n\t\tasyncio.run_coroutine_threadsafe(self._process_events(), self._event_loop)\n\n\tasync def _process_events(self) -&gt; None:\n\t\t\"\"\"Process events from the queue in the event loop's context.\"\"\"\n\t\tif self._processing:\n\t\t\treturn\n\n\t\tself._processing = True\n\t\ttry:\n\t\t\t# Get an event from the queue\n\t\t\twhile not self._event_queue.empty():\n\t\t\t\t_ = self._event_queue.get_nowait()\n\n\t\t\t\t# Cancel any existing debounce task\n\t\t\t\tif self._debounce_task and not self._debounce_task.done():\n\t\t\t\t\tself._debounce_task.cancel()\n\t\t\t\t\tlogger.debug(\"Cancelled existing debounce task due to new event.\")\n\n\t\t\t\t# Create a new debounce task within the event loop's context\n\t\t\t\tlogger.debug(f\"Scheduling new debounced callback with {self.debounce_delay}s delay.\")\n\t\t\t\tself._debounce_task = self._event_loop.create_task(self._debounced_callback())\n\t\tfinally:\n\t\t\tself._processing = False\n\t\t\tself._event_processed.set()\n\n\tasync def _debounced_callback(self) -&gt; None:\n\t\t\"\"\"Wait for the debounce period and then execute the callback.\"\"\"\n\t\ttry:\n\t\t\tawait asyncio.sleep(self.debounce_delay)\n\t\t\tlogger.info(\"Debounce delay finished, triggering sync callback.\")\n\t\t\tawait self.callback()\n\t\t\tself._last_event_time = time.monotonic()  # Update time after successful execution\n\t\t\tlogger.debug(\"Watcher callback executed successfully.\")\n\t\texcept asyncio.CancelledError:\n\t\t\tlogger.debug(\"Debounce task cancelled before execution.\")\n\t\t\t# Do not run the callback if cancelled\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error executing watcher callback\")\n\t\tfinally:\n\t\t\t# Clear the task reference once it's done\n\t\t\tself._debounce_task = None\n\n\tdef on_any_event(self, event: FileSystemEvent) -&gt; None:\n\t\t\"\"\"\n\t\tCatch all events and schedule the callback after debouncing.\n\n\t\tArgs:\n\t\t    event: The file system event.\n\n\t\t\"\"\"\n\t\tif event.is_directory:\n\t\t\treturn  # Ignore directory events for now, focus on file changes\n\n\t\t# Log the specific event detected\n\t\tevent_type = event.event_type\n\t\tsrc_path = getattr(event, \"src_path\", \"N/A\")\n\t\tdest_path = getattr(event, \"dest_path\", \"N/A\")  # For moved events\n\n\t\tif event_type == \"moved\":\n\t\t\tlogger.debug(f\"Detected file {event_type}: {src_path} -&gt; {dest_path}\")\n\t\telse:\n\t\t\tlogger.debug(f\"Detected file {event_type}: {src_path}\")\n\n\t\t# Schedule the callback in a thread-safe way\n\t\tself._schedule_callback()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.__init__","title":"__init__","text":"<pre><code>__init__(\n\tcallback: Callable[[], Coroutine[None, None, None]],\n\tdebounce_delay: float = 1.0,\n\tevent_loop: AbstractEventLoop | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the handler.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[], Coroutine[None, None, None]]</code> <p>An async function to call when changes are detected.</p> required <code>debounce_delay</code> <code>float</code> <p>Minimum time (seconds) between callback triggers.</p> <code>1.0</code> <code>event_loop</code> <code>AbstractEventLoop | None</code> <p>The asyncio event loop to use, or None to use the current one.</p> <code>None</code> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def __init__(\n\tself,\n\tcallback: Callable[[], Coroutine[None, None, None]],\n\tdebounce_delay: float = 1.0,\n\tevent_loop: asyncio.AbstractEventLoop | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the handler.\n\n\tArgs:\n\t    callback: An async function to call when changes are detected.\n\t    debounce_delay: Minimum time (seconds) between callback triggers.\n\t    event_loop: The asyncio event loop to use, or None to use the current one.\n\n\t\"\"\"\n\tself.callback = callback\n\tself.debounce_delay = debounce_delay\n\tself._last_event_time: float = 0\n\tself._debounce_task: asyncio.Task | None = None\n\t# Set up a thread-safe way to communicate with the event loop\n\tself._event_queue = queue.Queue()\n\tself._event_processed = threading.Event()\n\t# Store or get the event loop\n\tself._event_loop = event_loop or asyncio.get_event_loop()\n\t# Flag to track if we're in the process of handling an event\n\tself._processing = False\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.callback","title":"callback  <code>instance-attribute</code>","text":"<pre><code>callback = callback\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.debounce_delay","title":"debounce_delay  <code>instance-attribute</code>","text":"<pre><code>debounce_delay = debounce_delay\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.on_any_event","title":"on_any_event","text":"<pre><code>on_any_event(event: FileSystemEvent) -&gt; None\n</code></pre> <p>Catch all events and schedule the callback after debouncing.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>FileSystemEvent</code> <p>The file system event.</p> required Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def on_any_event(self, event: FileSystemEvent) -&gt; None:\n\t\"\"\"\n\tCatch all events and schedule the callback after debouncing.\n\n\tArgs:\n\t    event: The file system event.\n\n\t\"\"\"\n\tif event.is_directory:\n\t\treturn  # Ignore directory events for now, focus on file changes\n\n\t# Log the specific event detected\n\tevent_type = event.event_type\n\tsrc_path = getattr(event, \"src_path\", \"N/A\")\n\tdest_path = getattr(event, \"dest_path\", \"N/A\")  # For moved events\n\n\tif event_type == \"moved\":\n\t\tlogger.debug(f\"Detected file {event_type}: {src_path} -&gt; {dest_path}\")\n\telse:\n\t\tlogger.debug(f\"Detected file {event_type}: {src_path}\")\n\n\t# Schedule the callback in a thread-safe way\n\tself._schedule_callback()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher","title":"Watcher","text":"<p>Monitors a directory for changes and triggers a callback.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>class Watcher:\n\t\"\"\"Monitors a directory for changes and triggers a callback.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tpath_to_watch: str | Path,\n\t\ton_change_callback: Callable[[], Coroutine[None, None, None]],\n\t\tdebounce_delay: float = 1.0,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the watcher.\n\n\t\tArgs:\n\t\t    path_to_watch: The directory path to monitor.\n\t\t    on_change_callback: Async function to call upon detecting changes.\n\t\t    debounce_delay: Delay in seconds to avoid rapid firing of callbacks.\n\n\t\t\"\"\"\n\t\tself.observer = Observer()\n\t\tself.path_to_watch = Path(path_to_watch).resolve()\n\t\tif not self.path_to_watch.is_dir():\n\t\t\tmsg = f\"Path to watch must be a directory: {self.path_to_watch}\"\n\t\t\traise ValueError(msg)\n\n\t\t# Save the current event loop to use for callbacks\n\t\ttry:\n\t\t\tself._event_loop = asyncio.get_event_loop()\n\t\texcept RuntimeError:\n\t\t\t# If we're not in an event loop context, create a new one\n\t\t\tself._event_loop = asyncio.new_event_loop()\n\t\t\tasyncio.set_event_loop(self._event_loop)\n\n\t\tself.event_handler = FileChangeHandler(on_change_callback, debounce_delay, event_loop=self._event_loop)\n\t\tself._stop_event = anyio.Event()  # Initialize the event\n\n\tasync def start(self) -&gt; None:\n\t\t\"\"\"Start monitoring the directory.\"\"\"\n\t\tif not self.path_to_watch.exists():\n\t\t\tlogger.warning(f\"Watch path {self.path_to_watch} does not exist. Creating it.\")\n\t\t\tself.path_to_watch.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n\n\t\tself.observer.schedule(self.event_handler, str(self.path_to_watch), recursive=True)\n\t\tself.observer.start()\n\t\tlogger.info(f\"Started watching directory: {self.path_to_watch}\")\n\t\ttry:\n\t\t\t# Wait until the stop event is set\n\t\t\tawait self._stop_event.wait()\n\t\texcept KeyboardInterrupt:\n\t\t\tlogger.info(\"Watcher stopped by user (KeyboardInterrupt).\")\n\t\tfinally:\n\t\t\t# Ensure stop is called regardless of how wait() exits\n\t\t\tself.stop()\n\n\tdef stop(self) -&gt; None:\n\t\t\"\"\"Stop monitoring the directory.\"\"\"\n\t\tif self.observer.is_alive():\n\t\t\tself.observer.stop()\n\t\t\tself.observer.join()  # Wait for observer thread to finish\n\t\t\tlogger.info(\"Watchdog observer stopped.\")\n\t\t# Set the event to signal the start method to exit\n\t\tself._stop_event.set()\n\t\tlogger.info(\"Watcher stop event set.\")\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.__init__","title":"__init__","text":"<pre><code>__init__(\n\tpath_to_watch: str | Path,\n\ton_change_callback: Callable[\n\t\t[], Coroutine[None, None, None]\n\t],\n\tdebounce_delay: float = 1.0,\n) -&gt; None\n</code></pre> <p>Initialize the watcher.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_watch</code> <code>str | Path</code> <p>The directory path to monitor.</p> required <code>on_change_callback</code> <code>Callable[[], Coroutine[None, None, None]]</code> <p>Async function to call upon detecting changes.</p> required <code>debounce_delay</code> <code>float</code> <p>Delay in seconds to avoid rapid firing of callbacks.</p> <code>1.0</code> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def __init__(\n\tself,\n\tpath_to_watch: str | Path,\n\ton_change_callback: Callable[[], Coroutine[None, None, None]],\n\tdebounce_delay: float = 1.0,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the watcher.\n\n\tArgs:\n\t    path_to_watch: The directory path to monitor.\n\t    on_change_callback: Async function to call upon detecting changes.\n\t    debounce_delay: Delay in seconds to avoid rapid firing of callbacks.\n\n\t\"\"\"\n\tself.observer = Observer()\n\tself.path_to_watch = Path(path_to_watch).resolve()\n\tif not self.path_to_watch.is_dir():\n\t\tmsg = f\"Path to watch must be a directory: {self.path_to_watch}\"\n\t\traise ValueError(msg)\n\n\t# Save the current event loop to use for callbacks\n\ttry:\n\t\tself._event_loop = asyncio.get_event_loop()\n\texcept RuntimeError:\n\t\t# If we're not in an event loop context, create a new one\n\t\tself._event_loop = asyncio.new_event_loop()\n\t\tasyncio.set_event_loop(self._event_loop)\n\n\tself.event_handler = FileChangeHandler(on_change_callback, debounce_delay, event_loop=self._event_loop)\n\tself._stop_event = anyio.Event()  # Initialize the event\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.observer","title":"observer  <code>instance-attribute</code>","text":"<pre><code>observer = Observer()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.path_to_watch","title":"path_to_watch  <code>instance-attribute</code>","text":"<pre><code>path_to_watch = resolve()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.event_handler","title":"event_handler  <code>instance-attribute</code>","text":"<pre><code>event_handler = FileChangeHandler(\n\ton_change_callback,\n\tdebounce_delay,\n\tevent_loop=_event_loop,\n)\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start monitoring the directory.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>async def start(self) -&gt; None:\n\t\"\"\"Start monitoring the directory.\"\"\"\n\tif not self.path_to_watch.exists():\n\t\tlogger.warning(f\"Watch path {self.path_to_watch} does not exist. Creating it.\")\n\t\tself.path_to_watch.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n\n\tself.observer.schedule(self.event_handler, str(self.path_to_watch), recursive=True)\n\tself.observer.start()\n\tlogger.info(f\"Started watching directory: {self.path_to_watch}\")\n\ttry:\n\t\t# Wait until the stop event is set\n\t\tawait self._stop_event.wait()\n\texcept KeyboardInterrupt:\n\t\tlogger.info(\"Watcher stopped by user (KeyboardInterrupt).\")\n\tfinally:\n\t\t# Ensure stop is called regardless of how wait() exits\n\t\tself.stop()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop monitoring the directory.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def stop(self) -&gt; None:\n\t\"\"\"Stop monitoring the directory.\"\"\"\n\tif self.observer.is_alive():\n\t\tself.observer.stop()\n\t\tself.observer.join()  # Wait for observer thread to finish\n\t\tlogger.info(\"Watchdog observer stopped.\")\n\t# Set the event to signal the start method to exit\n\tself._stop_event.set()\n\tlogger.info(\"Watcher stop event set.\")\n</code></pre>"},{"location":"contributing/","title":"Development Setup","text":"<p>Before contributing, please read our Code of Conduct and Contributing Guidelines.</p> <ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/SarthakMishra/codemap.git\ncd codemap\n</code></pre> </li> <li> <p>Install Prerequisites:</p> <ul> <li>Task: Follow the official installation guide: https://taskfile.dev/installation/</li> <li>uv: Follow the official installation guide: https://docs.astral.sh/uv/getting-started/installation/ </li> <li>Python: Ensure you have Python 3.12 or later installed.</li> </ul> </li> <li> <p>Set up the Virtual Environment: <pre><code># Create a virtual environment using uv (creates .venv directory)\nuv venv\n\n# Activate the virtual environment\n# On Linux/macOS (bash/zsh):\nsource .venv/bin/activate\n# On Windows (Command Prompt):\n# .venv\\Scripts\\activate.bat\n# On Windows (PowerShell):\n# .venv\\Scripts\\Activate.ps1\n</code></pre></p> </li> <li> <p>Install Dependencies:     Install project dependencies, including development tools, using <code>uv</code>:     <pre><code># Installs dependencies from pyproject.toml including the 'dev' group\nuv sync --dev\n</code></pre></p> </li> <li> <p>Verify Setup:     You can list available development tasks using Task:     <pre><code>task -l\n</code></pre>     To run all checks and tests (similar to CI):     <pre><code>task ci\n</code></pre></p> </li> </ol> <p>For detailed contribution guidelines, branching strategy, and coding standards, please refer to our Contributing Guide. </p>"},{"location":"contributing/code-of-conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributing/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address, without their explicit permission</li> <li>Contacting individual members, contributors, or leaders privately, outside designated community mechanisms, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"contributing/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project maintainers. Please refer to the Contributing Guide for contact information if needed. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations. </p>"},{"location":"contributing/guidelines/","title":"Contributing to CodeMap","text":"<p>First off, thank you for considering contributing to CodeMap! We welcome contributions from everyone, and we're excited to see how you can help make this AI-powered developer toolkit even better.</p> <p>This document provides guidelines for contributing to the project. Please read it carefully to ensure a smooth and effective contribution process.</p>"},{"location":"contributing/guidelines/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Contributing to CodeMap</li> <li>Table of Contents</li> <li>How Can I Contribute?<ul> <li>Reporting Bugs</li> <li>Suggesting Enhancements</li> <li>Code Contributions</li> </ul> </li> <li>Getting Started</li> <li>Branching Strategy (Simplified Git Flow)<ul> <li>Core Branches</li> <li>Supporting Branches</li> <li>Workflow Examples</li> </ul> </li> <li>Code Contribution Workflow</li> <li>Coding Standards</li> <li>Testing</li> <li>Commit Message Guidelines</li> <li>Pull Request Process</li> <li>Release Process<ul> <li>Automatic Releases</li> <li>Release Preparation</li> <li>Hotfix Process</li> </ul> </li> <li>Questions?</li> </ul>"},{"location":"contributing/guidelines/#how-can-i-contribute","title":"How Can I Contribute?","text":""},{"location":"contributing/guidelines/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you encounter a bug, please help us by reporting it!</p> <ol> <li>Check Existing Issues: Before creating a new issue, please search the GitHub Issues to see if the bug has already been reported.</li> <li>Create a New Issue: If the bug hasn't been reported, create a new issue. Please include:<ul> <li>A clear and descriptive title.</li> <li>Your operating system and Python version.</li> <li>Steps to reproduce the bug reliably.</li> <li>What you expected to happen.</li> <li>What actually happened (including any error messages or tracebacks).</li> <li>Screenshots or code snippets if relevant.</li> </ul> </li> </ol>"},{"location":"contributing/guidelines/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>We welcome suggestions for new features or improvements to existing ones.</p> <ol> <li>Check Existing Issues/Discussions: Search the GitHub Issues and Discussions to see if your idea has already been proposed.</li> <li>Create a New Issue/Discussion: If not, open a new issue or start a discussion thread. Describe:<ul> <li>The enhancement you're proposing.</li> <li>The problem it solves or the use case it addresses.</li> <li>Any potential implementation ideas (optional).</li> </ul> </li> </ol>"},{"location":"contributing/guidelines/#code-contributions","title":"Code Contributions","text":"<p>If you'd like to contribute code (bug fixes, new features), please follow the workflow outlined below.</p>"},{"location":"contributing/guidelines/#getting-started","title":"Getting Started","text":"<p>Before you start coding, make sure you have set up your development environment correctly by following the Development Setup Guide.</p>"},{"location":"contributing/guidelines/#branching-strategy-simplified-git-flow","title":"Branching Strategy (Simplified Git Flow)","text":"<p>We use a simplified Git Flow model to manage branches and releases, with automated releases powered by Python Semantic Release.</p> <pre><code>gitGraph\n    commit\n    branch dev\n    checkout dev\n    commit\n\n    branch feature/new-feature\n    checkout feature/new-feature\n    commit\n    commit\n    checkout dev\n    merge feature/new-feature tag: \"v0.2.0-next.1\"\n\n    branch feature/another-feature\n    checkout feature/another-feature\n    commit\n    checkout dev\n    merge feature/another-feature tag: \"v0.2.0-next.2\"\n\n    branch release/v0.2.0\n    checkout release/v0.2.0\n    commit\n    checkout main\n    merge release/v0.2.0 tag: \"v0.2.0\"\n    checkout dev\n    merge main\n\n    branch hotfix/critical-fix\n    checkout hotfix/critical-fix\n    commit\n    checkout main\n    merge hotfix/critical-fix tag: \"v0.2.1\"\n    checkout dev\n    merge main\n</code></pre>"},{"location":"contributing/guidelines/#core-branches","title":"Core Branches","text":"<ul> <li><code>main</code>:<ul> <li>Represents the latest stable production-ready release.</li> <li>Pushes to <code>main</code> trigger automatic stable version releases.</li> <li>Protected branch with required reviews. Changes come via approved PRs from <code>release/*</code> or <code>hotfix/*</code> branches.</li> </ul> </li> <li><code>dev</code>:<ul> <li>The primary integration branch for ongoing development and upcoming features.</li> <li>Pushes to <code>dev</code> trigger automatic pre-release versions with the <code>-next</code> tag.</li> <li>All feature branches are merged into <code>dev</code>.</li> <li>Continuously tested via CI.</li> </ul> </li> </ul>"},{"location":"contributing/guidelines/#supporting-branches","title":"Supporting Branches","text":"<ul> <li>Feature branches (<code>feature/*</code>):<ul> <li>Branched off <code>dev</code>.</li> <li>Used for developing new features or significant changes.</li> <li>Named descriptively (e.g., <code>feature/add-pr-update-command</code>).</li> <li>Merged back into <code>dev</code> via Pull Requests (PRs).</li> </ul> </li> <li>Release branches (<code>release/*</code>):<ul> <li>Branched off <code>dev</code> when preparing for a new stable release.</li> <li>Used for final testing, documentation updates, and version stabilization.</li> <li>Format: <code>release/vX.Y.0</code> (e.g., <code>release/v1.2.0</code>).</li> <li>Merged into <code>main</code> via PR, which triggers automatic release.</li> <li>No need for manual version bumping as this is handled by semantic-release.</li> </ul> </li> <li>Hotfix branches (<code>hotfix/*</code>):<ul> <li>Branched off <code>main</code>.</li> <li>Used for critical bug fixes needed in the production version.</li> <li>Merged into <code>main</code> via PR, triggering automatic patch release.</li> <li>Also merged back into <code>dev</code> (usually by merging the updated <code>main</code>).</li> </ul> </li> </ul>"},{"location":"contributing/guidelines/#workflow-examples","title":"Workflow Examples","text":"<ol> <li> <p>New Feature Development:</p> <pre><code># Start from the dev branch\ngit checkout dev\ngit pull origin dev\n\n# Create your feature branch\ngit checkout -b feature/your-feature-name\n\n# --- Make your changes ---\n\n# Push your feature branch\ngit push -u origin feature/your-feature-name\n\n# Open a Pull Request to merge `feature/your-feature-name` into `dev`\n# When merged, a new pre-release version may be created automatically\n</code></pre> </li> <li> <p>Release Preparation:</p> <pre><code>git checkout dev\ngit pull origin dev\n\n# Create a release branch (no need to bump versions manually)\ngit checkout -b release/v1.3.0\n\n# Make any final adjustments, documentation updates, etc.\n# Push the release branch\ngit push -u origin release/v1.3.0\n\n# Create a PR from release/v1.3.0 to main\n# When the PR is approved and merged:\n# 1. A new release will be automatically created\n# 2. The package will be built and published to PyPI\n# 3. Main should be merged back to dev to sync the version changes\ngit checkout dev\ngit pull origin dev\ngit merge origin/main\ngit push origin dev\n</code></pre> </li> <li> <p>Hotfix Process:     <pre><code>git checkout main\ngit pull origin main\n\n# Create a hotfix branch\ngit checkout -b hotfix/critical-bug-fix\n\n# Fix the bug and commit using conventional commit format\n# (preferably using `codemap commit`)\n\n# Push the hotfix branch\ngit push -u origin hotfix/critical-bug-fix\n\n# Create a PR from hotfix/critical-bug-fix to main\n# When merged, a patch release will be automatically created\n\n# After the hotfix is released, sync changes back to dev\ngit checkout dev\ngit pull origin dev\ngit merge origin/main\ngit push origin dev\n</code></pre></p> </li> </ol>"},{"location":"contributing/guidelines/#code-contribution-workflow","title":"Code Contribution Workflow","text":"<ol> <li>Fork &amp; Clone: Fork the repository on GitHub and clone your fork locally.     <pre><code>git clone https://github.com/YOUR_USERNAME/codemap.git\ncd codemap\ngit remote add upstream https://github.com/SarthakMishra/codemap.git\n</code></pre></li> <li>Setup: Follow the Development Setup instructions.</li> <li>Branch: Create a new branch based on the correct base branch (<code>dev</code> for features/improvements, <code>main</code> only for agreed-upon hotfixes).     <pre><code># For features/improvements\ngit checkout dev\ngit pull upstream dev # Keep dev up-to-date\ngit checkout -b feature/your-descriptive-name\n\n# For hotfixes (usually maintainers)\n# git checkout main\n# git pull upstream main\n# git checkout -b hotfix/your-fix-name\n</code></pre></li> <li>Code: Make your changes. Write clean, well-commented code. Add or update tests as necessary.</li> <li>Format &amp; Lint: Ensure your code adheres to the project's style guidelines.     <pre><code>task format\ntask lint\n# Or run all checks\ntask ci\n</code></pre></li> <li>Test: Run the test suite to ensure your changes haven't broken anything.     <pre><code>task test\n# Check coverage\ntask coverage\n</code></pre></li> <li>Commit: Commit your changes using meaningful commit messages. We strongly encourage using the <code>codemap commit</code> command to generate conventional commit messages.     <pre><code># Stage your changes\ngit add .\n# Use the interactive commit tool\ncodemap commit\n# Or if you prefer manual commits, follow conventional commit format\n# git commit -m \"feat(cli): add option for custom output format\"\n</code></pre></li> <li>Push: Push your branch to your fork.     <pre><code>git push -u origin feature/your-descriptive-name\n</code></pre></li> <li>Pull Request: Open a Pull Request (PR) from your fork's branch to the <code>upstream/dev</code> branch (or <code>upstream/main</code> for hotfixes). Provide a clear description of your changes.</li> </ol>"},{"location":"contributing/guidelines/#coding-standards","title":"Coding Standards","text":"<ul> <li>Follow PEP 8 for Python code.</li> <li>Use type hints (<code>typing</code> module).</li> <li>Write docstrings for public modules, classes, and functions (see project docs rules).</li> <li>Use <code>ruff</code> for linting and formatting (<code>task format</code>, <code>task lint</code>).</li> </ul>"},{"location":"contributing/guidelines/#testing","title":"Testing","text":"<ul> <li>Write tests using <code>pytest</code>.</li> <li>Aim for good test coverage (<code>task coverage</code>).</li> <li>Ensure all tests pass (<code>task test</code>) before submitting a PR.</li> </ul>"},{"location":"contributing/guidelines/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>We follow the Conventional Commits specification.</p> <ul> <li>Format: <code>&lt;type&gt;[optional scope]: &lt;description&gt;</code></li> <li>Example: <code>feat(commit): add semantic diff splitting strategy</code></li> <li>Use <code>codemap commit</code>: The easiest way to ensure compliance is to use the built-in <code>codemap commit</code> command.</li> </ul>"},{"location":"contributing/guidelines/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure all CI checks (linting, testing) pass.</li> <li>Provide a clear title and description for your PR.</li> <li>Link any related issues.</li> <li>Request reviews from maintainers.</li> <li>Address any feedback promptly.</li> <li>Once approved, a maintainer will merge the PR.</li> </ol>"},{"location":"contributing/guidelines/#release-process","title":"Release Process","text":"<p>Releases are managed automatically using Python Semantic Release.</p>"},{"location":"contributing/guidelines/#automatic-releases","title":"Automatic Releases","text":"<ul> <li>Merging a PR into <code>dev</code> may trigger a pre-release (e.g., <code>v1.2.0-next.1</code>).</li> <li>Merging a PR from a <code>release/*</code> or <code>hotfix/*</code> branch into <code>main</code> will trigger a stable release (e.g., <code>v1.2.0</code> or <code>v1.2.1</code>).</li> <li>The release process includes:<ul> <li>Bumping the version based on commit messages.</li> <li>Generating a changelog.</li> <li>Tagging the commit in Git.</li> <li>Creating a GitHub Release.</li> <li>Building the package.</li> <li>Publishing to PyPI.</li> </ul> </li> </ul>"},{"location":"contributing/guidelines/#release-preparation","title":"Release Preparation","text":"<p>Maintainers will create <code>release/*</code> branches off <code>dev</code> when ready to stabilize for a release. This branch allows for final testing and documentation updates before merging to <code>main</code>.</p>"},{"location":"contributing/guidelines/#hotfix-process","title":"Hotfix Process","text":"<p>Critical bugs in <code>main</code> are fixed using <code>hotfix/*</code> branches, which are merged directly back into <code>main</code> to trigger a patch release.</p>"},{"location":"contributing/guidelines/#questions","title":"Questions?","text":"<p>If you have questions, feel free to open an issue or start a discussion on GitHub. </p>"},{"location":"usage/","title":"Usage Overview","text":"<p>This section covers the main commands provided by CodeMap:</p> <ul> <li>Generate Docs (<code>gen</code>): Learn how to create optimized documentation for your codebase.</li> <li>Smart Commit (<code>commit</code>): Discover how to use AI assistance for crafting meaningful Git commit messages.</li> <li>Pull Requests (<code>pr</code>): See how CodeMap helps streamline the creation and management of pull requests.</li> <li>Ask Questions (<code>ask</code>): Use AI to answer questions about your codebase, with chat and RAG support.</li> <li>Configuration (<code>conf</code>): Manage and generate your CodeMap configuration file.</li> <li>Index Codebase (<code>index</code>): Build and synchronize the codebase vector index for semantic search and RAG. </li> </ul>"},{"location":"usage/ask/","title":"Ask Questions (<code>ask</code>)","text":"<p>Ask questions about your codebase using Retrieval-Augmented Generation (RAG) and AI chat. This command lets you query your codebase for explanations, architecture, usage, and more, either in single-question or interactive chat mode.</p>"},{"location":"usage/ask/#command-options","title":"Command Options","text":"<pre><code>codemap ask [QUESTION] [OPTIONS]\n# Or using the alias:\ncm ask [QUESTION] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>QUESTION</code>: Your question about the codebase (optional if using interactive mode)</li> </ul> <p>Options:</p> <ul> <li><code>--interactive</code>, <code>-i</code>: Start an interactive chat session (multi-turn Q&amp;A)</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/ask/#examples","title":"Examples","text":"<pre><code># Ask a single question about the codebase\ncodemap ask \"Which module manages authentication?\"\n\n# Start an interactive chat session\ncm ask --interactive\n\n# Use the alias for a quick question\ncm ask \"How does the vector index work?\"\n</code></pre>"},{"location":"usage/ask/#features","title":"Features","text":"<ul> <li>Answers questions using semantic search and LLMs (RAG)</li> <li>Interactive chat mode for multi-turn conversations</li> <li>Supports both single-question and chat workflows</li> <li>Respects project configuration and environment variables</li> </ul>"},{"location":"usage/commit/","title":"Smart Commit (<code>commit</code>)","text":"<p>Create intelligent Git commits with AI-assisted message generation. The tool analyzes your changes, splits them into logical chunks, and generates meaningful commit messages using LLMs.</p>"},{"location":"usage/commit/#basic-usage","title":"Basic Usage","text":"<pre><code># Basic usage with default settings (interactive, semantic splitting)\ncodemap commit\n# Or using the alias:\ncm commit\n\n# Commit specific files or directories\ncodemap commit path/to/file.py path/to/dir/\n\n# Run in non-interactive mode (accepts all generated messages)\ncodemap commit --non-interactive\n\n# Bypass git hooks (e.g., pre-commit)\ncodemap commit --bypass-hooks\n</code></pre>"},{"location":"usage/commit/#command-options","title":"Command Options","text":"<pre><code>codemap commit [PATHS] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATHS</code>: Optional. One or more files or directories to include in the commit (defaults to all staged changes).</li> </ul> <p>Options:</p> <ul> <li><code>--non-interactive</code>, <code>-y</code>: Run in non-interactive mode (accepts all generated messages)</li> <li><code>--bypass-hooks</code>, <code>--no-verify</code>: Bypass git hooks with <code>--no-verify</code></li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/commit/#interactive-workflow","title":"Interactive Workflow","text":"<p>The commit command provides an interactive workflow that: 1. Analyzes your changes and splits them into logical chunks 2. Generates AI-powered commit messages for each chunk 3. Allows you to:    - Accept the generated message    - Edit the message before committing    - Regenerate the message    - Skip the chunk    - Exit the process</p>"},{"location":"usage/commit/#commit-linting-feature","title":"Commit Linting Feature","text":"<p>CodeMap includes automatic commit message linting to ensure your commit messages follow conventions:</p> <ol> <li>Automatic Validation: Generated commit messages are automatically validated against conventional commit standards.</li> <li>Linting Rules: Configurable in <code>.codemap.yml</code> (see Configuration).</li> <li>Auto-remediation: If a generated message fails linting, CodeMap attempts to regenerate a compliant message.</li> <li>Fallback Mechanism: If regeneration fails, the last message is used with linting status indicated.</li> </ol>"},{"location":"usage/commit/#commit-strategy","title":"Commit Strategy","text":"<p>The tool uses semantic analysis to group related changes together based on: - File relationships - Code content similarity - Directory structure - Common file patterns</p>"},{"location":"usage/commit/#examples","title":"Examples","text":"<pre><code># Basic interactive commit\ncodemap commit\n\n# Commit specific files\ncodemap commit path/to/file.py\n\n# Non-interactive commit with all changes\ncodemap commit --non-interactive\n\n# Commit with verbose logging\ncodemap commit -v\n\n# Bypass git hooks\ncodemap commit --bypass-hooks\n</code></pre>"},{"location":"usage/commit/#notes","title":"Notes","text":"<ul> <li>Direct commit message input (<code>--message</code>), model selection (<code>--model</code>), and diff strategy (<code>--strategy</code>) are not available as CLI options. These can be configured in <code>.codemap.yml</code>.</li> <li>The command is designed for semantic, AI-powered commit workflows.</li> </ul>"},{"location":"usage/configuration/","title":"Configuration","text":"<p>The <code>conf</code> command helps you create and manage your CodeMap configuration file (<code>.codemap.yml</code>). Use it to quickly generate a default config or overwrite an existing one.</p>"},{"location":"usage/configuration/#command-options","title":"Command Options","text":"<pre><code>codemap conf [OPTIONS]\n# Or using the alias:\ncm conf [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--force</code>, <code>-f</code>: Overwrite the existing configuration file if it already exists</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/configuration/#examples","title":"Examples","text":"<pre><code># Create a default .codemap.yml in the project root\ncodemap conf\n\n# Overwrite the config file if it already exists\ncm conf --force\n</code></pre>"},{"location":"usage/configuration/#all-available-options","title":"All Available Options","text":"<p>Below are all available configuration options with their default values. Commented options are advanced and can be enabled as needed.</p> <pre><code># LLM Configuration (applies globally unless overridden by command-specific LLM config)\nllm:\n  model: \"openai/gpt-4o-mini\"   # Default LLM model (provider:model-name)\n  # temperature: 0.5             # Lower for more deterministic outputs, higher for creativity\n  # max_output_tokens: 1024      # Maximum tokens in responses\n  # api_base: null               # Custom API base URL (e.g., for local LLMs or proxies)\n\n# Embedding Configuration\nembedding:\n  model_name: \"minishlab/potion-base-8M\"  # Only Model2Vec static models are supported\n  # dimension_metric: \"cosine\"            # Metric for dimension calculation\n  # max_retries: 3                        # Maximum retries for embedding requests\n  # retry_delay: 5                        # Delay in seconds between retries\n  # max_content_length: 5000               # Maximum characters per file chunk\n  # qdrant_batch_size: 100                 # Batch size for Qdrant uploads\n  # url: \"http://localhost:6333\"          # Qdrant server URL\n  # timeout: 30                            # Qdrant client timeout in seconds\n  # prefer_grpc: true                      # Prefer gRPC for Qdrant communication\n  # chunking:\n  #   max_hierarchy_depth: 2               # Maximum depth of code hierarchy to consider\n  #   max_file_lines: 1000                 # Maximum lines per file before splitting\n  # clustering:\n  #   method: \"agglomerative\"             # Clustering method: \"agglomerative\", \"dbscan\"\n  #   agglomerative:\n  #     metric: \"precomputed\"\n  #     distance_threshold: 0.3\n  #     linkage: \"complete\"\n  #   dbscan:\n  #     eps: 0.3\n  #     min_samples: 2\n  #     algorithm: \"auto\"\n  #     metric: \"precomputed\"\n\n# RAG (Retrieval Augmented Generation) Configuration\nrag:\n  max_context_length: 8000      # Maximum context length for the LLM\n  max_context_results: 10       # Maximum number of context results to return\n  similarity_threshold: 0.75    # Minimum similarity score (0-1) for relevance\n  # system_prompt: null         # Optional system prompt to guide the RAG model\n  include_file_content: true    # Include file content in context\n  include_metadata: true        # Include file metadata in context\n\n# Sync Configuration\nsync:\n  exclude_patterns:\n    - \"^node_modules/\"\n    - \"^.venv/\"\n    - \"^venv/\"\n    - \"^env/\"\n    - \"^__pycache__/\"\n    - \"^.mypy_cache/\"\n    - \"^.pytest_cache/\"\n    - \"^.ruff_cache/\"\n    - \"^dist/\"\n    - \"^build/\"\n    - \"^.git/\"\n    - \".pyc$\"\n    - \".pyo$\"\n    - \".so$\"\n    - \".dll$\"\n\n# Documentation Generation Settings ('gen' command)\ngen:\n  max_content_length: 5000       # Max content length per file (0 = unlimited)\n  use_gitignore: true            # Respect .gitignore patterns\n  output_dir: documentation      # Directory for generated docs\n  include_tree: true             # Include directory tree in output\n  include_entity_graph: true     # Include Mermaid entity relationship graph\n  semantic_analysis: true        # Enable semantic analysis using LSP\n  lod_level: docs                # Level of Detail: signatures, structure, docs, full\n  # mermaid_entities:\n  #   - module\n  #   - class\n  #   - function\n  #   - method\n  #   - constant\n  #   - variable\n  #   - import\n  # mermaid_relationships:\n  #   - declares\n  #   - imports\n  #   - calls\n  # mermaid_show_legend: true\n  # mermaid_remove_unconnected: false\n\n# Processor configuration\nprocessor:\n  enabled: true\n  max_workers: 4\n  ignored_patterns:\n    - \"**/.git/**\"\n    - \"**/__pycache__/**\"\n    - \"**/.venv/**\"\n    - \"**/node_modules/**\"\n    - \"**/*.pyc\"\n    - \"**/dist/**\"\n    - \"**/build/**\"\n  default_lod_level: signatures\n  # watcher:\n  #   enabled: true\n  #   debounce_delay: 1.0\n\n# Commit Feature Configuration ('commit' command)\ncommit:\n  strategy: semantic             # Diff splitting strategy: file, hunk, semantic\n  bypass_hooks: false            # Default for --bypass-hooks flag (--no-verify)\n  use_lod_context: true          # Use level of detail context\n  is_non_interactive: false      # Run in non-interactive mode\n  # diff_splitter:\n  #   similarity_threshold: 0.6\n  #   directory_similarity_threshold: 0.3\n  #   file_move_similarity_threshold: 0.85\n  #   min_chunks_for_consolidation: 2\n  #   max_chunks_before_consolidation: 20\n  #   max_file_size_for_llm: 50000\n  #   max_log_diff_size: 1000\n  #   default_code_extensions:\n  #     - js\n  #     - py\n  #     - ...\n  convention:\n    types:\n      - feat\n      - fix\n      - docs\n      - style\n      - refactor\n      - perf\n      - test\n      - build\n      - ci\n      - chore\n    scopes: []                   # Optional scopes (can be auto-derived if empty)\n    max_length: 72               # Max length for commit subject line\n  # lint:\n  #   header_max_length: { level: ERROR, rule: always, value: 100 }\n  #   type_enum: { level: ERROR, rule: always }\n  #   type_case: { level: ERROR, rule: always, value: lower-case }\n  #   subject_empty: { level: ERROR, rule: never }\n  #   subject_full_stop: { level: ERROR, rule: never, value: . }\n\n# Pull Request Configuration ('pr' command)\npr:\n  defaults:\n    base_branch: null            # Default base branch (null = repo default)\n    feature_prefix: \"feature/\"   # Default prefix for feature branches\n  strategy: github-flow          # Git workflow: github-flow, gitflow, trunk-based\n  # branch_mapping:\n  #   feature: { base: develop, prefix: \"feature/\" }\n  #   release: { base: main, prefix: \"release/\" }\n  #   hotfix: { base: main, prefix: \"hotfix/\" }\n  #   bugfix: { base: develop, prefix: \"bugfix/\" }\n  generate:\n    title_strategy: llm         # How to generate title: commits, llm, template\n    description_strategy: llm   # How to generate description: commits, llm, template\n    use_workflow_templates: true # Use built-in templates based on workflow/branch type?\n    # description_template: |\n    #   ## Changes\n    #   {changes}\n    #\n    #   ## Testing\n    #   {testing_instructions}\n    #\n    #   ## Screenshots\n    #   {screenshots}\n\n# Ask Command Configuration\nask:\n  interactive_chat: false        # Enable interactive chat mode for the 'ask' command\n</code></pre>"},{"location":"usage/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>The configuration is loaded in the following order (later sources override earlier ones):</p> <ol> <li>Default configuration from the package</li> <li><code>.codemap.yml</code> in the project root</li> <li>Custom config file specified with <code>--config</code></li> <li>Command-line arguments</li> </ol>"},{"location":"usage/configuration/#configuration-tips","title":"Configuration Tips","text":"<p>Refer to the main README section for detailed tips on configuring:</p> <ul> <li>Token Limits &amp; Content Length</li> <li>Git Integration (<code>use_gitignore</code>, <code>convention.scopes</code>, <code>bypass_hooks</code>)</li> <li>LLM Settings (<code>llm.model</code>, <code>llm.api_base</code>, <code>--model</code> flag)</li> <li>Commit Conventions &amp; Linting (<code>commit.convention</code>, <code>commit.lint</code>)</li> <li>PR Workflow Settings (<code>pr.strategy</code>, <code>pr.defaults</code>, <code>pr.branch_mapping</code>, <code>pr.generate</code>)</li> <li>Documentation Generation (<code>gen.*</code> settings and flags)</li> <li>Embedding and RAG settings for advanced semantic search</li> </ul>"},{"location":"usage/generate/","title":"Generate Markdown Docs (<code>gen</code>)","text":"<p>Generate optimized markdown documentation and directory structures for your project:</p>"},{"location":"usage/generate/#command-options","title":"Command Options","text":"<pre><code>codemap gen [PATH] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATH</code>: Path to the codebase to analyze (file or directory, must exist; defaults to current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--output</code>, <code>-o</code>: Output file path for the documentation (overrides config)</li> <li><code>--config</code>, <code>-c</code>: Path to custom configuration file</li> <li><code>--max-content-length</code>: Maximum content length for file display (set to 0 for unlimited, overrides config)</li> <li><code>--lod</code>: Level of Detail for code analysis (signatures, structure, docs, full). Default: <code>docs</code>. Overrides config.</li> <li><code>--semantic/--no-semantic</code>: Enable/disable semantic analysis using LSP. Default: enabled. Overrides config.</li> <li><code>--tree/--no-tree</code>, <code>-t</code>: Include/exclude directory tree in output. Overrides config (<code>gen.include_tree</code>).</li> <li><code>--entity-graph/--no-entity-graph</code>, <code>-e</code>: Include/exclude entity relationship graph (Mermaid) in output. Overrides config (<code>gen.include_entity_graph</code>).</li> <li><code>--mermaid-entities</code>: Comma-separated list of entity types (e.g., 'module,class,function'). Overrides config (<code>gen.mermaid_entities</code>).</li> <li><code>--mermaid-relationships</code>: Comma-separated list of relationship types (e.g., 'declares,imports,calls'). Overrides config (<code>gen.mermaid_relationships</code>).</li> <li><code>--mermaid-legend/--no-mermaid-legend</code>: Show/hide the legend in the Mermaid diagram. Overrides config (<code>gen.mermaid_show_legend</code>).</li> <li><code>--mermaid-unconnected/--no-mermaid-unconnected</code>: Remove/keep nodes with no connections in the Mermaid diagram. Overrides config (<code>gen.mermaid_remove_unconnected</code>).</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/generate/#examples","title":"Examples","text":"<pre><code># Generate documentation for current directory using defaults\ncodemap gen\n# Or using the alias:\ncm gen\n\n# Generate for a specific path with full detail and no semantic analysis\ncodemap gen /path/to/project --lod full --no-semantic\n\n# Generate docs with signatures only and custom Mermaid settings\ncm gen --lod signatures --mermaid-entities \"class,function\" --mermaid-relationships \"calls\"\n\n# Generate only directory tree (implicitly disables entity graph)\ncodemap gen --tree --no-entity-graph\n\n# Custom output location and content length\ncodemap gen -o ./docs/codebase.md --max-content-length 1500\n\n# Use custom configuration file\ncodemap gen --config custom-config.yml\n\n# Verbose mode for debugging\ncodemap gen -v\n</code></pre>"},{"location":"usage/generate/#output-structure","title":"Output Structure","text":"<p>The generated documentation includes: 1. Project overview and structure 2. Directory tree visualization 3. Token-optimized code summaries 4. File relationships and dependencies 5. Rich markdown formatting with syntax highlighting</p>"},{"location":"usage/generate/#file-processing","title":"File Processing","text":"<p>The generator: - Respects <code>.gitignore</code> patterns by default - Intelligently analyzes code structure - Optimizes content for token limits - Generates well-structured markdown - Handles various file types and languages </p>"},{"location":"usage/index_cmd/","title":"Index Codebase (<code>index</code>)","text":"<p>The <code>index</code> command processes your repository, generates code embeddings, and stores them in a vector database for semantic search and Retrieval-Augmented Generation (RAG). It can also watch for file changes and keep the index up to date.</p>"},{"location":"usage/index_cmd/#command-options","title":"Command Options","text":"<pre><code>codemap index [PATH] [OPTIONS]\n# Or using the alias:\ncm index [PATH] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATH</code>: Path to the repository root directory (defaults to current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--sync/--no-sync</code>: Synchronize the vector database with the current Git state on startup (default: sync enabled)</li> <li><code>--watch</code>, <code>-w</code>: Keep running and watch for file changes, automatically syncing the index</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/index_cmd/#examples","title":"Examples","text":"<pre><code># Index the current repository and sync with Git state\ncodemap index\n\n# Index a specific directory and watch for changes\ncm index /path/to/repo --watch\n\n# Index without syncing to Git state (faster, but may miss changes)\ncodemap index --no-sync\n</code></pre>"},{"location":"usage/index_cmd/#features","title":"Features","text":"<ul> <li>Processes your codebase and builds a semantic vector index</li> <li>Supports background file watching for live updates</li> <li>Integrates with CodeMap's RAG and AI chat features</li> <li>Respects configuration and .gitignore patterns</li> </ul> <p>Keeping your index up to date ensures the best results for AI-powered search and question answering. </p>"},{"location":"usage/pr/","title":"Pull Requests (<code>pr</code>)","text":"<p>The <code>codemap pr</code> command helps you create and manage pull requests with ease. It integrates with the existing <code>codemap commit</code> command to provide a seamless workflow from code changes to pull request creation.</p>"},{"location":"usage/pr/#pr-command-features","title":"PR Command Features","text":"<ul> <li>Create branches with intelligent naming based on your current changes</li> <li>Support for multiple Git workflow strategies (GitHub Flow, GitFlow, Trunk-Based)</li> <li>Rich branch visualization with metadata and relationships</li> <li>Smart base branch selection based on branch type</li> <li>Automatic content generation for different PR types (feature, release, hotfix)</li> <li>Workflow-specific PR templates based on branch type</li> <li>Interactive PR content editing with previews</li> <li>Update existing PRs with new commits</li> <li>Configurable via <code>.codemap.yml</code> for team-wide settings (see Configuration)</li> </ul>"},{"location":"usage/pr/#pr-command-requirements","title":"PR Command Requirements","text":"<ul> <li>Git repository with a remote named <code>origin</code></li> <li>GitHub CLI (<code>gh</code>) installed for PR creation and management</li> <li>Valid GitHub authentication for the <code>gh</code> CLI</li> </ul>"},{"location":"usage/pr/#creating-a-pr","title":"Creating a PR","text":"<pre><code>codemap pr create [OPTIONS]\n# Or using the alias:\ncm pr create [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--branch</code>, <code>-b</code>: Target branch name</li> <li><code>--type</code>, <code>-t</code>: Branch type (e.g., feature, release, hotfix, bugfix). Valid types depend on workflow strategy.</li> <li><code>--base</code>: Base branch for the PR (defaults to repo default or workflow-defined default)</li> <li><code>--title</code>: Pull request title</li> <li><code>--desc</code>, <code>-d</code>: Pull request description (file path or text)</li> <li><code>--no-commit</code>: Skip the commit process before creating PR</li> <li><code>--force-push</code>, <code>-f</code>: Force push the branch</li> <li><code>--workflow</code>, <code>-w</code>: Git workflow strategy (github-flow, gitflow, trunk-based). Overrides config (<code>pr.strategy</code>).</li> <li><code>--non-interactive</code>: Run in non-interactive mode</li> <li><code>--model</code>, <code>-m</code>: LLM model for content generation (overrides config <code>llm.model</code>).</li> <li><code>--bypass-hooks</code>, <code>--no-verify</code>: Bypass git hooks with <code>--no-verify</code></li> <li><code>--api-base</code>: (Advanced) Custom API base URL for LLM</li> <li><code>--api-key</code>: (Advanced) Custom API key for LLM</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/pr/#updating-a-pr","title":"Updating a PR","text":"<pre><code>codemap pr update [OPTIONS]\n# Or using the alias:\ncm pr update [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pr</code>: PR number to update (required if not updating PR for current branch)</li> <li><code>--title</code>: New PR title</li> <li><code>--desc</code>, <code>-d</code>: New PR description (file path or text)</li> <li><code>--force-push</code>, <code>-f</code>: Force push the branch (use with caution)</li> <li><code>--workflow</code>, <code>-w</code>: Git workflow strategy (github-flow, gitflow, trunk-based)</li> <li><code>--non-interactive</code>: Run in non-interactive mode</li> <li><code>--model</code>, <code>-m</code>: LLM model for content generation</li> <li><code>--bypass-hooks</code>, <code>--no-verify</code>: Bypass git hooks with <code>--no-verify</code></li> <li><code>--api-base</code>: (Advanced) Custom API base URL for LLM</li> <li><code>--api-key</code>: (Advanced) Custom API key for LLM</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul> <p>Warning</p> <p>--no-commit is NOT an option for 'update'</p>"},{"location":"usage/pr/#notes","title":"Notes","text":"<ul> <li><code>[PATH]</code> is not required; the command operates in the current repository by default.</li> <li>Advanced LLM options (<code>--api-base</code>, <code>--api-key</code>) are rarely needed unless using a custom or self-hosted LLM endpoint.</li> </ul>"},{"location":"usage/pr/#git-workflow-strategies","title":"Git Workflow Strategies","text":"<p>The PR command supports multiple Git workflow strategies:</p> <ol> <li>GitHub Flow (default)</li> <li>Simple, linear workflow</li> <li> <p>Feature branches merge directly to main</p> </li> <li> <p>GitFlow</p> </li> <li>Feature branches \u2192 develop</li> <li>Release branches \u2192 main</li> <li> <p>Hotfix branches \u2192 main (with back-merge to develop)</p> </li> <li> <p>Trunk-Based Development</p> </li> <li>Short-lived feature branches</li> <li>Emphasizes small, frequent PRs</li> </ol>"},{"location":"usage/pr/#pr-template-system","title":"PR Template System","text":"<p>CodeMap includes a robust PR template system that automatically generates appropriate titles and descriptions based on the selected workflow strategy, branch type, and changes being made. See the Configuration page for details on customizing templates.</p>"},{"location":"usage/pr/#examples","title":"Examples","text":"<pre><code># Create PR using workflow-specific templates (GitFlow)\ncodemap pr create --workflow gitflow --type feature\n\n# Create PR with custom title but workflow-based description\ncodemap pr create --title \"My Custom Title\" --workflow trunk-based\n\n# Override both the workflow template and use custom description\ncodemap pr create --desc \"Custom description with **markdown** support\"\n\n# Non-interactive PR creation with defined template usage\ncodemap pr create --non-interactive --workflow gitflow --type release\n\n# Update an existing PR by PR number\ncodemap pr update --pr 42 --title \"Update PR Title\"\n\n# Bypass git hooks when creating or updating a PR\ncodemap pr create --bypass-hooks\ncodemap pr update --bypass-hooks\n</code></pre>"}]}