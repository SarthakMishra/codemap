{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CodeMap: AI-Powered Developer Toolkit","text":"<p>Welcome to the CodeMap documentation!</p> <p>CodeMap is an AI-powered developer toolkit designed to enhance your development workflow. It offers features like token-optimized documentation generation, semantic code analysis, and streamlined Git operations with AI assistance.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Generate Documentation: Create optimized markdown documentation and visualize repository structures.</li> <li>Smart Commits: Get AI-generated commit messages based on semantic analysis of your changes.</li> <li>AI-Powered PRs: Streamline pull request creation and management with intelligent suggestions.</li> <li>Multi-LLM Support: Integrate with various LLM providers via LiteLLM.</li> </ul> <p>Explore the documentation sections to learn more:</p> <ul> <li>Installation</li> <li>Usage Guide</li> <li>Configuration</li> <li>Development Setup</li> <li>Contributing Guide</li> </ul>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<p>CodeMap relies on these excellent open-source libraries and models:</p>"},{"location":"acknowledgements/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>LiteLLM (&gt;=1.67.0) - Unified interface for LLM providers</li> <li>NumPy (&gt;=2.2.5) - Numerical computing for vector operations</li> <li>Pygments (&gt;=2.19.1) - Syntax highlighting for code snippets</li> <li>Python-dotenv (&gt;=1.1.0) - Environment variable management</li> <li>PyYAML (&gt;=6.0.2) - YAML parsing and configuration management</li> <li>Questionary (&gt;=2.1.0) - Interactive user prompts</li> <li>Requests (&gt;=2.32.3) - HTTP library for API interactions</li> <li>Rich (&gt;=14.0.0) - Beautiful terminal formatting and output</li> <li>Typer (&gt;=0.15.2) - Modern CLI framework for Python</li> <li>Typing Extensions (&gt;=4.13.2) - Backported typing features</li> <li>Sentence-Transformers (&gt;=4.1.0) - Text embeddings for semantic code analysis</li> </ul>"},{"location":"acknowledgements/#development-tools","title":"Development Tools","text":"<ul> <li>isort (&gt;=6.0.1) - Import sorting</li> <li>pylint (&gt;=3.3.6) - Code analysis</li> <li>pyright (&gt;=1.1.399) - Static type checking</li> <li>pytest (&gt;=8.3.5) - Testing framework</li> <li>pytest-cov (&gt;=6.1.1) - Test coverage reporting</li> <li>ruff (&gt;=0.11.6) - Fast Python linter</li> </ul>"},{"location":"acknowledgements/#models","title":"Models","text":"<ul> <li>Code Embeddings: Qodo/Qodo-Embed-1-1.5B - State-of-the-art embedding model optimized for code retrieval tasks.</li> <li>LLM Support: Compatible with various providers through LiteLLM including:</li> <li>OpenAI models</li> <li>Anthropic Claude models</li> <li>Groq models</li> <li>Mistral models</li> <li>Cohere models</li> <li>Together AI models</li> <li>OpenRouter providers</li> </ul>"},{"location":"acknowledgements/#special-thanks","title":"Special Thanks","text":"<ul> <li>Cursor</li> <li>OpenHands</li> <li>GitHub Actions</li> <li>Img Shields</li> <li>Codecov </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Warning</p> <p>CodeMap currently only supports Unix-based platforms (macOS, Linux). For Windows users, we recommend using Windows Subsystem for Linux (WSL).</p> <p>Tip</p> <p>After installation, you can use either <code>codemap</code> or the shorter alias <code>cm</code> to run the commands.</p>"},{"location":"installation/#installation-using-pipx-recommended","title":"Installation using pipx (Recommended)","text":"<p>Using <code>pipx</code> is recommended as it installs the package in an isolated environment and automatically manages the PATH.</p> <pre><code># Ensure pipx is installed (install it if you haven't)\n# python3 -m pip install --user pipx\n# python3 -m pipx ensurepath\n\n# Install codemap from PyPI\npipx install codemap\n</code></pre>"},{"location":"installation/#alternative-manual-installation-using-pip","title":"Alternative: Manual Installation using pip","text":"<p>If you prefer not to use <code>pipx</code>, you can install using <code>pip</code> directly:</p> <pre><code># Install with pip (user installation)\npip install --user codemap\n\n# Make sure your PATH includes the user bin directory\n# Add the following to your shell profile (e.g., ~/.bashrc, ~/.zshrc):\n# export PATH=\"$HOME/.local/bin:$PATH\"\n# Or find the correct path using: python3 -m site --user-base\n</code></pre>"},{"location":"installation/#development-version-latest-github","title":"Development Version (Latest GitHub)","text":"<p>If you want to try the latest development version with unreleased features:</p> <pre><code># Using pipx\npipx install git+https://github.com/SarthakMishra/codemap.git\n\n# Or using pip\npip install --user git+https://github.com/SarthakMishra/codemap.git\n</code></pre>"},{"location":"installation/#updating-codemap","title":"Updating CodeMap","text":"<p>To update CodeMap to the latest version:</p> <pre><code># If installed with pipx from PyPI\npipx upgrade codemap\n\n# If installed with pip from PyPI\npip install --user --upgrade codemap\n\n# If installed from GitHub\npipx upgrade codemap  # or\npip install --user --upgrade git+https://github.com/SarthakMishra/codemap.git\n</code></pre>"},{"location":"installation/#uninstalling","title":"Uninstalling","text":"<pre><code># If installed with pipx\npipx uninstall codemap\n\n# If installed with pip\npip uninstall codemap\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>CodeMap - AI-powered developer toolkit.</p>"},{"location":"api/#main-modules","title":"Main Modules","text":"<ul> <li>Cli - Command-line interface package for CodeMap.</li> <li>Config - Default configuration settings for the codemap tool.</li> <li>Db - Database management utilities using SQLModel.</li> <li>Gen - Code documentation generation package for CodeMap.</li> <li>Git - Git utilities for CodeMap.</li> <li>Llm - LLM module for CodeMap.</li> <li>Processor - CodeMap processor module.</li> <li>Utils - Utility module for CodeMap package.</li> <li>Watcher - Watcher module for CodeMap.</li> </ul>"},{"location":"api/config/","title":"Config","text":"<p>Default configuration settings for the codemap tool.</p>"},{"location":"api/config/#codemap.config.DEFAULT_CONFIG","title":"DEFAULT_CONFIG  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONFIG = {\n\t\"cache_dir\": \".codemap_cache\",\n\t\"llm\": {\n\t\t\"model\": \"openai/gpt-4o-mini\",\n\t\t\"api_base\": None,\n\t\t\"max_context_tokens\": 4000,\n\t\t\"use_lod_context\": True,\n\t\t\"use_batch_processing\": True,\n\t},\n\t\"embedding\": {\n\t\t\"model_name\": \"voyage-code-3\",\n\t\t\"dimension\": 1024,\n\t\t\"dimension_metric\": \"cosine\",\n\t\t\"max_retries\": 3,\n\t\t\"retry_delay\": 5,\n\t\t\"max_content_length\": 5000,\n\t\t\"qdrant_location\": \":memory:\",\n\t\t\"qdrant_collection_name\": \"codemap_vectors\",\n\t\t\"batch_size\": 32,\n\t\t\"qdrant_batch_size\": 100,\n\t\t\"url\": \"http://localhost:6333\",\n\t\t\"api_key\": None,\n\t\t\"timeout\": 30,\n\t\t\"prefer_grpc\": True,\n\t\t\"chunking\": {\n\t\t\t\"max_hierarchy_depth\": 2,\n\t\t\t\"max_file_lines\": 1000,\n\t\t},\n\t},\n\t\"rag\": {\n\t\t\"max_context_length\": 8000,\n\t\t\"max_context_results\": 10,\n\t\t\"similarity_threshold\": 0.75,\n\t\t\"system_prompt\": None,\n\t\t\"include_file_content\": True,\n\t\t\"include_metadata\": True,\n\t},\n\t\"sync\": {\n\t\t\"exclude_patterns\": [\n\t\t\t\"^node_modules/\",\n\t\t\t\"^\\\\.venv/\",\n\t\t\t\"^venv/\",\n\t\t\t\"^env/\",\n\t\t\t\"^__pycache__/\",\n\t\t\t\"^\\\\.mypy_cache/\",\n\t\t\t\"^\\\\.pytest_cache/\",\n\t\t\t\"^\\\\.ruff_cache/\",\n\t\t\t\"^dist/\",\n\t\t\t\"^build/\",\n\t\t\t\"^\\\\.git/\",\n\t\t\t\"\\\\.pyc$\",\n\t\t\t\"\\\\.pyo$\",\n\t\t\t\"\\\\.so$\",\n\t\t\t\"\\\\.dll$\",\n\t\t]\n\t},\n\t\"gen\": {\n\t\t\"max_content_length\": 5000,\n\t\t\"use_gitignore\": True,\n\t\t\"output_dir\": \"documentation\",\n\t\t\"include_tree\": True,\n\t\t\"include_entity_graph\": True,\n\t\t\"semantic_analysis\": True,\n\t\t\"lod_level\": \"docs\",\n\t\t\"mermaid_entities\": [\n\t\t\t\"module\",\n\t\t\t\"class\",\n\t\t\t\"function\",\n\t\t\t\"method\",\n\t\t\t\"constant\",\n\t\t\t\"variable\",\n\t\t\t\"import\",\n\t\t],\n\t\t\"mermaid_relationships\": [\n\t\t\t\"declares\",\n\t\t\t\"imports\",\n\t\t\t\"calls\",\n\t\t],\n\t\t\"mermaid_show_legend\": True,\n\t\t\"mermaid_remove_unconnected\": False,\n\t},\n\t\"processor\": {\n\t\t\"enabled\": True,\n\t\t\"max_workers\": 4,\n\t\t\"ignored_patterns\": [\n\t\t\t\"**/.git/**\",\n\t\t\t\"**/__pycache__/**\",\n\t\t\t\"**/.venv/**\",\n\t\t\t\"**/node_modules/**\",\n\t\t\t\"**/*.pyc\",\n\t\t\t\"**/dist/**\",\n\t\t\t\"**/build/**\",\n\t\t],\n\t\t\"default_lod_level\": \"signatures\",\n\t},\n\t\"commit\": {\n\t\t\"strategy\": \"file\",\n\t\t\"bypass_hooks\": False,\n\t\t\"diff_splitter\": {\n\t\t\t\"similarity_threshold\": 0.4,\n\t\t\t\"directory_similarity_threshold\": 0.3,\n\t\t\t\"min_chunks_for_consolidation\": 2,\n\t\t\t\"max_chunks_before_consolidation\": 20,\n\t\t\t\"max_file_size_for_llm\": 50000,\n\t\t\t\"max_log_diff_size\": 1000,\n\t\t\t\"model_name\": \"sarthak1/Qodo-Embed-M-1-1.5B-M2V-Distilled\",\n\t\t\t\"default_code_extensions\": [\n\t\t\t\t\"js\",\n\t\t\t\t\"jsx\",\n\t\t\t\t\"ts\",\n\t\t\t\t\"tsx\",\n\t\t\t\t\"py\",\n\t\t\t\t\"java\",\n\t\t\t\t\"c\",\n\t\t\t\t\"cpp\",\n\t\t\t\t\"h\",\n\t\t\t\t\"hpp\",\n\t\t\t\t\"cc\",\n\t\t\t\t\"cs\",\n\t\t\t\t\"go\",\n\t\t\t\t\"rb\",\n\t\t\t\t\"php\",\n\t\t\t\t\"rs\",\n\t\t\t\t\"swift\",\n\t\t\t\t\"scala\",\n\t\t\t\t\"kt\",\n\t\t\t\t\"sh\",\n\t\t\t\t\"pl\",\n\t\t\t\t\"pm\",\n\t\t\t],\n\t\t},\n\t\t\"convention\": {\n\t\t\t\"types\": [\n\t\t\t\t\"feat\",\n\t\t\t\t\"fix\",\n\t\t\t\t\"docs\",\n\t\t\t\t\"style\",\n\t\t\t\t\"refactor\",\n\t\t\t\t\"perf\",\n\t\t\t\t\"test\",\n\t\t\t\t\"build\",\n\t\t\t\t\"ci\",\n\t\t\t\t\"chore\",\n\t\t\t],\n\t\t\t\"scopes\": [],\n\t\t\t\"max_length\": 72,\n\t\t},\n\t\t\"lint\": {\n\t\t\t\"header_max_length\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t\t\"value\": 100,\n\t\t\t},\n\t\t\t\"header_case\": {\n\t\t\t\t\"level\": \"DISABLED\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t\t\"value\": \"lower-case\",\n\t\t\t},\n\t\t\t\"header_full_stop\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t\t\"value\": \".\",\n\t\t\t},\n\t\t\t\"type_enum\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t},\n\t\t\t\"type_case\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t\t\"value\": \"lower-case\",\n\t\t\t},\n\t\t\t\"type_empty\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t},\n\t\t\t\"scope_case\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t\t\"value\": \"lower-case\",\n\t\t\t},\n\t\t\t\"scope_empty\": {\n\t\t\t\t\"level\": \"DISABLED\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t},\n\t\t\t\"scope_enum\": {\n\t\t\t\t\"level\": \"DISABLED\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t},\n\t\t\t\"subject_case\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t\t\"value\": [\n\t\t\t\t\t\"sentence-case\",\n\t\t\t\t\t\"start-case\",\n\t\t\t\t\t\"pascal-case\",\n\t\t\t\t\t\"upper-case\",\n\t\t\t\t],\n\t\t\t},\n\t\t\t\"subject_empty\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t},\n\t\t\t\"subject_full_stop\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t\t\"value\": \".\",\n\t\t\t},\n\t\t\t\"subject_exclamation_mark\": {\n\t\t\t\t\"level\": \"DISABLED\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t},\n\t\t\t\"body_leading_blank\": {\n\t\t\t\t\"level\": \"WARNING\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t},\n\t\t\t\"body_empty\": {\n\t\t\t\t\"level\": \"DISABLED\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t},\n\t\t\t\"body_max_line_length\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t\t\"value\": 100,\n\t\t\t},\n\t\t\t\"footer_leading_blank\": {\n\t\t\t\t\"level\": \"WARNING\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t},\n\t\t\t\"footer_empty\": {\n\t\t\t\t\"level\": \"DISABLED\",\n\t\t\t\t\"rule\": \"never\",\n\t\t\t},\n\t\t\t\"footer_max_line_length\": {\n\t\t\t\t\"level\": \"ERROR\",\n\t\t\t\t\"rule\": \"always\",\n\t\t\t\t\"value\": 100,\n\t\t\t},\n\t\t},\n\t},\n\t\"pr\": {\n\t\t\"defaults\": {\n\t\t\t\"base_branch\": None,\n\t\t\t\"feature_prefix\": \"feature/\",\n\t\t},\n\t\t\"strategy\": \"github-flow\",\n\t\t\"branch_mapping\": {\n\t\t\t\"feature\": {\n\t\t\t\t\"base\": \"develop\",\n\t\t\t\t\"prefix\": \"feature/\",\n\t\t\t},\n\t\t\t\"release\": {\n\t\t\t\t\"base\": \"main\",\n\t\t\t\t\"prefix\": \"release/\",\n\t\t\t},\n\t\t\t\"hotfix\": {\"base\": \"main\", \"prefix\": \"hotfix/\"},\n\t\t\t\"bugfix\": {\n\t\t\t\t\"base\": \"develop\",\n\t\t\t\t\"prefix\": \"bugfix/\",\n\t\t\t},\n\t\t},\n\t\t\"generate\": {\n\t\t\t\"title_strategy\": \"commits\",\n\t\t\t\"description_strategy\": \"commits\",\n\t\t\t\"description_template\": dedent(\n\t\t\t\t\"\\t\\t\\t## Changes\\n\\t\\t\\t{changes}\\n\\n\\t\\t\\t## Testing\\n\\t\\t\\t{testing_instructions}\\n\\n\\t\\t\\t## Screenshots\\n\\t\\t\\t{screenshots}\\n\\t\\t\\t\"\n\t\t\t),\n\t\t\t\"use_workflow_templates\": True,\n\t\t},\n\t},\n\t\"ask\": {\"interactive_chat\": False},\n}\n</code></pre>"},{"location":"api/cli/","title":"Cli Overview","text":"<p>Command-line interface package for CodeMap.</p> <ul> <li>Ask Cmd - CLI command for asking questions about the codebase using RAG.</li> <li>Commit Cmd - Command for generating conventional commit messages from Git diffs.</li> <li>Gen Cmd - CLI command for generating code documentation.</li> <li>Index Cmd - CLI command for indexing repositories.</li> <li>Pr Cmd - CLI command for generating pull requests using the refactored lazy-loading pattern.</li> </ul>"},{"location":"api/cli/ask_cmd/","title":"Ask Cmd","text":"<p>CLI command for asking questions about the codebase using RAG.</p>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.QuestionArg","title":"QuestionArg  <code>module-attribute</code>","text":"<pre><code>QuestionArg = Annotated[\n\tstr | None,\n\tArgument(\n\t\thelp=\"Your question about the codebase (omit for interactive mode).\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.PathOpt","title":"PathOpt  <code>module-attribute</code>","text":"<pre><code>PathOpt = Annotated[\n\tPath | None,\n\tOption(\n\t\t\"--path\",\n\t\t\"-p\",\n\t\thelp=\"Path to the repository root (defaults to current directory).\",\n\t\texists=True,\n\t\tfile_okay=False,\n\t\tdir_okay=True,\n\t\tresolve_path=True,\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.ModelOpt","title":"ModelOpt  <code>module-attribute</code>","text":"<pre><code>ModelOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--model\",\n\t\t\"-m\",\n\t\thelp=\"LLM model to use (e.g., 'openai/gpt-4o-mini'). Overrides config.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.ApiBaseOpt","title":"ApiBaseOpt  <code>module-attribute</code>","text":"<pre><code>ApiBaseOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--api-base\", help=\"Override the LLM API base URL.\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.ApiKeyOpt","title":"ApiKeyOpt  <code>module-attribute</code>","text":"<pre><code>ApiKeyOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--api-key\",\n\t\thelp=\"Override the LLM API key (use environment variables for security).\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.InteractiveFlag","title":"InteractiveFlag  <code>module-attribute</code>","text":"<pre><code>InteractiveFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--interactive\",\n\t\t\"-i\",\n\t\thelp=\"Start an interactive chat session.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.VerboseFlag","title":"VerboseFlag  <code>module-attribute</code>","text":"<pre><code>VerboseFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--verbose\", \"-v\", help=\"Enable verbose logging.\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/ask_cmd/#codemap.cli.ask_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the ask command with the CLI app.</p> Source code in <code>src/codemap/cli/ask_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the ask command with the CLI app.\"\"\"\n\n\t@app.command(name=\"ask\")\n\t@asyncer.runnify  # Apply runnify directly to the command function\n\tasync def ask_command(\n\t\tquestion: QuestionArg = None,\n\t\tpath: PathOpt = None,\n\t\tmodel: ModelOpt = None,\n\t\tapi_base: ApiBaseOpt = None,\n\t\tapi_key: ApiKeyOpt = None,\n\t\tinteractive: InteractiveFlag = False,\n\t\tis_verbose: VerboseFlag = False,\n\t) -&gt; None:\n\t\t\"\"\"Ask questions about the codebase using Retrieval-Augmented Generation (RAG).\"\"\"\n\t\t# Defer heavy imports and logic to the implementation function\n\t\tawait _ask_command_impl(\n\t\t\tquestion=question,\n\t\t\tpath=path,\n\t\t\tmodel=model,\n\t\t\tapi_base=api_base,\n\t\t\tapi_key=api_key,\n\t\t\tinteractive=interactive,\n\t\t\tis_verbose=is_verbose,\n\t\t)\n</code></pre>"},{"location":"api/cli/commit_cmd/","title":"Commit Cmd","text":"<p>Command for generating conventional commit messages from Git diffs.</p>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.PathArg","title":"PathArg  <code>module-attribute</code>","text":"<pre><code>PathArg = Annotated[\n\tPath | None,\n\tArgument(\n\t\thelp=\"Path to repository or file to commit\",\n\t\texists=True,\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.MessageOpt","title":"MessageOpt  <code>module-attribute</code>","text":"<pre><code>MessageOpt = Annotated[\n\tstr | None,\n\tOption(\"--message\", \"-m\", help=\"Commit message\"),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.AllFilesFlag","title":"AllFilesFlag  <code>module-attribute</code>","text":"<pre><code>AllFilesFlag = Annotated[\n\tbool, Option(\"--all\", \"-a\", help=\"Commit all changes\")\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.ModelOpt","title":"ModelOpt  <code>module-attribute</code>","text":"<pre><code>ModelOpt = Annotated[\n\tstr,\n\tOption(\n\t\t\"--model\",\n\t\t\"-m\",\n\t\thelp=\"LLM model to use for message generation\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.NonInteractiveFlag","title":"NonInteractiveFlag  <code>module-attribute</code>","text":"<pre><code>NonInteractiveFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--non-interactive\",\n\t\t\"-y\",\n\t\thelp=\"Run in non-interactive mode\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.BypassHooksFlag","title":"BypassHooksFlag  <code>module-attribute</code>","text":"<pre><code>BypassHooksFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--bypass-hooks\",\n\t\t\"--no-verify\",\n\t\thelp=\"Bypass git hooks with --no-verify\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.VerboseFlag","title":"VerboseFlag  <code>module-attribute</code>","text":"<pre><code>VerboseFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--verbose\", \"-v\", help=\"Enable verbose logging\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.EmbeddingModelOpt","title":"EmbeddingModelOpt  <code>module-attribute</code>","text":"<pre><code>EmbeddingModelOpt = Annotated[\n\tstr,\n\tOption(\n\t\t\"--embedding-model\",\n\t\thelp=\"Model to use for embedding generation\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.ClusteringMethodOpt","title":"ClusteringMethodOpt  <code>module-attribute</code>","text":"<pre><code>ClusteringMethodOpt = Annotated[\n\tstr,\n\tOption(\n\t\t\"--clustering-method\",\n\t\thelp=\"Method to use for clustering ('agglomerative' or 'dbscan')\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.SimilarityThresholdOpt","title":"SimilarityThresholdOpt  <code>module-attribute</code>","text":"<pre><code>SimilarityThresholdOpt = Annotated[\n\tfloat,\n\tOption(\n\t\t\"--similarity-threshold\",\n\t\thelp=\"Threshold for group similarity to trigger merging (0.0-1.0)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/commit_cmd/#codemap.cli.commit_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the commit commands with the CLI app.</p> Source code in <code>src/codemap/cli/commit_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the commit commands with the CLI app.\"\"\"\n\n\t@app.command(name=\"commit\")\n\tdef semantic_commit_command(\n\t\tpath: PathArg = None,\n\t\tmodel: ModelOpt = \"gpt-4o-mini\",\n\t\tnon_interactive: NonInteractiveFlag = False,\n\t\tbypass_hooks: BypassHooksFlag = False,\n\t\tembedding_model: EmbeddingModelOpt = \"all-MiniLM-L6-v2\",\n\t\tclustering_method: ClusteringMethodOpt = \"agglomerative\",\n\t\tsimilarity_threshold: SimilarityThresholdOpt = 0.6,\n\t\tis_verbose: VerboseFlag = False,\n\t\tpathspecs: list[str] | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGenerate semantic commits by grouping related changes.\n\n\t\tThis command analyzes your changes, groups them semantically, and\n\t\tcreates multiple focused commits with AI-generated messages.\n\n\t\t\"\"\"\n\t\t# Defer heavy imports and logic to the implementation function\n\t\t_semantic_commit_command_impl(\n\t\t\tpath=path,\n\t\t\tmodel=model,\n\t\t\tnon_interactive=non_interactive,\n\t\t\tbypass_hooks=bypass_hooks,\n\t\t\tembedding_model=embedding_model,\n\t\t\tclustering_method=clustering_method,\n\t\t\tsimilarity_threshold=similarity_threshold,\n\t\t\tis_verbose=is_verbose,\n\t\t\tpathspecs=pathspecs,\n\t\t)\n</code></pre>"},{"location":"api/cli/gen_cmd/","title":"Gen Cmd","text":"<p>CLI command for generating code documentation.</p>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.PathArg","title":"PathArg  <code>module-attribute</code>","text":"<pre><code>PathArg = Annotated[\n\tPath,\n\tArgument(\n\t\texists=True,\n\t\thelp=\"Path to the codebase to analyze\",\n\t\tshow_default=True,\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.OutputOpt","title":"OutputOpt  <code>module-attribute</code>","text":"<pre><code>OutputOpt = Annotated[\n\tPath | None,\n\tOption(\n\t\t\"--output\",\n\t\t\"-o\",\n\t\thelp=\"Output file path (overrides config)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.ConfigOpt","title":"ConfigOpt  <code>module-attribute</code>","text":"<pre><code>ConfigOpt = Annotated[\n\tPath | None,\n\tOption(\"--config\", \"-c\", help=\"Path to config file\"),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MaxContentLengthOpt","title":"MaxContentLengthOpt  <code>module-attribute</code>","text":"<pre><code>MaxContentLengthOpt = Annotated[\n\tint | None,\n\tOption(\n\t\t\"--max-content-length\",\n\t\thelp=\"Maximum content length for file display (set to 0 for unlimited)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.TreeOpt","title":"TreeOpt  <code>module-attribute</code>","text":"<pre><code>TreeOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--tree/--no-tree\",\n\t\t\"-t\",\n\t\thelp=\"Include directory tree in output\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.VerboseFlag","title":"VerboseFlag  <code>module-attribute</code>","text":"<pre><code>VerboseFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--verbose\", \"-v\", help=\"Enable verbose logging\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.EntityGraphOpt","title":"EntityGraphOpt  <code>module-attribute</code>","text":"<pre><code>EntityGraphOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--entity-graph/--no-entity-graph\",\n\t\t\"-e\",\n\t\thelp=\"Include entity relationship graph in output\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.LODLevelOpt","title":"LODLevelOpt  <code>module-attribute</code>","text":"<pre><code>LODLevelOpt = Annotated[\n\tstr,\n\tOption(\n\t\t\"--lod\",\n\t\thelp=\"Level of Detail for code analysis (e.g., 'full', 'docs', 'signatures')\",\n\t\tcase_sensitive=False,\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidEntitiesOpt","title":"MermaidEntitiesOpt  <code>module-attribute</code>","text":"<pre><code>MermaidEntitiesOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--mermaid-entities\",\n\t\thelp=\"Comma-separated list of entity types to include in Mermaid graph (e.g., 'module,class,function')\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidRelationshipsOpt","title":"MermaidRelationshipsOpt  <code>module-attribute</code>","text":"<pre><code>MermaidRelationshipsOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--mermaid-relationships\",\n\t\thelp=\"Comma-separated list of relationship types to include in Mermaid graph (e.g., 'declares,imports,calls')\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidLegendOpt","title":"MermaidLegendOpt  <code>module-attribute</code>","text":"<pre><code>MermaidLegendOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--mermaid-legend/--no-mermaid-legend\",\n\t\thelp=\"Show/hide the legend in the Mermaid diagram\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.MermaidUnconnectedOpt","title":"MermaidUnconnectedOpt  <code>module-attribute</code>","text":"<pre><code>MermaidUnconnectedOpt = Annotated[\n\tbool | None,\n\tOption(\n\t\t\"--mermaid-unconnected/--no-mermaid-unconnected\",\n\t\thelp=\"Remove/keep nodes with no connections in the Mermaid diagram\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.SemanticAnalysisOpt","title":"SemanticAnalysisOpt  <code>module-attribute</code>","text":"<pre><code>SemanticAnalysisOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--semantic/--no-semantic\",\n\t\thelp=\"Enable/disable semantic analysis\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/gen_cmd/#codemap.cli.gen_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the gen command with the CLI app.</p> Source code in <code>src/codemap/cli/gen_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the gen command with the CLI app.\"\"\"\n\n\t@app.command(name=\"gen\")\n\tdef gen_command(\n\t\tpath: PathArg = Path(),\n\t\toutput: OutputOpt = None,\n\t\tconfig: ConfigOpt = None,\n\t\tmax_content_length: MaxContentLengthOpt = None,\n\t\tlod_level_str: LODLevelOpt = \"docs\",\n\t\tsemantic_analysis: SemanticAnalysisOpt = True,\n\t\ttree: TreeOpt = None,\n\t\tis_verbose: VerboseFlag = False,\n\t\tentity_graph: EntityGraphOpt = None,\n\t\tmermaid_entities_str: MermaidEntitiesOpt = None,\n\t\tmermaid_relationships_str: MermaidRelationshipsOpt = None,\n\t\tmermaid_show_legend_flag: MermaidLegendOpt = None,\n\t\tmermaid_remove_unconnected_flag: MermaidUnconnectedOpt = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGenerate code documentation.\n\n\t\tThis command processes a codebase and generates Markdown documentation\n\t\twith configurable level of detail.\n\n\t\tExamples:\n\t\t        codemap gen                      # Generate docs for current directory\n\t\t        codemap gen --lod full           # Generate full implementation docs\n\t\t        codemap gen --lod signatures     # Generate docs with signatures only\n\t\t        codemap gen --no-semantic        # Generate without semantic analysis\n\n\t\t\"\"\"\n\t\t# Defer all heavy imports by calling implementation function\n\t\t_gen_command_impl(\n\t\t\tpath=path,\n\t\t\toutput=output,\n\t\t\tconfig=config,\n\t\t\tmax_content_length=max_content_length,\n\t\t\tlod_level_str=lod_level_str,\n\t\t\tsemantic_analysis=semantic_analysis,\n\t\t\ttree=tree,\n\t\t\tis_verbose=is_verbose,\n\t\t\tentity_graph=entity_graph,\n\t\t\tmermaid_entities_str=mermaid_entities_str,\n\t\t\tmermaid_relationships_str=mermaid_relationships_str,\n\t\t\tmermaid_show_legend_flag=mermaid_show_legend_flag,\n\t\t\tmermaid_remove_unconnected_flag=mermaid_remove_unconnected_flag,\n\t\t)\n</code></pre>"},{"location":"api/cli/index_cmd/","title":"Index Cmd","text":"<p>CLI command for indexing repositories.</p>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.PathArg","title":"PathArg  <code>module-attribute</code>","text":"<pre><code>PathArg = Annotated[\n\tPath,\n\tArgument(\n\t\thelp=\"Path to the repository root directory.\",\n\t\texists=True,\n\t\tfile_okay=False,\n\t\tdir_okay=True,\n\t\treadable=True,\n\t\tresolve_path=True,\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.SyncOpt","title":"SyncOpt  <code>module-attribute</code>","text":"<pre><code>SyncOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--sync/--no-sync\",\n\t\thelp=\"Synchronize the vector database with the current Git state on startup.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.WatchOpt","title":"WatchOpt  <code>module-attribute</code>","text":"<pre><code>WatchOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--watch\",\n\t\t\"-w\",\n\t\thelp=\"Keep running and watch for file changes, automatically syncing the index.\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.LogLevelOpt","title":"LogLevelOpt  <code>module-attribute</code>","text":"<pre><code>LogLevelOpt = Annotated[\n\tstr,\n\tOption(\n\t\t\"--log-level\",\n\t\t\"-L\",\n\t\thelp=\"Set the logging level (e.g., DEBUG, INFO, WARNING).\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.VerboseFlag","title":"VerboseFlag  <code>module-attribute</code>","text":"<pre><code>VerboseFlag = Annotated[\n\tbool,\n\tOption(\n\t\t\"--verbose\", \"-v\", help=\"Enable verbose logging\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.ConfigOpt","title":"ConfigOpt  <code>module-attribute</code>","text":"<pre><code>ConfigOpt = Annotated[\n\tPath | None,\n\tOption(\"--config\", \"-c\", help=\"Path to config file\"),\n]\n</code></pre>"},{"location":"api/cli/index_cmd/#codemap.cli.index_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the index command with the CLI app.</p> Source code in <code>src/codemap/cli/index_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the index command with the CLI app.\"\"\"\n\n\t@app.command(name=\"index\")\n\tdef index_command(\n\t\tpath: PathArg = Path(),\n\t\tsync: SyncOpt = True,\n\t\twatch: WatchOpt = False,\n\t\tlog_level: LogLevelOpt = \"INFO\",\n\t\tis_verbose: VerboseFlag = False,\n\t\tconfig: ConfigOpt = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tIndex the repository: Process files, generate embeddings, and store in the vector database.\n\n\t\tOptionally, use --sync (default) to synchronize with the Git state on startup,\n\t\tand --watch (-w) to keep running and sync automatically on file changes.\n\t\t\"\"\"\n\t\t# Defer heavy imports and logic to the implementation function\n\t\t_index_command_impl(\n\t\t\tpath=path,\n\t\t\tsync=sync,\n\t\t\twatch=watch,\n\t\t\tlog_level=log_level,\n\t\t\tis_verbose=is_verbose,\n\t\t\tconfig=config,\n\t\t)\n</code></pre>"},{"location":"api/cli/pr_cmd/","title":"Pr Cmd","text":"<p>CLI command for generating pull requests using the refactored lazy-loading pattern.</p>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRAction","title":"PRAction","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Actions for PR command.</p> Source code in <code>src/codemap/cli/pr_cmd.py</code> <pre><code>class PRAction(str, Enum):\n\t\"\"\"Actions for PR command.\"\"\"\n\n\tCREATE = \"create\"\n\tUPDATE = \"update\"\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRAction.CREATE","title":"CREATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CREATE = 'create'\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRAction.UPDATE","title":"UPDATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UPDATE = 'update'\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.validate_workflow_strategy","title":"validate_workflow_strategy","text":"<pre><code>validate_workflow_strategy(value: str | None) -&gt; str | None\n</code></pre> <p>Validate workflow strategy - lightweight callback for typer.</p> Source code in <code>src/codemap/cli/pr_cmd.py</code> <pre><code>def validate_workflow_strategy(value: str | None) -&gt; str | None:\n\t\"\"\"Validate workflow strategy - lightweight callback for typer.\"\"\"\n\t# Avoid heavy imports like Console here\n\tvalid_strategies = [\"github-flow\", \"gitflow\", \"trunk-based\"]\n\tif value is None or value in valid_strategies:\n\t\treturn value\n\tmsg = f\"Invalid workflow strategy: {value}. Must be one of: {', '.join(valid_strategies)}\"\n\traise typer.BadParameter(msg)\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PathArg","title":"PathArg  <code>module-attribute</code>","text":"<pre><code>PathArg = Annotated[\n\tPath,\n\tArgument(\n\t\texists=True,\n\t\thelp=\"Path to the codebase to analyze\",\n\t\tshow_default=False,\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ActionArg","title":"ActionArg  <code>module-attribute</code>","text":"<pre><code>ActionArg = Annotated[\n\tPRAction,\n\tArgument(help=\"Action to perform: create or update\"),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.BranchNameOpt","title":"BranchNameOpt  <code>module-attribute</code>","text":"<pre><code>BranchNameOpt = Annotated[\n\tstr | None,\n\tOption(\"--branch\", \"-b\", help=\"Target branch name\"),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.BranchTypeOpt","title":"BranchTypeOpt  <code>module-attribute</code>","text":"<pre><code>BranchTypeOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--type\",\n\t\t\"-t\",\n\t\thelp=\"Branch type (feature, release, hotfix, bugfix)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.BaseBranchOpt","title":"BaseBranchOpt  <code>module-attribute</code>","text":"<pre><code>BaseBranchOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--base\",\n\t\thelp=\"Base branch for the PR (defaults to repo default)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.TitleOpt","title":"TitleOpt  <code>module-attribute</code>","text":"<pre><code>TitleOpt = Annotated[\n\tstr | None, Option(\"--title\", help=\"Pull request title\")\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.DescriptionOpt","title":"DescriptionOpt  <code>module-attribute</code>","text":"<pre><code>DescriptionOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--desc\",\n\t\t\"-d\",\n\t\thelp=\"Pull request description (file path or text)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.NoCommitOpt","title":"NoCommitOpt  <code>module-attribute</code>","text":"<pre><code>NoCommitOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--no-commit\",\n\t\thelp=\"Skip the commit process before creating PR\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ForcePushOpt","title":"ForcePushOpt  <code>module-attribute</code>","text":"<pre><code>ForcePushOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--force-push\", \"-f\", help=\"Force push the branch\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.PRNumberOpt","title":"PRNumberOpt  <code>module-attribute</code>","text":"<pre><code>PRNumberOpt = Annotated[\n\tint | None,\n\tOption(\n\t\t\"--pr\",\n\t\thelp=\"PR number to update (required for update action)\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.WorkflowOpt","title":"WorkflowOpt  <code>module-attribute</code>","text":"<pre><code>WorkflowOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--workflow\",\n\t\t\"-w\",\n\t\thelp=\"Git workflow strategy (github-flow, gitflow, trunk-based)\",\n\t\tcallback=validate_workflow_strategy,\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.NonInteractiveOpt","title":"NonInteractiveOpt  <code>module-attribute</code>","text":"<pre><code>NonInteractiveOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--non-interactive\",\n\t\thelp=\"Run in non-interactive mode\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ModelOpt","title":"ModelOpt  <code>module-attribute</code>","text":"<pre><code>ModelOpt = Annotated[\n\tstr | None,\n\tOption(\n\t\t\"--model\",\n\t\t\"-m\",\n\t\thelp=\"LLM model for content generation\",\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ApiBaseOpt","title":"ApiBaseOpt  <code>module-attribute</code>","text":"<pre><code>ApiBaseOpt = Annotated[\n\tstr | None,\n\tOption(\"--api-base\", help=\"API base URL for LLM\"),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.ApiKeyOpt","title":"ApiKeyOpt  <code>module-attribute</code>","text":"<pre><code>ApiKeyOpt = Annotated[\n\tstr | None, Option(\"--api-key\", help=\"API key for LLM\")\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.VerboseOpt","title":"VerboseOpt  <code>module-attribute</code>","text":"<pre><code>VerboseOpt = Annotated[\n\tbool,\n\tOption(\n\t\t\"--verbose\", \"-v\", help=\"Enable verbose logging\"\n\t),\n]\n</code></pre>"},{"location":"api/cli/pr_cmd/#codemap.cli.pr_cmd.register_command","title":"register_command","text":"<pre><code>register_command(app: Typer) -&gt; None\n</code></pre> <p>Register the pr command with the Typer app.</p> Source code in <code>src/codemap/cli/pr_cmd.py</code> <pre><code>def register_command(app: typer.Typer) -&gt; None:\n\t\"\"\"Register the pr command with the Typer app.\"\"\"\n\n\t@app.command(\"pr\")\n\tdef pr_command_entrypoint(\n\t\tpath_arg: PathArg = Path(),  # Default path to current dir\n\t\taction: ActionArg = PRAction.CREATE,\n\t\tbranch_name: BranchNameOpt = None,\n\t\tbranch_type: BranchTypeOpt = None,\n\t\tbase_branch: BaseBranchOpt = None,\n\t\ttitle: TitleOpt = None,\n\t\tdescription: DescriptionOpt = None,\n\t\tno_commit: NoCommitOpt = False,\n\t\tforce_push: ForcePushOpt = False,\n\t\tpr_number: PRNumberOpt = None,\n\t\tworkflow: WorkflowOpt = None,\n\t\tnon_interactive: NonInteractiveOpt = False,\n\t\tmodel: ModelOpt = None,\n\t\tapi_base: ApiBaseOpt = None,\n\t\tapi_key: ApiKeyOpt = None,\n\t\tis_verbose: VerboseOpt = False,\n\t) -&gt; None:\n\t\t\"\"\"Create or update a GitHub/GitLab pull request with generated content.\"\"\"\n\t\t_pr_command_impl(\n\t\t\tpath_arg=path_arg,\n\t\t\taction=action,\n\t\t\tbranch_name=branch_name,\n\t\t\tbranch_type=branch_type,\n\t\t\tbase_branch=base_branch,\n\t\t\ttitle=title,\n\t\t\tdescription=description,\n\t\t\tno_commit=no_commit,\n\t\t\tforce_push=force_push,\n\t\t\tpr_number=pr_number,\n\t\t\tworkflow=workflow,\n\t\t\tnon_interactive=non_interactive,\n\t\t\tmodel=model,\n\t\t\tapi_base=api_base,\n\t\t\tapi_key=api_key,\n\t\t\tis_verbose=is_verbose,\n\t\t)\n</code></pre>"},{"location":"api/db/","title":"Db Overview","text":"<p>Database management utilities using SQLModel.</p> <ul> <li>Client - Client interface for interacting with the database in CodeMap.</li> <li>Engine - Handles SQLModel engine creation and session management for CodeMap.</li> <li>Models - Database models for CodeMap using SQLModel.</li> </ul>"},{"location":"api/db/client/","title":"Client","text":"<p>Client interface for interacting with the database in CodeMap.</p>"},{"location":"api/db/client/#codemap.db.client.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient","title":"DatabaseClient","text":"<p>Provides high-level methods to interact with the CodeMap database.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>class DatabaseClient:\n\t\"\"\"Provides high-level methods to interact with the CodeMap database.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"\n\t\tInitializes the database client.\n\n\t\tIt sets up the client in an uninitialized state. The actual\n\t\tinitialization needs to be performed by calling the async initialize()\n\t\tmethod or waiting for _initialize_engine_if_needed to run when\n\t\trequired.\n\n\t\t\"\"\"\n\t\tself.engine = None  # Initialize engine as None\n\t\tself.initialized = False  # Flag to track initialization status\n\t\tself._init_task = None  # Store reference to initialization task\n\n\t\t# Initialize engine in event loop if possible\n\t\ttry:\n\t\t\tif asyncio.get_event_loop().is_running():\n\t\t\t\tself._init_task = asyncio.create_task(self.initialize())\n\t\t\telse:\n\t\t\t\t# In sync context, create a new event loop to initialize\n\t\t\t\tloop = asyncio.new_event_loop()\n\t\t\t\tloop.run_until_complete(self.initialize())\n\t\t\t\tloop.close()\n\t\texcept RuntimeError:\n\t\t\t# No event loop available, will initialize on first use\n\t\t\tlogger.debug(\"No event loop available during DatabaseClient init, will initialize on demand\")\n\n\tasync def initialize(self) -&gt; None:\n\t\t\"\"\"\n\t\tAsynchronously initialize the database client.\n\n\t\tThis should be called after creating the client and before using it.\n\n\t\t\"\"\"\n\t\tawait self._initialize_engine()\n\t\tself.initialized = True\n\t\tlogger.info(\"Database client successfully initialized\")\n\n\tasync def _initialize_engine(self) -&gt; None:\n\t\t\"\"\"Asynchronously gets the engine and creates tables.\"\"\"\n\t\tif self.engine is None:\n\t\t\ttry:\n\t\t\t\tself.engine = await get_engine()  # Await the async function\n\t\t\t\t# create_db_and_tables is synchronous, run it after engine is ready\n\t\t\t\tcreate_db_and_tables(self.engine)\n\t\t\t\tlogger.info(\"Database client initialized with PostgreSQL engine.\")\n\t\t\texcept RuntimeError:\n\t\t\t\tlogger.exception(\"Failed to initialize database engine\")\n\t\t\t\t# Decide how to handle this error - maybe raise, maybe set engine to None?\n\t\t\t\t# For now, re-raising to make the failure explicit.\n\t\t\t\traise\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"An unexpected error occurred during database initialization\")\n\t\t\t\traise\n\n\tasync def _initialize_engine_if_needed(self) -&gt; None:\n\t\t\"\"\"Ensure engine is initialized before use.\"\"\"\n\t\tif not self.initialized or self.engine is None:\n\t\t\tawait self.initialize()\n\n\tasync def cleanup(self) -&gt; None:\n\t\t\"\"\"\n\t\tAsynchronously cleanup the database client resources.\n\n\t\tThis should be called before discarding the client.\n\n\t\t\"\"\"\n\t\tif self.engine:\n\t\t\t# No need to close Engine in SQLAlchemy 2.0, but dispose will close connections\n\t\t\tif hasattr(self.engine, \"dispose\"):\n\t\t\t\tself.engine.dispose()\n\t\t\tself.engine = None\n\t\tself.initialized = False\n\t\tlogger.info(\"Database client cleaned up\")\n\n\t# Ensure engine is initialized before DB operations\n\tasync def _ensure_engine_initialized(self) -&gt; None:\n\t\t\"\"\"Ensures the database engine is properly initialized.\n\n\t\tThis method checks if the engine is initialized and attempts to initialize it if not.\n\t\tIf initialization fails, it logs an error and raises a RuntimeError.\n\n\t\tRaises:\n\t\t    RuntimeError: If database client initialization fails after attempting to initialize.\n\n\t\tReturns:\n\t\t    None: This method doesn't return anything but ensures engine is ready for use.\n\t\t\"\"\"\n\t\tif not self.initialized or self.engine is None:\n\t\t\tawait self._initialize_engine_if_needed()\n\t\t\tif not self.initialized or self.engine is None:\n\t\t\t\tmsg = \"Database client initialization failed.\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise RuntimeError(msg)\n\n\tdef add_chat_message(\n\t\tself,\n\t\tsession_id: str,\n\t\tuser_query: str,\n\t\tai_response: str | None = None,\n\t\tcontext: str | None = None,\n\t\ttool_calls: str | None = None,\n\t) -&gt; ChatHistory:\n\t\t\"\"\"\n\t\tAdds a chat message to the history.\n\n\t\tArgs:\n\t\t    session_id (str): The session identifier.\n\t\t    user_query (str): The user's query.\n\t\t    ai_response (Optional[str]): The AI's response.\n\t\t    context (Optional[str]): JSON string of context used.\n\t\t    tool_calls (Optional[str]): JSON string of tool calls made.\n\n\t\tReturns:\n\t\t    ChatHistory: The newly created chat history record.\n\n\t\t\"\"\"\n\t\t# Ensure engine is initialized - run in a new event loop if needed\n\t\tif not self.initialized or self.engine is None:\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\ttry:\n\t\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\t\tfinally:\n\t\t\t\tloop.close()\n\n\t\tif self.engine is None:\n\t\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\t\tmsg = \"Database engine is not initialized after check.\"\n\t\t\tlogger.error(msg)\n\t\t\traise RuntimeError(msg)\n\n\t\tchat_entry = ChatHistory(\n\t\t\tsession_id=session_id,\n\t\t\tuser_query=user_query,\n\t\t\tai_response=ai_response,\n\t\t\tcontext=context,\n\t\t\ttool_calls=tool_calls,\n\t\t)\n\t\ttry:\n\t\t\twith get_session(self.engine) as session:\n\t\t\t\tsession.add(chat_entry)\n\t\t\t\tsession.commit()\n\t\t\t\tsession.refresh(chat_entry)\n\t\t\t\tlogger.debug(f\"Added chat message for session {session_id} to DB (ID: {chat_entry.id}).\")\n\t\t\t\treturn chat_entry\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error adding chat message\")\n\t\t\traise  # Re-raise after logging\n\n\tdef get_chat_history(self, session_id: str, limit: int = 50) -&gt; list[ChatHistory]:\n\t\t\"\"\"\n\t\tRetrieves chat history for a session, ordered chronologically.\n\n\t\tArgs:\n\t\t    session_id (str): The session identifier.\n\t\t    limit (int): The maximum number of messages to return.\n\n\t\tReturns:\n\t\t    List[ChatHistory]: A list of chat history records.\n\n\t\t\"\"\"\n\t\t# Ensure engine is initialized - run in a new event loop if needed\n\t\tif not self.initialized or self.engine is None:\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\ttry:\n\t\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\t\tfinally:\n\t\t\t\tloop.close()\n\n\t\tif self.engine is None:\n\t\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\t\tmsg = \"Database engine is not initialized after check.\"\n\t\t\tlogger.error(msg)\n\t\t\traise RuntimeError(msg)\n\n\t\tstatement = (\n\t\t\tselect(ChatHistory)\n\t\t\t.where(ChatHistory.session_id == session_id)\n\t\t\t# Using type ignore as the linter incorrectly flags the type\n\t\t\t.order_by(asc(ChatHistory.timestamp))  # type: ignore[arg-type]\n\t\t\t.limit(limit)\n\t\t)\n\t\ttry:\n\t\t\twith get_session(self.engine) as session:\n\t\t\t\tresults = session.exec(statement).all()\n\t\t\t\tlogger.debug(f\"Retrieved {len(results)} messages for session {session_id}.\")\n\t\t\t\treturn list(results)\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error retrieving chat history\")\n\t\t\traise\n\n\tdef update_chat_response(self, message_id: int, ai_response: str) -&gt; bool:\n\t\t\"\"\"\n\t\tUpdates the AI response for a specific chat message.\n\n\t\tArgs:\n\t\t        message_id (int): The primary key ID of the chat message to update.\n\t\t        ai_response (str): The new AI response text.\n\n\t\tReturns:\n\t\t        bool: True if the update was successful, False otherwise.\n\n\t\t\"\"\"\n\t\t# Ensure engine is initialized - run in a new event loop if needed\n\t\tif not self.initialized or self.engine is None:\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\ttry:\n\t\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\t\tfinally:\n\t\t\t\tloop.close()\n\n\t\tif self.engine is None:\n\t\t\tlogger.error(\"Cannot update chat response: Database engine not initialized.\")\n\t\t\treturn False\n\n\t\ttry:\n\t\t\twith get_session(self.engine) as session:\n\t\t\t\tdb_entry = session.get(ChatHistory, message_id)\n\t\t\t\tif db_entry:\n\t\t\t\t\tdb_entry.ai_response = ai_response\n\t\t\t\t\tsession.commit()\n\t\t\t\t\tlogger.debug(f\"Updated DB entry {message_id} with AI response.\")\n\t\t\t\t\treturn True\n\t\t\t\tlogger.warning(f\"Chat message with ID {message_id} not found for update.\")\n\t\t\t\treturn False\n\t\texcept SQLAlchemyError:\n\t\t\tlogger.exception(f\"Database error updating chat response for message ID {message_id}\")\n\t\t\treturn False\n\t\texcept Exception:\n\t\t\tlogger.exception(f\"Unexpected error updating chat response for message ID {message_id}\")\n\t\t\treturn False\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initializes the database client.</p> <p>It sets up the client in an uninitialized state. The actual initialization needs to be performed by calling the async initialize() method or waiting for _initialize_engine_if_needed to run when required.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"\n\tInitializes the database client.\n\n\tIt sets up the client in an uninitialized state. The actual\n\tinitialization needs to be performed by calling the async initialize()\n\tmethod or waiting for _initialize_engine_if_needed to run when\n\trequired.\n\n\t\"\"\"\n\tself.engine = None  # Initialize engine as None\n\tself.initialized = False  # Flag to track initialization status\n\tself._init_task = None  # Store reference to initialization task\n\n\t# Initialize engine in event loop if possible\n\ttry:\n\t\tif asyncio.get_event_loop().is_running():\n\t\t\tself._init_task = asyncio.create_task(self.initialize())\n\t\telse:\n\t\t\t# In sync context, create a new event loop to initialize\n\t\t\tloop = asyncio.new_event_loop()\n\t\t\tloop.run_until_complete(self.initialize())\n\t\t\tloop.close()\n\texcept RuntimeError:\n\t\t# No event loop available, will initialize on first use\n\t\tlogger.debug(\"No event loop available during DatabaseClient init, will initialize on demand\")\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.engine","title":"engine  <code>instance-attribute</code>","text":"<pre><code>engine = None\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.initialized","title":"initialized  <code>instance-attribute</code>","text":"<pre><code>initialized = False\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize() -&gt; None\n</code></pre> <p>Asynchronously initialize the database client.</p> <p>This should be called after creating the client and before using it.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>async def initialize(self) -&gt; None:\n\t\"\"\"\n\tAsynchronously initialize the database client.\n\n\tThis should be called after creating the client and before using it.\n\n\t\"\"\"\n\tawait self._initialize_engine()\n\tself.initialized = True\n\tlogger.info(\"Database client successfully initialized\")\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Asynchronously cleanup the database client resources.</p> <p>This should be called before discarding the client.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>async def cleanup(self) -&gt; None:\n\t\"\"\"\n\tAsynchronously cleanup the database client resources.\n\n\tThis should be called before discarding the client.\n\n\t\"\"\"\n\tif self.engine:\n\t\t# No need to close Engine in SQLAlchemy 2.0, but dispose will close connections\n\t\tif hasattr(self.engine, \"dispose\"):\n\t\t\tself.engine.dispose()\n\t\tself.engine = None\n\tself.initialized = False\n\tlogger.info(\"Database client cleaned up\")\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.add_chat_message","title":"add_chat_message","text":"<pre><code>add_chat_message(\n\tsession_id: str,\n\tuser_query: str,\n\tai_response: str | None = None,\n\tcontext: str | None = None,\n\ttool_calls: str | None = None,\n) -&gt; ChatHistory\n</code></pre> <p>Adds a chat message to the history.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The session identifier.</p> required <code>user_query</code> <code>str</code> <p>The user's query.</p> required <code>ai_response</code> <code>Optional[str]</code> <p>The AI's response.</p> <code>None</code> <code>context</code> <code>Optional[str]</code> <p>JSON string of context used.</p> <code>None</code> <code>tool_calls</code> <code>Optional[str]</code> <p>JSON string of tool calls made.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChatHistory</code> <code>ChatHistory</code> <p>The newly created chat history record.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def add_chat_message(\n\tself,\n\tsession_id: str,\n\tuser_query: str,\n\tai_response: str | None = None,\n\tcontext: str | None = None,\n\ttool_calls: str | None = None,\n) -&gt; ChatHistory:\n\t\"\"\"\n\tAdds a chat message to the history.\n\n\tArgs:\n\t    session_id (str): The session identifier.\n\t    user_query (str): The user's query.\n\t    ai_response (Optional[str]): The AI's response.\n\t    context (Optional[str]): JSON string of context used.\n\t    tool_calls (Optional[str]): JSON string of tool calls made.\n\n\tReturns:\n\t    ChatHistory: The newly created chat history record.\n\n\t\"\"\"\n\t# Ensure engine is initialized - run in a new event loop if needed\n\tif not self.initialized or self.engine is None:\n\t\tloop = asyncio.new_event_loop()\n\t\ttry:\n\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\tfinally:\n\t\t\tloop.close()\n\n\tif self.engine is None:\n\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\tmsg = \"Database engine is not initialized after check.\"\n\t\tlogger.error(msg)\n\t\traise RuntimeError(msg)\n\n\tchat_entry = ChatHistory(\n\t\tsession_id=session_id,\n\t\tuser_query=user_query,\n\t\tai_response=ai_response,\n\t\tcontext=context,\n\t\ttool_calls=tool_calls,\n\t)\n\ttry:\n\t\twith get_session(self.engine) as session:\n\t\t\tsession.add(chat_entry)\n\t\t\tsession.commit()\n\t\t\tsession.refresh(chat_entry)\n\t\t\tlogger.debug(f\"Added chat message for session {session_id} to DB (ID: {chat_entry.id}).\")\n\t\t\treturn chat_entry\n\texcept Exception:\n\t\tlogger.exception(\"Error adding chat message\")\n\t\traise  # Re-raise after logging\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.get_chat_history","title":"get_chat_history","text":"<pre><code>get_chat_history(\n\tsession_id: str, limit: int = 50\n) -&gt; list[ChatHistory]\n</code></pre> <p>Retrieves chat history for a session, ordered chronologically.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The session identifier.</p> required <code>limit</code> <code>int</code> <p>The maximum number of messages to return.</p> <code>50</code> <p>Returns:</p> Type Description <code>list[ChatHistory]</code> <p>List[ChatHistory]: A list of chat history records.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def get_chat_history(self, session_id: str, limit: int = 50) -&gt; list[ChatHistory]:\n\t\"\"\"\n\tRetrieves chat history for a session, ordered chronologically.\n\n\tArgs:\n\t    session_id (str): The session identifier.\n\t    limit (int): The maximum number of messages to return.\n\n\tReturns:\n\t    List[ChatHistory]: A list of chat history records.\n\n\t\"\"\"\n\t# Ensure engine is initialized - run in a new event loop if needed\n\tif not self.initialized or self.engine is None:\n\t\tloop = asyncio.new_event_loop()\n\t\ttry:\n\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\tfinally:\n\t\t\tloop.close()\n\n\tif self.engine is None:\n\t\t# This should ideally not happen if _ensure_engine_initialized worked\n\t\tmsg = \"Database engine is not initialized after check.\"\n\t\tlogger.error(msg)\n\t\traise RuntimeError(msg)\n\n\tstatement = (\n\t\tselect(ChatHistory)\n\t\t.where(ChatHistory.session_id == session_id)\n\t\t# Using type ignore as the linter incorrectly flags the type\n\t\t.order_by(asc(ChatHistory.timestamp))  # type: ignore[arg-type]\n\t\t.limit(limit)\n\t)\n\ttry:\n\t\twith get_session(self.engine) as session:\n\t\t\tresults = session.exec(statement).all()\n\t\t\tlogger.debug(f\"Retrieved {len(results)} messages for session {session_id}.\")\n\t\t\treturn list(results)\n\texcept Exception:\n\t\tlogger.exception(\"Error retrieving chat history\")\n\t\traise\n</code></pre>"},{"location":"api/db/client/#codemap.db.client.DatabaseClient.update_chat_response","title":"update_chat_response","text":"<pre><code>update_chat_response(\n\tmessage_id: int, ai_response: str\n) -&gt; bool\n</code></pre> <p>Updates the AI response for a specific chat message.</p> <p>Parameters:</p> Name Type Description Default <code>message_id</code> <code>int</code> <p>The primary key ID of the chat message to update.</p> required <code>ai_response</code> <code>str</code> <p>The new AI response text.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the update was successful, False otherwise.</p> Source code in <code>src/codemap/db/client.py</code> <pre><code>def update_chat_response(self, message_id: int, ai_response: str) -&gt; bool:\n\t\"\"\"\n\tUpdates the AI response for a specific chat message.\n\n\tArgs:\n\t        message_id (int): The primary key ID of the chat message to update.\n\t        ai_response (str): The new AI response text.\n\n\tReturns:\n\t        bool: True if the update was successful, False otherwise.\n\n\t\"\"\"\n\t# Ensure engine is initialized - run in a new event loop if needed\n\tif not self.initialized or self.engine is None:\n\t\tloop = asyncio.new_event_loop()\n\t\ttry:\n\t\t\tloop.run_until_complete(self._ensure_engine_initialized())\n\t\tfinally:\n\t\t\tloop.close()\n\n\tif self.engine is None:\n\t\tlogger.error(\"Cannot update chat response: Database engine not initialized.\")\n\t\treturn False\n\n\ttry:\n\t\twith get_session(self.engine) as session:\n\t\t\tdb_entry = session.get(ChatHistory, message_id)\n\t\t\tif db_entry:\n\t\t\t\tdb_entry.ai_response = ai_response\n\t\t\t\tsession.commit()\n\t\t\t\tlogger.debug(f\"Updated DB entry {message_id} with AI response.\")\n\t\t\t\treturn True\n\t\t\tlogger.warning(f\"Chat message with ID {message_id} not found for update.\")\n\t\t\treturn False\n\texcept SQLAlchemyError:\n\t\tlogger.exception(f\"Database error updating chat response for message ID {message_id}\")\n\t\treturn False\n\texcept Exception:\n\t\tlogger.exception(f\"Unexpected error updating chat response for message ID {message_id}\")\n\t\treturn False\n</code></pre>"},{"location":"api/db/engine/","title":"Engine","text":"<p>Handles SQLModel engine creation and session management for CodeMap.</p>"},{"location":"api/db/engine/#codemap.db.engine.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/db/engine/#codemap.db.engine.get_engine","title":"get_engine  <code>async</code>","text":"<pre><code>get_engine(echo: bool = False) -&gt; Engine\n</code></pre> <p>Gets or creates the SQLAlchemy engine for the database.</p> <p>Tries to use PostgreSQL first, falling back to SQLite in-memory if PostgreSQL is not available. Ensures the PostgreSQL Docker container is running before creating the engine.</p> <p>Parameters:</p> Name Type Description Default <code>echo</code> <code>bool</code> <p>Whether to echo SQL statements to the log.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Engine</code> <code>Engine</code> <p>The SQLAlchemy Engine instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If neither PostgreSQL nor SQLite can be initialized.</p> Source code in <code>src/codemap/db/engine.py</code> <pre><code>async def get_engine(echo: bool = False) -&gt; Engine:\n\t\"\"\"\n\tGets or creates the SQLAlchemy engine for the database.\n\n\tTries to use PostgreSQL first, falling back to SQLite in-memory if PostgreSQL is not available.\n\tEnsures the PostgreSQL Docker container is running before creating the engine.\n\n\tArgs:\n\t        echo (bool): Whether to echo SQL statements to the log.\n\n\tReturns:\n\t        Engine: The SQLAlchemy Engine instance.\n\n\tRaises:\n\t        RuntimeError: If neither PostgreSQL nor SQLite can be initialized.\n\n\t\"\"\"\n\t# Check if we should skip Docker and use SQLite (for testing or non-Docker environments)\n\tuse_sqlite = os.environ.get(\"CODEMAP_USE_SQLITE\", \"\").lower() in (\"true\", \"1\", \"yes\")\n\n\tif use_sqlite:\n\t\treturn _create_sqlite_engine(echo)\n\n\t# Try PostgreSQL first\n\ttry:\n\t\treturn await _create_postgres_engine(echo)\n\texcept (RuntimeError, ConnectionError, TimeoutError, OSError) as e:\n\t\tlogger.warning(f\"PostgreSQL connection failed: {e}. Falling back to SQLite in-memory database.\")\n\t\treturn _create_sqlite_engine(echo)\n</code></pre>"},{"location":"api/db/engine/#codemap.db.engine.create_db_and_tables","title":"create_db_and_tables","text":"<pre><code>create_db_and_tables(engine_instance: Engine) -&gt; None\n</code></pre> <p>Creates the database and all tables defined in SQLModel models.</p> Source code in <code>src/codemap/db/engine.py</code> <pre><code>def create_db_and_tables(engine_instance: Engine) -&gt; None:\n\t\"\"\"Creates the database and all tables defined in SQLModel models.\"\"\"\n\tlogger.info(\"Ensuring database tables exist...\")\n\ttry:\n\t\tSQLModel.metadata.create_all(engine_instance)\n\t\tlogger.info(\"Database tables ensured.\")\n\texcept Exception:\n\t\tlogger.exception(\"Error creating database tables\")\n\t\traise\n</code></pre>"},{"location":"api/db/engine/#codemap.db.engine.get_session","title":"get_session","text":"<pre><code>get_session(\n\tengine_instance: Engine,\n) -&gt; Generator[Session, None, None]\n</code></pre> <p>Provides a context-managed SQLModel session from a given engine.</p> Source code in <code>src/codemap/db/engine.py</code> <pre><code>@contextmanager\ndef get_session(engine_instance: Engine) -&gt; Generator[Session, None, None]:\n\t\"\"\"Provides a context-managed SQLModel session from a given engine.\"\"\"\n\tsession = Session(engine_instance)\n\ttry:\n\t\tyield session\n\texcept Exception:\n\t\tlogger.exception(\"Session rollback due to exception\")\n\t\tsession.rollback()\n\t\traise\n\tfinally:\n\t\tsession.close()\n</code></pre>"},{"location":"api/db/models/","title":"Models","text":"<p>Database models for CodeMap using SQLModel.</p>"},{"location":"api/db/models/#codemap.db.models.ChatHistory","title":"ChatHistory","text":"<p>               Bases: <code>SQLModel</code></p> <p>Represents a single entry in the chat history table.</p> Source code in <code>src/codemap/db/models.py</code> <pre><code>class ChatHistory(SQLModel, table=True):\n\t\"\"\"Represents a single entry in the chat history table.\"\"\"\n\n\t__tablename__: str = \"chat_history\"  # type: ignore[assignment]\n\n\tid: int | None = Field(default=None, primary_key=True)\n\tsession_id: str = Field(index=True)\n\ttimestamp: datetime = Field(default_factory=datetime.utcnow, index=True)\n\tuser_query: str\n\tai_response: str | None = Field(default=None)\n\tcontext: str | None = Field(default=None)  # JSON string or similar\n\ttool_calls: str | None = Field(default=None)  # JSON string\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.__tablename__","title":"__tablename__  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>__tablename__: str = 'chat_history'\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: int | None = Field(default=None, primary_key=True)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.session_id","title":"session_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_id: str = Field(index=True)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: datetime = Field(\n\tdefault_factory=utcnow, index=True\n)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.user_query","title":"user_query  <code>instance-attribute</code>","text":"<pre><code>user_query: str\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.ai_response","title":"ai_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ai_response: str | None = Field(default=None)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.context","title":"context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>context: str | None = Field(default=None)\n</code></pre>"},{"location":"api/db/models/#codemap.db.models.ChatHistory.tool_calls","title":"tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_calls: str | None = Field(default=None)\n</code></pre>"},{"location":"api/gen/","title":"Gen Overview","text":"<p>Code documentation generation package for CodeMap.</p> <ul> <li>Command - Command implementation for code documentation generation.</li> <li>Generator - Code documentation generator implementation.</li> <li>Models - Models for the code generation module.</li> <li>Utils - Utility functions for the gen command.</li> </ul>"},{"location":"api/gen/command/","title":"Command","text":"<p>Command implementation for code documentation generation.</p>"},{"location":"api/gen/command/#codemap.gen.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.process_codebase","title":"process_codebase","text":"<pre><code>process_codebase(\n\ttarget_path: Path,\n\tconfig: GenConfig,\n\tprogress: Progress,\n\ttask_id: TaskID,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; tuple[list[LODEntity], dict]\n</code></pre> <p>Process a codebase using the LOD pipeline architecture.</p> <p>Parameters:</p> Name Type Description Default <code>target_path</code> <code>Path</code> <p>Path to the target codebase</p> required <code>config</code> <code>GenConfig</code> <p>Generation configuration</p> required <code>progress</code> <code>Progress</code> <p>Progress indicator</p> required <code>task_id</code> <code>TaskID</code> <p>Task ID for progress reporting</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[LODEntity], dict]</code> <p>Tuple of (list of LOD entities, metadata dict)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If processing fails</p> Source code in <code>src/codemap/gen/command.py</code> <pre><code>def process_codebase(\n\ttarget_path: Path,\n\tconfig: GenConfig,\n\tprogress: Progress,\n\ttask_id: TaskID,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; tuple[list[LODEntity], dict]:\n\t\"\"\"\n\tProcess a codebase using the LOD pipeline architecture.\n\n\tArgs:\n\t    target_path: Path to the target codebase\n\t    config: Generation configuration\n\t    progress: Progress indicator\n\t    task_id: Task ID for progress reporting\n\t    config_loader: Optional ConfigLoader instance to use\n\n\tReturns:\n\t    Tuple of (list of LOD entities, metadata dict)\n\n\tRaises:\n\t    RuntimeError: If processing fails\n\n\t\"\"\"\n\tlogger.info(\"Starting codebase processing for: %s\", target_path)\n\tprogress.update(task_id, description=\"Scanning files...\")\n\n\t# Get processor configuration from ConfigLoader\n\tif config_loader is None:\n\t\tconfig_loader = ConfigLoader()\n\t\tlogger.debug(\"Created new ConfigLoader instance in process_codebase\")\n\n\tprocessor_config = config_loader.get(\"processor\", {})\n\tgen_config_data = config_loader.get(\"gen\", {})\n\tmax_workers = processor_config.get(\"max_workers\", 4)\n\tlogger.debug(f\"Using max_workers: {max_workers} from configuration\")\n\n\t# Debug the command_arg if present\n\tif \"command_arg\" in gen_config_data:\n\t\tlogger.debug(f\"Found command_arg in gen_config_data: '{gen_config_data['command_arg']}'\")\n\telse:\n\t\tlogger.debug(\"No command_arg found in gen_config_data\")\n\n\ttry:\n\t\t# Need project root to correctly locate .gitignore\n\t\tproject_root = Path.cwd()  # Assuming CWD is project root\n\t\tall_paths = list(target_path.rglob(\"*\"))\n\n\t\t# Filter paths based on .gitignore patterns found in project_root\n\t\tfiltered_paths: Sequence[Path] = filter_paths_by_gitignore(all_paths, project_root)\n\n\t\t# Use the new utility function to process files and generate LOD entities\n\t\t# The utility function will handle parallel processing and progress updates\n\t\tentities = process_files_for_lod(\n\t\t\tpaths=filtered_paths,\n\t\t\tlod_level=config.lod_level,\n\t\t\tmax_workers=max_workers,  # Get from configuration\n\t\t\tprogress=progress,\n\t\t\ttask_id=task_id,\n\t\t)\n\texcept Exception as e:\n\t\tlogger.exception(\"Error during LOD file processing\")\n\t\terror_msg = f\"LOD processing failed: {e}\"\n\t\tshow_error(error_msg)\n\t\traise RuntimeError(error_msg) from e\n\n\t# Update counts based on actual processed entities\n\tprocessed_files = len(entities)\n\tlogger.info(f\"LOD processing complete. Generated {processed_files} entities.\")\n\t# total_files count is now handled within process_files_for_lod for progress\n\n\t# Generate repository metadata\n\tlanguages = {entity.language for entity in entities if entity.language}\n\t# Get total file count accurately from the filtered list *before* processing\n\ttotal_files_scanned = sum(1 for p in filtered_paths if p.is_file())\n\n\tmetadata: dict[str, Any] = {\n\t\t\"name\": target_path.name,\n\t\t\"target_path\": str(target_path.resolve()),  # Keep absolute target path for file path resolution\n\t\t\"original_path\": str(target_path),  # Store original path as provided in the command (could be relative)\n\t\t\"stats\": {\n\t\t\t\"total_files_scanned\": total_files_scanned,  # Total files scanned matching criteria\n\t\t\t\"processed_files\": processed_files,  # Files successfully processed for LOD\n\t\t\t\"total_lines\": sum(entity.end_line - entity.start_line + 1 for entity in entities),\n\t\t\t\"languages\": list(languages),\n\t\t},\n\t}\n\n\t# Add command_arg directly from config if available\n\tif \"command_arg\" in gen_config_data:\n\t\tmetadata[\"command_arg\"] = gen_config_data[\"command_arg\"]\n\t\tlogger.debug(f\"Adding command_arg to metadata: '{gen_config_data['command_arg']}'\")\n\n\t# Generate directory tree if requested\n\tif config.include_tree:\n\t\tmetadata[\"tree\"] = generate_tree(target_path, filtered_paths)\n\n\treturn entities, metadata\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand","title":"GenCommand","text":"<p>Main implementation of the gen command.</p> Source code in <code>src/codemap/gen/command.py</code> <pre><code>class GenCommand:\n\t\"\"\"Main implementation of the gen command.\"\"\"\n\n\tdef __init__(self, config: GenConfig, config_loader: ConfigLoader | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the gen command.\n\n\t\tArgs:\n\t\t    config: Generation configuration\n\t\t    config_loader: Optional ConfigLoader instance to use\n\n\t\t\"\"\"\n\t\tself.config = config\n\t\tself.config_loader = config_loader or ConfigLoader()\n\t\tlogger.debug(\"GenCommand initialized with ConfigLoader\")\n\n\tdef execute(self, target_path: Path, output_path: Path) -&gt; bool:\n\t\t\"\"\"\n\t\tExecute the gen command.\n\n\t\tArgs:\n\t\t    target_path: Path to the target codebase\n\t\t    output_path: Path to write the output\n\n\t\tReturns:\n\t\t    True if successful, False otherwise\n\n\t\t\"\"\"\n\t\tfrom rich.progress import BarColumn, Progress, TextColumn, TimeElapsedColumn\n\n\t\tfrom .generator import CodeMapGenerator\n\t\tfrom .utils import write_documentation\n\n\t\tstart_time = time.time()\n\n\t\ttry:\n\t\t\t# Create generator\n\t\t\tgenerator = CodeMapGenerator(self.config, output_path)\n\n\t\t\t# Process codebase with progress tracking\n\t\t\twith Progress(\n\t\t\t\tTextColumn(\"[progress.description]{task.description}\"),\n\t\t\t\tBarColumn(),\n\t\t\t\tTextColumn(\"{task.completed}/{task.total}\"),\n\t\t\t\tTimeElapsedColumn(),\n\t\t\t) as progress:\n\t\t\t\ttask_id = progress.add_task(\"Processing codebase...\", total=None)\n\n\t\t\t\t# Process the codebase\n\t\t\t\tentities, metadata = process_codebase(\n\t\t\t\t\ttarget_path, self.config, progress, task_id, config_loader=self.config_loader\n\t\t\t\t)\n\n\t\t\t\t# Extract the command_arg from config_loader and add it to metadata\n\t\t\t\tgen_config_data = self.config_loader.get(\"gen\", {})\n\t\t\t\tif \"command_arg\" in gen_config_data:\n\t\t\t\t\tmetadata[\"command_arg\"] = gen_config_data[\"command_arg\"]\n\n\t\t\t# Generate documentation\n\t\t\tconsole.print(\"[green]Processing complete. Generating documentation...\")\n\t\t\tcontent = generator.generate_documentation(entities, metadata)\n\n\t\t\t# Write documentation to output file\n\t\t\twrite_documentation(output_path, content)\n\n\t\t\t# Show completion message with timing\n\t\t\telapsed = time.time() - start_time\n\t\t\tconsole.print(f\"[green]Generation completed in {elapsed:.2f} seconds.\")\n\n\t\t\treturn True\n\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Error during gen command execution\")\n\t\t\t# Show a clean error message to the user\n\t\t\terror_msg = f\"Generation failed: {e!s}\"\n\t\t\tshow_error(error_msg)\n\t\t\treturn False\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig: GenConfig,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the gen command.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GenConfig</code> <p>Generation configuration</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> Source code in <code>src/codemap/gen/command.py</code> <pre><code>def __init__(self, config: GenConfig, config_loader: ConfigLoader | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize the gen command.\n\n\tArgs:\n\t    config: Generation configuration\n\t    config_loader: Optional ConfigLoader instance to use\n\n\t\"\"\"\n\tself.config = config\n\tself.config_loader = config_loader or ConfigLoader()\n\tlogger.debug(\"GenCommand initialized with ConfigLoader\")\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader or ConfigLoader()\n</code></pre>"},{"location":"api/gen/command/#codemap.gen.command.GenCommand.execute","title":"execute","text":"<pre><code>execute(target_path: Path, output_path: Path) -&gt; bool\n</code></pre> <p>Execute the gen command.</p> <p>Parameters:</p> Name Type Description Default <code>target_path</code> <code>Path</code> <p>Path to the target codebase</p> required <code>output_path</code> <code>Path</code> <p>Path to write the output</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/gen/command.py</code> <pre><code>def execute(self, target_path: Path, output_path: Path) -&gt; bool:\n\t\"\"\"\n\tExecute the gen command.\n\n\tArgs:\n\t    target_path: Path to the target codebase\n\t    output_path: Path to write the output\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\tfrom rich.progress import BarColumn, Progress, TextColumn, TimeElapsedColumn\n\n\tfrom .generator import CodeMapGenerator\n\tfrom .utils import write_documentation\n\n\tstart_time = time.time()\n\n\ttry:\n\t\t# Create generator\n\t\tgenerator = CodeMapGenerator(self.config, output_path)\n\n\t\t# Process codebase with progress tracking\n\t\twith Progress(\n\t\t\tTextColumn(\"[progress.description]{task.description}\"),\n\t\t\tBarColumn(),\n\t\t\tTextColumn(\"{task.completed}/{task.total}\"),\n\t\t\tTimeElapsedColumn(),\n\t\t) as progress:\n\t\t\ttask_id = progress.add_task(\"Processing codebase...\", total=None)\n\n\t\t\t# Process the codebase\n\t\t\tentities, metadata = process_codebase(\n\t\t\t\ttarget_path, self.config, progress, task_id, config_loader=self.config_loader\n\t\t\t)\n\n\t\t\t# Extract the command_arg from config_loader and add it to metadata\n\t\t\tgen_config_data = self.config_loader.get(\"gen\", {})\n\t\t\tif \"command_arg\" in gen_config_data:\n\t\t\t\tmetadata[\"command_arg\"] = gen_config_data[\"command_arg\"]\n\n\t\t# Generate documentation\n\t\tconsole.print(\"[green]Processing complete. Generating documentation...\")\n\t\tcontent = generator.generate_documentation(entities, metadata)\n\n\t\t# Write documentation to output file\n\t\twrite_documentation(output_path, content)\n\n\t\t# Show completion message with timing\n\t\telapsed = time.time() - start_time\n\t\tconsole.print(f\"[green]Generation completed in {elapsed:.2f} seconds.\")\n\n\t\treturn True\n\n\texcept Exception as e:\n\t\tlogger.exception(\"Error during gen command execution\")\n\t\t# Show a clean error message to the user\n\t\terror_msg = f\"Generation failed: {e!s}\"\n\t\tshow_error(error_msg)\n\t\treturn False\n</code></pre>"},{"location":"api/gen/generator/","title":"Generator","text":"<p>Code documentation generator implementation.</p>"},{"location":"api/gen/generator/#codemap.gen.generator.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator","title":"CodeMapGenerator","text":"<p>Generates code documentation based on LOD (Level of Detail).</p> Source code in <code>src/codemap/gen/generator.py</code> <pre><code>class CodeMapGenerator:\n\t\"\"\"Generates code documentation based on LOD (Level of Detail).\"\"\"\n\n\tdef __init__(self, config: GenConfig, output_path: Path) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the code map generator.\n\n\t\tArgs:\n\t\t    config: Generation configuration settings\n\t\t    output_path: Path to write the output\n\n\t\t\"\"\"\n\t\tself.config = config\n\t\tself.output_path = output_path\n\n\tdef _generate_mermaid_diagram(self, entities: list[LODEntity]) -&gt; str:\n\t\t\"\"\"Generate a Mermaid diagram string for entity relationships using subgraphs.\"\"\"\n\t\t# Convert config strings to lower case for case-insensitive comparison\n\t\tallowed_entities = {e.lower() for e in self.config.mermaid_entities} if self.config.mermaid_entities else None\n\t\tallowed_relationships = (\n\t\t\t{r.lower() for r in self.config.mermaid_relationships} if self.config.mermaid_relationships else None\n\t\t)\n\n\t\t# Helper to check if an entity type should be included\n\t\tdef should_include_entity(entity_type: EntityType) -&gt; bool:\n\t\t\t\"\"\"Determines if an entity type should be included in the diagram based on allowed entities.\n\n\t\t\tIf no allowed entities are specified in the config, all entities will be included. Otherwise,\n\t\t\tonly entities whose type matches one of the allowed entity types will be included.\n\n\t\t\tArgs:\n\t\t\t\tentity_type: The type of entity to check for inclusion.\n\n\t\t\tReturns:\n\t\t\t\tbool: True if the entity should be included, False otherwise.\n\t\t\t\"\"\"\n\t\t\tif not allowed_entities:\n\t\t\t\treturn True  # Include all if not specified\n\t\t\treturn entity_type.name.lower() in allowed_entities\n\n\t\tdef should_include_relationship(relationship_type: str) -&gt; bool:\n\t\t\t\"\"\"Determines if a relationship type should be included in the diagram based on allowed relationships.\n\n\t\t\tIf no allowed relationships are specified in the config, all relationships will be included. Otherwise,\n\t\t\tonly relationships whose type matches one of the allowed relationship types will be included.\n\n\t\t\tArgs:\n\t\t\t\trelationship_type: The type of relationship to check for inclusion.\n\n\t\t\tReturns:\n\t\t\t\tbool: True if the relationship should be included, False otherwise.\n\t\t\t\"\"\"\n\t\t\tif not allowed_relationships:\n\t\t\t\treturn True  # Include all if not specified\n\t\t\treturn relationship_type.lower() in allowed_relationships\n\n\t\t# --- Data Structures --- #\n\t\t# node_id -&gt; (definition_line, class_name) for regular nodes\n\t\tnode_definitions: dict[str, tuple[str, str]] = {}\n\t\t# subgraph_id -&gt; (label, type, list_of_contained_node_ids)\n\t\tsubgraph_definitions: dict[str, tuple[str, str, list[str]]] = {}\n\t\t# subgraph_id -&gt; parent_subgraph_id (for nesting)\n\t\tsubgraph_hierarchy: dict[str, str] = {}\n\t\t# Edges (parent_id, child_id, label, type)\n\t\tedges: list[tuple[str, str, str, str]] = []\n\t\t# Track processed entities/subgraphs to avoid duplicates\n\t\tprocessed_ids = set()\n\t\t# Map entity ID to entity object\n\t\tentity_map: dict[str, LODEntity] = {}\n\t\t# Track which nodes/subgraphs are connected by edges\n\t\tconnected_ids = set()\n\t\t# Map simple function/method names to their full node IDs\n\t\tname_to_node_ids: dict[str, list[str]] = {}\n\t\t# Keep track of nodes defined outside any subgraph (like external imports)\n\t\tglobal_nodes: set[str] = set()\n\n\t\tinternal_paths = {str(e.metadata.get(\"file_path\")) for e in entities if e.metadata.get(\"file_path\")}\n\n\t\tdef get_node_id(entity: LODEntity) -&gt; str:\n\t\t\t\"\"\"Generates a unique node ID for an entity in Mermaid diagram format.\n\n\t\t\tThe ID is constructed from the entity's file path, start line, and name/type,\n\t\t\tand is sanitized to be Mermaid-compatible (alphanumeric + underscore only).\n\n\t\t\tArgs:\n\t\t\t\tentity: The entity to generate an ID for.\n\n\t\t\tReturns:\n\t\t\t\tstr: A Mermaid-compatible node ID string.\n\t\t\t\"\"\"\n\t\t\tfile_path_str = entity.metadata.get(\"file_path\", \"unknown_file\")\n\t\t\tbase_id = f\"{file_path_str}_{entity.start_line}_{entity.name or entity.entity_type.name}\"\n\t\t\t# Ensure Mermaid compatibility (alphanumeric + underscore)\n\t\t\treturn \"\".join(c if c.isalnum() else \"_\" for c in base_id)\n\n\t\tdef process_entity_recursive(entity: LODEntity, current_subgraph_id: str | None = None) -&gt; None:\n\t\t\t\"\"\"Recursively processes an entity to build Mermaid diagram components.\n\n\t\t\tThis function handles:\n\t\t\t- Creating subgraphs for modules and classes\n\t\t\t- Creating nodes for functions, methods, variables and constants\n\t\t\t- Processing dependencies (imports)\n\t\t\t- Establishing relationships between entities\n\t\t\t- Recursively processing child entities\n\n\t\t\tArgs:\n\t\t\t\tentity: The entity to process\n\t\t\t\tcurrent_subgraph_id: The ID of the current subgraph context (if any)\n\n\t\t\tReturns:\n\t\t\t\tNone: Modifies various data structures in the closure:\n\t\t\t\t\t- node_definitions: Dictionary of node definitions\n\t\t\t\t\t- subgraph_definitions: Dictionary of subgraph definitions\n\t\t\t\t\t- subgraph_hierarchy: Dictionary of subgraph parent-child relationships\n\t\t\t\t\t- edges: List of edges between nodes/subgraphs\n\t\t\t\t\t- processed_ids: Set of processed entity IDs\n\t\t\t\t\t- entity_map: Dictionary mapping node IDs to entities\n\t\t\t\t\t- connected_ids: Set of connected node/subgraph IDs\n\t\t\t\t\t- name_to_node_ids: Dictionary mapping names to node IDs\n\t\t\t\t\t- global_nodes: Set of nodes defined outside subgraphs\n\t\t\t\"\"\"\n\t\t\tnonlocal processed_ids, connected_ids, global_nodes\n\n\t\t\tentity_node_id = get_node_id(entity)\n\n\t\t\tif entity.entity_type == EntityType.UNKNOWN or entity_node_id in processed_ids:\n\t\t\t\treturn\n\n\t\t\tprocessed_ids.add(entity_node_id)\n\t\t\tentity_map[entity_node_id] = entity\n\t\t\tinclude_this_entity = should_include_entity(entity.entity_type)\n\n\t\t\tnext_subgraph_id = current_subgraph_id\n\n\t\t\t# --- Handle Subgraphs (Module, Class) --- #\n\t\t\tif entity.entity_type in (EntityType.MODULE, EntityType.CLASS) and include_this_entity:\n\t\t\t\tsubgraph_label = _escape_mermaid_label(\n\t\t\t\t\tentity.name or Path(entity.metadata.get(\"file_path\", \"unknown\")).name\n\t\t\t\t)\n\t\t\t\tsubgraph_type = \"moduleSubgraph\" if entity.entity_type == EntityType.MODULE else \"classSubgraph\"\n\t\t\t\tif entity.entity_type == EntityType.MODULE and current_subgraph_id:  # Nested module\n\t\t\t\t\tsubgraph_type = \"submoduleSubgraph\"\n\n\t\t\t\tsubgraph_definitions[entity_node_id] = (subgraph_label, subgraph_type, [])\n\t\t\t\tif current_subgraph_id:\n\t\t\t\t\tsubgraph_hierarchy[entity_node_id] = current_subgraph_id\n\t\t\t\t# Mark the container as potentially connected if it has children or dependencies\n\t\t\t\tconnected_ids.add(entity_node_id)\n\t\t\t\tnext_subgraph_id = entity_node_id  # Children belong to this new subgraph\n\n\t\t\t# --- Handle Nodes (Functions, Methods, Vars, Consts, Imports) --- #\n\t\t\telif include_this_entity and entity.entity_type != EntityType.IMPORT:  # Imports handled separately\n\t\t\t\tnode_definition = \"\"\n\t\t\t\tnode_class = \"\"\n\t\t\t\tlabel = _escape_mermaid_label(entity.name or f\"({entity.entity_type.name.lower()})\")\n\n\t\t\t\tif entity.entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\t\t\t\tnode_definition = f'{entity_node_id}(\"{label}\")'\n\t\t\t\t\tnode_class = \"funcNode\"\n\t\t\t\telif entity.entity_type == EntityType.CONSTANT:\n\t\t\t\t\tnode_definition = f'{entity_node_id}[\"{label}\"]'\n\t\t\t\t\tnode_class = \"constNode\"\n\t\t\t\telif entity.entity_type == EntityType.VARIABLE:\n\t\t\t\t\tnode_definition = f'{entity_node_id}[\"{label}\"]'\n\t\t\t\t\tnode_class = \"varNode\"\n\t\t\t\t# Add other types if needed\n\n\t\t\t\tif node_definition and entity_node_id not in node_definitions:\n\t\t\t\t\tnode_definitions[entity_node_id] = (node_definition, node_class)\n\t\t\t\t\tif current_subgraph_id:\n\t\t\t\t\t\tsubgraph_definitions[current_subgraph_id][2].append(entity_node_id)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Should not happen often if root is module, but handle just in case\n\t\t\t\t\t\tglobal_nodes.add(entity_node_id)\n\n\t\t\t# --- Add to Name Map (for call edges) --- #\n\t\t\tif entity.entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\t\t\tname = entity.name\n\t\t\t\tif name:\n\t\t\t\t\tif name not in name_to_node_ids:\n\t\t\t\t\t\tname_to_node_ids[name] = []\n\t\t\t\t\tname_to_node_ids[name].append(entity_node_id)\n\n\t\t\t# --- Process Dependencies (Imports) --- #\n\t\t\tdependencies = entity.metadata.get(\"dependencies\", [])\n\t\t\t# Imports are associated with the module/class they are in, or globally if nowhere else\n\n\t\t\tif dependencies and should_include_relationship(\"imports\"):\n\t\t\t\tfor dep in dependencies:\n\t\t\t\t\tis_external = not dep.startswith(\".\") and not any(\n\t\t\t\t\t\tdep.startswith(str(p).replace(\"\\\\\", \"/\"))\n\t\t\t\t\t\tfor p in internal_paths\n\t\t\t\t\t\tif p  # Handle path separators\n\t\t\t\t\t)\n\t\t\t\t\tdep_id = \"dep_\" + \"\".join(c if c.isalnum() else \"_\" for c in dep)\n\t\t\t\t\tdep_label = _escape_mermaid_label(dep)\n\n\t\t\t\t\tif dep_id not in node_definitions and dep_id not in processed_ids:\n\t\t\t\t\t\tprocessed_ids.add(dep_id)  # Mark as processed to avoid duplicate definitions\n\t\t\t\t\t\tdep_class = \"externalImportNode\" if is_external else \"internalImportNode\"\n\t\t\t\t\t\tnode_shape = f'((\"{dep_label}\"))' if is_external else f'[\"{dep_label}\"]'\n\t\t\t\t\t\tnode_definitions[dep_id] = (f\"{dep_id}{node_shape}\", dep_class)\n\t\t\t\t\t\tglobal_nodes.add(dep_id)  # Imports are defined globally\n\n\t\t\t\t\t# Add edge from the importing *container* (subgraph) to the dependency\n\t\t\t\t\t# Use the entity_node_id of the *module* or *class* for the source of the import edge\n\t\t\t\t\tsource_node_id = (\n\t\t\t\t\t\tentity_node_id\n\t\t\t\t\t\tif entity.entity_type in (EntityType.MODULE, EntityType.CLASS)\n\t\t\t\t\t\telse current_subgraph_id\n\t\t\t\t\t)\n\t\t\t\t\tif source_node_id and source_node_id != dep_id:  # Check source_node_id validity\n\t\t\t\t\t\tedge_tuple = (source_node_id, dep_id, \"imports\", \"import\")\n\t\t\t\t\t\tif edge_tuple not in edges:\n\t\t\t\t\t\t\tedges.append(edge_tuple)\n\t\t\t\t\t\t\tconnected_ids.add(source_node_id)\n\t\t\t\t\t\t\tconnected_ids.add(dep_id)\n\n\t\t\t# --- Process Children Recursively --- #\n\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\tprocess_entity_recursive(child, next_subgraph_id)\n\n\t\t\t\t# --- Define Parent Edge (Declares) --- #\n\t\t\t\t# Edge from container (subgraph) to child node/subgraph\n\t\t\t\tchild_node_id = get_node_id(child)\n\t\t\t\tif (\n\t\t\t\t\tnext_subgraph_id  # Ensure there is a parent subgraph\n\t\t\t\t\tand child.entity_type != EntityType.UNKNOWN\n\t\t\t\t\tand child_node_id in processed_ids  # Ensure child was processed (not filtered out)\n\t\t\t\t\tand should_include_relationship(\"declares\")\n\t\t\t\t):\n\t\t\t\t\t# Only add edge if child is *directly* contained (node or subgraph)\n\t\t\t\t\tis_child_node = child_node_id in node_definitions\n\t\t\t\t\tis_child_subgraph = child_node_id in subgraph_definitions\n\n\t\t\t\t\tif is_child_node or is_child_subgraph:\n\t\t\t\t\t\tedge_tuple = (next_subgraph_id, child_node_id, \"declares\", \"declare\")\n\t\t\t\t\t\tif edge_tuple not in edges:\n\t\t\t\t\t\t\tedges.append(edge_tuple)\n\t\t\t\t\t\t\tconnected_ids.add(next_subgraph_id)\n\t\t\t\t\t\t\tconnected_ids.add(child_node_id)\n\n\t\t# --- Main Processing Loop --- #\n\t\tfor entity in entities:\n\t\t\t# Start processing from top-level modules\n\t\t\tif entity.entity_type == EntityType.MODULE and entity.metadata.get(\"file_path\"):\n\t\t\t\tprocess_entity_recursive(entity, current_subgraph_id=None)\n\n\t\t# --- Define Call Edges --- #\n\t\tif should_include_relationship(\"calls\"):\n\t\t\tfor caller_node_id, caller_entity in entity_map.items():\n\t\t\t\t# Check if the caller is a function/method node that was actually defined\n\t\t\t\tif caller_node_id in node_definitions and caller_entity.entity_type in (\n\t\t\t\t\tEntityType.FUNCTION,\n\t\t\t\t\tEntityType.METHOD,\n\t\t\t\t):\n\t\t\t\t\tcalls = caller_entity.metadata.get(\"calls\", [])\n\t\t\t\t\tfor called_name in calls:\n\t\t\t\t\t\t# Try matching full name first, then simple name\n\t\t\t\t\t\tpossible_target_ids = []\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\tcalled_name in name_to_node_ids\n\t\t\t\t\t\t):  # Full name match? (e.g., class.method) - Less likely with simple parsing\n\t\t\t\t\t\t\tpossible_target_ids.extend(name_to_node_ids[called_name])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsimple_called_name = called_name.split(\".\")[-1]\n\t\t\t\t\t\t\tif simple_called_name in name_to_node_ids:\n\t\t\t\t\t\t\t\tpossible_target_ids.extend(name_to_node_ids[simple_called_name])\n\n\t\t\t\t\t\tfor target_node_id in possible_target_ids:\n\t\t\t\t\t\t\t# Ensure target is also a defined node and not the caller itself\n\t\t\t\t\t\t\tif target_node_id in node_definitions and caller_node_id != target_node_id:\n\t\t\t\t\t\t\t\tedge_tuple = (caller_node_id, target_node_id, \"calls\", \"call\")\n\t\t\t\t\t\t\t\tif edge_tuple not in edges:\n\t\t\t\t\t\t\t\t\tedges.append(edge_tuple)\n\t\t\t\t\t\t\t\t\tconnected_ids.add(caller_node_id)\n\t\t\t\t\t\t\t\t\tconnected_ids.add(target_node_id)\n\n\t\t# --- Assemble Final Mermaid String --- #\n\t\tmermaid_lines = [\"graph LR\"]  # Or TD for Top-Down if preferred\n\n\t\t# --- Define Style Strings (Instead of classDef) ---\n\t\tstyle_map = {\n\t\t\t# Node Styles\n\t\t\t\"funcNode\": \"fill:#007bff,stroke:#FFF,stroke-width:1px,color:white\",  # Blue\n\t\t\t\"constNode\": \"fill:#6f42c1,stroke:#FFF,stroke-width:1px,color:white\",  # Purple\n\t\t\t\"varNode\": \"fill:#fd7e14,stroke:#FFF,stroke-width:1px,color:white\",  # Orange\n\t\t\t\"internalImportNode\": \"fill:#20c997,stroke:#FFF,stroke-width:1px,color:white\",  # Teal\n\t\t\t\"externalImportNode\": \"fill:#ffc107,stroke:#333,stroke-width:1px,color:#333\",  # Yellow\n\t\t\t# Subgraph Styles\n\t\t\t\"moduleSubgraph\": \"fill:#121630,color:#FFF\",  # Dark Grey BG\n\t\t\t\"submoduleSubgraph\": \"fill:#2a122e,color:#FFF\",  # Lighter Grey BG\n\t\t\t\"classSubgraph\": \"fill:#100f5e,color:#FFF\",  # Light Green BG\n\t\t}\n\n\t\t# --- Render Logic --- #\n\t\trendered_elements = set()  # Track IDs of things actually rendered\n\t\toutput_lines = []\n\t\tstyle_lines = []  # Collect style commands separately\n\t\tused_style_keys = set()  # Track which styles (funcNode, classSubgraph etc.) are used\n\n\t\t# Function to recursively render subgraphs and their nodes\n\t\tdef render_subgraph(subgraph_id: str, indent: str = \"\") -&gt; None:\n\t\t\t\"\"\"Recursively renders a Mermaid subgraph and its contents.\n\n\t\t\tArgs:\n\t\t\t\tsubgraph_id: The ID of the subgraph to render\n\t\t\t\tindent: String used for indentation in the output (default: \"\")\n\n\t\t\tReturns:\n\t\t\t\tNone: Output is written to output_lines and style_lines lists\n\n\t\t\tSide Effects:\n\t\t\t\t- Adds to rendered_elements set to track rendered items\n\t\t\t\t- Appends lines to output_lines for Mermaid graph definition\n\t\t\t\t- Appends style commands to style_lines\n\t\t\t\t- Updates used_style_keys with any styles actually used\n\t\t\t\"\"\"\n\t\t\tif subgraph_id in rendered_elements:\n\t\t\t\treturn\n\t\t\trendered_elements.add(subgraph_id)\n\n\t\t\tlabel, sg_type, contained_node_ids = subgraph_definitions[subgraph_id]\n\t\t\toutput_lines.append(f'{indent}subgraph {subgraph_id}[\"{label}\"]')\n\t\t\toutput_lines.append(f\"{indent}  direction LR\")  # Or TD\n\n\t\t\t# Render nodes inside this subgraph\n\t\t\tfor node_id in contained_node_ids:\n\t\t\t\tif node_id in node_definitions:\n\t\t\t\t\t# Apply filtering if enabled\n\t\t\t\t\tif self.config.mermaid_remove_unconnected and node_id not in connected_ids:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif node_id in rendered_elements:\n\t\t\t\t\t\tcontinue  # Should not happen, but safeguard\n\t\t\t\t\trendered_elements.add(node_id)\n\n\t\t\t\t\tdefinition, node_class = node_definitions[node_id]\n\t\t\t\t\toutput_lines.append(f\"{indent}  {definition}\")\n\t\t\t\t\tif node_class in style_map:\n\t\t\t\t\t\tstyle_lines.append(f\"{indent}  style {node_id} {style_map[node_class]}\")\n\t\t\t\t\t\tused_style_keys.add(node_class)  # Track used style\n\n\t\t\t# Render nested subgraphs\n\t\t\tnested_subgraphs = [sid for sid, parent_id in subgraph_hierarchy.items() if parent_id == subgraph_id]\n\t\t\tfor nested_id in sorted(nested_subgraphs):  # Sort for consistent output\n\t\t\t\t# Apply filtering if enabled - check if the subgraph itself or any node inside it is connected\n\t\t\t\tis_nested_connected = subgraph_id in connected_ids or any(\n\t\t\t\t\tnid in connected_ids for nid in subgraph_definitions[nested_id][2]\n\t\t\t\t)\n\t\t\t\tif self.config.mermaid_remove_unconnected and not is_nested_connected:\n\t\t\t\t\tcontinue\n\t\t\t\trender_subgraph(nested_id, indent + \"  \")\n\n\t\t\toutput_lines.append(f\"{indent}end\")\n\t\t\t# Apply style definition to subgraph *after* end\n\t\t\tif sg_type in style_map:\n\t\t\t\tstyle_lines.append(f\"{indent}style {subgraph_id} {style_map[sg_type]}\")\n\t\t\t\tused_style_keys.add(sg_type)  # Track used style\n\n\t\t# --- Define Global Nodes (Imports primarily) ---\n\t\toutput_lines.append(\"\\n  %% Global Nodes\")\n\t\tfor node_id in sorted(global_nodes):\n\t\t\tif node_id in node_definitions:\n\t\t\t\t# Apply filtering if enabled\n\t\t\t\tif self.config.mermaid_remove_unconnected and node_id not in connected_ids:\n\t\t\t\t\tcontinue\n\t\t\t\tif node_id in rendered_elements:\n\t\t\t\t\tcontinue\n\t\t\t\trendered_elements.add(node_id)\n\n\t\t\t\tdefinition, node_class = node_definitions[node_id]\n\t\t\t\toutput_lines.append(f\"  {definition}\")\n\t\t\t\tif node_class in style_map:\n\t\t\t\t\tstyle_lines.append(f\"  style {node_id} {style_map[node_class]}\")\n\t\t\t\t\tused_style_keys.add(node_class)  # Track used style\n\n\t\t# --- Render Top-Level Subgraphs ---\n\t\toutput_lines.append(\"\\n  %% Subgraphs\")\n\t\ttop_level_subgraphs = [sg_id for sg_id in subgraph_definitions if sg_id not in subgraph_hierarchy]\n\t\tfor sg_id in sorted(top_level_subgraphs):\n\t\t\t# Apply filtering if enabled - check if the subgraph itself or any node inside it is connected\n\t\t\tis_sg_connected = sg_id in connected_ids or any(\n\t\t\t\tnid in connected_ids for nid in subgraph_definitions[sg_id][2]\n\t\t\t)\n\t\t\tif self.config.mermaid_remove_unconnected and not is_sg_connected:\n\t\t\t\tcontinue\n\t\t\trender_subgraph(sg_id)\n\n\t\t# --- Render Edges --- #\n\t\toutput_lines.append(\"\\n  %% Edges\")\n\t\tlink_styles = []\n\t\tcall_edge_indices = []\n\t\timport_edge_indices = []\n\t\tdeclare_edge_indices = []\n\n\t\tfiltered_edges = []\n\t\tfor _i, (source_id, target_id, label, edge_type) in enumerate(edges):\n\t\t\t# Ensure both source and target were actually rendered (or are subgraphs that contain rendered nodes)\n\t\t\tsource_exists = source_id in rendered_elements or source_id in subgraph_definitions\n\t\t\ttarget_exists = target_id in rendered_elements or target_id in subgraph_definitions\n\n\t\t\tif source_exists and target_exists:\n\t\t\t\tedge_str = \"\"\n\t\t\t\tif edge_type == \"import\":\n\t\t\t\t\tedge_str = f\"  {source_id} -.-&gt;|{label}| {target_id}\"\n\t\t\t\t\timport_edge_indices.append(len(filtered_edges))  # Index in the filtered list\n\t\t\t\telif edge_type == \"call\":\n\t\t\t\t\tedge_str = f\"  {source_id} --&gt;|{label}| {target_id}\"\n\t\t\t\t\tcall_edge_indices.append(len(filtered_edges))\n\t\t\t\telif edge_type == \"declare\":\n\t\t\t\t\t# Make declare edges less prominent\n\t\t\t\t\tedge_str = f\"  {source_id} --- {target_id}\"  # Simple line, no label needed visually\n\t\t\t\t\tdeclare_edge_indices.append(len(filtered_edges))\n\t\t\t\telse:  # Default or unknown edge type\n\t\t\t\t\tedge_str = f\"  {source_id} --&gt; {target_id}\"\n\n\t\t\t\tif edge_str:\n\t\t\t\t\tfiltered_edges.append(edge_str)\n\n\t\toutput_lines.extend(sorted(filtered_edges))  # Sort for consistency\n\n\t\t# --- Apply Link Styles --- #\n\t\tif call_edge_indices or import_edge_indices or declare_edge_indices:\n\t\t\toutput_lines.append(\"\\n  %% Link Styles\")\n\t\t\tlink_styles.extend(\n\t\t\t\t[f\"  linkStyle {idx} stroke:#28a745,stroke-width:2px;\" for idx in call_edge_indices]\n\t\t\t)  # Green\n\t\t\tlink_styles.extend(\n\t\t\t\t[\n\t\t\t\t\tf\"  linkStyle {idx} stroke:#ffc107,stroke-width:1px,stroke-dasharray: 5 5;\"\n\t\t\t\t\tfor idx in import_edge_indices\n\t\t\t\t]\n\t\t\t)  # Yellow dashed\n\t\t\tlink_styles.extend(\n\t\t\t\t[f\"  linkStyle {idx} stroke:#adb5bd,stroke-width:1px;\" for idx in declare_edge_indices]\n\t\t\t)  # Gray thin\n\n\t\t\toutput_lines.extend(link_styles)\n\n\t\t# --- Generate Dynamic Legend (if enabled) ---\n\t\tlegend_lines = []\n\t\tlegend_style_lines = []\n\t\tif self.config.mermaid_show_legend and used_style_keys:\n\t\t\tlegend_lines.append(\"\\n  %% Legend\")\n\t\t\tlegend_lines.append(\"  subgraph Legend\")\n\t\t\tlegend_lines.append(\"    direction LR\")\n\n\t\t\t# Define all possible legend items and their corresponding style keys\n\t\t\tlegend_item_definitions = {\n\t\t\t\t\"legend_module\": (\"moduleSubgraph\", '[\"Module/File\"]'),\n\t\t\t\t\"legend_submodule\": (\"submoduleSubgraph\", '[\"Sub-Module\"]'),\n\t\t\t\t\"legend_class\": (\"classSubgraph\", '[\"Class\"]'),\n\t\t\t\t\"legend_func\": (\"funcNode\", '(\"Function/Method\")'),\n\t\t\t\t\"legend_const\": (\"constNode\", '[\"Constant\"]'),\n\t\t\t\t\"legend_var\": (\"varNode\", '[\"Variable\"]'),\n\t\t\t\t\"legend_import_int\": (\"internalImportNode\", '[\"Internal Import\"]'),\n\t\t\t\t\"legend_import_ext\": (\"externalImportNode\", '((\"External Import\"))'),\n\t\t\t}\n\n\t\t\tfor legend_id, (style_key, definition) in legend_item_definitions.items():\n\t\t\t\t# Only add legend item if its corresponding style was actually used in the graph\n\t\t\t\tif style_key in used_style_keys:\n\t\t\t\t\tlegend_lines.append(f\"    {legend_id}{definition}\")\n\t\t\t\t\t# Also add its style command to the list of styles\n\t\t\t\t\tif style_key in style_map:\n\t\t\t\t\t\tlegend_style_lines.append(f\"  style {legend_id} {style_map[style_key]}\")\n\n\t\t\tlegend_lines.append(\"  end\")\n\t\t\tlegend_lines.append(\"\")  # Add a blank line after legend\n\n\t\t# --- Assemble Final Output --- #\n\t\tmermaid_lines.extend(legend_lines)  # Add legend definitions (if any)\n\t\tmermaid_lines.extend(output_lines)  # Add main graph structure and edges\n\n\t\t# Append all collected style commands at the end\n\t\tall_style_lines = style_lines + legend_style_lines\n\t\tif all_style_lines:\n\t\t\tmermaid_lines.append(\"\\n  %% Styles\")\n\t\t\tmermaid_lines.extend(sorted(all_style_lines))\n\n\t\treturn \"\\n\".join(mermaid_lines)\n\n\tdef generate_documentation(self, entities: list[LODEntity], metadata: dict) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate markdown documentation from the processed LOD entities.\n\n\t\tArgs:\n\t\t    entities: List of LOD entities\n\t\t    metadata: Repository metadata\n\n\t\tReturns:\n\t\t    Generated documentation as string\n\n\t\t\"\"\"\n\t\tcontent = []\n\n\t\t# Add header with repository information\n\t\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\t\toriginal_path = metadata.get(\"original_path\", \"\")\n\t\tcommand_arg = metadata.get(\"command_arg\", \"\")\n\n\t\t# Debug logging to see what values we're receiving\n\t\tlogger.debug(\n\t\t\tf\"Metadata values for heading: \"\n\t\t\tf\"command_arg='{command_arg}', \"\n\t\t\tf\"original_path='{original_path}', \"\n\t\t\tf\"target_path='{target_path_str}'\"\n\t\t)\n\n\t\t# Use the exact command argument if available\n\t\tif command_arg:\n\t\t\trepo_name = command_arg\n\t\t# Fall back to original path if available\n\t\telif original_path:\n\t\t\trepo_name = original_path\n\t\t# Further fallback to just the directory name\n\t\telif target_path_str:\n\t\t\trepo_name = Path(target_path_str).name\n\t\telse:\n\t\t\trepo_name = metadata.get(\"name\", \"Repository\")\n\n\t\tcontent.append(f\"# `{repo_name}` Documentation\")\n\t\tcontent.append(f\"\\nGenerated on: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\t\tif \"description\" in metadata:\n\t\t\tcontent.append(\"\\n\" + metadata.get(\"description\", \"\"))\n\n\t\t# Add repository statistics\n\t\tif \"stats\" in metadata:\n\t\t\tstats = metadata[\"stats\"]\n\t\t\tcontent.append(\"\\n## Document Statistics\")\n\t\t\tcontent.append(f\"- Total files scanned: {stats.get('total_files_scanned', 0)}\")\n\t\t\tcontent.append(f\"- Total lines of code: {stats.get('total_lines', 0)}\")\n\t\t\tcontent.append(f\"- Languages: {', '.join(stats.get('languages', []))}\")\n\n\t\t# Add directory structure if requested\n\t\tif self.config.include_tree and \"tree\" in metadata:\n\t\t\tcontent.append(\"\\n## Directory Structure\")\n\t\t\tcontent.append(\"```\")\n\t\t\tcontent.append(metadata[\"tree\"])\n\t\t\tcontent.append(\"```\")\n\n\t\t# Add Mermaid diagram if entities exist and config enables it\n\t\tif entities and self.config.include_entity_graph:\n\t\t\tcontent.append(\"\\n## Entity Relationships\")\n\t\t\tcontent.append(\"```mermaid\")\n\t\t\tmermaid_diagram = self._generate_mermaid_diagram(entities)\n\t\t\tcontent.append(mermaid_diagram)\n\t\t\tcontent.append(\"```\")\n\n\t\t# Add table of contents for the scanned files\n\t\tcontent.append(\"\\n## Scanned Files\")\n\n\t\t# Group entities by file\n\t\tfiles: dict[Path, list[LODEntity]] = {}\n\n\t\t# Get the target path from metadata\n\t\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\t\ttarget_path = Path(target_path_str) if target_path_str else None\n\n\t\tfor entity in entities:\n\t\t\tfile_path = Path(entity.metadata.get(\"file_path\", \"\"))\n\t\t\tif not file_path.name:\n\t\t\t\tcontinue\n\n\t\t\tif file_path not in files:\n\t\t\t\tfiles[file_path] = []\n\t\t\tfiles[file_path].append(entity)\n\n\t\t# Create TOC entries with properly formatted relative paths\n\t\tfor i, file_path in enumerate(sorted(files.keys()), 1):\n\t\t\t# Get path relative to the target directory\n\t\t\ttry:\n\t\t\t\tif target_path and target_path.exists():\n\t\t\t\t\t# Get the relative path from the target directory\n\t\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\t\tfilename = file_path.name\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t\t# Handle paths with subdirectories\n\t\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\t\tanchor = clean_filename\n\t\t\t\telse:\n\t\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\t\tanchor = clean_filename\n\n\t\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{anchor})\")\n\t\t\texcept ValueError:\n\t\t\t\t# If relative_to fails, just use the filename\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{clean_filename})\")\n\n\t\t# Add code documentation grouped by file\n\t\tcontent.append(\"\\n## Code Documentation\")\n\n\t\t# Helper function to format a single entity recursively\n\t\tdef format_entity_recursive(entity: LODEntity, level: int) -&gt; list[str]:\n\t\t\t\"\"\"Recursively formats an entity and its children into markdown documentation.\n\n\t\t\tArgs:\n\t\t\t\tentity: The entity to format\n\t\t\t\tlevel: The current indentation level in the hierarchy\n\n\t\t\tReturns:\n\t\t\t\tA list of markdown-formatted strings representing the entity and its children\n\t\t\t\"\"\"\n\t\t\tentity_content = []\n\t\t\tindent = \"  \" * level\n\t\t\tlist_prefix = f\"{indent}- \"\n\n\t\t\t# Basic entry: Type and Name/Signature\n\t\t\tentry_line = f\"{list_prefix}**{entity.entity_type.name.capitalize()}**: `{entity.name}`\"\n\t\t\tif self.config.lod_level.value &gt;= LODLevel.STRUCTURE.value and entity.signature:\n\t\t\t\tentry_line = f\"{list_prefix}**{entity.entity_type.name.capitalize()}**: `{entity.signature}`\"\n\t\t\t# Special handling for comments\n\t\t\telif entity.entity_type == EntityType.COMMENT and entity.content:\n\t\t\t\tcomment_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\t# Format as italicized blockquote\n\t\t\t\tentity_content.extend([f\"{indent}&gt; *{line.strip()}*\" for line in comment_lines])\n\t\t\t\tentry_line = None  # Don't print the default entry line\n\t\t\telif not entity.name and entity.entity_type == EntityType.MODULE:\n\t\t\t\t# Skip module node if it has no name (handled by file heading)\n\t\t\t\t# Don't add the list item itself\n\t\t\t\tentry_line = None  # Don't print the default entry line\n\n\t\t\t# Add the generated entry line if it wasn't skipped\n\t\t\tif entry_line:\n\t\t\t\tentity_content.append(entry_line)\n\n\t\t\t# Add Docstring if level is DOCS or FULL (and not a comment)\n\t\t\tif (\n\t\t\t\tentity.entity_type != EntityType.COMMENT\n\t\t\t\tand self.config.lod_level.value &gt;= LODLevel.DOCS.value\n\t\t\t\tand entity.docstring\n\t\t\t):\n\t\t\t\tdocstring_lines = entity.docstring.strip().split(\"\\n\")\n\t\t\t\t# Format docstring lines with proper indentation\n\t\t\t\tentity_content.append(f\"{indent}  &gt;\")\n\t\t\t\tentity_content.extend([f\"{indent}  &gt; {line}\" for line in docstring_lines])\n\n\t\t\t# Add Content if level is FULL\n\t\t\tif self.config.lod_level.value &gt;= LODLevel.FULL.value and entity.content:\n\t\t\t\tcontent_lang = entity.language or \"\"\n\t\t\t\tentity_content.append(f\"{indent}  ```{content_lang}\")\n\t\t\t\t# Indent content lines as well\n\t\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\t\tentity_content.extend([f\"{indent}  {line}\" for line in content_lines])\n\t\t\t\tentity_content.append(f\"{indent}  ```\")\n\n\t\t\t# Recursively format children\n\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t# Skip unknown children\n\t\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\tentity_content.extend(format_entity_recursive(child, level + 1))\n\n\t\t\treturn entity_content\n\n\t\tfirst_file = True\n\t\tfor i, (file_path, file_entities) in enumerate(sorted(files.items()), 1):\n\t\t\t# Add a divider before each file section except the first one\n\t\t\tif not first_file:\n\t\t\t\tcontent.append(\"\\n---\")  # Horizontal rule\n\t\t\tfirst_file = False\n\n\t\t\t# Get path relative to the target directory\n\t\t\ttry:\n\t\t\t\tif target_path and target_path.exists():\n\t\t\t\t\t# Get the relative path from the target directory\n\t\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\t\tfilename = file_path.name\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t\t# Handle paths with subdirectories\n\t\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\t\tanchor = clean_filename\n\t\t\t\telse:\n\t\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\t\tanchor = clean_filename\n\n\t\t\t\t# Add a custom ID to the heading to match our anchor\n\t\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\t\t\texcept ValueError:\n\t\t\t\t# If relative_to fails, just use the filename\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\n\t\t\t# Sort top-level entities by line number\n\t\t\tsorted_entities = sorted(file_entities, key=lambda e: e.start_line)\n\n\t\t\tif self.config.lod_level == LODLevel.SIGNATURES:\n\t\t\t\t# Level 1: Only top-level signatures\n\t\t\t\tfor entity in sorted_entities:\n\t\t\t\t\tif entity.entity_type in (\n\t\t\t\t\t\tEntityType.CLASS,\n\t\t\t\t\t\tEntityType.FUNCTION,\n\t\t\t\t\t\tEntityType.METHOD,\n\t\t\t\t\t\tEntityType.INTERFACE,\n\t\t\t\t\t\tEntityType.MODULE,\n\t\t\t\t\t):\n\t\t\t\t\t\tcontent.append(f\"\\n#### {entity.name or '(Module Level)'}\")\n\t\t\t\t\t\tif entity.signature:\n\t\t\t\t\t\t\tsig_lang = entity.language or \"\"\n\t\t\t\t\t\t\tcontent.append(f\"\\n```{sig_lang}\")\n\t\t\t\t\t\t\tcontent.append(entity.signature)\n\t\t\t\t\t\t\tcontent.append(\"```\")\n\t\t\telse:\n\t\t\t\t# Levels 2, 3, 4: Use recursive formatting\n\t\t\t\tfor entity in sorted_entities:\n\t\t\t\t\t# Process top-level entities (usually MODULE, but could be others if file has only one class/func)\n\t\t\t\t\tif entity.entity_type == EntityType.MODULE:\n\t\t\t\t\t\t# If it's the module, start recursion from its children\n\t\t\t\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t\t\t\t# Skip unknown children\n\t\t\t\t\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\t\t\t\tcontent.extend(format_entity_recursive(child, level=0))\n\t\t\t\t\t# Handle cases where the top-level entity isn't MODULE (e.g., a file with just one class)\n\t\t\t\t\t# Skip if unknown\n\t\t\t\t\telif entity.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\t\tcontent.extend(format_entity_recursive(entity, level=0))\n\n\t\treturn \"\\n\".join(content)\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator.__init__","title":"__init__","text":"<pre><code>__init__(config: GenConfig, output_path: Path) -&gt; None\n</code></pre> <p>Initialize the code map generator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GenConfig</code> <p>Generation configuration settings</p> required <code>output_path</code> <code>Path</code> <p>Path to write the output</p> required Source code in <code>src/codemap/gen/generator.py</code> <pre><code>def __init__(self, config: GenConfig, output_path: Path) -&gt; None:\n\t\"\"\"\n\tInitialize the code map generator.\n\n\tArgs:\n\t    config: Generation configuration settings\n\t    output_path: Path to write the output\n\n\t\"\"\"\n\tself.config = config\n\tself.output_path = output_path\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator.output_path","title":"output_path  <code>instance-attribute</code>","text":"<pre><code>output_path = output_path\n</code></pre>"},{"location":"api/gen/generator/#codemap.gen.generator.CodeMapGenerator.generate_documentation","title":"generate_documentation","text":"<pre><code>generate_documentation(\n\tentities: list[LODEntity], metadata: dict\n) -&gt; str\n</code></pre> <p>Generate markdown documentation from the processed LOD entities.</p> <p>Parameters:</p> Name Type Description Default <code>entities</code> <code>list[LODEntity]</code> <p>List of LOD entities</p> required <code>metadata</code> <code>dict</code> <p>Repository metadata</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated documentation as string</p> Source code in <code>src/codemap/gen/generator.py</code> <pre><code>def generate_documentation(self, entities: list[LODEntity], metadata: dict) -&gt; str:\n\t\"\"\"\n\tGenerate markdown documentation from the processed LOD entities.\n\n\tArgs:\n\t    entities: List of LOD entities\n\t    metadata: Repository metadata\n\n\tReturns:\n\t    Generated documentation as string\n\n\t\"\"\"\n\tcontent = []\n\n\t# Add header with repository information\n\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\toriginal_path = metadata.get(\"original_path\", \"\")\n\tcommand_arg = metadata.get(\"command_arg\", \"\")\n\n\t# Debug logging to see what values we're receiving\n\tlogger.debug(\n\t\tf\"Metadata values for heading: \"\n\t\tf\"command_arg='{command_arg}', \"\n\t\tf\"original_path='{original_path}', \"\n\t\tf\"target_path='{target_path_str}'\"\n\t)\n\n\t# Use the exact command argument if available\n\tif command_arg:\n\t\trepo_name = command_arg\n\t# Fall back to original path if available\n\telif original_path:\n\t\trepo_name = original_path\n\t# Further fallback to just the directory name\n\telif target_path_str:\n\t\trepo_name = Path(target_path_str).name\n\telse:\n\t\trepo_name = metadata.get(\"name\", \"Repository\")\n\n\tcontent.append(f\"# `{repo_name}` Documentation\")\n\tcontent.append(f\"\\nGenerated on: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\tif \"description\" in metadata:\n\t\tcontent.append(\"\\n\" + metadata.get(\"description\", \"\"))\n\n\t# Add repository statistics\n\tif \"stats\" in metadata:\n\t\tstats = metadata[\"stats\"]\n\t\tcontent.append(\"\\n## Document Statistics\")\n\t\tcontent.append(f\"- Total files scanned: {stats.get('total_files_scanned', 0)}\")\n\t\tcontent.append(f\"- Total lines of code: {stats.get('total_lines', 0)}\")\n\t\tcontent.append(f\"- Languages: {', '.join(stats.get('languages', []))}\")\n\n\t# Add directory structure if requested\n\tif self.config.include_tree and \"tree\" in metadata:\n\t\tcontent.append(\"\\n## Directory Structure\")\n\t\tcontent.append(\"```\")\n\t\tcontent.append(metadata[\"tree\"])\n\t\tcontent.append(\"```\")\n\n\t# Add Mermaid diagram if entities exist and config enables it\n\tif entities and self.config.include_entity_graph:\n\t\tcontent.append(\"\\n## Entity Relationships\")\n\t\tcontent.append(\"```mermaid\")\n\t\tmermaid_diagram = self._generate_mermaid_diagram(entities)\n\t\tcontent.append(mermaid_diagram)\n\t\tcontent.append(\"```\")\n\n\t# Add table of contents for the scanned files\n\tcontent.append(\"\\n## Scanned Files\")\n\n\t# Group entities by file\n\tfiles: dict[Path, list[LODEntity]] = {}\n\n\t# Get the target path from metadata\n\ttarget_path_str = metadata.get(\"target_path\", \"\")\n\ttarget_path = Path(target_path_str) if target_path_str else None\n\n\tfor entity in entities:\n\t\tfile_path = Path(entity.metadata.get(\"file_path\", \"\"))\n\t\tif not file_path.name:\n\t\t\tcontinue\n\n\t\tif file_path not in files:\n\t\t\tfiles[file_path] = []\n\t\tfiles[file_path].append(entity)\n\n\t# Create TOC entries with properly formatted relative paths\n\tfor i, file_path in enumerate(sorted(files.keys()), 1):\n\t\t# Get path relative to the target directory\n\t\ttry:\n\t\t\tif target_path and target_path.exists():\n\t\t\t\t# Get the relative path from the target directory\n\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\tfilename = file_path.name\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t# Handle paths with subdirectories\n\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\telse:\n\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\tanchor = clean_filename\n\t\t\telse:\n\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tanchor = clean_filename\n\n\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{anchor})\")\n\t\texcept ValueError:\n\t\t\t# If relative_to fails, just use the filename\n\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\tcontent.append(f\"{i}. [{rel_path_str}](#{clean_filename})\")\n\n\t# Add code documentation grouped by file\n\tcontent.append(\"\\n## Code Documentation\")\n\n\t# Helper function to format a single entity recursively\n\tdef format_entity_recursive(entity: LODEntity, level: int) -&gt; list[str]:\n\t\t\"\"\"Recursively formats an entity and its children into markdown documentation.\n\n\t\tArgs:\n\t\t\tentity: The entity to format\n\t\t\tlevel: The current indentation level in the hierarchy\n\n\t\tReturns:\n\t\t\tA list of markdown-formatted strings representing the entity and its children\n\t\t\"\"\"\n\t\tentity_content = []\n\t\tindent = \"  \" * level\n\t\tlist_prefix = f\"{indent}- \"\n\n\t\t# Basic entry: Type and Name/Signature\n\t\tentry_line = f\"{list_prefix}**{entity.entity_type.name.capitalize()}**: `{entity.name}`\"\n\t\tif self.config.lod_level.value &gt;= LODLevel.STRUCTURE.value and entity.signature:\n\t\t\tentry_line = f\"{list_prefix}**{entity.entity_type.name.capitalize()}**: `{entity.signature}`\"\n\t\t# Special handling for comments\n\t\telif entity.entity_type == EntityType.COMMENT and entity.content:\n\t\t\tcomment_lines = entity.content.strip().split(\"\\n\")\n\t\t\t# Format as italicized blockquote\n\t\t\tentity_content.extend([f\"{indent}&gt; *{line.strip()}*\" for line in comment_lines])\n\t\t\tentry_line = None  # Don't print the default entry line\n\t\telif not entity.name and entity.entity_type == EntityType.MODULE:\n\t\t\t# Skip module node if it has no name (handled by file heading)\n\t\t\t# Don't add the list item itself\n\t\t\tentry_line = None  # Don't print the default entry line\n\n\t\t# Add the generated entry line if it wasn't skipped\n\t\tif entry_line:\n\t\t\tentity_content.append(entry_line)\n\n\t\t# Add Docstring if level is DOCS or FULL (and not a comment)\n\t\tif (\n\t\t\tentity.entity_type != EntityType.COMMENT\n\t\t\tand self.config.lod_level.value &gt;= LODLevel.DOCS.value\n\t\t\tand entity.docstring\n\t\t):\n\t\t\tdocstring_lines = entity.docstring.strip().split(\"\\n\")\n\t\t\t# Format docstring lines with proper indentation\n\t\t\tentity_content.append(f\"{indent}  &gt;\")\n\t\t\tentity_content.extend([f\"{indent}  &gt; {line}\" for line in docstring_lines])\n\n\t\t# Add Content if level is FULL\n\t\tif self.config.lod_level.value &gt;= LODLevel.FULL.value and entity.content:\n\t\t\tcontent_lang = entity.language or \"\"\n\t\t\tentity_content.append(f\"{indent}  ```{content_lang}\")\n\t\t\t# Indent content lines as well\n\t\t\tcontent_lines = entity.content.strip().split(\"\\n\")\n\t\t\tentity_content.extend([f\"{indent}  {line}\" for line in content_lines])\n\t\t\tentity_content.append(f\"{indent}  ```\")\n\n\t\t# Recursively format children\n\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t# Skip unknown children\n\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\tentity_content.extend(format_entity_recursive(child, level + 1))\n\n\t\treturn entity_content\n\n\tfirst_file = True\n\tfor i, (file_path, file_entities) in enumerate(sorted(files.items()), 1):\n\t\t# Add a divider before each file section except the first one\n\t\tif not first_file:\n\t\t\tcontent.append(\"\\n---\")  # Horizontal rule\n\t\tfirst_file = False\n\n\t\t# Get path relative to the target directory\n\t\ttry:\n\t\t\tif target_path and target_path.exists():\n\t\t\t\t# Get the relative path from the target directory\n\t\t\t\trel_path = file_path.relative_to(target_path)\n\n\t\t\t\t# Format the path with a leading slash for files directly in the target directory\n\t\t\t\trel_path_str = f\"/{rel_path}\"\n\n\t\t\t\t# Create a clean anchor ID by converting to lowercase and removing all special characters\n\t\t\t\t# including underscores, to create standard anchor IDs\n\t\t\t\tfilename = file_path.name\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in filename)\n\n\t\t\t\t# Handle paths with subdirectories\n\t\t\t\tif len(rel_path.parts) &gt; 1:\n\t\t\t\t\t# Get directory name and filename\n\t\t\t\t\tdirectory = rel_path.parts[-2]  # Last directory before the file\n\t\t\t\t\tanchor = f\"{directory}{clean_filename}\"  # e.g., \"raginitpy\"\n\t\t\t\telse:\n\t\t\t\t\t# Just use the clean filename for files at root\n\t\t\t\t\tanchor = clean_filename\n\t\t\telse:\n\t\t\t\t# Fall back to just the filename if target path is not available\n\t\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\t\tanchor = clean_filename\n\n\t\t\t# Add a custom ID to the heading to match our anchor\n\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\t\texcept ValueError:\n\t\t\t# If relative_to fails, just use the filename\n\t\t\trel_path_str = f\"/{file_path.name}\"\n\t\t\tclean_filename = \"\".join(c.lower() if c.isalnum() else \"\" for c in file_path.name)\n\t\t\tcontent.append(f\"\\n### {i}. {rel_path_str}\")\n\n\t\t# Sort top-level entities by line number\n\t\tsorted_entities = sorted(file_entities, key=lambda e: e.start_line)\n\n\t\tif self.config.lod_level == LODLevel.SIGNATURES:\n\t\t\t# Level 1: Only top-level signatures\n\t\t\tfor entity in sorted_entities:\n\t\t\t\tif entity.entity_type in (\n\t\t\t\t\tEntityType.CLASS,\n\t\t\t\t\tEntityType.FUNCTION,\n\t\t\t\t\tEntityType.METHOD,\n\t\t\t\t\tEntityType.INTERFACE,\n\t\t\t\t\tEntityType.MODULE,\n\t\t\t\t):\n\t\t\t\t\tcontent.append(f\"\\n#### {entity.name or '(Module Level)'}\")\n\t\t\t\t\tif entity.signature:\n\t\t\t\t\t\tsig_lang = entity.language or \"\"\n\t\t\t\t\t\tcontent.append(f\"\\n```{sig_lang}\")\n\t\t\t\t\t\tcontent.append(entity.signature)\n\t\t\t\t\t\tcontent.append(\"```\")\n\t\telse:\n\t\t\t# Levels 2, 3, 4: Use recursive formatting\n\t\t\tfor entity in sorted_entities:\n\t\t\t\t# Process top-level entities (usually MODULE, but could be others if file has only one class/func)\n\t\t\t\tif entity.entity_type == EntityType.MODULE:\n\t\t\t\t\t# If it's the module, start recursion from its children\n\t\t\t\t\tfor child in sorted(entity.children, key=lambda e: e.start_line):\n\t\t\t\t\t\t# Skip unknown children\n\t\t\t\t\t\tif child.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\t\t\tcontent.extend(format_entity_recursive(child, level=0))\n\t\t\t\t# Handle cases where the top-level entity isn't MODULE (e.g., a file with just one class)\n\t\t\t\t# Skip if unknown\n\t\t\t\telif entity.entity_type != EntityType.UNKNOWN:\n\t\t\t\t\tcontent.extend(format_entity_recursive(entity, level=0))\n\n\treturn \"\\n\".join(content)\n</code></pre>"},{"location":"api/gen/models/","title":"Models","text":"<p>Models for the code generation module.</p>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig","title":"GenConfig  <code>dataclass</code>","text":"<p>Configuration settings for the 'gen' command.</p> Source code in <code>src/codemap/gen/models.py</code> <pre><code>@dataclass\nclass GenConfig:\n\t\"\"\"Configuration settings for the 'gen' command.\"\"\"\n\n\t# Fields without default values\n\tmax_content_length: int\n\tuse_gitignore: bool\n\toutput_dir: Path\n\tsemantic_analysis: bool\n\tlod_level: LODLevel\n\n\t# Fields with default values\n\tinclude_tree: bool = True\n\tinclude_entity_graph: bool = True\n\tmermaid_entities: list[str] = field(default_factory=list)\n\tmermaid_relationships: list[str] = field(default_factory=list)\n\tmermaid_show_legend: bool = True\n\tmermaid_remove_unconnected: bool = False\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n\tmax_content_length: int,\n\tuse_gitignore: bool,\n\toutput_dir: Path,\n\tsemantic_analysis: bool,\n\tlod_level: LODLevel,\n\tinclude_tree: bool = True,\n\tinclude_entity_graph: bool = True,\n\tmermaid_entities: list[str] = list(),\n\tmermaid_relationships: list[str] = list(),\n\tmermaid_show_legend: bool = True,\n\tmermaid_remove_unconnected: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.max_content_length","title":"max_content_length  <code>instance-attribute</code>","text":"<pre><code>max_content_length: int\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.use_gitignore","title":"use_gitignore  <code>instance-attribute</code>","text":"<pre><code>use_gitignore: bool\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.output_dir","title":"output_dir  <code>instance-attribute</code>","text":"<pre><code>output_dir: Path\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.semantic_analysis","title":"semantic_analysis  <code>instance-attribute</code>","text":"<pre><code>semantic_analysis: bool\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.lod_level","title":"lod_level  <code>instance-attribute</code>","text":"<pre><code>lod_level: LODLevel\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.include_tree","title":"include_tree  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_tree: bool = True\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.include_entity_graph","title":"include_entity_graph  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_entity_graph: bool = True\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.mermaid_entities","title":"mermaid_entities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_entities: list[str] = field(default_factory=list)\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.mermaid_relationships","title":"mermaid_relationships  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_relationships: list[str] = field(\n\tdefault_factory=list\n)\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.mermaid_show_legend","title":"mermaid_show_legend  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_show_legend: bool = True\n</code></pre>"},{"location":"api/gen/models/#codemap.gen.models.GenConfig.mermaid_remove_unconnected","title":"mermaid_remove_unconnected  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mermaid_remove_unconnected: bool = False\n</code></pre>"},{"location":"api/gen/utils/","title":"Utils","text":"<p>Utility functions for the gen command.</p>"},{"location":"api/gen/utils/#codemap.gen.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.process_files_for_lod","title":"process_files_for_lod","text":"<pre><code>process_files_for_lod(\n\tpaths: Sequence[Path],\n\tlod_level: LODLevel,\n\tmax_workers: int = 4,\n\tprogress: Progress | None = None,\n\ttask_id: TaskID | None = None,\n) -&gt; list[LODEntity]\n</code></pre> <p>Process a list of file paths to generate LOD entities in parallel.</p> <p>Bypasses the main ProcessingPipeline and uses LODGenerator directly.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Sequence[Path]</code> <p>Sequence of file paths to process.</p> required <code>lod_level</code> <code>LODLevel</code> <p>The level of detail required.</p> required <code>max_workers</code> <code>int</code> <p>Maximum number of parallel worker threads.</p> <code>4</code> <code>progress</code> <code>Progress | None</code> <p>Optional rich Progress object for updates.</p> <code>None</code> <code>task_id</code> <code>TaskID | None</code> <p>Optional TaskID for the specific progress task.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[LODEntity]</code> <p>A list of successfully generated LODEntity objects.</p> Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def process_files_for_lod(\n\tpaths: Sequence[Path],\n\tlod_level: LODLevel,\n\tmax_workers: int = 4,\n\tprogress: Progress | None = None,\n\ttask_id: TaskID | None = None,\n) -&gt; list[LODEntity]:\n\t\"\"\"\n\tProcess a list of file paths to generate LOD entities in parallel.\n\n\tBypasses the main ProcessingPipeline and uses LODGenerator directly.\n\n\tArgs:\n\t        paths: Sequence of file paths to process.\n\t        lod_level: The level of detail required.\n\t        max_workers: Maximum number of parallel worker threads.\n\t        progress: Optional rich Progress object for updates.\n\t        task_id: Optional TaskID for the specific progress task.\n\n\tReturns:\n\t        A list of successfully generated LODEntity objects.\n\n\t\"\"\"\n\tlod_generator = LODGenerator()\n\tlod_entities = []\n\tfutures: list[Future[LODEntity | None]] = []\n\tfiles_to_process = [p for p in paths if p.is_file()]\n\n\tif progress and task_id is not None:\n\t\t# Update total files count accurately based on files we will process\n\t\tprogress.update(\n\t\t\ttask_id, total=len(files_to_process), description=f\"Processing {len(files_to_process)} files for LOD...\"\n\t\t)\n\n\twith ThreadPoolExecutor(max_workers=max_workers) as executor:\n\t\tfor file_path in files_to_process:\n\t\t\tfuture = executor.submit(_process_single_file_lod, file_path, lod_level, lod_generator)\n\t\t\tfutures.append(future)\n\n\t\tfor future in as_completed(futures):\n\t\t\tresult = future.result()\n\t\t\tif result:\n\t\t\t\tlod_entities.append(result)\n\t\t\tif progress and task_id is not None:\n\t\t\t\t# Advance progress for each completed future (success or fail)\n\t\t\t\tprogress.advance(task_id)\n\n\t# Ensure progress bar completes if it was used\n\tif progress and task_id is not None:\n\t\t# Final update in case of discrepancies or immediate completion\n\t\tprogress.update(\n\t\t\ttask_id, completed=len(futures), description=f\"LOD processing complete. Found {len(lod_entities)} entities.\"\n\t\t)\n\n\tlogger.info(\"Finished LOD processing. Generated %d entities.\", len(lod_entities))\n\treturn lod_entities\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.generate_tree","title":"generate_tree","text":"<pre><code>generate_tree(\n\ttarget_path: Path, filtered_paths: Sequence[Path]\n) -&gt; str\n</code></pre> <p>Generate a directory tree representation.</p> <p>Parameters:</p> Name Type Description Default <code>target_path</code> <code>Path</code> <p>Root path</p> required <code>filtered_paths</code> <code>Sequence[Path]</code> <p>List of filtered absolute paths within target_path</p> required <p>Returns:</p> Type Description <code>str</code> <p>Tree representation as string</p> Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def generate_tree(target_path: Path, filtered_paths: Sequence[Path]) -&gt; str:\n\t\"\"\"\n\tGenerate a directory tree representation.\n\n\tArgs:\n\t    target_path: Root path\n\t    filtered_paths: List of filtered **absolute** paths within target_path\n\n\tReturns:\n\t    Tree representation as string\n\n\t\"\"\"\n\t# Build a nested dictionary representing the file structure\n\ttree = {}\n\n\t# Process directories first to ensure complete structure\n\t# Sort paths to process directories first, then files, all in alphabetical order\n\tsorted_paths = sorted(filtered_paths, key=lambda p: (p.is_file(), str(p).lower()))\n\n\t# Ensure target_path itself is in the structure if it's not already\n\tdir_paths = [p for p in sorted_paths if p.is_dir()]\n\n\t# Add all directories first to ensure complete structure\n\tfor abs_path in dir_paths:\n\t\t# Ensure we only process paths within the target_path\n\t\ttry:\n\t\t\trel_path = abs_path.relative_to(target_path)\n\t\t\tdir_parts = rel_path.parts\n\n\t\t\tcurrent_level = tree\n\t\t\tfor _i, part in enumerate(dir_parts):\n\t\t\t\tif part not in current_level:\n\t\t\t\t\tcurrent_level[part] = {}\n\t\t\t\tcurrent_level = current_level[part]\n\t\texcept ValueError:\n\t\t\tcontinue  # Skip paths not under target_path\n\n\t# Then add files to the structure\n\tfile_paths = [p for p in sorted_paths if p.is_file()]\n\tfor abs_path in file_paths:\n\t\ttry:\n\t\t\trel_path = abs_path.relative_to(target_path)\n\t\t\tparts = rel_path.parts\n\n\t\t\tcurrent_level = tree\n\t\t\tfor i, part in enumerate(parts):\n\t\t\t\tif i == len(parts) - 1:  # Last part (file)\n\t\t\t\t\tcurrent_level[part] = \"file\"\n\t\t\t\telse:\n\t\t\t\t\t# Create directory levels if they don't exist\n\t\t\t\t\tif part not in current_level:\n\t\t\t\t\t\tcurrent_level[part] = {}\n\n\t\t\t\t\t# Get reference to the next level\n\t\t\t\t\tnext_level = current_level[part]\n\n\t\t\t\t\t# Handle case where a file might exist with the same name as a directory part\n\t\t\t\t\tif not isinstance(next_level, dict):\n\t\t\t\t\t\t# This shouldn't happen with proper directory structure, but handle just in case\n\t\t\t\t\t\tlogger.warning(f\"Name conflict: {part} is both a file and a directory in path {rel_path}\")\n\t\t\t\t\t\t# Convert to dictionary with special file marker\n\t\t\t\t\t\tcurrent_level[part] = {\"__file__\": True}\n\n\t\t\t\t\tcurrent_level = current_level[part]\n\t\texcept ValueError:\n\t\t\tcontinue  # Skip paths not under target_path\n\n\t# Get just the target directory name to display at the root of the tree\n\t# rather than the full absolute path\n\ttarget_dir_name = target_path.name\n\n\t# Initialize tree_lines with just the directory name at the root\n\ttree_lines = [target_dir_name]\n\n\t# Recursive function to generate formatted tree lines\n\tdef format_level(level: dict, prefix: str = \"\") -&gt; None:\n\t\t\"\"\"Recursively formats a directory tree level into ASCII tree representation.\n\n\t\tArgs:\n\t\t    level: Dictionary representing the current directory level, where keys are names\n\t\t        and values are either subdirectories (dicts) or files (strings).\n\t\t    prefix: String used for indentation and tree connectors in the output. Defaults to \"\".\n\n\t\tReturns:\n\t\t    None: Modifies the tree_lines list in the closure by appending formatted lines.\n\n\t\tNote:\n\t\t    - Directories are sorted before files\n\t\t    - Items are sorted alphabetically within their type group\n\t\t    - Special markers (like \"__file__\") are skipped\n\t\t\"\"\"\n\t\t# Sort items: directories first (dictionaries), then files (strings)\n\t\tsorted_items = sorted(level.items(), key=lambda x: (not isinstance(x[1], dict), x[0].lower()))\n\n\t\tfor i, (name, item_type) in enumerate(sorted_items):\n\t\t\tis_last_item = i == len(sorted_items) - 1\n\t\t\tconnector = \"\u2514\u2500\u2500 \" if is_last_item else \"\u251c\u2500\u2500 \"\n\n\t\t\tif name == \"__file__\":\n\t\t\t\t# Skip special markers\n\t\t\t\tcontinue\n\n\t\t\tif isinstance(item_type, dict):  # It's a directory\n\t\t\t\ttree_lines.append(f\"{prefix}{connector}{name}/\")\n\t\t\t\tnew_prefix = prefix + (\"    \" if is_last_item else \"\u2502   \")\n\t\t\t\tformat_level(item_type, new_prefix)\n\t\t\telse:  # It's a file\n\t\t\t\ttree_lines.append(f\"{prefix}{connector}{name}\")\n\n\t# Start formatting from the root\n\tformat_level(tree)\n\n\t# If tree only contains the target path (no files/directories under it)\n\tif len(tree_lines) == 1:\n\t\treturn tree_lines[0] + \"/\"\n\n\treturn \"\\n\".join(tree_lines)\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.determine_output_path","title":"determine_output_path","text":"<pre><code>determine_output_path(\n\tproject_root: Path,\n\toutput: Path | None,\n\tconfig_data: dict,\n) -&gt; Path\n</code></pre> <p>Determine the output path for documentation.</p> <p>Parameters:</p> Name Type Description Default <code>project_root</code> <code>Path</code> <p>Root directory of the project</p> required <code>output</code> <code>Path | None</code> <p>Optional output path from command line</p> required <code>config_data</code> <code>dict</code> <p>Gen-specific configuration data</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The determined output path</p> Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def determine_output_path(project_root: Path, output: Path | None, config_data: dict) -&gt; Path:\n\t\"\"\"\n\tDetermine the output path for documentation.\n\n\tArgs:\n\t    project_root: Root directory of the project\n\t    output: Optional output path from command line\n\t    config_data: Gen-specific configuration data\n\n\tReturns:\n\t    The determined output path\n\n\t\"\"\"\n\tfrom datetime import UTC, datetime\n\n\t# If output is provided, use it directly\n\tif output:\n\t\treturn output.resolve()\n\n\t# Check for output file in config\n\tif \"output_file\" in config_data:\n\t\toutput_file = Path(config_data[\"output_file\"])\n\t\tif output_file.is_absolute():\n\t\t\treturn output_file\n\t\treturn project_root / output_file\n\n\t# Get output directory from config\n\toutput_dir = config_data.get(\"output_dir\", \"documentation\")\n\n\t# If output_dir is absolute, use it directly\n\toutput_dir_path = Path(output_dir)\n\tif not output_dir_path.is_absolute():\n\t\t# Otherwise, create the output directory in the project root\n\t\toutput_dir_path = project_root / output_dir\n\n\toutput_dir_path.mkdir(parents=True, exist_ok=True)\n\n\t# Generate a filename with timestamp\n\ttimestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n\tfilename = f\"documentation_{timestamp}.md\"\n\n\treturn output_dir_path / filename\n</code></pre>"},{"location":"api/gen/utils/#codemap.gen.utils.write_documentation","title":"write_documentation","text":"<pre><code>write_documentation(\n\toutput_path: Path, documentation: str\n) -&gt; None\n</code></pre> <p>Write documentation to the specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path</code> <p>Path to write documentation to</p> required <code>documentation</code> <code>str</code> <p>Documentation content to write</p> required Source code in <code>src/codemap/gen/utils.py</code> <pre><code>def write_documentation(output_path: Path, documentation: str) -&gt; None:\n\t\"\"\"\n\tWrite documentation to the specified output path.\n\n\tArgs:\n\t    output_path: Path to write documentation to\n\t    documentation: Documentation content to write\n\n\t\"\"\"\n\tfrom codemap.utils.cli_utils import console, ensure_directory_exists, show_error\n\n\ttry:\n\t\t# Ensure parent directory exists\n\t\tensure_directory_exists(output_path.parent)\n\t\toutput_path.write_text(documentation)\n\t\tconsole.print(f\"[green]Documentation written to {output_path}\")\n\texcept (PermissionError, OSError) as e:\n\t\tshow_error(f\"Error writing documentation to {output_path}: {e!s}\")\n\t\traise\n</code></pre>"},{"location":"api/git/","title":"Git Overview","text":"<p>Git utilities for CodeMap.</p> <ul> <li>Commit Generator - Commit message generation package for CodeMap.</li> <li>Commit Linter - Commit linter package for validating git commit messages according to conventional commits.</li> <li>Diff Splitter - Diff splitting package for CodeMap.</li> <li>Interactive - Interactive commit interface for CodeMap.</li> <li>Pr Generator - PR generation package for CodeMap.</li> <li>Semantic Grouping - Semantic grouping implementation for the CodeMap project.</li> <li>Utils - Git utilities for CodeMap.</li> </ul>"},{"location":"api/git/interactive/","title":"Interactive","text":"<p>Interactive commit interface for CodeMap.</p>"},{"location":"api/git/interactive/#codemap.git.interactive.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.MAX_PREVIEW_LENGTH","title":"MAX_PREVIEW_LENGTH  <code>module-attribute</code>","text":"<pre><code>MAX_PREVIEW_LENGTH = 200\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.MAX_PREVIEW_LINES","title":"MAX_PREVIEW_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_PREVIEW_LINES = 10\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction","title":"ChunkAction","text":"<p>               Bases: <code>Enum</code></p> <p>Possible actions for a diff chunk.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>class ChunkAction(Enum):\n\t\"\"\"Possible actions for a diff chunk.\"\"\"\n\n\tCOMMIT = auto()\n\tEDIT = auto()\n\tSKIP = auto()\n\tABORT = auto()\n\tREGENERATE = auto()\n\tEXIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.COMMIT","title":"COMMIT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMMIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.EDIT","title":"EDIT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EDIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.SKIP","title":"SKIP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKIP = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.ABORT","title":"ABORT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ABORT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.REGENERATE","title":"REGENERATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REGENERATE = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkAction.EXIT","title":"EXIT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXIT = auto()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult","title":"ChunkResult  <code>dataclass</code>","text":"<p>Result of processing a diff chunk.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>@dataclass\nclass ChunkResult:\n\t\"\"\"Result of processing a diff chunk.\"\"\"\n\n\taction: ChunkAction\n\tmessage: str | None = None\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult.__init__","title":"__init__","text":"<pre><code>__init__(\n\taction: ChunkAction, message: str | None = None\n) -&gt; None\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: ChunkAction\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.ChunkResult.message","title":"message  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>message: str | None = None\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI","title":"CommitUI","text":"<p>Interactive UI for the commit process.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>class CommitUI:\n\t\"\"\"Interactive UI for the commit process.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the commit UI.\"\"\"\n\t\tself.console = Console()\n\n\tdef display_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a diff chunk to the user.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk to display\n\t\t    index: The 0-based index of the current chunk\n\t\t    total: The total number of chunks\n\n\t\t\"\"\"\n\t\t# Build file information\n\t\tfile_info = Text(\"Files: \", style=\"blue\")\n\t\tfile_info.append(\", \".join(chunk.files))\n\n\t\t# Calculate changes\n\t\tadded = len(\n\t\t\t[line for line in chunk.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t\t)\n\t\tremoved = len(\n\t\t\t[line for line in chunk.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t\t)\n\t\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\t\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t\t# Prepare diff content\n\t\tpanel_content = chunk.content\n\t\tif not panel_content.strip():\n\t\t\tpanel_content = \"No content diff available (e.g., new file or mode change)\"\n\n\t\t# Truncate to maximum of MAX_PREVIEW_LINES lines\n\t\tcontent_lines = panel_content.splitlines()\n\t\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\t\tpanel_content = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\n\t\tdiff_content = Text(\"\\n\" + panel_content)\n\n\t\t# Determine title for the panel - use provided index and total\n\t\tpanel_title = f\"[bold]Commit {index + 1} of {total}[/bold]\"\n\n\t\t# Create content for the panel conditionally\n\t\tif getattr(chunk, \"description\", None):\n\t\t\t# If there's a description, create a combined panel\n\t\t\tif getattr(chunk, \"is_llm_generated\", False):\n\t\t\t\tmessage_title = \"[bold blue]Proposed message (AI)[/]\"\n\t\t\t\tmessage_style = \"blue\"\n\t\t\telse:\n\t\t\t\tmessage_title = \"[bold yellow]Proposed message (Simple)[/]\"\n\t\t\t\tmessage_style = \"yellow\"\n\n\t\t\t# Create separate panels and print them\n\t\t\t# First, print the diff panel\n\t\t\tdiff_panel = Panel(\n\t\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\t\ttitle=panel_title,\n\t\t\t\tborder_style=\"cyan\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(diff_panel)\n\n\t\t\t# Print divider\n\t\t\tself.console.print(Rule(style=\"dim\"))\n\n\t\t\t# Then print the message panel\n\t\t\tmessage_panel = Panel(\n\t\t\t\tText(str(chunk.description), style=\"green\"),\n\t\t\t\ttitle=message_title,\n\t\t\t\tborder_style=message_style,\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(message_panel)\n\t\telse:\n\t\t\t# If no description, just print the diff panel\n\t\t\tpanel = Panel(\n\t\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\t\ttitle=panel_title,\n\t\t\t\tborder_style=\"cyan\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print()\n\t\t\tself.console.print(panel)\n\t\t\tself.console.print()\n\n\tdef display_group(self, group: SemanticGroup, index: int = 0, total: int = 1) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a semantic group to the user.\n\n\t\tArgs:\n\t\t        group: SemanticGroup to display\n\t\t        index: The 0-based index of the current group\n\t\t        total: The total number of groups\n\n\t\t\"\"\"\n\t\t# Build file information\n\t\tfile_list = \"\\n\".join([f\"  - {file}\" for file in group.files])\n\t\tfile_info = Text(f\"Files ({len(group.files)}):\\n\", style=\"blue\")\n\t\tfile_info.append(file_list)\n\n\t\t# Prepare diff preview - show first few lines of diff content\n\t\tdiff_preview = group.content\n\t\tcontent_lines = diff_preview.splitlines()\n\t\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\t\tdiff_preview = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\t\tdiff_content = Text(\"\\n\\nDiff Preview:\\n\", style=\"blue\")\n\t\tdiff_content.append(diff_preview)\n\n\t\t# Calculate changes\n\t\tadded = len(\n\t\t\t[line for line in group.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t\t)\n\t\tremoved = len(\n\t\t\t[line for line in group.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t\t)\n\t\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\t\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t\t# Determine title for the panel\n\t\tpanel_title = f\"[bold]Group {index + 1} of {total}[/bold]\"\n\n\t\t# Create diff panel\n\t\tdiff_panel = Panel(\n\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\ttitle=panel_title,\n\t\t\tborder_style=\"cyan\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(diff_panel)\n\n\t\t# Print divider\n\t\tself.console.print(Rule(style=\"dim\"))\n\n\t\t# Create message panel if message exists\n\t\tif hasattr(group, \"message\") and group.message:\n\t\t\t# Create message panel\n\t\t\tmessage_panel = Panel(\n\t\t\t\tText(str(group.message), style=\"green\"),\n\t\t\t\ttitle=\"[bold blue]Generated message[/]\",\n\t\t\t\tborder_style=\"green\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(message_panel)\n\t\telse:\n\t\t\tself.console.print(\n\t\t\t\tPanel(\n\t\t\t\t\tText(\"No message generated yet\", style=\"dim\"),\n\t\t\t\t\ttitle=\"[bold]Message[/]\",\n\t\t\t\t\tborder_style=\"yellow\",\n\t\t\t\t\texpand=True,\n\t\t\t\t\twidth=self.console.width,\n\t\t\t\t\tpadding=(1, 2),\n\t\t\t\t)\n\t\t\t)\n\n\tdef display_message(self, message: str, is_llm_generated: bool = False) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a commit message to the user.\n\n\t\tArgs:\n\t\t    message: The commit message to display\n\t\t    is_llm_generated: Whether the message was generated by an LLM\n\n\t\t\"\"\"\n\t\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\t\tmessage_panel = Panel(\n\t\t\tText(message, style=\"green\"),\n\t\t\ttitle=f\"[bold {'blue' if is_llm_generated else 'yellow'}]Proposed message ({tag})[/]\",\n\t\t\tborder_style=\"blue\" if is_llm_generated else \"yellow\",\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\n\tdef get_user_action(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tGet the user's desired action for the current chunk.\n\n\t\tReturns:\n\t\t    ChunkAction indicating what to do with the chunk\n\n\t\t\"\"\"\n\t\t# Define options with their display text and corresponding action\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Commit with this message\", ChunkAction.COMMIT),\n\t\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\n\t\t# Use questionary to get the user's choice\n\t\tresult = questionary.select(\n\t\t\t\"What would you like to do?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tdefault=options[0][0],  # Set \"Commit with this message\" as default\n\t\t\tqmark=\"\u00bb\",\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\n\t\t# Map the result back to the ChunkAction\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\n\t\t# Fallback (should never happen)\n\t\treturn ChunkAction.EXIT\n\n\tdef get_user_action_on_lint_failure(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tGet the user's desired action when linting fails.\n\n\t\tReturns:\n\t\t    ChunkAction indicating what to do.\n\n\t\t\"\"\"\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t\t(\"Bypass linter and commit with --no-verify\", ChunkAction.COMMIT),\n\t\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\t\tresult = questionary.select(\n\t\t\t\"Linting failed. What would you like to do?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tqmark=\"?\u00bb\",  # Use a different qmark to indicate failure state\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\t\treturn ChunkAction.EXIT  # Fallback\n\n\tdef edit_message(self, current_message: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet an edited commit message from the user.\n\n\t\tArgs:\n\t\t    current_message: Current commit message\n\n\t\tReturns:\n\t\t    Edited commit message\n\n\t\t\"\"\"\n\t\tself.console.print(\"\\n[bold blue]Edit commit message:[/]\")\n\t\tself.console.print(\"[dim]Press Enter to keep current message[/]\")\n\t\treturn Prompt.ask(\"Message\", default=current_message)\n\n\tdef process_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; ChunkResult:\n\t\t\"\"\"\n\t\tProcess a single diff chunk interactively.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk to process\n\t\t    index: The 0-based index of the current chunk\n\t\t    total: The total number of chunks\n\n\t\tReturns:\n\t\t    ChunkResult with the user's action and any modified message\n\n\t\t\"\"\"\n\t\t# Display the combined diff and message panel\n\t\tself.display_chunk(chunk, index, total)\n\n\t\t# Now get the user's action through questionary (without displaying another message panel)\n\t\taction = self.get_user_action()\n\n\t\tif action == ChunkAction.EDIT:\n\t\t\tmessage = self.edit_message(chunk.description or \"\")\n\t\t\treturn ChunkResult(ChunkAction.COMMIT, message)\n\n\t\tif action == ChunkAction.COMMIT:\n\t\t\treturn ChunkResult(action, chunk.description)\n\n\t\treturn ChunkResult(action)\n\n\tdef confirm_abort(self) -&gt; bool:\n\t\t\"\"\"\n\t\tAsk the user to confirm aborting the commit process.\n\n\t\tReturns:\n\t\t    True if the user confirms, False otherwise\n\n\t\tRaises:\n\t\t    typer.Exit: When the user confirms exiting\n\n\t\t\"\"\"\n\t\tconfirmed = Confirm.ask(\n\t\t\t\"\\n[bold yellow]Are you sure you want to exit without committing?[/]\",\n\t\t\tdefault=False,\n\t\t)\n\n\t\tif confirmed:\n\t\t\tself.console.print(\"[yellow]Exiting commit process...[/yellow]\")\n\t\t\t# Use a zero exit code to indicate a successful (intended) exit\n\t\t\t# This prevents error messages from showing when exiting\n\t\t\traise typer.Exit(code=0)\n\n\t\treturn False\n\n\tdef confirm_bypass_hooks(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tAsk the user what to do when git hooks fail.\n\n\t\tReturns:\n\t\t    ChunkAction indicating what to do next\n\n\t\t\"\"\"\n\t\tself.console.print(\"\\n[bold yellow]Git hooks failed.[/]\")\n\t\tself.console.print(\"[yellow]This may be due to linting or other pre-commit checks.[/]\")\n\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Force commit and bypass hooks\", ChunkAction.COMMIT),\n\t\t\t(\"Regenerate message and try again\", ChunkAction.REGENERATE),\n\t\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\n\t\tresult = questionary.select(\n\t\t\t\"What would you like to do?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tqmark=\"\u00bb\",\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\n\t\t# Fallback (should never happen)\n\t\treturn ChunkAction.EXIT\n\n\tdef show_success(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow a success message.\n\n\t\tArgs:\n\t\t    message: Message to display\n\n\t\t\"\"\"\n\t\tself.console.print(f\"\\n[bold green]\u2713[/] {message}\")\n\n\tdef show_warning(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow a warning message to the user.\n\n\t\tArgs:\n\t\t    message: Warning message to display\n\n\t\t\"\"\"\n\t\tself.console.print(f\"\\n[bold yellow]\u26a0[/] {message}\")\n\n\tdef show_error(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow an error message to the user.\n\n\t\tArgs:\n\t\t    message: Error message to display\n\n\t\t\"\"\"\n\t\tif \"No changes to commit\" in message:\n\t\t\t# This is an informational message, not an error\n\t\t\tself.console.print(f\"[yellow]{message}[/yellow]\")\n\t\telse:\n\t\t\t# This is a real error\n\t\t\tself.console.print(f\"[red]Error:[/red] {message}\")\n\n\tdef show_skipped(self, files: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tShow which files were skipped.\n\n\t\tArgs:\n\t\t    files: List of skipped files\n\n\t\t\"\"\"\n\t\tif files:\n\t\t\tself.console.print(\"\\n[yellow]Skipped changes in:[/]\")\n\t\t\tfor file in files:\n\t\t\t\tself.console.print(f\"  \u2022 {file}\")\n\n\tdef show_message(self, message: str) -&gt; None:\n\t\t\"\"\"\n\t\tShow a general informational message.\n\n\t\tArgs:\n\t\t    message: Message to display\n\n\t\t\"\"\"\n\t\tself.console.print(f\"\\n{message}\")\n\n\tdef show_regenerating(self) -&gt; None:\n\t\t\"\"\"Show message indicating message regeneration.\"\"\"\n\t\tself.console.print(\"\\n[yellow]Regenerating commit message...[/yellow]\")\n\n\tdef show_all_committed(self) -&gt; None:\n\t\t\"\"\"Show message indicating all changes are committed.\"\"\"\n\t\tself.console.print(\"[green]\u2713[/green] All changes committed!\")\n\n\tdef show_all_done(self) -&gt; None:\n\t\t\"\"\"\n\t\tShow a final success message when the process completes.\n\n\t\tThis is an alias for show_all_committed for now, but could be\n\t\tcustomized.\n\n\t\t\"\"\"\n\t\tself.show_all_committed()\n\n\tdef show_lint_errors(self, errors: list[str]) -&gt; None:\n\t\t\"\"\"Display linting errors to the user.\"\"\"\n\t\tself.console.print(\"[bold red]Commit message failed linting:[/bold red]\")\n\t\tfor error in errors:\n\t\t\tself.console.print(f\"  - {error}\")\n\n\tdef confirm_commit_with_lint_errors(self) -&gt; bool:\n\t\t\"\"\"Ask the user if they want to commit despite lint errors.\"\"\"\n\t\treturn questionary.confirm(\"Commit message has lint errors. Commit anyway?\", default=False).ask()\n\n\tdef confirm_exit(self) -&gt; bool:\n\t\t\"\"\"Ask the user to confirm exiting without committing.\"\"\"\n\t\treturn questionary.confirm(\"Are you sure you want to exit without committing?\", default=False).ask()\n\n\tdef display_failed_lint_message(self, message: str, lint_errors: list[str], is_llm_generated: bool = False) -&gt; None:\n\t\t\"\"\"\n\t\tDisplay a commit message that failed linting, along with the errors.\n\n\t\tArgs:\n\t\t    message: The commit message to display.\n\t\t    lint_errors: List of linting error messages.\n\t\t    is_llm_generated: Whether the message was generated by an LLM.\n\n\t\t\"\"\"\n\t\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\t\tmessage_panel = Panel(\n\t\t\tText(message, style=\"yellow\"),  # Use yellow style for the message text\n\t\t\ttitle=f\"[bold yellow]Proposed message ({tag}) - LINTING FAILED[/]\",\n\t\t\tborder_style=\"yellow\",  # Yellow border to indicate warning/failure\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\n\t\t# Display lint errors below\n\t\tif lint_errors:\n\t\t\terror_text = Text(\"\\n\".join([f\"- {err}\" for err in lint_errors]), style=\"red\")\n\t\t\terror_panel = Panel(\n\t\t\t\terror_text,\n\t\t\t\ttitle=\"[bold red]Linting Errors[/]\",\n\t\t\t\tborder_style=\"red\",\n\t\t\t\texpand=False,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t\tself.console.print(error_panel)\n\n\tdef get_group_action(self) -&gt; ChunkAction:\n\t\t\"\"\"\n\t\tGet the user's desired action for the current semantic group.\n\n\t\tReturns:\n\t\t        ChunkAction indicating what to do with the group\n\n\t\t\"\"\"\n\t\t# Define options with their display text and corresponding action\n\t\toptions: list[tuple[str, ChunkAction]] = [\n\t\t\t(\"Commit this group\", ChunkAction.COMMIT),\n\t\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t\t]\n\n\t\t# Use questionary to get the user's choice\n\t\tresult = questionary.select(\n\t\t\t\"What would you like to do with this group?\",\n\t\t\tchoices=[option[0] for option in options],\n\t\t\tdefault=options[0][0],  # Set \"Commit this group\" as default\n\t\t\tqmark=\"\u00bb\",\n\t\t\tuse_indicator=True,\n\t\t\tuse_arrow_keys=True,\n\t\t).ask()\n\n\t\t# Map the result back to the ChunkAction\n\t\tfor option, action in options:\n\t\t\tif option == result:\n\t\t\t\treturn action\n\n\t\t# Fallback (should never happen)\n\t\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the commit UI.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the commit UI.\"\"\"\n\tself.console = Console()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.console","title":"console  <code>instance-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_chunk","title":"display_chunk","text":"<pre><code>display_chunk(\n\tchunk: DiffChunk, index: int = 0, total: int = 1\n) -&gt; None\n</code></pre> <p>Display a diff chunk to the user.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>DiffChunk to display</p> required <code>index</code> <code>int</code> <p>The 0-based index of the current chunk</p> <code>0</code> <code>total</code> <code>int</code> <p>The total number of chunks</p> <code>1</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; None:\n\t\"\"\"\n\tDisplay a diff chunk to the user.\n\n\tArgs:\n\t    chunk: DiffChunk to display\n\t    index: The 0-based index of the current chunk\n\t    total: The total number of chunks\n\n\t\"\"\"\n\t# Build file information\n\tfile_info = Text(\"Files: \", style=\"blue\")\n\tfile_info.append(\", \".join(chunk.files))\n\n\t# Calculate changes\n\tadded = len(\n\t\t[line for line in chunk.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t)\n\tremoved = len(\n\t\t[line for line in chunk.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t)\n\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t# Prepare diff content\n\tpanel_content = chunk.content\n\tif not panel_content.strip():\n\t\tpanel_content = \"No content diff available (e.g., new file or mode change)\"\n\n\t# Truncate to maximum of MAX_PREVIEW_LINES lines\n\tcontent_lines = panel_content.splitlines()\n\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\tpanel_content = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\n\tdiff_content = Text(\"\\n\" + panel_content)\n\n\t# Determine title for the panel - use provided index and total\n\tpanel_title = f\"[bold]Commit {index + 1} of {total}[/bold]\"\n\n\t# Create content for the panel conditionally\n\tif getattr(chunk, \"description\", None):\n\t\t# If there's a description, create a combined panel\n\t\tif getattr(chunk, \"is_llm_generated\", False):\n\t\t\tmessage_title = \"[bold blue]Proposed message (AI)[/]\"\n\t\t\tmessage_style = \"blue\"\n\t\telse:\n\t\t\tmessage_title = \"[bold yellow]Proposed message (Simple)[/]\"\n\t\t\tmessage_style = \"yellow\"\n\n\t\t# Create separate panels and print them\n\t\t# First, print the diff panel\n\t\tdiff_panel = Panel(\n\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\ttitle=panel_title,\n\t\t\tborder_style=\"cyan\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(diff_panel)\n\n\t\t# Print divider\n\t\tself.console.print(Rule(style=\"dim\"))\n\n\t\t# Then print the message panel\n\t\tmessage_panel = Panel(\n\t\t\tText(str(chunk.description), style=\"green\"),\n\t\t\ttitle=message_title,\n\t\t\tborder_style=message_style,\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\telse:\n\t\t# If no description, just print the diff panel\n\t\tpanel = Panel(\n\t\t\tGroup(file_info, changes_info, diff_content),\n\t\t\ttitle=panel_title,\n\t\t\tborder_style=\"cyan\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print()\n\t\tself.console.print(panel)\n\t\tself.console.print()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_group","title":"display_group","text":"<pre><code>display_group(\n\tgroup: SemanticGroup, index: int = 0, total: int = 1\n) -&gt; None\n</code></pre> <p>Display a semantic group to the user.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>SemanticGroup</code> <p>SemanticGroup to display</p> required <code>index</code> <code>int</code> <p>The 0-based index of the current group</p> <code>0</code> <code>total</code> <code>int</code> <p>The total number of groups</p> <code>1</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_group(self, group: SemanticGroup, index: int = 0, total: int = 1) -&gt; None:\n\t\"\"\"\n\tDisplay a semantic group to the user.\n\n\tArgs:\n\t        group: SemanticGroup to display\n\t        index: The 0-based index of the current group\n\t        total: The total number of groups\n\n\t\"\"\"\n\t# Build file information\n\tfile_list = \"\\n\".join([f\"  - {file}\" for file in group.files])\n\tfile_info = Text(f\"Files ({len(group.files)}):\\n\", style=\"blue\")\n\tfile_info.append(file_list)\n\n\t# Prepare diff preview - show first few lines of diff content\n\tdiff_preview = group.content\n\tcontent_lines = diff_preview.splitlines()\n\tif len(content_lines) &gt; MAX_PREVIEW_LINES:\n\t\tremaining_lines = len(content_lines) - MAX_PREVIEW_LINES\n\t\tdiff_preview = \"\\n\".join(content_lines[:MAX_PREVIEW_LINES]) + f\"\\n... ({remaining_lines} more lines)\"\n\tdiff_content = Text(\"\\n\\nDiff Preview:\\n\", style=\"blue\")\n\tdiff_content.append(diff_preview)\n\n\t# Calculate changes\n\tadded = len(\n\t\t[line for line in group.content.splitlines() if line.startswith(\"+\") and not line.startswith(\"+++\")]\n\t)\n\tremoved = len(\n\t\t[line for line in group.content.splitlines() if line.startswith(\"-\") and not line.startswith(\"---\")]\n\t)\n\tchanges_info = Text(\"\\nChanges: \", style=\"blue\")\n\tchanges_info.append(f\"{added} added, {removed} removed\")\n\n\t# Determine title for the panel\n\tpanel_title = f\"[bold]Group {index + 1} of {total}[/bold]\"\n\n\t# Create diff panel\n\tdiff_panel = Panel(\n\t\tGroup(file_info, changes_info, diff_content),\n\t\ttitle=panel_title,\n\t\tborder_style=\"cyan\",\n\t\texpand=True,\n\t\twidth=self.console.width,\n\t\tpadding=(1, 2),\n\t)\n\tself.console.print(diff_panel)\n\n\t# Print divider\n\tself.console.print(Rule(style=\"dim\"))\n\n\t# Create message panel if message exists\n\tif hasattr(group, \"message\") and group.message:\n\t\t# Create message panel\n\t\tmessage_panel = Panel(\n\t\t\tText(str(group.message), style=\"green\"),\n\t\t\ttitle=\"[bold blue]Generated message[/]\",\n\t\t\tborder_style=\"green\",\n\t\t\texpand=True,\n\t\t\twidth=self.console.width,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(message_panel)\n\telse:\n\t\tself.console.print(\n\t\t\tPanel(\n\t\t\t\tText(\"No message generated yet\", style=\"dim\"),\n\t\t\t\ttitle=\"[bold]Message[/]\",\n\t\t\t\tborder_style=\"yellow\",\n\t\t\t\texpand=True,\n\t\t\t\twidth=self.console.width,\n\t\t\t\tpadding=(1, 2),\n\t\t\t)\n\t\t)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_message","title":"display_message","text":"<pre><code>display_message(\n\tmessage: str, is_llm_generated: bool = False\n) -&gt; None\n</code></pre> <p>Display a commit message to the user.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to display</p> required <code>is_llm_generated</code> <code>bool</code> <p>Whether the message was generated by an LLM</p> <code>False</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_message(self, message: str, is_llm_generated: bool = False) -&gt; None:\n\t\"\"\"\n\tDisplay a commit message to the user.\n\n\tArgs:\n\t    message: The commit message to display\n\t    is_llm_generated: Whether the message was generated by an LLM\n\n\t\"\"\"\n\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\tmessage_panel = Panel(\n\t\tText(message, style=\"green\"),\n\t\ttitle=f\"[bold {'blue' if is_llm_generated else 'yellow'}]Proposed message ({tag})[/]\",\n\t\tborder_style=\"blue\" if is_llm_generated else \"yellow\",\n\t\texpand=False,\n\t\tpadding=(1, 2),\n\t)\n\tself.console.print(message_panel)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.get_user_action","title":"get_user_action","text":"<pre><code>get_user_action() -&gt; ChunkAction\n</code></pre> <p>Get the user's desired action for the current chunk.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do with the chunk</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def get_user_action(self) -&gt; ChunkAction:\n\t\"\"\"\n\tGet the user's desired action for the current chunk.\n\n\tReturns:\n\t    ChunkAction indicating what to do with the chunk\n\n\t\"\"\"\n\t# Define options with their display text and corresponding action\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Commit with this message\", ChunkAction.COMMIT),\n\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\n\t# Use questionary to get the user's choice\n\tresult = questionary.select(\n\t\t\"What would you like to do?\",\n\t\tchoices=[option[0] for option in options],\n\t\tdefault=options[0][0],  # Set \"Commit with this message\" as default\n\t\tqmark=\"\u00bb\",\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\n\t# Map the result back to the ChunkAction\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\n\t# Fallback (should never happen)\n\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.get_user_action_on_lint_failure","title":"get_user_action_on_lint_failure","text":"<pre><code>get_user_action_on_lint_failure() -&gt; ChunkAction\n</code></pre> <p>Get the user's desired action when linting fails.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def get_user_action_on_lint_failure(self) -&gt; ChunkAction:\n\t\"\"\"\n\tGet the user's desired action when linting fails.\n\n\tReturns:\n\t    ChunkAction indicating what to do.\n\n\t\"\"\"\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t(\"Bypass linter and commit with --no-verify\", ChunkAction.COMMIT),\n\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t(\"Skip this chunk\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\tresult = questionary.select(\n\t\t\"Linting failed. What would you like to do?\",\n\t\tchoices=[option[0] for option in options],\n\t\tqmark=\"?\u00bb\",  # Use a different qmark to indicate failure state\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\treturn ChunkAction.EXIT  # Fallback\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.edit_message","title":"edit_message","text":"<pre><code>edit_message(current_message: str) -&gt; str\n</code></pre> <p>Get an edited commit message from the user.</p> <p>Parameters:</p> Name Type Description Default <code>current_message</code> <code>str</code> <p>Current commit message</p> required <p>Returns:</p> Type Description <code>str</code> <p>Edited commit message</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def edit_message(self, current_message: str) -&gt; str:\n\t\"\"\"\n\tGet an edited commit message from the user.\n\n\tArgs:\n\t    current_message: Current commit message\n\n\tReturns:\n\t    Edited commit message\n\n\t\"\"\"\n\tself.console.print(\"\\n[bold blue]Edit commit message:[/]\")\n\tself.console.print(\"[dim]Press Enter to keep current message[/]\")\n\treturn Prompt.ask(\"Message\", default=current_message)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.process_chunk","title":"process_chunk","text":"<pre><code>process_chunk(\n\tchunk: DiffChunk, index: int = 0, total: int = 1\n) -&gt; ChunkResult\n</code></pre> <p>Process a single diff chunk interactively.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>DiffChunk to process</p> required <code>index</code> <code>int</code> <p>The 0-based index of the current chunk</p> <code>0</code> <code>total</code> <code>int</code> <p>The total number of chunks</p> <code>1</code> <p>Returns:</p> Type Description <code>ChunkResult</code> <p>ChunkResult with the user's action and any modified message</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def process_chunk(self, chunk: DiffChunk, index: int = 0, total: int = 1) -&gt; ChunkResult:\n\t\"\"\"\n\tProcess a single diff chunk interactively.\n\n\tArgs:\n\t    chunk: DiffChunk to process\n\t    index: The 0-based index of the current chunk\n\t    total: The total number of chunks\n\n\tReturns:\n\t    ChunkResult with the user's action and any modified message\n\n\t\"\"\"\n\t# Display the combined diff and message panel\n\tself.display_chunk(chunk, index, total)\n\n\t# Now get the user's action through questionary (without displaying another message panel)\n\taction = self.get_user_action()\n\n\tif action == ChunkAction.EDIT:\n\t\tmessage = self.edit_message(chunk.description or \"\")\n\t\treturn ChunkResult(ChunkAction.COMMIT, message)\n\n\tif action == ChunkAction.COMMIT:\n\t\treturn ChunkResult(action, chunk.description)\n\n\treturn ChunkResult(action)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_abort","title":"confirm_abort","text":"<pre><code>confirm_abort() -&gt; bool\n</code></pre> <p>Ask the user to confirm aborting the commit process.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the user confirms, False otherwise</p> <p>Raises:</p> Type Description <code>Exit</code> <p>When the user confirms exiting</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_abort(self) -&gt; bool:\n\t\"\"\"\n\tAsk the user to confirm aborting the commit process.\n\n\tReturns:\n\t    True if the user confirms, False otherwise\n\n\tRaises:\n\t    typer.Exit: When the user confirms exiting\n\n\t\"\"\"\n\tconfirmed = Confirm.ask(\n\t\t\"\\n[bold yellow]Are you sure you want to exit without committing?[/]\",\n\t\tdefault=False,\n\t)\n\n\tif confirmed:\n\t\tself.console.print(\"[yellow]Exiting commit process...[/yellow]\")\n\t\t# Use a zero exit code to indicate a successful (intended) exit\n\t\t# This prevents error messages from showing when exiting\n\t\traise typer.Exit(code=0)\n\n\treturn False\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_bypass_hooks","title":"confirm_bypass_hooks","text":"<pre><code>confirm_bypass_hooks() -&gt; ChunkAction\n</code></pre> <p>Ask the user what to do when git hooks fail.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do next</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_bypass_hooks(self) -&gt; ChunkAction:\n\t\"\"\"\n\tAsk the user what to do when git hooks fail.\n\n\tReturns:\n\t    ChunkAction indicating what to do next\n\n\t\"\"\"\n\tself.console.print(\"\\n[bold yellow]Git hooks failed.[/]\")\n\tself.console.print(\"[yellow]This may be due to linting or other pre-commit checks.[/]\")\n\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Force commit and bypass hooks\", ChunkAction.COMMIT),\n\t\t(\"Regenerate message and try again\", ChunkAction.REGENERATE),\n\t\t(\"Edit message manually\", ChunkAction.EDIT),\n\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\n\tresult = questionary.select(\n\t\t\"What would you like to do?\",\n\t\tchoices=[option[0] for option in options],\n\t\tqmark=\"\u00bb\",\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\n\t# Fallback (should never happen)\n\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_success","title":"show_success","text":"<pre><code>show_success(message: str) -&gt; None\n</code></pre> <p>Show a success message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_success(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow a success message.\n\n\tArgs:\n\t    message: Message to display\n\n\t\"\"\"\n\tself.console.print(f\"\\n[bold green]\u2713[/] {message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_warning","title":"show_warning","text":"<pre><code>show_warning(message: str) -&gt; None\n</code></pre> <p>Show a warning message to the user.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_warning(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow a warning message to the user.\n\n\tArgs:\n\t    message: Warning message to display\n\n\t\"\"\"\n\tself.console.print(f\"\\n[bold yellow]\u26a0[/] {message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_error","title":"show_error","text":"<pre><code>show_error(message: str) -&gt; None\n</code></pre> <p>Show an error message to the user.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_error(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow an error message to the user.\n\n\tArgs:\n\t    message: Error message to display\n\n\t\"\"\"\n\tif \"No changes to commit\" in message:\n\t\t# This is an informational message, not an error\n\t\tself.console.print(f\"[yellow]{message}[/yellow]\")\n\telse:\n\t\t# This is a real error\n\t\tself.console.print(f\"[red]Error:[/red] {message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_skipped","title":"show_skipped","text":"<pre><code>show_skipped(files: list[str]) -&gt; None\n</code></pre> <p>Show which files were skipped.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of skipped files</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_skipped(self, files: list[str]) -&gt; None:\n\t\"\"\"\n\tShow which files were skipped.\n\n\tArgs:\n\t    files: List of skipped files\n\n\t\"\"\"\n\tif files:\n\t\tself.console.print(\"\\n[yellow]Skipped changes in:[/]\")\n\t\tfor file in files:\n\t\t\tself.console.print(f\"  \u2022 {file}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_message","title":"show_message","text":"<pre><code>show_message(message: str) -&gt; None\n</code></pre> <p>Show a general informational message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to display</p> required Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_message(self, message: str) -&gt; None:\n\t\"\"\"\n\tShow a general informational message.\n\n\tArgs:\n\t    message: Message to display\n\n\t\"\"\"\n\tself.console.print(f\"\\n{message}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_regenerating","title":"show_regenerating","text":"<pre><code>show_regenerating() -&gt; None\n</code></pre> <p>Show message indicating message regeneration.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_regenerating(self) -&gt; None:\n\t\"\"\"Show message indicating message regeneration.\"\"\"\n\tself.console.print(\"\\n[yellow]Regenerating commit message...[/yellow]\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_all_committed","title":"show_all_committed","text":"<pre><code>show_all_committed() -&gt; None\n</code></pre> <p>Show message indicating all changes are committed.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_all_committed(self) -&gt; None:\n\t\"\"\"Show message indicating all changes are committed.\"\"\"\n\tself.console.print(\"[green]\u2713[/green] All changes committed!\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_all_done","title":"show_all_done","text":"<pre><code>show_all_done() -&gt; None\n</code></pre> <p>Show a final success message when the process completes.</p> <p>This is an alias for show_all_committed for now, but could be customized.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_all_done(self) -&gt; None:\n\t\"\"\"\n\tShow a final success message when the process completes.\n\n\tThis is an alias for show_all_committed for now, but could be\n\tcustomized.\n\n\t\"\"\"\n\tself.show_all_committed()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.show_lint_errors","title":"show_lint_errors","text":"<pre><code>show_lint_errors(errors: list[str]) -&gt; None\n</code></pre> <p>Display linting errors to the user.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def show_lint_errors(self, errors: list[str]) -&gt; None:\n\t\"\"\"Display linting errors to the user.\"\"\"\n\tself.console.print(\"[bold red]Commit message failed linting:[/bold red]\")\n\tfor error in errors:\n\t\tself.console.print(f\"  - {error}\")\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_commit_with_lint_errors","title":"confirm_commit_with_lint_errors","text":"<pre><code>confirm_commit_with_lint_errors() -&gt; bool\n</code></pre> <p>Ask the user if they want to commit despite lint errors.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_commit_with_lint_errors(self) -&gt; bool:\n\t\"\"\"Ask the user if they want to commit despite lint errors.\"\"\"\n\treturn questionary.confirm(\"Commit message has lint errors. Commit anyway?\", default=False).ask()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.confirm_exit","title":"confirm_exit","text":"<pre><code>confirm_exit() -&gt; bool\n</code></pre> <p>Ask the user to confirm exiting without committing.</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def confirm_exit(self) -&gt; bool:\n\t\"\"\"Ask the user to confirm exiting without committing.\"\"\"\n\treturn questionary.confirm(\"Are you sure you want to exit without committing?\", default=False).ask()\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.display_failed_lint_message","title":"display_failed_lint_message","text":"<pre><code>display_failed_lint_message(\n\tmessage: str,\n\tlint_errors: list[str],\n\tis_llm_generated: bool = False,\n) -&gt; None\n</code></pre> <p>Display a commit message that failed linting, along with the errors.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to display.</p> required <code>lint_errors</code> <code>list[str]</code> <p>List of linting error messages.</p> required <code>is_llm_generated</code> <code>bool</code> <p>Whether the message was generated by an LLM.</p> <code>False</code> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def display_failed_lint_message(self, message: str, lint_errors: list[str], is_llm_generated: bool = False) -&gt; None:\n\t\"\"\"\n\tDisplay a commit message that failed linting, along with the errors.\n\n\tArgs:\n\t    message: The commit message to display.\n\t    lint_errors: List of linting error messages.\n\t    is_llm_generated: Whether the message was generated by an LLM.\n\n\t\"\"\"\n\ttag = \"AI\" if is_llm_generated else \"Simple\"\n\tmessage_panel = Panel(\n\t\tText(message, style=\"yellow\"),  # Use yellow style for the message text\n\t\ttitle=f\"[bold yellow]Proposed message ({tag}) - LINTING FAILED[/]\",\n\t\tborder_style=\"yellow\",  # Yellow border to indicate warning/failure\n\t\texpand=False,\n\t\tpadding=(1, 2),\n\t)\n\tself.console.print(message_panel)\n\n\t# Display lint errors below\n\tif lint_errors:\n\t\terror_text = Text(\"\\n\".join([f\"- {err}\" for err in lint_errors]), style=\"red\")\n\t\terror_panel = Panel(\n\t\t\terror_text,\n\t\t\ttitle=\"[bold red]Linting Errors[/]\",\n\t\t\tborder_style=\"red\",\n\t\t\texpand=False,\n\t\t\tpadding=(1, 2),\n\t\t)\n\t\tself.console.print(error_panel)\n</code></pre>"},{"location":"api/git/interactive/#codemap.git.interactive.CommitUI.get_group_action","title":"get_group_action","text":"<pre><code>get_group_action() -&gt; ChunkAction\n</code></pre> <p>Get the user's desired action for the current semantic group.</p> <p>Returns:</p> Type Description <code>ChunkAction</code> <p>ChunkAction indicating what to do with the group</p> Source code in <code>src/codemap/git/interactive.py</code> <pre><code>def get_group_action(self) -&gt; ChunkAction:\n\t\"\"\"\n\tGet the user's desired action for the current semantic group.\n\n\tReturns:\n\t        ChunkAction indicating what to do with the group\n\n\t\"\"\"\n\t# Define options with their display text and corresponding action\n\toptions: list[tuple[str, ChunkAction]] = [\n\t\t(\"Commit this group\", ChunkAction.COMMIT),\n\t\t(\"Edit message and commit\", ChunkAction.EDIT),\n\t\t(\"Regenerate message\", ChunkAction.REGENERATE),\n\t\t(\"Skip this group\", ChunkAction.SKIP),\n\t\t(\"Exit without committing\", ChunkAction.EXIT),\n\t]\n\n\t# Use questionary to get the user's choice\n\tresult = questionary.select(\n\t\t\"What would you like to do with this group?\",\n\t\tchoices=[option[0] for option in options],\n\t\tdefault=options[0][0],  # Set \"Commit this group\" as default\n\t\tqmark=\"\u00bb\",\n\t\tuse_indicator=True,\n\t\tuse_arrow_keys=True,\n\t).ask()\n\n\t# Map the result back to the ChunkAction\n\tfor option, action in options:\n\t\tif option == result:\n\t\t\treturn action\n\n\t# Fallback (should never happen)\n\treturn ChunkAction.EXIT\n</code></pre>"},{"location":"api/git/utils/","title":"Utils","text":"<p>Git utilities for CodeMap.</p>"},{"location":"api/git/utils/#codemap.git.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff","title":"GitDiff  <code>dataclass</code>","text":"<p>Represents a Git diff chunk.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>@dataclass\nclass GitDiff:\n\t\"\"\"Represents a Git diff chunk.\"\"\"\n\n\tfiles: list[str]\n\tcontent: str\n\tis_staged: bool = False\n\tis_untracked: bool = False\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.__init__","title":"__init__","text":"<pre><code>__init__(\n\tfiles: list[str],\n\tcontent: str,\n\tis_staged: bool = False,\n\tis_untracked: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: list[str]\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.is_staged","title":"is_staged  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_staged: bool = False\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitDiff.is_untracked","title":"is_untracked  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_untracked: bool = False\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.GitError","title":"GitError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for Git-related errors.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>class GitError(Exception):\n\t\"\"\"Custom exception for Git-related errors.\"\"\"\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.run_git_command","title":"run_git_command","text":"<pre><code>run_git_command(\n\tcommand: list[str],\n\tcwd: Path | str | None = None,\n\tenvironment: dict[str, str] | None = None,\n) -&gt; str\n</code></pre> <p>Run a git command and return its output.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>list[str]</code> <p>Command to run as a list of string arguments</p> required <code>cwd</code> <code>Path | str | None</code> <p>Working directory to run the command in</p> <code>None</code> <code>environment</code> <code>dict[str, str] | None</code> <p>Environment variables to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The output from the command</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If the git command fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def run_git_command(\n\tcommand: list[str],\n\tcwd: Path | str | None = None,\n\tenvironment: dict[str, str] | None = None,\n) -&gt; str:\n\t\"\"\"\n\tRun a git command and return its output.\n\n\tArgs:\n\t    command: Command to run as a list of string arguments\n\t    cwd: Working directory to run the command in\n\t    environment: Environment variables to use\n\n\tReturns:\n\t    The output from the command\n\n\tRaises:\n\t    GitError: If the git command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Using subprocess.run with a list of arguments is safe since we're not using shell=True\n\t\t# and the command is not being built from untrusted input\n\t\tresult = subprocess.run(  # noqa: S603\n\t\t\tcommand,\n\t\t\tcwd=cwd,\n\t\t\tcapture_output=True,\n\t\t\ttext=True,\n\t\t\tcheck=True,\n\t\t\tenv=environment,\n\t\t)\n\t\treturn result.stdout.strip()\n\texcept subprocess.CalledProcessError as e:\n\t\t# Check if this is a pre-commit hook failure for commit - handled specially by the UI\n\t\tif command and len(command) &gt; 1 and command[1] == \"commit\":\n\t\t\tif \"pre-commit\" in (e.stderr or \"\"):\n\t\t\t\t# This is a pre-commit hook failure - which is handled by the UI, so don't log as exception\n\t\t\t\tlogger.warning(\"Git hooks failed: %s\", e.stderr)\n\t\t\t\tmsg = f\"{e.stderr}\"\n\t\t\t\traise GitError(msg) from e\n\t\t\t# Regular commit error\n\t\t\tlogger.exception(\"Git command failed: %s\", \" \".join(command))\n\n\t\tcmd_str = \" \".join(command)\n\t\terror_output = e.stderr or \"\"\n\t\terror_msg = f\"Git command failed: {cmd_str}\\n{error_output}\"\n\t\tlogger.exception(error_msg)\n\t\traise GitError(error_output or error_msg) from e\n\texcept Exception as e:\n\t\terror_msg = f\"Error running git command: {e}\"\n\t\tlogger.exception(error_msg)\n\t\traise GitError(error_msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.get_repo_root","title":"get_repo_root","text":"<pre><code>get_repo_root(path: Path | None = None) -&gt; Path\n</code></pre> <p>Get the root directory of the Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Optional path to start searching from</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to repository root</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If not in a Git repository</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_repo_root(path: Path | None = None) -&gt; Path:\n\t\"\"\"\n\tGet the root directory of the Git repository.\n\n\tArgs:\n\t    path: Optional path to start searching from\n\n\tReturns:\n\t    Path to repository root\n\n\tRaises:\n\t    GitError: If not in a Git repository\n\n\t\"\"\"\n\ttry:\n\t\tresult = run_git_command([\"git\", \"rev-parse\", \"--show-toplevel\"], path)\n\t\treturn Path(result.strip())\n\texcept GitError as e:\n\t\tmsg = \"Not in a Git repository\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.validate_repo_path","title":"validate_repo_path","text":"<pre><code>validate_repo_path(path: Path | None = None) -&gt; Path | None\n</code></pre> <p>Validate and return the repository path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Optional path to validate (defaults to current directory)</p> <code>None</code> <p>Returns:</p> Type Description <code>Path | None</code> <p>Path to the repository root if valid, None otherwise</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def validate_repo_path(path: Path | None = None) -&gt; Path | None:\n\t\"\"\"\n\tValidate and return the repository path.\n\n\tArgs:\n\t    path: Optional path to validate (defaults to current directory)\n\n\tReturns:\n\t    Path to the repository root if valid, None otherwise\n\n\t\"\"\"\n\ttry:\n\t\t# If no path provided, use current directory\n\t\tif path is None:\n\t\t\tpath = Path.cwd()\n\n\t\t# Get the repository root\n\t\treturn get_repo_root(path)\n\texcept GitError:\n\t\treturn None\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.get_staged_diff","title":"get_staged_diff","text":"<pre><code>get_staged_diff() -&gt; GitDiff\n</code></pre> <p>Get the diff of staged changes.</p> <p>Returns:</p> Type Description <code>GitDiff</code> <p>GitDiff object containing staged changes</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_staged_diff() -&gt; GitDiff:\n\t\"\"\"\n\tGet the diff of staged changes.\n\n\tReturns:\n\t    GitDiff object containing staged changes\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Get list of staged files\n\t\tstaged_files = run_git_command([\"git\", \"diff\", \"--cached\", \"--name-only\"]).splitlines()\n\n\t\t# Get the actual diff\n\t\tdiff_content = run_git_command([\"git\", \"diff\", \"--cached\"])\n\n\t\treturn GitDiff(\n\t\t\tfiles=staged_files,\n\t\t\tcontent=diff_content,\n\t\t\tis_staged=True,\n\t\t)\n\texcept GitError as e:\n\t\tmsg = \"Failed to get staged changes\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.get_unstaged_diff","title":"get_unstaged_diff","text":"<pre><code>get_unstaged_diff() -&gt; GitDiff\n</code></pre> <p>Get the diff of unstaged changes.</p> <p>Returns:</p> Type Description <code>GitDiff</code> <p>GitDiff object containing unstaged changes</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_unstaged_diff() -&gt; GitDiff:\n\t\"\"\"\n\tGet the diff of unstaged changes.\n\n\tReturns:\n\t    GitDiff object containing unstaged changes\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Get list of modified files\n\t\tmodified_files = run_git_command([\"git\", \"diff\", \"--name-only\"]).splitlines()\n\n\t\t# Get the actual diff\n\t\tdiff_content = run_git_command([\"git\", \"diff\"])\n\n\t\treturn GitDiff(\n\t\t\tfiles=modified_files,\n\t\t\tcontent=diff_content,\n\t\t\tis_staged=False,\n\t\t)\n\texcept GitError as e:\n\t\tmsg = \"Failed to get unstaged changes\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.stage_files","title":"stage_files","text":"<pre><code>stage_files(files: list[str]) -&gt; None\n</code></pre> <p>Stage the specified files.</p> <p>This function intelligently handles both existing and deleted files: - For existing files, it uses <code>git add</code> - For files that no longer exist but are tracked by git, it uses <code>git rm</code> - For files that no longer exist but are still in index, it uses <code>git rm --cached</code></p> <p>This prevents errors when trying to stage files that have been deleted but not yet tracked in git.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of files to stage</p> required <p>Raises:</p> Type Description <code>GitError</code> <p>If staging fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def stage_files(files: list[str]) -&gt; None:\n\t\"\"\"\n\tStage the specified files.\n\n\tThis function intelligently handles both existing and deleted files:\n\t- For existing files, it uses `git add`\n\t- For files that no longer exist but are tracked by git, it uses `git rm`\n\t- For files that no longer exist but are still in index, it uses `git rm --cached`\n\n\tThis prevents errors when trying to stage files that have been deleted\n\tbut not yet tracked in git.\n\n\tArgs:\n\t    files: List of files to stage\n\n\tRaises:\n\t    GitError: If staging fails\n\n\t\"\"\"\n\tif not files:\n\t\tlogger.warning(\"No files provided to stage_files\")\n\t\treturn\n\n\t# Keep track of all errors to report at the end\n\terrors = []\n\n\ttry:\n\t\t# 1. Get information about file status\n\t\t# ====================================\n\t\tgit_status_info = {}\n\t\ttracked_files = set()\n\t\tindex_files = set()\n\n\t\t# 1.1 Get git status information\n\t\ttry:\n\t\t\tstatus_output = run_git_command([\"git\", \"status\", \"--porcelain\"])\n\t\t\tfor line in status_output.splitlines():\n\t\t\t\t# Ensure line is a string, not bytes\n\t\t\t\tline_str = line if isinstance(line, str) else line.decode(\"utf-8\")\n\t\t\t\tif not line_str:\n\t\t\t\t\tcontinue\n\n\t\t\t\tstatus = line_str[:2]\n\t\t\t\tfile_path = line_str[3:].strip()\n\t\t\t\tgit_status_info[file_path] = status\n\t\texcept GitError:\n\t\t\terrors.append(\"Failed to get git status information\")\n\n\t\t# 1.2 Get tracked files\n\t\ttry:\n\t\t\ttracked_files_output = run_git_command([\"git\", \"ls-files\"])\n\t\t\ttracked_files = set(tracked_files_output.splitlines())\n\t\texcept GitError:\n\t\t\terrors.append(\"Failed to get list of tracked files\")\n\n\t\t# 1.3 Get index files\n\t\ttry:\n\t\t\tindex_files_output = run_git_command([\"git\", \"ls-files\", \"--stage\"])\n\t\t\tindex_files = {line.split()[-1] for line in index_files_output.splitlines() if line.strip()}\n\t\texcept GitError:\n\t\t\terrors.append(\"Failed to get list of files in git index\")\n\n\t\t# 2. Filter and categorize files\n\t\t# ==============================\n\t\t# Filter out invalid filenames\n\t\tvalid_files = [\n\t\t\tfile\n\t\t\tfor file in files\n\t\t\tif not (any(char in file for char in [\"*\", \"+\", \"{\", \"}\", \"\\\\\"]) or file.startswith('\"'))\n\t\t]\n\n\t\t# Skip any invalid filenames that were filtered out\n\t\tfor file in files:\n\t\t\tif file not in valid_files:\n\t\t\t\tlogger.warning(\"Skipping invalid filename: %s\", file)\n\n\t\t# Categorize files\n\t\texisting_files = []\n\t\tdeleted_tracked_files = []\n\t\tdeleted_index_files = []\n\t\tuntracked_nonexistent_files = []\n\n\t\tfor file in valid_files:\n\t\t\tpath = Path(file)\n\t\t\tif path.exists():\n\t\t\t\texisting_files.append(file)\n\t\t\telif file in tracked_files:\n\t\t\t\tdeleted_tracked_files.append(file)\n\t\t\telif file in index_files:\n\t\t\t\tdeleted_index_files.append(file)\n\t\t\telse:\n\t\t\t\tuntracked_nonexistent_files.append(file)\n\t\t\t\tlogger.warning(\"Skipping file %s: Does not exist and is not tracked by git\", file)\n\n\t\t# Log the categorized files\n\t\tlogger.debug(\"Existing files (%d): %s\", len(existing_files), existing_files)\n\t\tlogger.debug(\"Deleted tracked files (%d): %s\", len(deleted_tracked_files), deleted_tracked_files)\n\t\tlogger.debug(\"Deleted index files (%d): %s\", len(deleted_index_files), deleted_index_files)\n\n\t\t# 3. Process each file category\n\t\t# =============================\n\t\t# 3.1 Add existing files\n\t\tif existing_files:\n\t\t\ttry:\n\t\t\t\trun_git_command([\"git\", \"add\", *existing_files])\n\t\t\t\tlogger.debug(\"Added %d existing files\", len(existing_files))\n\t\t\texcept GitError as e:\n\t\t\t\terrors.append(f\"Failed to add existing files: {e!s}\")\n\n\t\t# 3.2 Remove deleted tracked files\n\t\tfor file in deleted_tracked_files:\n\t\t\tcmd = [\"git\", \"rm\", file]\n\t\t\ttry:\n\t\t\t\trun_git_command(cmd)\n\t\t\t\tlogger.debug(\"Removed deleted tracked file: %s\", file)\n\t\t\texcept GitError as e:\n\t\t\t\tif \"did not match any files\" in str(e):\n\t\t\t\t\t# File exists in tracked_files but can't be found, try with --cached\n\t\t\t\t\tdeleted_index_files.append(file)\n\t\t\t\telse:\n\t\t\t\t\terrors.append(f\"Failed to remove deleted tracked file {file}: {e!s}\")\n\n\t\t# 3.3 Remove files from index\n\t\tif deleted_index_files:\n\t\t\ttry:\n\t\t\t\trun_git_command([\"git\", \"rm\", \"--cached\", *deleted_index_files])\n\t\t\t\tlogger.debug(\"Removed %d files from index\", len(deleted_index_files))\n\t\t\texcept GitError as e:\n\t\t\t\terrors.append(f\"Failed to remove files from index: {e!s}\")\n\n\t\t# 4. Report errors if any occurred\n\t\t# ================================\n\t\tif errors:\n\t\t\terror_msg = \"; \".join(errors)\n\t\t\tmsg = f\"Errors while staging files: {error_msg}\"\n\t\t\tlogger.error(msg)\n\t\t\traise GitError(msg)\n\n\texcept GitError:\n\t\t# Pass through GitError exceptions\n\t\traise\n\texcept Exception as e:\n\t\t# Wrap other exceptions in GitError\n\t\tmsg = f\"Unexpected error staging files: {e}\"\n\t\tlogger.exception(msg)\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.commit","title":"commit","text":"<pre><code>commit(message: str) -&gt; None\n</code></pre> <p>Create a commit with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Commit message</p> required <p>Raises:</p> Type Description <code>GitError</code> <p>If commit fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def commit(message: str) -&gt; None:\n\t\"\"\"\n\tCreate a commit with the given message.\n\n\tArgs:\n\t    message: Commit message\n\n\tRaises:\n\t    GitError: If commit fails\n\n\t\"\"\"\n\ttry:\n\t\t# For commit messages, we need to ensure they're properly quoted\n\t\t# Use a shell command directly to ensure proper quoting\n\t\timport shlex\n\n\t\tquoted_message = shlex.quote(message)\n\t\tshell_command = f\"git commit -m {quoted_message}\"\n\n\t\t# Using shell=True is necessary for proper handling of quoted commit messages\n\t\t# Security is maintained by using shlex.quote to escape user input\n\t\tsubprocess.run(  # noqa: S602\n\t\t\tshell_command,\n\t\t\tcwd=None,  # Use current dir\n\t\t\tcapture_output=True,\n\t\t\ttext=True,\n\t\t\tcheck=True,\n\t\t\tshell=True,  # Using shell=True for this operation\n\t\t)\n\texcept subprocess.CalledProcessError as e:\n\t\tmsg = f\"Failed to create commit: {e.stderr}\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.get_other_staged_files","title":"get_other_staged_files","text":"<pre><code>get_other_staged_files(\n\ttargeted_files: list[str],\n) -&gt; list[str]\n</code></pre> <p>Get staged files that are not part of the targeted files.</p> <p>Parameters:</p> Name Type Description Default <code>targeted_files</code> <code>list[str]</code> <p>List of files that are meant to be committed</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of other staged files that might be committed inadvertently</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_other_staged_files(targeted_files: list[str]) -&gt; list[str]:\n\t\"\"\"\n\tGet staged files that are not part of the targeted files.\n\n\tArgs:\n\t    targeted_files: List of files that are meant to be committed\n\n\tReturns:\n\t    List of other staged files that might be committed inadvertently\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Get all staged files\n\t\tall_staged = run_git_command([\"git\", \"diff\", \"--cached\", \"--name-only\"]).splitlines()\n\n\t\t# Filter out the targeted files\n\t\treturn [f for f in all_staged if f not in targeted_files]\n\texcept GitError as e:\n\t\tmsg = \"Failed to check for other staged files\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.stash_staged_changes","title":"stash_staged_changes","text":"<pre><code>stash_staged_changes(exclude_files: list[str]) -&gt; bool\n</code></pre> <p>Temporarily stash staged changes except for specified files.</p> <p>This is used to ensure only specific files are committed when other files might be mistakenly staged.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_files</code> <code>list[str]</code> <p>Files to exclude from stashing (to keep staged)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether stashing was performed</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git operations fail</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def stash_staged_changes(exclude_files: list[str]) -&gt; bool:\n\t\"\"\"\n\tTemporarily stash staged changes except for specified files.\n\n\tThis is used to ensure only specific files are committed when other\n\tfiles might be mistakenly staged.\n\n\tArgs:\n\t    exclude_files: Files to exclude from stashing (to keep staged)\n\n\tReturns:\n\t    Whether stashing was performed\n\n\tRaises:\n\t    GitError: If git operations fail\n\n\t\"\"\"\n\ttry:\n\t\t# First check if there are any other staged files\n\t\tother_files = get_other_staged_files(exclude_files)\n\t\tif not other_files:\n\t\t\treturn False\n\n\t\t# Create a temporary index to save current state\n\t\trun_git_command([\"git\", \"stash\", \"push\", \"--keep-index\", \"--message\", \"CodeMap: temporary stash for commit\"])\n\texcept GitError as e:\n\t\tmsg = \"Failed to stash other staged changes\"\n\t\traise GitError(msg) from e\n\telse:\n\t\treturn True\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.unstash_changes","title":"unstash_changes","text":"<pre><code>unstash_changes() -&gt; None\n</code></pre> <p>Restore previously stashed changes.</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git operations fail</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def unstash_changes() -&gt; None:\n\t\"\"\"\n\tRestore previously stashed changes.\n\n\tRaises:\n\t    GitError: If git operations fail\n\n\t\"\"\"\n\ttry:\n\t\tstash_list = run_git_command([\"git\", \"stash\", \"list\"])\n\t\tif \"CodeMap: temporary stash for commit\" in stash_list:\n\t\t\trun_git_command([\"git\", \"stash\", \"pop\"])\n\texcept GitError as e:\n\t\tmsg = \"Failed to restore stashed changes; you may need to manually run 'git stash pop'\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.commit_only_files","title":"commit_only_files","text":"<pre><code>commit_only_files(\n\tfiles: list[str],\n\tmessage: str,\n\t*,\n\tcommit_options: list[str] | None = None,\n\tignore_hooks: bool = False,\n) -&gt; list[str]\n</code></pre> <p>Commit only the specified files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of files to commit</p> required <code>message</code> <code>str</code> <p>Commit message</p> required <code>commit_options</code> <code>list[str] | None</code> <p>Additional commit options</p> <code>None</code> <code>ignore_hooks</code> <code>bool</code> <p>Whether to ignore Git hooks</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of other staged files that weren't committed</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def commit_only_files(\n\tfiles: list[str], message: str, *, commit_options: list[str] | None = None, ignore_hooks: bool = False\n) -&gt; list[str]:\n\t\"\"\"\n\tCommit only the specified files.\n\n\tArgs:\n\t    files: List of files to commit\n\t    message: Commit message\n\t    commit_options: Additional commit options\n\t    ignore_hooks: Whether to ignore Git hooks\n\n\tReturns:\n\t    List of other staged files that weren't committed\n\n\t\"\"\"\n\ttry:\n\t\t# Get status to check for deleted files\n\t\tstatus_cmd = [\"git\", \"status\", \"--porcelain\"]\n\t\tresult = subprocess.run(  # noqa: S603\n\t\t\tstatus_cmd,\n\t\t\tcapture_output=True,\n\t\t\ttext=True,\n\t\t\tcheck=True,\n\t\t\tshell=False,  # Explicitly set shell=False for security\n\t\t)\n\t\tstatus_output = result.stdout.strip()\n\n\t\t# Extract files from status output\n\t\tstatus_files = {}\n\t\tfor line in status_output.splitlines():\n\t\t\tif not line.strip():\n\t\t\t\tcontinue\n\t\t\tstatus = line[:2].strip()\n\t\t\tfile_path = line[3:].strip()\n\n\t\t\t# Handle renamed files\n\t\t\tif isinstance(file_path, bytes):\n\t\t\t\tfile_path = file_path.decode(\"utf-8\")\n\n\t\t\tif \" -&gt; \" in file_path:\n\t\t\t\tfile_path = file_path.split(\" -&gt; \")[1]\n\n\t\t\tstatus_files[file_path] = status\n\n\t\t# Stage all files - our improved stage_files function can handle both existing and deleted files\n\t\tstage_files(files)\n\n\t\t# Get other staged files\n\t\tother_staged = get_other_staged_files(files)\n\n\t\t# Commit the changes\n\t\tcommit_cmd = [\"git\", \"commit\", \"-m\", message]\n\n\t\tif commit_options:\n\t\t\tcommit_cmd.extend(commit_options)\n\n\t\tif ignore_hooks:\n\t\t\tcommit_cmd.append(\"--no-verify\")\n\n\t\ttry:\n\t\t\tsubprocess.run(  # noqa: S603\n\t\t\t\tcommit_cmd,\n\t\t\t\tcheck=True,\n\t\t\t\tcapture_output=True,\n\t\t\t\ttext=True,\n\t\t\t\tshell=False,  # Explicitly set shell=False for security\n\t\t\t)\n\t\t\tlogger.info(\"Created commit with message: %s\", message)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\t# Capture stderr and stdout for better error reporting\n\t\t\terror_msg = f\"Git commit command failed. Command: '{' '.join(commit_cmd)}'\"\n\n\t\t\tif e.stderr:\n\t\t\t\terror_msg += f\"\\n\\nGit Error Output:\\n{e.stderr.strip()}\"\n\t\t\tif e.stdout:\n\t\t\t\terror_msg += f\"\\n\\nCommand Output:\\n{e.stdout.strip()}\"\n\n\t\t\tlogger.exception(\"Failed to create commit: %s\", error_msg)\n\t\t\traise GitError(error_msg) from e\n\n\t\treturn other_staged\n\texcept GitError:\n\t\t# Re-raise GitErrors directly\n\t\traise\n\texcept Exception as e:\n\t\terror_msg = f\"Error in commit_only_files: {e!s}\"\n\t\tlogger.exception(error_msg)\n\t\traise GitError(error_msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.get_untracked_files","title":"get_untracked_files","text":"<pre><code>get_untracked_files() -&gt; list[str]\n</code></pre> <p>Get a list of untracked files in the repository.</p> <p>These are files that are not yet tracked by Git (new files that haven't been staged).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of untracked file paths</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_untracked_files() -&gt; list[str]:\n\t\"\"\"\n\tGet a list of untracked files in the repository.\n\n\tThese are files that are not yet tracked by Git (new files that haven't been staged).\n\n\tReturns:\n\t    List of untracked file paths\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Use ls-files with --others to get untracked files and --exclude-standard to respect gitignore\n\t\treturn run_git_command([\"git\", \"ls-files\", \"--others\", \"--exclude-standard\"]).splitlines()\n\texcept GitError as e:\n\t\tmsg = \"Failed to get untracked files\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.unstage_files","title":"unstage_files","text":"<pre><code>unstage_files(files: list[str]) -&gt; None\n</code></pre> <p>Unstage the specified files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of files to unstage</p> required <p>Raises:</p> Type Description <code>GitError</code> <p>If unstaging fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def unstage_files(files: list[str]) -&gt; None:\n\t\"\"\"\n\tUnstage the specified files.\n\n\tArgs:\n\t    files: List of files to unstage\n\n\tRaises:\n\t    GitError: If unstaging fails\n\n\t\"\"\"\n\ttry:\n\t\trun_git_command([\"git\", \"restore\", \"--staged\", *files])\n\texcept GitError as e:\n\t\tmsg = f\"Failed to unstage files: {', '.join(files)}\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.switch_branch","title":"switch_branch","text":"<pre><code>switch_branch(branch_name: str) -&gt; None\n</code></pre> <p>Switch the current Git branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>The name of the branch to switch to.</p> required <p>Raises:</p> Type Description <code>GitError</code> <p>If the git checkout command fails.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def switch_branch(branch_name: str) -&gt; None:\n\t\"\"\"\n\tSwitch the current Git branch.\n\n\tArgs:\n\t    branch_name: The name of the branch to switch to.\n\n\tRaises:\n\t    GitError: If the git checkout command fails.\n\n\t\"\"\"\n\ttry:\n\t\tcommand = [\"git\", \"checkout\", branch_name]\n\t\tlogger.debug(\"Running command: %s\", shlex.join(command))\n\t\tresult = subprocess.run(command, capture_output=True, text=True, check=True, cwd=get_repo_root())  # noqa: S603\n\t\tlogger.debug(\"Switch branch stdout: %s\", result.stdout)\n\t\tlogger.debug(\"Switch branch stderr: %s\", result.stderr)\n\texcept subprocess.CalledProcessError as e:\n\t\terror_message = f\"Failed to switch to branch '{branch_name}': {e.stderr}\"\n\t\tlogger.exception(error_message)\n\t\traise GitError(error_message) from e\n\texcept FileNotFoundError as e:\n\t\terror_message = \"Git command not found. Ensure Git is installed and in PATH.\"\n\t\tlogger.exception(error_message)\n\t\traise GitError(error_message) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.get_current_branch","title":"get_current_branch","text":"<pre><code>get_current_branch() -&gt; str\n</code></pre> <p>Get the name of the current branch.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the current branch</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def get_current_branch() -&gt; str:\n\t\"\"\"\n\tGet the name of the current branch.\n\n\tReturns:\n\t    Name of the current branch\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\treturn run_git_command([\"git\", \"branch\", \"--show-current\"]).strip()\n\texcept GitError as e:\n\t\tmsg = \"Failed to get current branch\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/utils/#codemap.git.utils.is_git_ignored","title":"is_git_ignored","text":"<pre><code>is_git_ignored(file_path: str) -&gt; bool\n</code></pre> <p>Check if a file is ignored by Git.</p> Source code in <code>src/codemap/git/utils.py</code> <pre><code>def is_git_ignored(file_path: str) -&gt; bool:\n\t\"\"\"Check if a file is ignored by Git.\"\"\"\n\ttry:\n\t\treturn run_git_command([\"git\", \"check-ignore\", file_path]).strip() == \"\"\n\texcept GitError:\n\t\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/","title":"Commit Generator Overview","text":"<p>Commit message generation package for CodeMap.</p> <ul> <li>Command - Main commit command implementation for CodeMap.</li> <li>Generator - Generator module for commit messages.</li> <li>Prompts - Prompt templates for commit message generation.</li> <li>Schemas - Schemas and data structures for commit message generation.</li> <li>Utils - Utility functions for commit message generation.</li> </ul>"},{"location":"api/git/commit_generator/command/","title":"Command","text":"<p>Main commit command implementation for CodeMap.</p>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MAX_FILES_BEFORE_BATCHING","title":"MAX_FILES_BEFORE_BATCHING  <code>module-attribute</code>","text":"<pre><code>MAX_FILES_BEFORE_BATCHING = 10\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MAX_FILE_CONTENT_LINES","title":"MAX_FILE_CONTENT_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_FILE_CONTENT_LINES = 300\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MAX_TOTAL_CONTENT_LINES","title":"MAX_TOTAL_CONTENT_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_TOTAL_CONTENT_LINES = 1000\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.MIN_PORCELAIN_LINE_LENGTH","title":"MIN_PORCELAIN_LINE_LENGTH  <code>module-attribute</code>","text":"<pre><code>MIN_PORCELAIN_LINE_LENGTH = 3\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.ExitCommandError","title":"ExitCommandError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception to signal an exit command.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>class ExitCommandError(Exception):\n\t\"\"\"Exception to signal an exit command.\"\"\"\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand","title":"CommitCommand","text":"<p>Handles the commit command workflow.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>class CommitCommand:\n\t\"\"\"Handles the commit command workflow.\"\"\"\n\n\tdef __init__(self, path: Path | None = None, model: str = \"gpt-4o-mini\", bypass_hooks: bool = False) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the commit command.\n\n\t\tArgs:\n\t\t    path: Optional path to start from\n\t\t    model: LLM model to use for commit message generation\n\t\t    bypass_hooks: Whether to bypass git hooks with --no-verify\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.repo_root = get_repo_root(path)\n\t\t\tself.ui: CommitUI = CommitUI()\n\t\t\tself.splitter = DiffSplitter(self.repo_root)\n\t\t\tself.target_files = []  # Initialize target_files attribute\n\n\t\t\t# Store the current branch at initialization to ensure we don't switch branches unexpectedly\n\t\t\ttry:\n\t\t\t\tself.original_branch = get_current_branch()\n\t\t\texcept (ImportError, GitError):\n\t\t\t\tself.original_branch = None\n\n\t\t\t# Create LLM client and configs\n\t\t\tfrom codemap.llm import create_client\n\t\t\tfrom codemap.utils.config_loader import ConfigLoader\n\n\t\t\tconfig_loader = ConfigLoader(repo_root=self.repo_root)\n\t\t\tllm_client = create_client(repo_path=self.repo_root, model=model)\n\n\t\t\t# Create the commit message generator with required parameters\n\t\t\tself.message_generator = CommitMessageGenerator(\n\t\t\t\trepo_root=self.repo_root,\n\t\t\t\tllm_client=llm_client,\n\t\t\t\tprompt_template=DEFAULT_PROMPT_TEMPLATE,\n\t\t\t\tconfig_loader=config_loader,\n\t\t\t)\n\n\t\t\tself.error_state = None  # Tracks reason for failure: \"failed\", \"aborted\", etc.\n\t\t\tself.bypass_hooks = bypass_hooks  # Whether to bypass git hooks with --no-verify\n\t\texcept GitError as e:\n\t\t\traise RuntimeError(str(e)) from e\n\n\tdef _get_changes(self) -&gt; list[GitDiff]:\n\t\t\"\"\"\n\t\tGet staged, unstaged, and untracked changes, generating a GitDiff object per file.\n\n\t\tReturns:\n\t\t    List of GitDiff objects, each representing changes for a single file.\n\n\t\tRaises:\n\t\t    RuntimeError: If Git operations fail.\n\n\t\t\"\"\"\n\t\tchanges: list[GitDiff] = []\n\t\tprocessed_files: set[str] = set()  # Track files already added\n\n\t\ttry:\n\t\t\t# 1. Get Staged Changes (Per File)\n\t\t\tstaged_files = run_git_command([\"git\", \"diff\", \"--cached\", \"--name-only\"]).splitlines()\n\t\t\tif staged_files:\n\t\t\t\tlogger.debug(\"Found %d staged files. Fetching diffs individually...\", len(staged_files))\n\t\t\t\tfor file_path in staged_files:\n\t\t\t\t\tif file_path in processed_files:\n\t\t\t\t\t\tcontinue  # Avoid duplicates if somehow listed again\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfile_diff_content = run_git_command([\"git\", \"diff\", \"--cached\", \"--\", file_path])\n\t\t\t\t\t\tchanges.append(GitDiff(files=[file_path], content=file_diff_content, is_staged=True))\n\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\tlogger.warning(\"Could not get staged diff for %s: %s\", file_path, e)\n\n\t\t\t# 2. Get Unstaged Changes (Per File for files not already staged)\n\t\t\tunstaged_files = run_git_command([\"git\", \"diff\", \"--name-only\"]).splitlines()\n\t\t\tif unstaged_files:\n\t\t\t\tlogger.debug(\"Found %d unstaged files. Fetching diffs individually...\", len(unstaged_files))\n\t\t\t\tfor file_path in unstaged_files:\n\t\t\t\t\t# Only process unstaged if not already captured as staged\n\t\t\t\t\tif file_path not in processed_files:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tfile_diff_content = run_git_command([\"git\", \"diff\", \"--\", file_path])\n\t\t\t\t\t\t\tchanges.append(GitDiff(files=[file_path], content=file_diff_content, is_staged=False))\n\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\t\tlogger.warning(\"Could not get unstaged diff for %s: %s\", file_path, e)\n\n\t\t\t# 3. Get Untracked Files (Per File, content formatted as diff)\n\t\t\tuntracked_files_paths = get_untracked_files()\n\t\t\tif untracked_files_paths:\n\t\t\t\tlogger.debug(\"Found %d untracked files. Reading content...\", len(untracked_files_paths))\n\t\t\t\ttotal_content_lines = 0\n\n\t\t\t\tfor file_path in untracked_files_paths:\n\t\t\t\t\t# Only process untracked if not already captured as staged/unstaged (edge case)\n\t\t\t\t\tif file_path not in processed_files:\n\t\t\t\t\t\tabs_path = self.repo_root / file_path\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tcontent = read_file_content(abs_path)\n\t\t\t\t\t\t\tif content is not None:\n\t\t\t\t\t\t\t\tcontent_lines = content.splitlines()\n\t\t\t\t\t\t\t\toriginal_line_count = len(content_lines)\n\t\t\t\t\t\t\t\tneeds_total_truncation_notice = False\n\n\t\t\t\t\t\t\t\t# File-level truncation\n\t\t\t\t\t\t\t\tif len(content_lines) &gt; MAX_FILE_CONTENT_LINES:\n\t\t\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\t\t\t\"Untracked file %s is large (%d lines), truncating to %d lines\",\n\t\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t\t\tlen(content_lines),\n\t\t\t\t\t\t\t\t\t\tMAX_FILE_CONTENT_LINES,\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\ttruncation_msg = (\n\t\t\t\t\t\t\t\t\t\tf\"[... {len(content_lines) - MAX_FILE_CONTENT_LINES} more lines truncated ...]\"\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\tcontent_lines = content_lines[:MAX_FILE_CONTENT_LINES]\n\t\t\t\t\t\t\t\t\tcontent_lines.append(truncation_msg)\n\n\t\t\t\t\t\t\t\t# Total content truncation check\n\t\t\t\t\t\t\t\tif total_content_lines + len(content_lines) &gt; MAX_TOTAL_CONTENT_LINES:\n\t\t\t\t\t\t\t\t\tremaining_lines = MAX_TOTAL_CONTENT_LINES - total_content_lines\n\t\t\t\t\t\t\t\t\tif remaining_lines &gt; 0:\n\t\t\t\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\t\t\t\t\"Total untracked content size exceeded limit. Truncating %s to %d lines\",\n\t\t\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t\t\t\tremaining_lines,\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t\tcontent_lines = content_lines[:remaining_lines]\n\t\t\t\t\t\t\t\t\t\tneeds_total_truncation_notice = True\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t# No space left at all, skip this file and subsequent ones\n\t\t\t\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\t\t\t\"Max total untracked lines reached. Skipping remaining untracked files.\"\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\t\t\t# Format content for the diff\n\t\t\t\t\t\t\t\tformatted_content = [\"--- /dev/null\", f\"+++ b/{file_path}\"]\n\t\t\t\t\t\t\t\tformatted_content.extend(f\"+{line}\" for line in content_lines)\n\t\t\t\t\t\t\t\tif needs_total_truncation_notice:\n\t\t\t\t\t\t\t\t\tformatted_content.append(\n\t\t\t\t\t\t\t\t\t\t\"+[... Further untracked files truncated due to total size limits ...]\"\n\t\t\t\t\t\t\t\t\t)\n\n\t\t\t\t\t\t\t\tfile_content_str = \"\\n\".join(formatted_content)\n\t\t\t\t\t\t\t\tchanges.append(\n\t\t\t\t\t\t\t\t\tGitDiff(\n\t\t\t\t\t\t\t\t\t\tfiles=[file_path], content=file_content_str, is_staged=False, is_untracked=True\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\ttotal_content_lines += len(content_lines)\n\t\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\t\t\t\tlogger.debug(\n\t\t\t\t\t\t\t\t\t\"Added content for untracked file %s (%d lines / %d original).\",\n\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t\tlen(content_lines),\n\t\t\t\t\t\t\t\t\toriginal_line_count,\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t# File content is None or empty\n\t\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\t\"Untracked file %s could not be read or is empty. Creating entry without content.\",\n\t\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tchanges.append(\n\t\t\t\t\t\t\t\t\tGitDiff(files=[file_path], content=\"\", is_staged=False, is_untracked=True)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\t\t\t\t\t\texcept (OSError, UnicodeDecodeError) as file_read_error:\n\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\"Could not read untracked file %s: %s. Creating entry without content.\",\n\t\t\t\t\t\t\t\tfile_path,\n\t\t\t\t\t\t\t\tfile_read_error,\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\tchanges.append(GitDiff(files=[file_path], content=\"\", is_staged=False, is_untracked=True))\n\t\t\t\t\t\t\tprocessed_files.add(file_path)\n\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get repository changes: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise RuntimeError(msg) from e\n\n\t\treturn changes\n\n\tdef _perform_commit(self, chunk: DiffChunk, message: str) -&gt; bool:\n\t\t\"\"\"\n\t\tPerform the actual commit operation.\n\n\t\tArgs:\n\t\t    chunk: The chunk to commit\n\t\t    message: Commit message to use\n\n\t\tReturns:\n\t\t    True if successful, False otherwise\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Commit only the files specified in the chunk\n\t\t\tcommit_only_files(chunk.files, message, ignore_hooks=self.bypass_hooks)\n\t\t\tself.ui.show_success(f\"Committed {len(chunk.files)} files.\")\n\t\t\treturn True\n\t\texcept GitError as e:\n\t\t\terror_msg = f\"Error during commit: {e}\"\n\t\t\tself.ui.show_error(error_msg)\n\t\t\tlogger.exception(error_msg)\n\t\t\tself.error_state = \"failed\"\n\t\t\treturn False\n\n\tdef _process_chunk(self, chunk: DiffChunk, index: int, total_chunks: int) -&gt; bool:\n\t\t\"\"\"\n\t\tProcess a single chunk interactively.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk to process\n\t\t    index: The 0-based index of the current chunk\n\t\t    total_chunks: The total number of chunks\n\n\t\tReturns:\n\t\t    True if processing should continue, False to abort or on failure.\n\n\t\tRaises:\n\t\t    typer.Exit: If user chooses to exit.\n\n\t\t\"\"\"\n\t\tlogger.debug(\n\t\t\t\"Processing chunk - Chunk ID: %s, Index: %d/%d, Files: %s\",\n\t\t\tid(chunk),\n\t\t\tindex + 1,\n\t\t\ttotal_chunks,\n\t\t\tchunk.files,\n\t\t)\n\n\t\t# Clear previous generation state if any\n\t\tchunk.description = None\n\t\tchunk.is_llm_generated = False\n\n\t\twhile True:  # Loop to allow regeneration/editing\n\t\t\tmessage = \"\"\n\t\t\tused_llm = False\n\t\t\tpassed_linting = True  # Assume true unless linting happens and fails\n\t\t\tlint_messages: list[str] = []  # Initialize lint messages list\n\n\t\t\t# Generate message (potentially with linting retries)\n\t\t\ttry:\n\t\t\t\t# Generate message using the updated method\n\t\t\t\tmessage, used_llm, passed_linting, lint_messages = self.message_generator.generate_message_with_linting(\n\t\t\t\t\tchunk\n\t\t\t\t)\n\t\t\t\tchunk.description = message\n\t\t\t\tchunk.is_llm_generated = used_llm\n\t\t\texcept (LLMError, RuntimeError) as e:\n\t\t\t\tlogger.exception(\"Failed during message generation for chunk\")\n\t\t\t\tself.ui.show_error(f\"Error generating message: {e}\")\n\t\t\t\t# Offer to skip or exit after generation error\n\t\t\t\tif not questionary.confirm(\"Skip this chunk and continue?\", default=True).ask():\n\t\t\t\t\tself.error_state = \"aborted\"\n\t\t\t\t\treturn False  # Abort\n\t\t\t\t# If user chooses to skip after generation error, we continue to the next chunk\n\t\t\t\treturn True\n\n\t\t\t# -------- Handle Linting Result and User Action ---------\n\t\t\tif not passed_linting:\n\t\t\t\t# Display the diff chunk info first\n\t\t\t\tself.ui.display_chunk(chunk, index, total_chunks)\n\t\t\t\t# Display the failed message and lint errors\n\t\t\t\tself.ui.display_failed_lint_message(message, lint_messages, used_llm)\n\t\t\t\t# Ask user what to do on failure\n\t\t\t\taction = self.ui.get_user_action_on_lint_failure()\n\t\t\telse:\n\t\t\t\t# Display the valid message and diff chunk\n\t\t\t\tself.ui.display_chunk(chunk, index, total_chunks)  # Pass correct index and total\n\t\t\t\t# Ask user what to do with the valid message\n\t\t\t\taction = self.ui.get_user_action()\n\n\t\t\t# -------- Process User Action ---------\n\t\t\tif action == ChunkAction.COMMIT:\n\t\t\t\t# Commit with the current message (which is valid if we got here via the 'else' block)\n\t\t\t\tif self._perform_commit(chunk, message):\n\t\t\t\t\treturn True  # Continue to next chunk\n\t\t\t\tself.error_state = \"failed\"\n\t\t\t\treturn False  # Abort on commit failure\n\t\t\tif action == ChunkAction.EDIT:\n\t\t\t\t# Allow user to edit the message\n\t\t\t\tcurrent_message = chunk.description or \"\"  # Default to empty string if None\n\t\t\t\tedited_message = self.ui.edit_message(current_message)\n\t\t\t\tcleaned_edited_message = clean_message_for_linting(edited_message)\n\t\t\t\tedited_is_valid, _ = lint_commit_message(cleaned_edited_message)\n\t\t\t\t# Convert error_message to list for compatibility with the rest of the code\n\t\t\t\tif edited_is_valid:\n\t\t\t\t\t# Commit with the user-edited, now valid message\n\t\t\t\t\tif self._perform_commit(chunk, cleaned_edited_message):\n\t\t\t\t\t\treturn True  # Continue to next chunk\n\t\t\t\t\tself.error_state = \"failed\"\n\t\t\t\t\treturn False  # Abort on commit failure\n\t\t\t\t# If edited message is still invalid, show errors and loop back\n\t\t\t\tself.ui.show_warning(\"Edited message still failed linting.\")\n\t\t\t\t# Update state for the next loop iteration to show the edited (but invalid) message\n\t\t\t\tchunk.description = edited_message\n\t\t\t\tchunk.is_llm_generated = False  # Mark as not LLM-generated\n\t\t\t\tcontinue  # Go back to the start of the while loop\n\t\t\tif action == ChunkAction.REGENERATE:\n\t\t\t\tself.ui.show_regenerating()\n\t\t\t\tchunk.description = None  # Clear description before regenerating\n\t\t\t\tchunk.is_llm_generated = False\n\t\t\t\tcontinue  # Go back to the start of the while loop to regenerate\n\t\t\tif action == ChunkAction.SKIP:\n\t\t\t\tself.ui.show_skipped(chunk.files)\n\t\t\t\treturn True  # Continue to next chunk\n\t\t\tif action == ChunkAction.EXIT:\n\t\t\t\tif self.ui.confirm_exit():\n\t\t\t\t\tself.error_state = \"aborted\"\n\t\t\t\t\t# Returning False signals to stop processing chunks\n\t\t\t\t\treturn False\n\t\t\t\t# If user cancels exit, loop back to show the chunk again\n\t\t\t\tcontinue\n\n\t\t\t# Should not be reached\n\t\t\tlogger.error(\"Unhandled action in _process_chunk: %s\", action)\n\t\t\treturn False\n\n\tdef process_all_chunks(self, chunks: list[DiffChunk], grand_total: int, interactive: bool = True) -&gt; bool:\n\t\t\"\"\"\n\t\tProcess all generated chunks.\n\n\t\tArgs:\n\t\t    chunks: List of DiffChunk objects to process\n\t\t    grand_total: Total number of chunks initially generated\n\t\t    interactive: Whether to run in interactive mode\n\n\t\tReturns:\n\t\t    True if all chunks were processed successfully, False otherwise\n\n\t\t\"\"\"\n\t\tif not chunks:\n\t\t\tself.ui.show_error(\"No diff chunks found to process.\")\n\t\t\treturn False\n\n\t\tsuccess = True\n\t\tfor i, chunk in enumerate(chunks):\n\t\t\tif interactive:\n\t\t\t\ttry:\n\t\t\t\t\tif not self._process_chunk(chunk, i, grand_total):\n\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\tbreak\n\t\t\t\texcept typer.Exit:\n\t\t\t\t\t# User chose to exit via typer.Exit(), which is expected\n\t\t\t\t\tsuccess = False  # Indicate not all chunks were processed\n\t\t\t\t\tbreak\n\t\t\t\texcept RuntimeError as e:\n\t\t\t\t\tself.ui.show_error(f\"Runtime error processing chunk: {e}\")\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\t# Non-interactive mode: generate and attempt commit\n\t\t\t\ttry:\n\t\t\t\t\tmessage, _, passed_linting, _ = self.message_generator.generate_message_with_linting(chunk)\n\t\t\t\t\tif not passed_linting:\n\t\t\t\t\t\tlogger.warning(\"Generated message failed linting in non-interactive mode: %s\", message)\n\t\t\t\t\t\t# Decide behavior: skip, commit anyway, fail? Let's skip for now.\n\t\t\t\t\t\tself.ui.show_skipped(chunk.files)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif not self._perform_commit(chunk, message):\n\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\tbreak\n\t\t\t\texcept (LLMError, RuntimeError, GitError) as e:\n\t\t\t\t\tself.ui.show_error(f\"Error processing chunk non-interactively: {e}\")\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\n\t\treturn success\n\n\tdef run(self, interactive: bool = True) -&gt; bool:\n\t\t\"\"\"\n\t\tRun the commit command workflow.\n\n\t\tArgs:\n\t\t    interactive: Whether to run in interactive mode. Defaults to True.\n\n\t\tReturns:\n\t\t    True if the process completed (even if aborted), False on unexpected error.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\twith loading_spinner(\"Analyzing changes...\"):\n\t\t\t\tchanges = self._get_changes()\n\n\t\t\tif not changes:\n\t\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\t\treturn True\n\n\t\t\t# Process each diff separately to avoid parsing issues\n\t\t\tchunks = []\n\n\t\t\tfor diff in changes:\n\t\t\t\t# Process each diff individually\n\t\t\t\tdiff_chunks, _ = self.splitter.split_diff(diff)\n\t\t\t\tchunks.extend(diff_chunks)\n\n\t\t\ttotal_chunks = len(chunks)\n\t\t\tlogger.info(\"Split files into %d chunks.\", total_chunks)\n\n\t\t\tif not chunks:\n\t\t\t\t# Import DiffChunk for clarity\n\n\t\t\t\t# If no target files available, try to detect modified files\n\t\t\t\tif not self.target_files:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t# Get staged files\n\t\t\t\t\t\tstaged_output = run_git_command([\"git\", \"diff\", \"--cached\", \"--name-only\"])\n\t\t\t\t\t\tif staged_output.strip():\n\t\t\t\t\t\t\tself.target_files.extend(staged_output.splitlines())\n\n\t\t\t\t\t\t# Get unstaged but tracked files\n\t\t\t\t\t\tunstaged_output = run_git_command([\"git\", \"diff\", \"--name-only\"])\n\t\t\t\t\t\tif unstaged_output.strip():\n\t\t\t\t\t\t\tself.target_files.extend(unstaged_output.splitlines())\n\n\t\t\t\t\t\t# Get untracked files\n\t\t\t\t\t\tuntracked_files = get_untracked_files()\n\t\t\t\t\t\tif untracked_files:\n\t\t\t\t\t\t\tself.target_files.extend(untracked_files)\n\n\t\t\t\t\t\t# Remove duplicates\n\t\t\t\t\t\tself.target_files = list(set(self.target_files))\n\n\t\t\t\t\t\tif self.target_files:\n\t\t\t\t\t\t\tlogger.info(f\"Using detected modified files: {self.target_files}\")\n\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\tlogger.warning(f\"Error while getting modified files: {e}\")\n\n\t\t\t\t# Use helper method to create fallback chunks\n\t\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t\t# If still no chunks, return error\n\t\t\t\tif not chunks:\n\t\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\t\treturn False\n\n\t\t\t# Process chunks, passing the interactive flag\n\t\t\tsuccess = self.process_all_chunks(chunks, total_chunks, interactive=interactive)\n\n\t\t\tif self.error_state == \"aborted\":\n\t\t\t\tself.ui.show_message(\"Commit process aborted by user.\")\n\t\t\t\treturn True  # Abort is considered a valid exit\n\t\t\tif self.error_state == \"failed\":\n\t\t\t\tself.ui.show_error(\"Commit process failed due to errors.\")\n\t\t\t\treturn False\n\t\t\tif not success:\n\t\t\t\t# If process_all_chunks returned False without setting error_state\n\t\t\t\tself.ui.show_error(\"Commit process failed.\")\n\t\t\t\treturn False\n\t\t\tself.ui.show_all_done()\n\t\t\treturn True\n\n\t\texcept RuntimeError as e:\n\t\t\tself.ui.show_error(str(e))\n\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\t\tlogger.exception(\"Unexpected error in commit command run loop\")\n\t\t\treturn False\n\t\tfinally:\n\t\t\t# Restore original branch if it was changed\n\t\t\tif self.original_branch:\n\t\t\t\ttry:\n\t\t\t\t\t# get_current_branch is already imported\n\t\t\t\t\t# switch_branch is imported from codemap.git.utils now\n\t\t\t\t\tcurrent = get_current_branch()\n\t\t\t\t\tif current != self.original_branch:\n\t\t\t\t\t\tlogger.info(\"Restoring original branch: %s\", self.original_branch)\n\t\t\t\t\t\tswitch_branch(self.original_branch)\n\t\t\t\texcept (GitError, Exception) as e:\n\t\t\t\t\tlogger.warning(\"Could not restore original branch %s: %s\", self.original_branch, e)\n\n\tdef _try_create_fallback_chunks(self, files: list[str]) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tTry to create fallback chunks for files when regular splitting fails.\n\n\t\tArgs:\n\t\t\tfiles: List of file paths to process\n\n\t\tReturns:\n\t\t\tList of created DiffChunk objects\n\t\t\"\"\"\n\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\tchunks = []\n\n\t\t# Get all tracked files from git\n\t\ttry:\n\t\t\tall_tracked_files = run_git_command([\"git\", \"ls-files\"]).splitlines()\n\n\t\t\t# If files has incorrect paths (e.g., \"rc/\" instead of \"src/\"), attempt to fix them\n\t\t\tcorrected_files = []\n\t\t\tfor file in files:\n\t\t\t\t# Check if file exists as is\n\t\t\t\tif file in all_tracked_files:\n\t\t\t\t\tcorrected_files.append(file)\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Try to find a similar file in tracked files\n\t\t\t\tif file.startswith(\"rc/\") and file.replace(\"rc/\", \"src/\") in all_tracked_files:\n\t\t\t\t\tcorrected_file = file.replace(\"rc/\", \"src/\")\n\t\t\t\t\tlogger.info(f\"Corrected file path from {file} to {corrected_file}\")\n\t\t\t\t\tcorrected_files.append(corrected_file)\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Add other potential corrections here\n\t\t\t\t# For example, check for case-insensitive matches\n\n\t\t\t\tlogger.warning(f\"Could not find a matching tracked file for {file}\")\n\n\t\t\t# Update files with corrected paths if needed\n\t\t\tif corrected_files:\n\t\t\t\tfiles = corrected_files\n\t\t\t\tlogger.info(f\"Using corrected file paths: {files}\")\n\n\t\t\t# Now try to create chunks with the corrected paths\n\t\t\tfor file in files:\n\t\t\t\ttry:\n\t\t\t\t\t# Try unstaged changes first\n\t\t\t\t\tfile_diff = run_git_command([\"git\", \"diff\", \"HEAD\", \"--\", file])\n\t\t\t\t\tif file_diff.strip():\n\t\t\t\t\t\tlogger.debug(f\"Created individual chunk for {file}\")\n\t\t\t\t\t\tchunks.append(DiffChunk(files=[file], content=file_diff))\n\t\t\t\t\t\tcontinue  # Skip to next file if we found a diff\n\n\t\t\t\t\t# Then try staged changes\n\t\t\t\t\tfile_diff = run_git_command([\"git\", \"diff\", \"--cached\", \"--\", file])\n\t\t\t\t\tif file_diff.strip():\n\t\t\t\t\t\tlogger.debug(f\"Created individual chunk for staged {file}\")\n\t\t\t\t\t\tchunks.append(DiffChunk(files=[file], content=file_diff))\n\t\t\t\texcept GitError:\n\t\t\t\t\tlogger.warning(f\"Could not get diff for {file}\")\n\t\texcept GitError as e:\n\t\t\tlogger.warning(f\"Error while trying to fix file paths: {e}\")\n\n\t\t# If still no chunks but we have files, create empty chunks as last resort\n\t\tif not chunks and files:\n\t\t\tlogger.warning(\"No diffs found, creating minimal placeholder chunks\")\n\t\t\tfor file in files:\n\t\t\t\t# Create a minimal diff just to allow the process to continue\n\t\t\t\tplaceholder_diff = f\"--- a/{file}\\n+++ b/{file}\\n@@ -1 +1 @@\\n No content change detected\"\n\t\t\t\tchunks.append(DiffChunk(files=[file], content=placeholder_diff))\n\t\t\t\tlogger.debug(f\"Created placeholder chunk for {file}\")\n\n\t\treturn chunks\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\tpath: Path | None = None,\n\tmodel: str = \"gpt-4o-mini\",\n\tbypass_hooks: bool = False,\n) -&gt; None\n</code></pre> <p>Initialize the commit command.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Optional path to start from</p> <code>None</code> <code>model</code> <code>str</code> <p>LLM model to use for commit message generation</p> <code>'gpt-4o-mini'</code> <code>bypass_hooks</code> <code>bool</code> <p>Whether to bypass git hooks with --no-verify</p> <code>False</code> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def __init__(self, path: Path | None = None, model: str = \"gpt-4o-mini\", bypass_hooks: bool = False) -&gt; None:\n\t\"\"\"\n\tInitialize the commit command.\n\n\tArgs:\n\t    path: Optional path to start from\n\t    model: LLM model to use for commit message generation\n\t    bypass_hooks: Whether to bypass git hooks with --no-verify\n\n\t\"\"\"\n\ttry:\n\t\tself.repo_root = get_repo_root(path)\n\t\tself.ui: CommitUI = CommitUI()\n\t\tself.splitter = DiffSplitter(self.repo_root)\n\t\tself.target_files = []  # Initialize target_files attribute\n\n\t\t# Store the current branch at initialization to ensure we don't switch branches unexpectedly\n\t\ttry:\n\t\t\tself.original_branch = get_current_branch()\n\t\texcept (ImportError, GitError):\n\t\t\tself.original_branch = None\n\n\t\t# Create LLM client and configs\n\t\tfrom codemap.llm import create_client\n\t\tfrom codemap.utils.config_loader import ConfigLoader\n\n\t\tconfig_loader = ConfigLoader(repo_root=self.repo_root)\n\t\tllm_client = create_client(repo_path=self.repo_root, model=model)\n\n\t\t# Create the commit message generator with required parameters\n\t\tself.message_generator = CommitMessageGenerator(\n\t\t\trepo_root=self.repo_root,\n\t\t\tllm_client=llm_client,\n\t\t\tprompt_template=DEFAULT_PROMPT_TEMPLATE,\n\t\t\tconfig_loader=config_loader,\n\t\t)\n\n\t\tself.error_state = None  # Tracks reason for failure: \"failed\", \"aborted\", etc.\n\t\tself.bypass_hooks = bypass_hooks  # Whether to bypass git hooks with --no-verify\n\texcept GitError as e:\n\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = get_repo_root(path)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.ui","title":"ui  <code>instance-attribute</code>","text":"<pre><code>ui: CommitUI = CommitUI()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.splitter","title":"splitter  <code>instance-attribute</code>","text":"<pre><code>splitter = DiffSplitter(repo_root)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.target_files","title":"target_files  <code>instance-attribute</code>","text":"<pre><code>target_files = []\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.original_branch","title":"original_branch  <code>instance-attribute</code>","text":"<pre><code>original_branch = get_current_branch()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.message_generator","title":"message_generator  <code>instance-attribute</code>","text":"<pre><code>message_generator = CommitMessageGenerator(\n\trepo_root=repo_root,\n\tllm_client=llm_client,\n\tprompt_template=DEFAULT_PROMPT_TEMPLATE,\n\tconfig_loader=config_loader,\n)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.error_state","title":"error_state  <code>instance-attribute</code>","text":"<pre><code>error_state = None\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.bypass_hooks","title":"bypass_hooks  <code>instance-attribute</code>","text":"<pre><code>bypass_hooks = bypass_hooks\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.process_all_chunks","title":"process_all_chunks","text":"<pre><code>process_all_chunks(\n\tchunks: list[DiffChunk],\n\tgrand_total: int,\n\tinteractive: bool = True,\n) -&gt; bool\n</code></pre> <p>Process all generated chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of DiffChunk objects to process</p> required <code>grand_total</code> <code>int</code> <p>Total number of chunks initially generated</p> required <code>interactive</code> <code>bool</code> <p>Whether to run in interactive mode</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if all chunks were processed successfully, False otherwise</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def process_all_chunks(self, chunks: list[DiffChunk], grand_total: int, interactive: bool = True) -&gt; bool:\n\t\"\"\"\n\tProcess all generated chunks.\n\n\tArgs:\n\t    chunks: List of DiffChunk objects to process\n\t    grand_total: Total number of chunks initially generated\n\t    interactive: Whether to run in interactive mode\n\n\tReturns:\n\t    True if all chunks were processed successfully, False otherwise\n\n\t\"\"\"\n\tif not chunks:\n\t\tself.ui.show_error(\"No diff chunks found to process.\")\n\t\treturn False\n\n\tsuccess = True\n\tfor i, chunk in enumerate(chunks):\n\t\tif interactive:\n\t\t\ttry:\n\t\t\t\tif not self._process_chunk(chunk, i, grand_total):\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\t\t\texcept typer.Exit:\n\t\t\t\t# User chose to exit via typer.Exit(), which is expected\n\t\t\t\tsuccess = False  # Indicate not all chunks were processed\n\t\t\t\tbreak\n\t\t\texcept RuntimeError as e:\n\t\t\t\tself.ui.show_error(f\"Runtime error processing chunk: {e}\")\n\t\t\t\tsuccess = False\n\t\t\t\tbreak\n\t\telse:\n\t\t\t# Non-interactive mode: generate and attempt commit\n\t\t\ttry:\n\t\t\t\tmessage, _, passed_linting, _ = self.message_generator.generate_message_with_linting(chunk)\n\t\t\t\tif not passed_linting:\n\t\t\t\t\tlogger.warning(\"Generated message failed linting in non-interactive mode: %s\", message)\n\t\t\t\t\t# Decide behavior: skip, commit anyway, fail? Let's skip for now.\n\t\t\t\t\tself.ui.show_skipped(chunk.files)\n\t\t\t\t\tcontinue\n\t\t\t\tif not self._perform_commit(chunk, message):\n\t\t\t\t\tsuccess = False\n\t\t\t\t\tbreak\n\t\t\texcept (LLMError, RuntimeError, GitError) as e:\n\t\t\t\tself.ui.show_error(f\"Error processing chunk non-interactively: {e}\")\n\t\t\t\tsuccess = False\n\t\t\t\tbreak\n\n\treturn success\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.CommitCommand.run","title":"run","text":"<pre><code>run(interactive: bool = True) -&gt; bool\n</code></pre> <p>Run the commit command workflow.</p> <p>Parameters:</p> Name Type Description Default <code>interactive</code> <code>bool</code> <p>Whether to run in interactive mode. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the process completed (even if aborted), False on unexpected error.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def run(self, interactive: bool = True) -&gt; bool:\n\t\"\"\"\n\tRun the commit command workflow.\n\n\tArgs:\n\t    interactive: Whether to run in interactive mode. Defaults to True.\n\n\tReturns:\n\t    True if the process completed (even if aborted), False on unexpected error.\n\n\t\"\"\"\n\ttry:\n\t\twith loading_spinner(\"Analyzing changes...\"):\n\t\t\tchanges = self._get_changes()\n\n\t\tif not changes:\n\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\treturn True\n\n\t\t# Process each diff separately to avoid parsing issues\n\t\tchunks = []\n\n\t\tfor diff in changes:\n\t\t\t# Process each diff individually\n\t\t\tdiff_chunks, _ = self.splitter.split_diff(diff)\n\t\t\tchunks.extend(diff_chunks)\n\n\t\ttotal_chunks = len(chunks)\n\t\tlogger.info(\"Split files into %d chunks.\", total_chunks)\n\n\t\tif not chunks:\n\t\t\t# Import DiffChunk for clarity\n\n\t\t\t# If no target files available, try to detect modified files\n\t\t\tif not self.target_files:\n\t\t\t\ttry:\n\t\t\t\t\t# Get staged files\n\t\t\t\t\tstaged_output = run_git_command([\"git\", \"diff\", \"--cached\", \"--name-only\"])\n\t\t\t\t\tif staged_output.strip():\n\t\t\t\t\t\tself.target_files.extend(staged_output.splitlines())\n\n\t\t\t\t\t# Get unstaged but tracked files\n\t\t\t\t\tunstaged_output = run_git_command([\"git\", \"diff\", \"--name-only\"])\n\t\t\t\t\tif unstaged_output.strip():\n\t\t\t\t\t\tself.target_files.extend(unstaged_output.splitlines())\n\n\t\t\t\t\t# Get untracked files\n\t\t\t\t\tuntracked_files = get_untracked_files()\n\t\t\t\t\tif untracked_files:\n\t\t\t\t\t\tself.target_files.extend(untracked_files)\n\n\t\t\t\t\t# Remove duplicates\n\t\t\t\t\tself.target_files = list(set(self.target_files))\n\n\t\t\t\t\tif self.target_files:\n\t\t\t\t\t\tlogger.info(f\"Using detected modified files: {self.target_files}\")\n\t\t\t\texcept GitError as e:\n\t\t\t\t\tlogger.warning(f\"Error while getting modified files: {e}\")\n\n\t\t\t# Use helper method to create fallback chunks\n\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t# If still no chunks, return error\n\t\t\tif not chunks:\n\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\treturn False\n\n\t\t# Process chunks, passing the interactive flag\n\t\tsuccess = self.process_all_chunks(chunks, total_chunks, interactive=interactive)\n\n\t\tif self.error_state == \"aborted\":\n\t\t\tself.ui.show_message(\"Commit process aborted by user.\")\n\t\t\treturn True  # Abort is considered a valid exit\n\t\tif self.error_state == \"failed\":\n\t\t\tself.ui.show_error(\"Commit process failed due to errors.\")\n\t\t\treturn False\n\t\tif not success:\n\t\t\t# If process_all_chunks returned False without setting error_state\n\t\t\tself.ui.show_error(\"Commit process failed.\")\n\t\t\treturn False\n\t\tself.ui.show_all_done()\n\t\treturn True\n\n\texcept RuntimeError as e:\n\t\tself.ui.show_error(str(e))\n\t\treturn False\n\texcept Exception as e:\n\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\tlogger.exception(\"Unexpected error in commit command run loop\")\n\t\treturn False\n\tfinally:\n\t\t# Restore original branch if it was changed\n\t\tif self.original_branch:\n\t\t\ttry:\n\t\t\t\t# get_current_branch is already imported\n\t\t\t\t# switch_branch is imported from codemap.git.utils now\n\t\t\t\tcurrent = get_current_branch()\n\t\t\t\tif current != self.original_branch:\n\t\t\t\t\tlogger.info(\"Restoring original branch: %s\", self.original_branch)\n\t\t\t\t\tswitch_branch(self.original_branch)\n\t\t\texcept (GitError, Exception) as e:\n\t\t\t\tlogger.warning(\"Could not restore original branch %s: %s\", self.original_branch, e)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand","title":"SemanticCommitCommand","text":"<p>               Bases: <code>CommitCommand</code></p> <p>Handles the semantic commit command workflow.</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>class SemanticCommitCommand(CommitCommand):\n\t\"\"\"Handles the semantic commit command workflow.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tpath: Path | None = None,\n\t\tmodel: str = \"gpt-4o-mini\",\n\t\tbypass_hooks: bool = False,\n\t\tembedding_model: str = \"all-MiniLM-L6-v2\",\n\t\tclustering_method: str = \"agglomerative\",\n\t\tsimilarity_threshold: float = 0.6,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the semantic commit command.\n\n\t\tArgs:\n\t\t        path: Optional path to start from\n\t\t        model: LLM model to use for commit message generation\n\t\t        bypass_hooks: Whether to bypass git hooks with --no-verify\n\t\t        embedding_model: Model to use for generating embeddings\n\t\t        clustering_method: Method to use for clustering (\"agglomerative\" or \"dbscan\")\n\t\t        similarity_threshold: Threshold for group similarity to trigger merging\n\n\t\t\"\"\"\n\t\tsuper().__init__(path, model, bypass_hooks)\n\n\t\t# Import semantic grouping components\n\t\tfrom codemap.git.semantic_grouping.clusterer import DiffClusterer\n\t\tfrom codemap.git.semantic_grouping.embedder import DiffEmbedder\n\t\tfrom codemap.git.semantic_grouping.resolver import FileIntegrityResolver\n\n\t\t# Initialize semantic grouping components\n\t\tself.embedder = DiffEmbedder(model_name=embedding_model)\n\t\tself.clusterer = DiffClusterer(method=clustering_method)\n\t\tself.resolver = FileIntegrityResolver(similarity_threshold=similarity_threshold)\n\n\t\t# Track state for commits\n\t\tself.committed_files: set[str] = set()\n\t\tself.is_pathspec_mode = False\n\t\tself.all_repo_files: set[str] = set()\n\t\tself.target_files: list[str] = []\n\n\tdef _get_target_files(self, pathspecs: list[str] | None = None) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet the list of target files based on pathspecs.\n\n\t\tArgs:\n\t\t        pathspecs: Optional list of path specifications\n\n\t\tReturns:\n\t\t        List of file paths\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tcmd = [\"git\", \"status\", \"--porcelain=v1\", \"-uall\"]\n\t\t\tif pathspecs:\n\t\t\t\tcmd.extend([\"--\", *pathspecs])\n\t\t\t\tself.is_pathspec_mode = True\n\n\t\t\toutput = run_git_command(cmd)\n\n\t\t\t# Parse porcelain output to get file paths\n\t\t\ttarget_files = []\n\t\t\tfor line in output.splitlines():\n\t\t\t\tif not line or len(line) &lt; MIN_PORCELAIN_LINE_LENGTH:\n\t\t\t\t\tcontinue\n\n\t\t\t\tstatus = line[:2]\n\t\t\t\tfile_path = line[3:].strip()\n\n\t\t\t\t# Handle renamed files\n\t\t\t\tif status.startswith(\"R\"):\n\t\t\t\t\t# Extract the new file name after the arrow\n\t\t\t\t\tfile_path = file_path.split(\" -&gt; \")[1]\n\n\t\t\t\ttarget_files.append(file_path)\n\n\t\t\t# If in pathspec mode, get all repo files for later use\n\t\t\tif self.is_pathspec_mode:\n\t\t\t\tself.all_repo_files = set(run_git_command([\"git\", \"ls-files\"]).splitlines())\n\n\t\t\treturn target_files\n\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get target files: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _prepare_untracked_files(self, target_files: list[str]) -&gt; list[str]:\n\t\t\"\"\"\n\t\tPrepare untracked files for diffing by adding them to the index.\n\n\t\tArgs:\n\t\t        target_files: List of target file paths\n\n\t\tReturns:\n\t\t        List of untracked files that were prepared\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get untracked files\n\t\t\tuntracked_files = get_untracked_files()\n\n\t\t\t# Filter to only those in target_files\n\t\t\tuntracked_targets = [f for f in untracked_files if f in target_files]\n\n\t\t\tif untracked_targets:\n\t\t\t\t# Add untracked files to the index (but not staging area)\n\t\t\t\trun_git_command([\"git\", \"add\", \"-N\", \"--\", *untracked_targets])\n\n\t\t\treturn untracked_targets\n\n\t\texcept GitError as e:\n\t\t\tlogger.warning(\"Error preparing untracked files: %s\", e)\n\t\t\treturn []\n\n\tdef _get_combined_diff(self, target_files: list[str]) -&gt; GitDiff:\n\t\t\"\"\"\n\t\tGet the combined diff for all target files.\n\n\t\tArgs:\n\t\t        target_files: List of target file paths\n\n\t\tReturns:\n\t\t        GitDiff object with the combined diff\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get diff against HEAD for all target files\n\t\t\tdiff_content = run_git_command([\"git\", \"diff\", \"HEAD\", \"--\", *target_files])\n\n\t\t\treturn GitDiff(files=target_files, content=diff_content)\n\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get combined diff: {e}\"\n\t\t\tlogger.exception(msg)\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _create_semantic_groups(self, chunks: list[DiffChunk]) -&gt; list[SemanticGroup]:\n\t\t\"\"\"\n\t\tCreate semantic groups from diff chunks.\n\n\t\tArgs:\n\t\t        chunks: List of DiffChunk objects\n\n\t\tReturns:\n\t\t        List of SemanticGroup objects\n\n\t\t\"\"\"\n\t\t# Shortcut for small changes - bypass embedding process\n\t\tif len(chunks) &lt;= 3:  # Threshold for \"small changes\" # noqa: PLR2004\n\t\t\tlogger.info(\"Small number of chunks detected (%d), bypassing embedding process\", len(chunks))\n\t\t\t# Create a single semantic group with all chunks\n\t\t\tsingle_group = SemanticGroup(chunks=chunks)\n\t\t\t# Extract all file names from chunks\n\t\t\tfiles_set = set()\n\t\t\tfor chunk in chunks:\n\t\t\t\tfiles_set.update(chunk.files)\n\t\t\tsingle_group.files = list(files_set)\n\t\t\t# Combine all content\n\t\t\tcombined_content = \"\\n\".join(chunk.content for chunk in chunks)\n\t\t\tsingle_group.content = combined_content\n\t\t\treturn [single_group]\n\n\t\t# Generate embeddings for chunks\n\t\tchunk_embedding_tuples = self.embedder.embed_chunks(chunks)\n\t\tchunk_embeddings = {ce[0]: ce[1] for ce in chunk_embedding_tuples}\n\n\t\t# Cluster chunks\n\t\tcluster_lists = self.clusterer.cluster(chunk_embedding_tuples)\n\n\t\t# Create initial semantic groups\n\t\tinitial_groups = [SemanticGroup(chunks=cluster) for cluster in cluster_lists]\n\n\t\t# Resolve file integrity constraints\n\t\treturn self.resolver.resolve_violations(initial_groups, chunk_embeddings)\n\n\tdef _generate_group_messages(self, groups: list[SemanticGroup]) -&gt; list[SemanticGroup]:\n\t\t\"\"\"\n\t\tGenerate commit messages for semantic groups.\n\n\t\tArgs:\n\t\t        groups: List of SemanticGroup objects\n\n\t\tReturns:\n\t\t        List of SemanticGroup objects with messages\n\n\t\t\"\"\"\n\t\tfrom codemap.git.semantic_grouping.batch_processor import batch_generate_messages\n\n\t\t# Get config loader and settings\n\t\tconfig_loader = self.message_generator.get_config_loader()\n\t\tllm_config = config_loader.get(\"llm\", {})\n\t\tuse_batch_processing = llm_config.get(\"use_batch_processing\", True)\n\t\tmodel = llm_config.get(\"model\", \"openai/gpt-4o-mini\")\n\n\t\t# Handle batch processing if enabled and we have multiple groups\n\t\tif use_batch_processing and len(groups) &gt; 1:\n\t\t\ttry:\n\t\t\t\tlogger.info(f\"Using batch processing for {len(groups)} semantic groups\")\n\t\t\t\t# Get the prompt template from the message generator\n\t\t\t\tprompt_template = self.message_generator.prompt_template\n\n\t\t\t\t# Run the batch processing\n\t\t\t\treturn batch_generate_messages(\n\t\t\t\t\tgroups=groups, prompt_template=prompt_template, config_loader=config_loader, model=model\n\t\t\t\t)\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Batch processing failed\")\n\t\t\t\t# Show warning message in UI when falling back\n\t\t\t\tself.ui.show_warning(\"Batch processing failed. Falling back to individual message generation.\")\n\t\t\t\tlogger.info(\"Falling back to individual message generation\")\n\t\t\t\t# Fall back to individual message generation\n\n\t\t# Process groups individually\n\t\tfrom codemap.git.diff_splitter import DiffChunk\n\t\tfrom codemap.git.semantic_grouping.context_processor import process_chunks_with_lod\n\n\t\t# Get max token limit and settings from message generator's config\n\t\tmax_tokens = llm_config.get(\"max_context_tokens\", 4000)\n\t\tuse_lod_context = llm_config.get(\"use_lod_context\", True)\n\n\t\tfor group in groups:\n\t\t\ttry:\n\t\t\t\t# Create temporary DiffChunks from the group's chunks\n\t\t\t\tif use_lod_context and len(group.chunks) &gt; 1:\n\t\t\t\t\tlogger.debug(\"Processing semantic group with %d chunks using LOD context\", len(group.chunks))\n\t\t\t\t\ttry:\n\t\t\t\t\t\t# Process all chunks in the group with LOD context processor\n\t\t\t\t\t\toptimized_content = process_chunks_with_lod(group.chunks, max_tokens)\n\n\t\t\t\t\t\tif optimized_content:\n\t\t\t\t\t\t\t# Create a temporary chunk with the optimized content\n\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=optimized_content)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t# Fallback: create a temp chunk with original content\n\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\t\texcept Exception:\n\t\t\t\t\t\tlogger.exception(\"Error in LOD context processing\")\n\t\t\t\t\t\t# Fallback to original content\n\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\telse:\n\t\t\t\t\t# Use the original group content\n\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\n\t\t\t\t# Generate message with linting\n\t\t\t\t# We ignore linting status - SemanticCommitCommand is less strict\n\t\t\t\tmessage, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\n\t\t\t\t# Store the message with the group\n\t\t\t\tgroup.message = message\n\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error generating message for group\")\n\t\t\t\t# Use a fallback message\n\t\t\t\tgroup.message = f\"update: changes to {len(group.files)} files\"\n\n\t\treturn groups\n\n\tdef _stage_and_commit_group(self, group: SemanticGroup) -&gt; bool:\n\t\t\"\"\"\n\t\tStage and commit a semantic group.\n\n\t\tArgs:\n\t\t        group: SemanticGroup to commit\n\n\t\tReturns:\n\t\t        bool: Whether the commit was successful\n\n\t\t\"\"\"\n\t\t# Get files in this group\n\t\tgroup_files = group.files\n\n\t\ttry:\n\t\t\t# First, unstage any previously staged files\n\t\t\t# This ensures we only commit the current group\n\t\t\trun_git_command([\"git\", \"reset\"])\n\n\t\t\t# Add the group files to the index\n\t\t\trun_git_command([\"git\", \"add\", \"--\", *group_files])\n\n\t\t\t# Create the commit with the group message\n\t\t\tcommit_cmd = [\"git\", \"commit\", \"-m\", group.message or \"\"]\n\n\t\t\t# Add --no-verify if bypass_hooks is set\n\t\t\tif self.bypass_hooks:\n\t\t\t\tcommit_cmd.append(\"--no-verify\")\n\n\t\t\ttry:\n\t\t\t\trun_git_command(commit_cmd)\n\n\t\t\t\t# Mark files as committed\n\t\t\t\tself.committed_files.update(group_files)\n\t\t\t\treturn True\n\t\t\texcept GitError as commit_error:\n\t\t\t\t# Check if this is a pre-commit hook failure\n\t\t\t\tif \"pre-commit\" in str(commit_error) and not self.bypass_hooks:\n\t\t\t\t\t# Show the error message for clarity\n\t\t\t\t\terror_msg = str(commit_error)\n\t\t\t\t\tif \"conventional commit\" in error_msg.lower() or \"lint\" in error_msg.lower():\n\t\t\t\t\t\t# Extract the lint errors if possible\n\t\t\t\t\t\tlint_errors = [\n\t\t\t\t\t\t\tline.strip()\n\t\t\t\t\t\t\tfor line in error_msg.splitlines()\n\t\t\t\t\t\t\tif line.strip() and not line.startswith(\"Command\") and \"returned non-zero\" not in line\n\t\t\t\t\t\t]\n\n\t\t\t\t\t\t# Show the message with lint warnings\n\t\t\t\t\t\tmessage = group.message or \"\"  # Use empty string if None\n\t\t\t\t\t\tself.ui.display_failed_lint_message(message, lint_errors, is_llm_generated=True)\n\n\t\t\t\t\t\t# Present options specific to lint failures\n\t\t\t\t\t\tlint_action = self.ui.get_user_action_on_lint_failure()\n\n\t\t\t\t\t\tif lint_action == ChunkAction.REGENERATE:\n\t\t\t\t\t\t\tself.ui.show_regenerating()\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t# Create temporary DiffChunk for regeneration\n\t\t\t\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\n\t\t\t\t\t\t\t\t# Use the linting-aware prompt this time\n\t\t\t\t\t\t\t\tmessage, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\t\t\t\t\t\t\t\tgroup.message = message\n\n\t\t\t\t\t\t\t\t# Try again with the new message\n\t\t\t\t\t\t\t\treturn self._stage_and_commit_group(group)\n\t\t\t\t\t\t\texcept (LLMError, GitError, RuntimeError) as e:\n\t\t\t\t\t\t\t\tself.ui.show_error(f\"Error regenerating message: {e}\")\n\t\t\t\t\t\t\t\treturn False\n\t\t\t\t\t\telif lint_action == ChunkAction.COMMIT:\n\t\t\t\t\t\t\t# User chose to bypass the linter\n\t\t\t\t\t\t\tself.ui.show_message(\"Bypassing linter and committing with --no-verify\")\n\t\t\t\t\t\t\tcommit_cmd.append(\"--no-verify\")\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\trun_git_command(commit_cmd)\n\t\t\t\t\t\t\t\t# Mark files as committed\n\t\t\t\t\t\t\t\tself.committed_files.update(group_files)\n\t\t\t\t\t\t\t\treturn True\n\t\t\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\t\t\tself.ui.show_error(f\"Commit failed even with --no-verify: {e}\")\n\t\t\t\t\t\t\t\treturn False\n\t\t\t\t\t\telif lint_action == ChunkAction.EDIT:\n\t\t\t\t\t\t\tedited_message = self.ui.edit_message(group.message or \"\")  # Empty string as fallback\n\t\t\t\t\t\t\tgroup.message = edited_message\n\t\t\t\t\t\t\treturn self._stage_and_commit_group(group)\n\t\t\t\t\t\telif lint_action == ChunkAction.SKIP:\n\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\t\treturn False\n\t\t\t\t\t\telif lint_action == ChunkAction.EXIT:\n\t\t\t\t\t\t\tif self.ui.confirm_exit():\n\t\t\t\t\t\t\t\traise ExitCommandError from None\n\t\t\t\t\t\t\treturn False\n\n\t\t\t\t\t# Generic pre-commit hook failure (not specifically commit message linting)\n\t\t\t\t\thook_action = self.ui.confirm_bypass_hooks()\n\n\t\t\t\t\tif hook_action == ChunkAction.COMMIT:\n\t\t\t\t\t\t# User chose to bypass the hooks\n\t\t\t\t\t\tself.ui.show_message(\"Bypassing Git hooks and committing with --no-verify\")\n\t\t\t\t\t\tcommit_cmd.append(\"--no-verify\")\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\trun_git_command(commit_cmd)\n\t\t\t\t\t\t\t# Mark files as committed\n\t\t\t\t\t\t\tself.committed_files.update(group_files)\n\t\t\t\t\t\t\treturn True\n\t\t\t\t\t\texcept GitError as e:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Commit failed even with --no-verify: {e}\")\n\t\t\t\t\t\t\treturn False\n\t\t\t\t\telif hook_action == ChunkAction.REGENERATE:\n\t\t\t\t\t\tself.ui.show_regenerating()\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t# Create temporary DiffChunk for regeneration\n\t\t\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\n\t\t\t\t\t\t\t# Use the linting-aware prompt this time\n\t\t\t\t\t\t\tmessage, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\t\t\t\t\t\t\tgroup.message = message\n\n\t\t\t\t\t\t\t# Try again with the new message\n\t\t\t\t\t\t\treturn self._stage_and_commit_group(group)\n\t\t\t\t\t\texcept (LLMError, GitError, RuntimeError) as e:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Error regenerating message: {e}\")\n\t\t\t\t\t\t\treturn False\n\t\t\t\t\telif hook_action == ChunkAction.EDIT:\n\t\t\t\t\t\tedited_message = self.ui.edit_message(group.message or \"\")  # Empty string as fallback\n\t\t\t\t\t\tgroup.message = edited_message\n\t\t\t\t\t\treturn self._stage_and_commit_group(group)\n\t\t\t\t\telif hook_action == ChunkAction.SKIP:\n\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\treturn False\n\t\t\t\t\telif hook_action == ChunkAction.EXIT:\n\t\t\t\t\t\tif self.ui.confirm_exit():\n\t\t\t\t\t\t\traise ExitCommandError from None\n\t\t\t\t\t\treturn False\n\n\t\t\t\t# Either not a pre-commit hook error or user declined to bypass\n\t\t\t\tself.ui.show_error(f\"Failed to commit: {commit_error}\")\n\t\t\t\treturn False\n\n\t\texcept GitError as e:\n\t\t\tself.ui.show_error(f\"Git operation failed: {e}\")\n\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\tself.ui.show_error(f\"Unexpected error during commit: {e}\")\n\t\t\tlogger.exception(\"Unexpected error in _stage_and_commit_group\")\n\t\t\treturn False\n\n\tdef run(self, interactive: bool = True, pathspecs: list[str] | None = None) -&gt; bool:\n\t\t\"\"\"\n\t\tRun the semantic commit command workflow.\n\n\t\tArgs:\n\t\t        interactive: Whether to run in interactive mode\n\t\t        pathspecs: Optional list of path specifications\n\n\t\tReturns:\n\t\t        bool: Whether the process completed successfully\n\n\t\t\"\"\"\n\t\tcommitted_count = 0  # Initialize this at the beginning of the method\n\n\t\ttry:\n\t\t\t# Get target files\n\t\t\twith loading_spinner(\"Analyzing repository...\"):\n\t\t\t\tself.target_files = self._get_target_files(pathspecs)\n\n\t\t\t\tif not self.target_files:\n\t\t\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\t\t\treturn True\n\n\t\t\t\t# Prepare untracked files\n\t\t\t\tself._prepare_untracked_files(self.target_files)\n\n\t\t\t\t# Get combined diff\n\t\t\t\tcombined_diff = self._get_combined_diff(self.target_files)\n\n\t\t\t\t# Log diff details for debugging\n\t\t\t\tlogger.debug(f\"Combined diff size: {len(combined_diff.content)} characters\")\n\t\t\t\tlogger.debug(f\"Target files: {len(self.target_files)} files\")\n\n\t\t\t\t# Import DiffChunk before using it\n\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t# Split diff into chunks\n\t\t\t\tchunks, _ = self.splitter.split_diff(combined_diff)\n\t\t\t\tlogger.debug(f\"Initial chunks created: {len(chunks)}\")\n\n\t\t\t\t# If no chunks created but we have combined diff content, create a single chunk\n\t\t\t\tif not chunks and combined_diff.content.strip():\n\t\t\t\t\tlogger.info(\"No chunks created from splitter, creating a single chunk\")\n\t\t\t\t\tchunks = [DiffChunk(files=self.target_files, content=combined_diff.content)]\n\n\t\t\t\t# Last resort: try creating individual chunks for each file\n\t\t\t\tif not chunks:\n\t\t\t\t\tlogger.info(\"Attempting to create individual file chunks\")\n\t\t\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t\t# If still no chunks, return error\n\t\t\t\tif not chunks:\n\t\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\t\treturn False\n\n\t\t\t\tlogger.info(f\"Final chunk count: {len(chunks)}\")\n\n\t\t\t# Create semantic groups\n\t\t\twith loading_spinner(\"Creating semantic groups...\"):\n\t\t\t\t# Special case for very few files - create a single group\n\t\t\t\tif len(chunks) &lt;= 2:  # noqa: PLR2004\n\t\t\t\t\tlogger.info(\"Small number of chunks detected, creating a single semantic group\")\n\t\t\t\t\t# Create a single semantic group with all chunks\n\t\t\t\t\tsingle_group = SemanticGroup(chunks=chunks)\n\t\t\t\t\t# Extract all file names from chunks\n\t\t\t\t\tfiles_set = set()\n\t\t\t\t\tfor chunk in chunks:\n\t\t\t\t\t\tfiles_set.update(chunk.files)\n\t\t\t\t\tsingle_group.files = list(files_set)\n\t\t\t\t\tgroups = [single_group]\n\t\t\t\telse:\n\t\t\t\t\t# Normal case - use clustering\n\t\t\t\t\tgroups = self._create_semantic_groups(chunks)\n\n\t\t\t\tif not groups:\n\t\t\t\t\tself.ui.show_error(\"Failed to create semantic groups.\")\n\t\t\t\t\treturn False\n\n\t\t\t\t# Generate messages for groups\n\t\t\t\tgroups = self._generate_group_messages(groups)\n\n\t\t\t# Process groups\n\t\t\tself.ui.show_message(f\"Found {len(groups)} semantic groups of changes.\")\n\n\t\t\tsuccess = True\n\n\t\t\tfor i, group in enumerate(groups):\n\t\t\t\tif interactive:\n\t\t\t\t\t# Display group info with improved UI\n\t\t\t\t\tself.ui.display_group(group, i, len(groups))\n\n\t\t\t\t\t# Get user action\n\t\t\t\t\taction = self.ui.get_group_action()\n\n\t\t\t\t\tif action == ChunkAction.COMMIT:\n\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\telif action == ChunkAction.EDIT:\n\t\t\t\t\t\t# Allow user to edit the message\n\t\t\t\t\t\tcurrent_message = group.message or \"\"  # Default to empty string if None\n\t\t\t\t\t\tedited_message = self.ui.edit_message(current_message)\n\t\t\t\t\t\tgroup.message = edited_message\n\n\t\t\t\t\t\t# Commit immediately after editing\n\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\telif action == ChunkAction.REGENERATE:\n\t\t\t\t\t\tself.ui.show_regenerating()\n\t\t\t\t\t\t# Re-generate the message\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\t\t\t\tmessage, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\t\t\t\t\t\t\tgroup.message = message\n\n\t\t\t\t\t\t\t# Show the regenerated message\n\t\t\t\t\t\t\tself.ui.display_group(group, i, len(groups))\n\t\t\t\t\t\t\tif questionary.confirm(\"Commit with regenerated message?\", default=True).ask():\n\t\t\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\texcept (LLMError, GitError, RuntimeError) as e:\n\t\t\t\t\t\t\tself.ui.show_error(f\"Error regenerating message: {e}\")\n\t\t\t\t\t\t\tif questionary.confirm(\"Skip this group?\", default=True).ask():\n\t\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\telif action == ChunkAction.SKIP:\n\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\telif action == ChunkAction.EXIT and self.ui.confirm_exit():\n\t\t\t\t\t\t# This is a user-initiated exit, should not be considered a failure\n\t\t\t\t\t\tself.ui.show_message(\"Commit process exited by user.\")\n\t\t\t\t\t\treturn True  # Return true to indicate normal exit, not failure\n\t\t\t\telse:\n\t\t\t\t\t# In non-interactive mode, commit each group immediately\n\t\t\t\t\tgroup.message = group.message or f\"update: changes to {len(group.files)} files\"\n\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\tsuccess = False\n\n\t\t\tif committed_count &gt; 0:\n\t\t\t\tself.ui.show_message(f\"Successfully committed {committed_count} semantic groups.\")\n\t\t\t\tself.ui.show_all_done()\n\t\t\telse:\n\t\t\t\tself.ui.show_message(\"No changes were committed.\")\n\n\t\t\treturn success\n\t\texcept ExitCommandError:\n\t\t\t# User requested to exit during lint failure handling\n\t\t\treturn committed_count &gt; 0\n\t\texcept RuntimeError as e:\n\t\t\tself.ui.show_error(str(e))\n\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\t\tlogger.exception(\"Unexpected error in semantic commit command\")\n\t\t\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\tpath: Path | None = None,\n\tmodel: str = \"gpt-4o-mini\",\n\tbypass_hooks: bool = False,\n\tembedding_model: str = \"all-MiniLM-L6-v2\",\n\tclustering_method: str = \"agglomerative\",\n\tsimilarity_threshold: float = 0.6,\n) -&gt; None\n</code></pre> <p>Initialize the semantic commit command.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Optional path to start from</p> <code>None</code> <code>model</code> <code>str</code> <p>LLM model to use for commit message generation</p> <code>'gpt-4o-mini'</code> <code>bypass_hooks</code> <code>bool</code> <p>Whether to bypass git hooks with --no-verify</p> <code>False</code> <code>embedding_model</code> <code>str</code> <p>Model to use for generating embeddings</p> <code>'all-MiniLM-L6-v2'</code> <code>clustering_method</code> <code>str</code> <p>Method to use for clustering (\"agglomerative\" or \"dbscan\")</p> <code>'agglomerative'</code> <code>similarity_threshold</code> <code>float</code> <p>Threshold for group similarity to trigger merging</p> <code>0.6</code> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def __init__(\n\tself,\n\tpath: Path | None = None,\n\tmodel: str = \"gpt-4o-mini\",\n\tbypass_hooks: bool = False,\n\tembedding_model: str = \"all-MiniLM-L6-v2\",\n\tclustering_method: str = \"agglomerative\",\n\tsimilarity_threshold: float = 0.6,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the semantic commit command.\n\n\tArgs:\n\t        path: Optional path to start from\n\t        model: LLM model to use for commit message generation\n\t        bypass_hooks: Whether to bypass git hooks with --no-verify\n\t        embedding_model: Model to use for generating embeddings\n\t        clustering_method: Method to use for clustering (\"agglomerative\" or \"dbscan\")\n\t        similarity_threshold: Threshold for group similarity to trigger merging\n\n\t\"\"\"\n\tsuper().__init__(path, model, bypass_hooks)\n\n\t# Import semantic grouping components\n\tfrom codemap.git.semantic_grouping.clusterer import DiffClusterer\n\tfrom codemap.git.semantic_grouping.embedder import DiffEmbedder\n\tfrom codemap.git.semantic_grouping.resolver import FileIntegrityResolver\n\n\t# Initialize semantic grouping components\n\tself.embedder = DiffEmbedder(model_name=embedding_model)\n\tself.clusterer = DiffClusterer(method=clustering_method)\n\tself.resolver = FileIntegrityResolver(similarity_threshold=similarity_threshold)\n\n\t# Track state for commits\n\tself.committed_files: set[str] = set()\n\tself.is_pathspec_mode = False\n\tself.all_repo_files: set[str] = set()\n\tself.target_files: list[str] = []\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.embedder","title":"embedder  <code>instance-attribute</code>","text":"<pre><code>embedder = DiffEmbedder(model_name=embedding_model)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.clusterer","title":"clusterer  <code>instance-attribute</code>","text":"<pre><code>clusterer = DiffClusterer(method=clustering_method)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.resolver","title":"resolver  <code>instance-attribute</code>","text":"<pre><code>resolver = FileIntegrityResolver(\n\tsimilarity_threshold=similarity_threshold\n)\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.committed_files","title":"committed_files  <code>instance-attribute</code>","text":"<pre><code>committed_files: set[str] = set()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.is_pathspec_mode","title":"is_pathspec_mode  <code>instance-attribute</code>","text":"<pre><code>is_pathspec_mode = False\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.all_repo_files","title":"all_repo_files  <code>instance-attribute</code>","text":"<pre><code>all_repo_files: set[str] = set()\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.target_files","title":"target_files  <code>instance-attribute</code>","text":"<pre><code>target_files: list[str] = []\n</code></pre>"},{"location":"api/git/commit_generator/command/#codemap.git.commit_generator.command.SemanticCommitCommand.run","title":"run","text":"<pre><code>run(\n\tinteractive: bool = True,\n\tpathspecs: list[str] | None = None,\n) -&gt; bool\n</code></pre> <p>Run the semantic commit command workflow.</p> <p>Parameters:</p> Name Type Description Default <code>interactive</code> <code>bool</code> <p>Whether to run in interactive mode</p> <code>True</code> <code>pathspecs</code> <code>list[str] | None</code> <p>Optional list of path specifications</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the process completed successfully</p> Source code in <code>src/codemap/git/commit_generator/command.py</code> <pre><code>def run(self, interactive: bool = True, pathspecs: list[str] | None = None) -&gt; bool:\n\t\"\"\"\n\tRun the semantic commit command workflow.\n\n\tArgs:\n\t        interactive: Whether to run in interactive mode\n\t        pathspecs: Optional list of path specifications\n\n\tReturns:\n\t        bool: Whether the process completed successfully\n\n\t\"\"\"\n\tcommitted_count = 0  # Initialize this at the beginning of the method\n\n\ttry:\n\t\t# Get target files\n\t\twith loading_spinner(\"Analyzing repository...\"):\n\t\t\tself.target_files = self._get_target_files(pathspecs)\n\n\t\t\tif not self.target_files:\n\t\t\t\tself.ui.show_message(\"No changes detected to commit.\")\n\t\t\t\treturn True\n\n\t\t\t# Prepare untracked files\n\t\t\tself._prepare_untracked_files(self.target_files)\n\n\t\t\t# Get combined diff\n\t\t\tcombined_diff = self._get_combined_diff(self.target_files)\n\n\t\t\t# Log diff details for debugging\n\t\t\tlogger.debug(f\"Combined diff size: {len(combined_diff.content)} characters\")\n\t\t\tlogger.debug(f\"Target files: {len(self.target_files)} files\")\n\n\t\t\t# Import DiffChunk before using it\n\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t# Split diff into chunks\n\t\t\tchunks, _ = self.splitter.split_diff(combined_diff)\n\t\t\tlogger.debug(f\"Initial chunks created: {len(chunks)}\")\n\n\t\t\t# If no chunks created but we have combined diff content, create a single chunk\n\t\t\tif not chunks and combined_diff.content.strip():\n\t\t\t\tlogger.info(\"No chunks created from splitter, creating a single chunk\")\n\t\t\t\tchunks = [DiffChunk(files=self.target_files, content=combined_diff.content)]\n\n\t\t\t# Last resort: try creating individual chunks for each file\n\t\t\tif not chunks:\n\t\t\t\tlogger.info(\"Attempting to create individual file chunks\")\n\t\t\t\tchunks = self._try_create_fallback_chunks(self.target_files)\n\n\t\t\t# If still no chunks, return error\n\t\t\tif not chunks:\n\t\t\t\tself.ui.show_error(\"Failed to split changes into manageable chunks.\")\n\t\t\t\treturn False\n\n\t\t\tlogger.info(f\"Final chunk count: {len(chunks)}\")\n\n\t\t# Create semantic groups\n\t\twith loading_spinner(\"Creating semantic groups...\"):\n\t\t\t# Special case for very few files - create a single group\n\t\t\tif len(chunks) &lt;= 2:  # noqa: PLR2004\n\t\t\t\tlogger.info(\"Small number of chunks detected, creating a single semantic group\")\n\t\t\t\t# Create a single semantic group with all chunks\n\t\t\t\tsingle_group = SemanticGroup(chunks=chunks)\n\t\t\t\t# Extract all file names from chunks\n\t\t\t\tfiles_set = set()\n\t\t\t\tfor chunk in chunks:\n\t\t\t\t\tfiles_set.update(chunk.files)\n\t\t\t\tsingle_group.files = list(files_set)\n\t\t\t\tgroups = [single_group]\n\t\t\telse:\n\t\t\t\t# Normal case - use clustering\n\t\t\t\tgroups = self._create_semantic_groups(chunks)\n\n\t\t\tif not groups:\n\t\t\t\tself.ui.show_error(\"Failed to create semantic groups.\")\n\t\t\t\treturn False\n\n\t\t\t# Generate messages for groups\n\t\t\tgroups = self._generate_group_messages(groups)\n\n\t\t# Process groups\n\t\tself.ui.show_message(f\"Found {len(groups)} semantic groups of changes.\")\n\n\t\tsuccess = True\n\n\t\tfor i, group in enumerate(groups):\n\t\t\tif interactive:\n\t\t\t\t# Display group info with improved UI\n\t\t\t\tself.ui.display_group(group, i, len(groups))\n\n\t\t\t\t# Get user action\n\t\t\t\taction = self.ui.get_group_action()\n\n\t\t\t\tif action == ChunkAction.COMMIT:\n\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\tsuccess = False\n\t\t\t\telif action == ChunkAction.EDIT:\n\t\t\t\t\t# Allow user to edit the message\n\t\t\t\t\tcurrent_message = group.message or \"\"  # Default to empty string if None\n\t\t\t\t\tedited_message = self.ui.edit_message(current_message)\n\t\t\t\t\tgroup.message = edited_message\n\n\t\t\t\t\t# Commit immediately after editing\n\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\tsuccess = False\n\t\t\t\telif action == ChunkAction.REGENERATE:\n\t\t\t\t\tself.ui.show_regenerating()\n\t\t\t\t\t# Re-generate the message\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfrom codemap.git.diff_splitter import DiffChunk\n\n\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\t\t\tmessage, _, _, _ = self.message_generator.generate_message_with_linting(temp_chunk)\n\t\t\t\t\t\tgroup.message = message\n\n\t\t\t\t\t\t# Show the regenerated message\n\t\t\t\t\t\tself.ui.display_group(group, i, len(groups))\n\t\t\t\t\t\tif questionary.confirm(\"Commit with regenerated message?\", default=True).ask():\n\t\t\t\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\t\t\t\tcommitted_count += 1\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\t\t\t\tsuccess = False\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\texcept (LLMError, GitError, RuntimeError) as e:\n\t\t\t\t\t\tself.ui.show_error(f\"Error regenerating message: {e}\")\n\t\t\t\t\t\tif questionary.confirm(\"Skip this group?\", default=True).ask():\n\t\t\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsuccess = False\n\t\t\t\telif action == ChunkAction.SKIP:\n\t\t\t\t\tself.ui.show_skipped(group.files)\n\t\t\t\telif action == ChunkAction.EXIT and self.ui.confirm_exit():\n\t\t\t\t\t# This is a user-initiated exit, should not be considered a failure\n\t\t\t\t\tself.ui.show_message(\"Commit process exited by user.\")\n\t\t\t\t\treturn True  # Return true to indicate normal exit, not failure\n\t\t\telse:\n\t\t\t\t# In non-interactive mode, commit each group immediately\n\t\t\t\tgroup.message = group.message or f\"update: changes to {len(group.files)} files\"\n\t\t\t\tself.ui.show_message(f\"\\nCommitting: {group.message}\")\n\t\t\t\tif self._stage_and_commit_group(group):\n\t\t\t\t\tcommitted_count += 1\n\t\t\t\telse:\n\t\t\t\t\tself.ui.show_error(f\"Failed to commit group: {group.message}\")\n\t\t\t\t\tsuccess = False\n\n\t\tif committed_count &gt; 0:\n\t\t\tself.ui.show_message(f\"Successfully committed {committed_count} semantic groups.\")\n\t\t\tself.ui.show_all_done()\n\t\telse:\n\t\t\tself.ui.show_message(\"No changes were committed.\")\n\n\t\treturn success\n\texcept ExitCommandError:\n\t\t# User requested to exit during lint failure handling\n\t\treturn committed_count &gt; 0\n\texcept RuntimeError as e:\n\t\tself.ui.show_error(str(e))\n\t\treturn False\n\texcept Exception as e:\n\t\tself.ui.show_error(f\"An unexpected error occurred: {e}\")\n\t\tlogger.exception(\"Unexpected error in semantic commit command\")\n\t\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/generator/","title":"Generator","text":"<p>Generator module for commit messages.</p>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.MAX_DEBUG_CONTENT_LENGTH","title":"MAX_DEBUG_CONTENT_LENGTH  <code>module-attribute</code>","text":"<pre><code>MAX_DEBUG_CONTENT_LENGTH = 100\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.EXPECTED_PARTS_COUNT","title":"EXPECTED_PARTS_COUNT  <code>module-attribute</code>","text":"<pre><code>EXPECTED_PARTS_COUNT = 2\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator","title":"CommitMessageGenerator","text":"<p>Generates commit messages using LLMs.</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>class CommitMessageGenerator:\n\t\"\"\"Generates commit messages using LLMs.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_root: Path,\n\t\tllm_client: LLMClient,\n\t\tprompt_template: str,\n\t\tconfig_loader: ConfigLoader,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the commit message generator.\n\n\t\tArgs:\n\t\t    repo_root: Root directory of the Git repository\n\t\t    llm_client: LLMClient instance to use\n\t\t    prompt_template: Custom prompt template to use\n\t\t    config_loader: ConfigLoader instance to use for configuration\n\n\t\t\"\"\"\n\t\tself.repo_root = repo_root\n\t\tself.prompt_template = prompt_template\n\t\tself._config_loader = config_loader\n\t\tself.client = llm_client\n\n\t\t# Add commit template to client\n\t\tself.client.set_template(\"commit\", self.prompt_template)\n\n\t\t# Get max token limit from config\n\t\tllm_config = self._config_loader.get(\"llm\", {})\n\t\tself.max_tokens = llm_config.get(\"max_context_tokens\", 4000)\n\n\t\t# Flag to control whether to use the LOD-based context processing\n\t\tself.use_lod_context = llm_config.get(\"use_lod_context\", True)\n\n\tdef extract_file_info(self, chunk: DiffChunk) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tExtract file information from the diff chunk.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk object to extract information from\n\n\t\tReturns:\n\t\t    Dictionary with information about files\n\n\t\t\"\"\"\n\t\tfile_info = {}\n\t\tfiles = chunk.files\n\t\tfor file in files:\n\t\t\tif not isinstance(file, str):\n\t\t\t\tcontinue  # Skip non-string file entries\n\t\t\tfile_path = self.repo_root / file\n\t\t\tif not file_path.exists():\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\textension = file_path.suffix.lstrip(\".\")\n\t\t\t\tfile_info[file] = {\n\t\t\t\t\t\"extension\": extension,\n\t\t\t\t\t\"directory\": str(file_path.parent.relative_to(self.repo_root)),\n\t\t\t\t}\n\t\t\t\tpath_parts = file_path.parts\n\t\t\t\tif len(path_parts) &gt; 1:\n\t\t\t\t\tif \"src\" in path_parts:\n\t\t\t\t\t\tidx = path_parts.index(\"src\")\n\t\t\t\t\t\tif idx + 1 &lt; len(path_parts):\n\t\t\t\t\t\t\tfile_info[file][\"module\"] = path_parts[idx + 1]\n\t\t\t\t\telif \"tests\" in path_parts:\n\t\t\t\t\t\tfile_info[file][\"module\"] = \"tests\"\n\t\t\texcept (ValueError, IndexError, TypeError):\n\t\t\t\tcontinue\n\t\treturn file_info\n\n\tdef get_commit_convention(self) -&gt; dict[str, Any]:\n\t\t\"\"\"Get commit convention settings from config.\"\"\"\n\t\t# Use the centralized ConfigLoader to get the convention\n\t\treturn self._config_loader.get_commit_convention()\n\n\tdef _prepare_prompt(self, chunk: DiffChunk) -&gt; str:\n\t\t\"\"\"\n\t\tPrepare the prompt for the LLM.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk object to prepare prompt for\n\n\t\tReturns:\n\t\t    Prepared prompt with diff and file information\n\n\t\t\"\"\"\n\t\tfile_info = self.extract_file_info(chunk)\n\t\tconvention = self.get_commit_convention()\n\n\t\t# Get the diff content\n\t\tdiff_content = chunk.content\n\n\t\t# Use the LOD-based context processor if enabled\n\t\tif self.use_lod_context:\n\t\t\tlogger.debug(\"Using LOD-based context processing\")\n\t\t\ttry:\n\t\t\t\t# Process the chunk with LOD to optimize context length\n\t\t\t\tenhanced_diff_content = process_chunks_with_lod([chunk], self.max_tokens)\n\n\t\t\t\tif enhanced_diff_content:\n\t\t\t\t\tdiff_content = enhanced_diff_content\n\t\t\t\t\tlogger.debug(\"LOD context processing successful\")\n\t\t\t\telse:\n\t\t\t\t\tlogger.debug(\"LOD processing returned empty result, using original content\")\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error during LOD context processing\")\n\t\t\t\t# Continue with the original content if LOD processing fails\n\t\telse:\n\t\t\t# Use the original binary file detection logic\n\t\t\tbinary_files = []\n\t\t\tfor file_path in chunk.files:\n\t\t\t\tif file_path in file_info:\n\t\t\t\t\textension = file_info[file_path].get(\"extension\", \"\").lower()\n\t\t\t\t\t# Common binary file extensions\n\t\t\t\t\tbinary_extensions = {\n\t\t\t\t\t\t\"png\",\n\t\t\t\t\t\t\"jpg\",\n\t\t\t\t\t\t\"jpeg\",\n\t\t\t\t\t\t\"gif\",\n\t\t\t\t\t\t\"bmp\",\n\t\t\t\t\t\t\"tiff\",\n\t\t\t\t\t\t\"ico\",\n\t\t\t\t\t\t\"webp\",  # Images\n\t\t\t\t\t\t\"mp3\",\n\t\t\t\t\t\t\"wav\",\n\t\t\t\t\t\t\"ogg\",\n\t\t\t\t\t\t\"flac\",\n\t\t\t\t\t\t\"aac\",  # Audio\n\t\t\t\t\t\t\"mp4\",\n\t\t\t\t\t\t\"avi\",\n\t\t\t\t\t\t\"mkv\",\n\t\t\t\t\t\t\"mov\",\n\t\t\t\t\t\t\"webm\",  # Video\n\t\t\t\t\t\t\"pdf\",\n\t\t\t\t\t\t\"doc\",\n\t\t\t\t\t\t\"docx\",\n\t\t\t\t\t\t\"xls\",\n\t\t\t\t\t\t\"xlsx\",\n\t\t\t\t\t\t\"ppt\",\n\t\t\t\t\t\t\"pptx\",  # Documents\n\t\t\t\t\t\t\"zip\",\n\t\t\t\t\t\t\"tar\",\n\t\t\t\t\t\t\"gz\",\n\t\t\t\t\t\t\"rar\",\n\t\t\t\t\t\t\"7z\",  # Archives\n\t\t\t\t\t\t\"exe\",\n\t\t\t\t\t\t\"dll\",\n\t\t\t\t\t\t\"so\",\n\t\t\t\t\t\t\"dylib\",  # Binaries\n\t\t\t\t\t\t\"ttf\",\n\t\t\t\t\t\t\"otf\",\n\t\t\t\t\t\t\"woff\",\n\t\t\t\t\t\t\"woff2\",  # Fonts\n\t\t\t\t\t\t\"db\",\n\t\t\t\t\t\t\"sqlite\",\n\t\t\t\t\t\t\"mdb\",  # Databases\n\t\t\t\t\t}\n\n\t\t\t\t\tif extension in binary_extensions:\n\t\t\t\t\t\tbinary_files.append(file_path)\n\n\t\t\t\t# For absolute paths, try to check if the file is binary\n\t\t\t\tabs_path = self.repo_root / file_path\n\t\t\t\ttry:\n\t\t\t\t\tif abs_path.exists():\n\t\t\t\t\t\tfrom codemap.utils.file_utils import is_binary_file\n\n\t\t\t\t\t\tif is_binary_file(abs_path) and file_path not in binary_files:\n\t\t\t\t\t\t\tbinary_files.append(file_path)\n\t\t\t\texcept (OSError, PermissionError) as e:\n\t\t\t\t\t# If any error occurs during binary check, log it and continue\n\t\t\t\t\tlogger.debug(\"Error checking if %s is binary: %s\", file_path, str(e))\n\n\t\t\t# If we have binary files or no diff content, enhance the prompt\n\t\t\tenhanced_diff_content = diff_content\n\t\t\tif not diff_content or binary_files:\n\t\t\t\t# Create a specialized header for binary files\n\t\t\t\tbinary_files_header = \"\"\n\t\t\t\tif binary_files:\n\t\t\t\t\tbinary_files_header = \"BINARY FILES DETECTED:\\n\"\n\t\t\t\t\tfor binary_file in binary_files:\n\t\t\t\t\t\textension = file_info.get(binary_file, {}).get(\"extension\", \"unknown\")\n\t\t\t\t\t\tbinary_files_header += f\"- {binary_file} (binary {extension} file)\\n\"\n\t\t\t\t\tbinary_files_header += \"\\n\"\n\n\t\t\t\t# If no diff content, create a more informative message about binary files\n\t\t\t\tif not diff_content:\n\t\t\t\t\tfile_descriptions = []\n\t\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\t\tif file_path in binary_files:\n\t\t\t\t\t\t\textension = file_info.get(file_path, {}).get(\"extension\", \"unknown\")\n\t\t\t\t\t\t\tfile_descriptions.append(f\"{file_path} (binary {extension} file)\")\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\textension = file_info.get(file_path, {}).get(\"extension\", \"\")\n\t\t\t\t\t\t\tfile_descriptions.append(f\"{file_path} ({extension} file)\")\n\n\t\t\t\t\tenhanced_diff_content = (\n\t\t\t\t\t\tf\"{binary_files_header}This chunk contains changes to the following files \"\n\t\t\t\t\t\tf\"with no visible diff content (likely binary changes):\\n\"\n\t\t\t\t\t)\n\t\t\t\t\tfor desc in file_descriptions:\n\t\t\t\t\t\tenhanced_diff_content += f\"- {desc}\\n\"\n\t\t\t\telse:\n\t\t\t\t\t# If there is diff content but also binary files, add the binary files header\n\t\t\t\t\tenhanced_diff_content = binary_files_header + diff_content\n\n\t\t\tdiff_content = enhanced_diff_content\n\n\t\t# Create a context dict with default values for template variables\n\t\tcontext = {\n\t\t\t\"diff\": diff_content,\n\t\t\t\"files\": file_info,\n\t\t\t\"convention\": convention,\n\t\t\t\"schema\": COMMIT_MESSAGE_SCHEMA,\n\t\t\t\"original_message\": \"\",  # Default value for original_message\n\t\t\t\"lint_errors\": \"\",  # Default value for lint_errors\n\t\t}\n\n\t\t# Prepare and return the prompt\n\t\treturn prepare_prompt(\n\t\t\ttemplate=self.prompt_template,\n\t\t\tdiff_content=diff_content,\n\t\t\tfile_info=file_info,\n\t\t\tconvention=convention,\n\t\t\textra_context=context,  # Pass the context with default values\n\t\t)\n\n\tdef format_json_to_commit_message(self, content: str) -&gt; str:\n\t\t\"\"\"\n\t\tFormat a JSON string as a conventional commit message.\n\n\t\tArgs:\n\t\t    content: JSON content string from LLM response\n\n\t\tReturns:\n\t\t    Formatted commit message string\n\n\t\t\"\"\"\n\n\t\tdef _raise_validation_error(message: str) -&gt; None:\n\t\t\t\"\"\"Helper to raise ValueError with consistent message.\"\"\"\n\t\t\tlogger.warning(\"LLM response validation failed: %s\", message)\n\t\t\tmsg = message\n\t\t\traise ValueError(msg)\n\n\t\ttry:\n\t\t\t# Try to parse the content as JSON\n\t\t\tdebug_content = (\n\t\t\t\tcontent[:MAX_DEBUG_CONTENT_LENGTH] + \"...\" if len(content) &gt; MAX_DEBUG_CONTENT_LENGTH else content\n\t\t\t)\n\t\t\tlogger.debug(\"Parsing JSON content: %s\", debug_content)\n\n\t\t\t# Handle both direct JSON objects and strings containing JSON\n\t\t\tif not content.strip().startswith(\"{\"):\n\t\t\t\t# Extract JSON if it's wrapped in other text\n\t\t\t\timport re\n\n\t\t\t\tjson_match = re.search(r\"({.*})\", content, re.DOTALL)\n\t\t\t\tif json_match:\n\t\t\t\t\tcontent = json_match.group(1)\n\n\t\t\tmessage_data = json.loads(content)\n\t\t\tlogger.debug(\"Parsed JSON: %s\", message_data)\n\n\t\t\t# Basic Schema Validation\n\t\t\tif not isinstance(message_data, dict):\n\t\t\t\t_raise_validation_error(\"JSON response is not an object\")\n\n\t\t\tif not message_data.get(\"type\") or not message_data.get(\"description\"):\n\t\t\t\t_raise_validation_error(\"Missing required fields in JSON response\")\n\n\t\t\t# Extract components with validation/defaults\n\t\t\tcommit_type = str(message_data[\"type\"]).lower().strip()\n\n\t\t\t# Check for valid commit type (from the config)\n\t\t\tvalid_types = self._config_loader.get_commit_convention().get(\"types\", [])\n\t\t\tif valid_types and commit_type not in valid_types:\n\t\t\t\tlogger.warning(\"Invalid commit type: %s. Valid types: %s\", commit_type, valid_types)\n\t\t\t\t# Try to find a valid type as fallback\n\t\t\t\tif \"feat\" in valid_types:\n\t\t\t\t\tcommit_type = \"feat\"\n\t\t\t\telif \"fix\" in valid_types:\n\t\t\t\t\tcommit_type = \"fix\"\n\t\t\t\telif len(valid_types) &gt; 0:\n\t\t\t\t\tcommit_type = valid_types[0]\n\t\t\t\tlogger.debug(\"Using fallback commit type: %s\", commit_type)\n\n\t\t\tscope = message_data.get(\"scope\")\n\t\t\tif scope is not None:\n\t\t\t\tscope = str(scope).lower().strip()\n\n\t\t\tdescription = str(message_data[\"description\"]).lower().strip()\n\n\t\t\t# Ensure description doesn't start with another type prefix\n\t\t\tfor valid_type in valid_types:\n\t\t\t\tif description.startswith(f\"{valid_type}:\"):\n\t\t\t\t\t# Remove the duplicate type prefix from description\n\t\t\t\t\tdescription = description.split(\":\", 1)[1].strip()\n\t\t\t\t\tlogger.debug(\"Removed duplicate type prefix from description: %s\", description)\n\t\t\t\t\tbreak\n\n\t\t\tbody = message_data.get(\"body\")\n\t\t\tif body is not None:\n\t\t\t\tbody = str(body).strip()\n\t\t\tis_breaking = bool(message_data.get(\"breaking\", False))\n\n\t\t\t# Format the header\n\t\t\theader = f\"{commit_type}\"\n\t\t\tif scope:\n\t\t\t\theader += f\"({scope})\"\n\t\t\tif is_breaking:\n\t\t\t\theader += \"!\"\n\t\t\theader += f\": {description}\"\n\n\t\t\t# Ensure compliance with commit format regex\n\t\t\t# The regex requires a space after the colon, and the format should be &lt;type&gt;(&lt;scope&gt;)!: &lt;description&gt;\n\t\t\tif \": \" not in header:\n\t\t\t\tparts = header.split(\":\")\n\t\t\t\tif len(parts) == EXPECTED_PARTS_COUNT:\n\t\t\t\t\theader = f\"{parts[0]}: {parts[1].strip()}\"\n\n\t\t\t# Validation check against regex pattern\n\t\t\timport re\n\n\t\t\tfrom codemap.git.commit_linter.constants import COMMIT_REGEX\n\n\t\t\t# If header doesn't match the expected format, log and try to fix it\n\t\t\tif not COMMIT_REGEX.match(header):\n\t\t\t\tlogger.warning(\"Generated header doesn't match commit format: %s\", header)\n\t\t\t\t# As a fallback, recreate with a simpler format\n\t\t\t\tsimple_header = f\"{commit_type}\"\n\t\t\t\tif scope:\n\t\t\t\t\tsimple_header += f\"({scope})\"\n\t\t\t\tif is_breaking:\n\t\t\t\t\tsimple_header += \"!\"\n\t\t\t\tsimple_header += f\": {description}\"\n\t\t\t\theader = simple_header\n\t\t\t\tlogger.debug(\"Fixed header to: %s\", header)\n\n\t\t\t# Build the complete message\n\t\t\tmessage_parts = [header]\n\n\t\t\t# Add body if provided\n\t\t\tif body:\n\t\t\t\tmessage_parts.append(\"\")  # Empty line between header and body\n\t\t\t\tmessage_parts.append(body)\n\n\t\t\t# Carefully filter only breaking change footers\n\t\t\tfooters = message_data.get(\"footers\", [])\n\t\t\tbreaking_change_footers = []\n\n\t\t\tif isinstance(footers, list):\n\t\t\t\tbreaking_change_footers = [\n\t\t\t\t\tfooter\n\t\t\t\t\tfor footer in footers\n\t\t\t\t\tif isinstance(footer, dict)\n\t\t\t\t\tand footer.get(\"token\", \"\").upper() in (\"BREAKING CHANGE\", \"BREAKING-CHANGE\")\n\t\t\t\t]\n\n\t\t\tif breaking_change_footers:\n\t\t\t\tif not body:\n\t\t\t\t\tmessage_parts.append(\"\")  # Empty line before footers if no body\n\t\t\t\telse:\n\t\t\t\t\tmessage_parts.append(\"\")  # Empty line between body and footers\n\n\t\t\t\tfor footer in breaking_change_footers:\n\t\t\t\t\ttoken = footer.get(\"token\", \"\")\n\t\t\t\t\tvalue = footer.get(\"value\", \"\")\n\t\t\t\t\tmessage_parts.append(f\"{token}: {value}\")\n\n\t\t\tmessage = \"\\n\".join(message_parts)\n\t\t\tlogger.debug(\"Formatted commit message: %s\", message)\n\t\t\treturn message\n\n\t\texcept (json.JSONDecodeError, ValueError, TypeError, AttributeError) as e:\n\t\t\t# If parsing or validation fails, return the content as-is, but cleaned\n\t\t\tlogger.warning(\"Error formatting JSON to commit message: %s. Using raw content.\", str(e))\n\t\t\treturn content.strip()\n\n\tdef fallback_generation(self, chunk: DiffChunk) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate a fallback commit message without LLM.\n\n\t\tThis is used when LLM-based generation fails or is disabled.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk object to generate message for\n\n\t\tReturns:\n\t\t    Generated commit message\n\n\t\t\"\"\"\n\t\tcommit_type = \"chore\"\n\n\t\t# Get files directly from the chunk object\n\t\tfiles = chunk.files\n\n\t\t# Filter only strings (defensive, though DiffChunk.files should be list[str])\n\t\tstring_files = [f for f in files if isinstance(f, str)]\n\n\t\tfor file in string_files:\n\t\t\tif file.startswith(\"tests/\"):\n\t\t\t\tcommit_type = \"test\"\n\t\t\t\tbreak\n\t\t\tif file.startswith(\"docs/\") or file.endswith(\".md\"):\n\t\t\t\tcommit_type = \"docs\"\n\t\t\t\tbreak\n\n\t\t# Get content directly from the chunk object\n\t\tcontent = chunk.content\n\n\t\tif isinstance(content, str) and (\"fix\" in content.lower() or \"bug\" in content.lower()):\n\t\t\tcommit_type = \"fix\"  # Be slightly smarter about 'fix' type\n\n\t\t# Use chunk description if available and seems specific (not just placeholder)\n\t\tchunk_desc = chunk.description\n\t\tplaceholder_descs = [\"update files\", \"changes in\", \"hunk in\", \"new file:\"]\n\t\t# Ensure chunk_desc is not None before calling lower()\n\t\tuse_chunk_desc = chunk_desc and not any(p in chunk_desc.lower() for p in placeholder_descs)\n\n\t\tif use_chunk_desc and chunk_desc:  # Add explicit check for chunk_desc\n\t\t\tdescription = chunk_desc\n\t\t\t# Attempt to extract a type from the chunk description if possible\n\t\t\t# Ensure chunk_desc is not None before calling lower() and split()\n\t\t\tif chunk_desc.lower().startswith(\n\t\t\t\t(\"feat\", \"fix\", \"refactor\", \"docs\", \"test\", \"chore\", \"style\", \"perf\", \"ci\", \"build\")\n\t\t\t):\n\t\t\t\tparts = chunk_desc.split(\":\", 1)\n\t\t\t\tif len(parts) &gt; 1:\n\t\t\t\t\tcommit_type = parts[0].split(\"(\")[0].strip().lower()  # Extract type before scope\n\t\t\t\t\tdescription = parts[1].strip()\n\t\telse:\n\t\t\t# Generate description based on file count/path if no specific chunk desc\n\t\t\tdescription = \"update files\"  # Default\n\t\t\tif string_files:\n\t\t\t\tif len(string_files) == 1:\n\t\t\t\t\tdescription = f\"update {string_files[0]}\"\n\t\t\t\telse:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcommon_dir = os.path.commonpath(string_files)\n\t\t\t\t\t\t# Make common_dir relative to repo root if possible\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tcommon_dir_rel = os.path.relpath(common_dir, self.repo_root)\n\t\t\t\t\t\t\tif common_dir_rel and common_dir_rel != \".\":\n\t\t\t\t\t\t\t\tdescription = f\"update files in {common_dir_rel}\"\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\t\t\t\t\t\texcept ValueError:  # Happens if paths are on different drives (unlikely in repo)\n\t\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\t\t\t\t\texcept (ValueError, TypeError):  # commonpath fails on empty list or mixed types\n\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\t\tmessage = f\"{commit_type}: {description}\"\n\t\tlogger.debug(\"Generated fallback message: %s\", message)\n\t\treturn message\n\n\tdef generate_message(self, chunk: DiffChunk) -&gt; tuple[str, bool]:\n\t\t\"\"\"\n\t\tGenerate a commit message for a diff chunk.\n\n\t\tArgs:\n\t\t    chunk: Diff chunk to generate message for\n\n\t\tReturns:\n\t\t    Generated message and success flag\n\n\t\t\"\"\"\n\t\t# Prepare prompt with chunk data\n\t\ttry:\n\t\t\tprompt = self._prepare_prompt(chunk)\n\t\t\tlogger.debug(\"Prompt prepared successfully\")\n\n\t\t\t# Generate message using configured LLM provider\n\t\t\tmessage = self._call_llm_api(prompt)\n\t\t\tlogger.debug(\"LLM generated message: %s\", message)\n\n\t\t\t# Return generated message with success flag\n\t\t\treturn message, True\n\t\texcept (ValueError, TypeError, KeyError, LLMError):\n\t\t\tlogger.exception(\"Error during LLM generation\")\n\t\t\t# Fall back to heuristic generation\n\t\t\treturn self.fallback_generation(chunk), False\n\n\tdef _call_llm_api(self, prompt: str) -&gt; str:\n\t\t\"\"\"\n\t\tCall the LLM API with the given prompt.\n\n\t\tArgs:\n\t\t    prompt: Prompt to send to the LLM\n\n\t\tReturns:\n\t\t    Raw response content from the LLM\n\n\t\tRaises:\n\t\t    LLMError: If the API call fails\n\n\t\t\"\"\"\n\t\t# Directly use the generate_text method from the LLMClient\n\t\treturn self.client.generate_text(prompt=prompt, json_schema=COMMIT_MESSAGE_SCHEMA)\n\n\tdef generate_message_with_linting(\n\t\tself, chunk: DiffChunk, retry_count: int = 1, max_retries: int = 3\n\t) -&gt; tuple[str, bool, bool, list[str]]:\n\t\t\"\"\"\n\t\tGenerate a commit message with linting verification.\n\n\t\tArgs:\n\t\t        chunk: The DiffChunk to generate a message for\n\t\t        retry_count: Current retry count (default: 1)\n\t\t        max_retries: Maximum number of retries for linting (default: 3)\n\n\t\tReturns:\n\t\t        Tuple of (message, used_llm, passed_linting, lint_messages)\n\n\t\t\"\"\"\n\t\t# First, generate the initial message\n\t\tinitial_lint_messages: list[str] = []  # Store initial messages\n\t\ttry:\n\t\t\tmessage, used_llm = self.generate_message(chunk)\n\t\t\tlogger.debug(\"Generated initial message: %s\", message)\n\n\t\t\t# Clean the message before linting\n\t\t\tmessage = clean_message_for_linting(message)\n\n\t\t\t# Check if the message passes linting\n\t\t\tis_valid, error_message = lint_commit_message(\n\t\t\t\tmessage, repo_root=self.repo_root, config_loader=self._config_loader\n\t\t\t)\n\t\t\tinitial_lint_messages = [error_message] if error_message is not None else []\n\t\t\tlogger.debug(\"Lint result: valid=%s, messages=%s\", is_valid, initial_lint_messages)\n\n\t\t\tif is_valid or retry_count &gt;= max_retries:\n\t\t\t\t# Return empty list if valid, or initial messages if max retries reached\n\t\t\t\treturn message, used_llm, is_valid, [] if is_valid else initial_lint_messages\n\n\t\t\t# Prepare the diff content\n\t\t\tdiff_content = chunk.content\n\t\t\tif not diff_content:\n\t\t\t\t# Check if we have binary files in the chunk\n\t\t\t\tbinary_files = []\n\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\t# First check file extension\n\t\t\t\t\textension = \"\"\n\t\t\t\t\tfile_info = self.extract_file_info(chunk)\n\t\t\t\t\tif file_path in file_info:\n\t\t\t\t\t\textension = file_info[file_path].get(\"extension\", \"\").lower()\n\t\t\t\t\t\tbinary_extensions = {\n\t\t\t\t\t\t\t\"png\",\n\t\t\t\t\t\t\t\"jpg\",\n\t\t\t\t\t\t\t\"jpeg\",\n\t\t\t\t\t\t\t\"gif\",\n\t\t\t\t\t\t\t\"bmp\",\n\t\t\t\t\t\t\t\"ico\",\n\t\t\t\t\t\t\t\"webp\",\n\t\t\t\t\t\t\t\"mp3\",\n\t\t\t\t\t\t\t\"wav\",\n\t\t\t\t\t\t\t\"mp4\",\n\t\t\t\t\t\t\t\"avi\",\n\t\t\t\t\t\t\t\"mov\",\n\t\t\t\t\t\t\t\"pdf\",\n\t\t\t\t\t\t\t\"zip\",\n\t\t\t\t\t\t\t\"tar\",\n\t\t\t\t\t\t\t\"gz\",\n\t\t\t\t\t\t\t\"exe\",\n\t\t\t\t\t\t\t\"dll\",\n\t\t\t\t\t\t\t\"so\",\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif extension in binary_extensions:\n\t\t\t\t\t\t\tbinary_files.append(file_path)\n\n\t\t\t\t\t# Also try to detect binary files directly\n\t\t\t\t\tabs_path = self.repo_root / file_path\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif abs_path.exists():\n\t\t\t\t\t\t\tfrom codemap.utils.file_utils import is_binary_file\n\n\t\t\t\t\t\t\tif is_binary_file(abs_path) and file_path not in binary_files:\n\t\t\t\t\t\t\t\tbinary_files.append(file_path)\n\t\t\t\t\texcept (OSError, PermissionError) as e:\n\t\t\t\t\t\t# If any error occurs during binary check, log it and continue\n\t\t\t\t\t\tlogger.debug(\"Error checking if %s is binary: %s\", file_path, str(e))\n\n\t\t\t\tif binary_files:\n\t\t\t\t\t# Create a more descriptive message for binary files\n\t\t\t\t\tdiff_content = \"Binary files detected in this chunk:\\n\"\n\t\t\t\t\tfor binary_file in binary_files:\n\t\t\t\t\t\tdiff_content += f\"- {binary_file}\\n\"\n\t\t\t\telse:\n\t\t\t\t\t# Generic fallback for empty diff with no binary files detected\n\t\t\t\t\tdiff_content = \"Empty diff (likely modified binary files)\"\n\n\t\t\tlogger.info(\"Regenerating message with linting feedback (attempt %d/%d)\", retry_count, max_retries)\n\n\t\t\ttry:\n\t\t\t\t# Prepare the enhanced prompt for regeneration\n\t\t\t\tlint_template = get_lint_prompt_template()\n\t\t\t\tenhanced_prompt = prepare_lint_prompt(\n\t\t\t\t\ttemplate=lint_template,\n\t\t\t\t\tfile_info=self.extract_file_info(chunk),  # Use self\n\t\t\t\t\tconvention=self.get_commit_convention(),  # Use self\n\t\t\t\t\tlint_messages=initial_lint_messages,  # Use initial messages for feedback\n\t\t\t\t\toriginal_message=message,  # Pass the original message that failed linting\n\t\t\t\t)\n\n\t\t\t\t# Generate message with the enhanced prompt\n\t\t\t\tregenerated_message = self._call_llm_api(enhanced_prompt)\n\t\t\t\tlogger.debug(\"Regenerated message (RAW LLM output): %s\", regenerated_message)\n\n\t\t\t\t# Format from JSON to commit message format\n\t\t\t\tregenerated_message = self.format_json_to_commit_message(regenerated_message)\n\t\t\t\tlogger.debug(\"Formatted message: %s\", regenerated_message)\n\n\t\t\t\t# Clean and recheck linting\n\t\t\t\tcleaned_message = clean_message_for_linting(regenerated_message)\n\t\t\t\tlogger.debug(\"Cleaned message for linting: %s\", cleaned_message)\n\n\t\t\t\t# Check if the message passes linting\n\t\t\t\tfinal_is_valid, error_message = lint_commit_message(\n\t\t\t\t\tcleaned_message, repo_root=self.repo_root, config_loader=self._config_loader\n\t\t\t\t)\n\t\t\t\tfinal_lint_messages = [error_message] if error_message is not None else []\n\t\t\t\tlogger.debug(\"Regenerated lint result: valid=%s, messages=%s\", final_is_valid, final_lint_messages)\n\n\t\t\t\t# Return final result and messages (empty if valid)\n\t\t\t\treturn cleaned_message, True, final_is_valid, [] if final_is_valid else final_lint_messages\n\t\t\texcept (ValueError, TypeError, KeyError, LLMError, json.JSONDecodeError):\n\t\t\t\t# If regeneration fails, log it and return the original message and its lint errors\n\t\t\t\tlogger.exception(\"Error during message regeneration\")\n\t\t\t\treturn message, used_llm, False, initial_lint_messages  # Return original message and errors\n\t\texcept (ValueError, TypeError, KeyError, LLMError, json.JSONDecodeError):\n\t\t\t# If generation fails completely, use a fallback (fallback doesn't lint, so return True, empty messages)\n\t\t\tlogger.exception(\"Error during message generation\")\n\t\t\tmessage = self.fallback_generation(chunk)\n\t\t\treturn message, False, True, []  # Fallback assumes valid, no lint messages\n\n\tdef get_config_loader(self) -&gt; ConfigLoader:\n\t\t\"\"\"\n\t\tGet the ConfigLoader instance used by this generator.\n\n\t\tReturns:\n\t\t    ConfigLoader instance\n\n\t\t\"\"\"\n\t\treturn self._config_loader\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_root: Path,\n\tllm_client: LLMClient,\n\tprompt_template: str,\n\tconfig_loader: ConfigLoader,\n) -&gt; None\n</code></pre> <p>Initialize the commit message generator.</p> <p>Parameters:</p> Name Type Description Default <code>repo_root</code> <code>Path</code> <p>Root directory of the Git repository</p> required <code>llm_client</code> <code>LLMClient</code> <p>LLMClient instance to use</p> required <code>prompt_template</code> <code>str</code> <p>Custom prompt template to use</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance to use for configuration</p> required Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def __init__(\n\tself,\n\trepo_root: Path,\n\tllm_client: LLMClient,\n\tprompt_template: str,\n\tconfig_loader: ConfigLoader,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the commit message generator.\n\n\tArgs:\n\t    repo_root: Root directory of the Git repository\n\t    llm_client: LLMClient instance to use\n\t    prompt_template: Custom prompt template to use\n\t    config_loader: ConfigLoader instance to use for configuration\n\n\t\"\"\"\n\tself.repo_root = repo_root\n\tself.prompt_template = prompt_template\n\tself._config_loader = config_loader\n\tself.client = llm_client\n\n\t# Add commit template to client\n\tself.client.set_template(\"commit\", self.prompt_template)\n\n\t# Get max token limit from config\n\tllm_config = self._config_loader.get(\"llm\", {})\n\tself.max_tokens = llm_config.get(\"max_context_tokens\", 4000)\n\n\t# Flag to control whether to use the LOD-based context processing\n\tself.use_lod_context = llm_config.get(\"use_lod_context\", True)\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = repo_root\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = llm_client\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens = get('max_context_tokens', 4000)\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.use_lod_context","title":"use_lod_context  <code>instance-attribute</code>","text":"<pre><code>use_lod_context = get('use_lod_context', True)\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.extract_file_info","title":"extract_file_info","text":"<pre><code>extract_file_info(chunk: DiffChunk) -&gt; dict[str, Any]\n</code></pre> <p>Extract file information from the diff chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>Diff chunk object to extract information from</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with information about files</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def extract_file_info(self, chunk: DiffChunk) -&gt; dict[str, Any]:\n\t\"\"\"\n\tExtract file information from the diff chunk.\n\n\tArgs:\n\t    chunk: Diff chunk object to extract information from\n\n\tReturns:\n\t    Dictionary with information about files\n\n\t\"\"\"\n\tfile_info = {}\n\tfiles = chunk.files\n\tfor file in files:\n\t\tif not isinstance(file, str):\n\t\t\tcontinue  # Skip non-string file entries\n\t\tfile_path = self.repo_root / file\n\t\tif not file_path.exists():\n\t\t\tcontinue\n\t\ttry:\n\t\t\textension = file_path.suffix.lstrip(\".\")\n\t\t\tfile_info[file] = {\n\t\t\t\t\"extension\": extension,\n\t\t\t\t\"directory\": str(file_path.parent.relative_to(self.repo_root)),\n\t\t\t}\n\t\t\tpath_parts = file_path.parts\n\t\t\tif len(path_parts) &gt; 1:\n\t\t\t\tif \"src\" in path_parts:\n\t\t\t\t\tidx = path_parts.index(\"src\")\n\t\t\t\t\tif idx + 1 &lt; len(path_parts):\n\t\t\t\t\t\tfile_info[file][\"module\"] = path_parts[idx + 1]\n\t\t\t\telif \"tests\" in path_parts:\n\t\t\t\t\tfile_info[file][\"module\"] = \"tests\"\n\t\texcept (ValueError, IndexError, TypeError):\n\t\t\tcontinue\n\treturn file_info\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.get_commit_convention","title":"get_commit_convention","text":"<pre><code>get_commit_convention() -&gt; dict[str, Any]\n</code></pre> <p>Get commit convention settings from config.</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def get_commit_convention(self) -&gt; dict[str, Any]:\n\t\"\"\"Get commit convention settings from config.\"\"\"\n\t# Use the centralized ConfigLoader to get the convention\n\treturn self._config_loader.get_commit_convention()\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.format_json_to_commit_message","title":"format_json_to_commit_message","text":"<pre><code>format_json_to_commit_message(content: str) -&gt; str\n</code></pre> <p>Format a JSON string as a conventional commit message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>JSON content string from LLM response</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted commit message string</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def format_json_to_commit_message(self, content: str) -&gt; str:\n\t\"\"\"\n\tFormat a JSON string as a conventional commit message.\n\n\tArgs:\n\t    content: JSON content string from LLM response\n\n\tReturns:\n\t    Formatted commit message string\n\n\t\"\"\"\n\n\tdef _raise_validation_error(message: str) -&gt; None:\n\t\t\"\"\"Helper to raise ValueError with consistent message.\"\"\"\n\t\tlogger.warning(\"LLM response validation failed: %s\", message)\n\t\tmsg = message\n\t\traise ValueError(msg)\n\n\ttry:\n\t\t# Try to parse the content as JSON\n\t\tdebug_content = (\n\t\t\tcontent[:MAX_DEBUG_CONTENT_LENGTH] + \"...\" if len(content) &gt; MAX_DEBUG_CONTENT_LENGTH else content\n\t\t)\n\t\tlogger.debug(\"Parsing JSON content: %s\", debug_content)\n\n\t\t# Handle both direct JSON objects and strings containing JSON\n\t\tif not content.strip().startswith(\"{\"):\n\t\t\t# Extract JSON if it's wrapped in other text\n\t\t\timport re\n\n\t\t\tjson_match = re.search(r\"({.*})\", content, re.DOTALL)\n\t\t\tif json_match:\n\t\t\t\tcontent = json_match.group(1)\n\n\t\tmessage_data = json.loads(content)\n\t\tlogger.debug(\"Parsed JSON: %s\", message_data)\n\n\t\t# Basic Schema Validation\n\t\tif not isinstance(message_data, dict):\n\t\t\t_raise_validation_error(\"JSON response is not an object\")\n\n\t\tif not message_data.get(\"type\") or not message_data.get(\"description\"):\n\t\t\t_raise_validation_error(\"Missing required fields in JSON response\")\n\n\t\t# Extract components with validation/defaults\n\t\tcommit_type = str(message_data[\"type\"]).lower().strip()\n\n\t\t# Check for valid commit type (from the config)\n\t\tvalid_types = self._config_loader.get_commit_convention().get(\"types\", [])\n\t\tif valid_types and commit_type not in valid_types:\n\t\t\tlogger.warning(\"Invalid commit type: %s. Valid types: %s\", commit_type, valid_types)\n\t\t\t# Try to find a valid type as fallback\n\t\t\tif \"feat\" in valid_types:\n\t\t\t\tcommit_type = \"feat\"\n\t\t\telif \"fix\" in valid_types:\n\t\t\t\tcommit_type = \"fix\"\n\t\t\telif len(valid_types) &gt; 0:\n\t\t\t\tcommit_type = valid_types[0]\n\t\t\tlogger.debug(\"Using fallback commit type: %s\", commit_type)\n\n\t\tscope = message_data.get(\"scope\")\n\t\tif scope is not None:\n\t\t\tscope = str(scope).lower().strip()\n\n\t\tdescription = str(message_data[\"description\"]).lower().strip()\n\n\t\t# Ensure description doesn't start with another type prefix\n\t\tfor valid_type in valid_types:\n\t\t\tif description.startswith(f\"{valid_type}:\"):\n\t\t\t\t# Remove the duplicate type prefix from description\n\t\t\t\tdescription = description.split(\":\", 1)[1].strip()\n\t\t\t\tlogger.debug(\"Removed duplicate type prefix from description: %s\", description)\n\t\t\t\tbreak\n\n\t\tbody = message_data.get(\"body\")\n\t\tif body is not None:\n\t\t\tbody = str(body).strip()\n\t\tis_breaking = bool(message_data.get(\"breaking\", False))\n\n\t\t# Format the header\n\t\theader = f\"{commit_type}\"\n\t\tif scope:\n\t\t\theader += f\"({scope})\"\n\t\tif is_breaking:\n\t\t\theader += \"!\"\n\t\theader += f\": {description}\"\n\n\t\t# Ensure compliance with commit format regex\n\t\t# The regex requires a space after the colon, and the format should be &lt;type&gt;(&lt;scope&gt;)!: &lt;description&gt;\n\t\tif \": \" not in header:\n\t\t\tparts = header.split(\":\")\n\t\t\tif len(parts) == EXPECTED_PARTS_COUNT:\n\t\t\t\theader = f\"{parts[0]}: {parts[1].strip()}\"\n\n\t\t# Validation check against regex pattern\n\t\timport re\n\n\t\tfrom codemap.git.commit_linter.constants import COMMIT_REGEX\n\n\t\t# If header doesn't match the expected format, log and try to fix it\n\t\tif not COMMIT_REGEX.match(header):\n\t\t\tlogger.warning(\"Generated header doesn't match commit format: %s\", header)\n\t\t\t# As a fallback, recreate with a simpler format\n\t\t\tsimple_header = f\"{commit_type}\"\n\t\t\tif scope:\n\t\t\t\tsimple_header += f\"({scope})\"\n\t\t\tif is_breaking:\n\t\t\t\tsimple_header += \"!\"\n\t\t\tsimple_header += f\": {description}\"\n\t\t\theader = simple_header\n\t\t\tlogger.debug(\"Fixed header to: %s\", header)\n\n\t\t# Build the complete message\n\t\tmessage_parts = [header]\n\n\t\t# Add body if provided\n\t\tif body:\n\t\t\tmessage_parts.append(\"\")  # Empty line between header and body\n\t\t\tmessage_parts.append(body)\n\n\t\t# Carefully filter only breaking change footers\n\t\tfooters = message_data.get(\"footers\", [])\n\t\tbreaking_change_footers = []\n\n\t\tif isinstance(footers, list):\n\t\t\tbreaking_change_footers = [\n\t\t\t\tfooter\n\t\t\t\tfor footer in footers\n\t\t\t\tif isinstance(footer, dict)\n\t\t\t\tand footer.get(\"token\", \"\").upper() in (\"BREAKING CHANGE\", \"BREAKING-CHANGE\")\n\t\t\t]\n\n\t\tif breaking_change_footers:\n\t\t\tif not body:\n\t\t\t\tmessage_parts.append(\"\")  # Empty line before footers if no body\n\t\t\telse:\n\t\t\t\tmessage_parts.append(\"\")  # Empty line between body and footers\n\n\t\t\tfor footer in breaking_change_footers:\n\t\t\t\ttoken = footer.get(\"token\", \"\")\n\t\t\t\tvalue = footer.get(\"value\", \"\")\n\t\t\t\tmessage_parts.append(f\"{token}: {value}\")\n\n\t\tmessage = \"\\n\".join(message_parts)\n\t\tlogger.debug(\"Formatted commit message: %s\", message)\n\t\treturn message\n\n\texcept (json.JSONDecodeError, ValueError, TypeError, AttributeError) as e:\n\t\t# If parsing or validation fails, return the content as-is, but cleaned\n\t\tlogger.warning(\"Error formatting JSON to commit message: %s. Using raw content.\", str(e))\n\t\treturn content.strip()\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.fallback_generation","title":"fallback_generation","text":"<pre><code>fallback_generation(chunk: DiffChunk) -&gt; str\n</code></pre> <p>Generate a fallback commit message without LLM.</p> <p>This is used when LLM-based generation fails or is disabled.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>Diff chunk object to generate message for</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated commit message</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def fallback_generation(self, chunk: DiffChunk) -&gt; str:\n\t\"\"\"\n\tGenerate a fallback commit message without LLM.\n\n\tThis is used when LLM-based generation fails or is disabled.\n\n\tArgs:\n\t    chunk: Diff chunk object to generate message for\n\n\tReturns:\n\t    Generated commit message\n\n\t\"\"\"\n\tcommit_type = \"chore\"\n\n\t# Get files directly from the chunk object\n\tfiles = chunk.files\n\n\t# Filter only strings (defensive, though DiffChunk.files should be list[str])\n\tstring_files = [f for f in files if isinstance(f, str)]\n\n\tfor file in string_files:\n\t\tif file.startswith(\"tests/\"):\n\t\t\tcommit_type = \"test\"\n\t\t\tbreak\n\t\tif file.startswith(\"docs/\") or file.endswith(\".md\"):\n\t\t\tcommit_type = \"docs\"\n\t\t\tbreak\n\n\t# Get content directly from the chunk object\n\tcontent = chunk.content\n\n\tif isinstance(content, str) and (\"fix\" in content.lower() or \"bug\" in content.lower()):\n\t\tcommit_type = \"fix\"  # Be slightly smarter about 'fix' type\n\n\t# Use chunk description if available and seems specific (not just placeholder)\n\tchunk_desc = chunk.description\n\tplaceholder_descs = [\"update files\", \"changes in\", \"hunk in\", \"new file:\"]\n\t# Ensure chunk_desc is not None before calling lower()\n\tuse_chunk_desc = chunk_desc and not any(p in chunk_desc.lower() for p in placeholder_descs)\n\n\tif use_chunk_desc and chunk_desc:  # Add explicit check for chunk_desc\n\t\tdescription = chunk_desc\n\t\t# Attempt to extract a type from the chunk description if possible\n\t\t# Ensure chunk_desc is not None before calling lower() and split()\n\t\tif chunk_desc.lower().startswith(\n\t\t\t(\"feat\", \"fix\", \"refactor\", \"docs\", \"test\", \"chore\", \"style\", \"perf\", \"ci\", \"build\")\n\t\t):\n\t\t\tparts = chunk_desc.split(\":\", 1)\n\t\t\tif len(parts) &gt; 1:\n\t\t\t\tcommit_type = parts[0].split(\"(\")[0].strip().lower()  # Extract type before scope\n\t\t\t\tdescription = parts[1].strip()\n\telse:\n\t\t# Generate description based on file count/path if no specific chunk desc\n\t\tdescription = \"update files\"  # Default\n\t\tif string_files:\n\t\t\tif len(string_files) == 1:\n\t\t\t\tdescription = f\"update {string_files[0]}\"\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tcommon_dir = os.path.commonpath(string_files)\n\t\t\t\t\t# Make common_dir relative to repo root if possible\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcommon_dir_rel = os.path.relpath(common_dir, self.repo_root)\n\t\t\t\t\t\tif common_dir_rel and common_dir_rel != \".\":\n\t\t\t\t\t\t\tdescription = f\"update files in {common_dir_rel}\"\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\t\t\t\t\texcept ValueError:  # Happens if paths are on different drives (unlikely in repo)\n\t\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\t\t\t\texcept (ValueError, TypeError):  # commonpath fails on empty list or mixed types\n\t\t\t\t\tdescription = f\"update {len(string_files)} files\"\n\n\tmessage = f\"{commit_type}: {description}\"\n\tlogger.debug(\"Generated fallback message: %s\", message)\n\treturn message\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.generate_message","title":"generate_message","text":"<pre><code>generate_message(chunk: DiffChunk) -&gt; tuple[str, bool]\n</code></pre> <p>Generate a commit message for a diff chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>Diff chunk to generate message for</p> required <p>Returns:</p> Type Description <code>tuple[str, bool]</code> <p>Generated message and success flag</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def generate_message(self, chunk: DiffChunk) -&gt; tuple[str, bool]:\n\t\"\"\"\n\tGenerate a commit message for a diff chunk.\n\n\tArgs:\n\t    chunk: Diff chunk to generate message for\n\n\tReturns:\n\t    Generated message and success flag\n\n\t\"\"\"\n\t# Prepare prompt with chunk data\n\ttry:\n\t\tprompt = self._prepare_prompt(chunk)\n\t\tlogger.debug(\"Prompt prepared successfully\")\n\n\t\t# Generate message using configured LLM provider\n\t\tmessage = self._call_llm_api(prompt)\n\t\tlogger.debug(\"LLM generated message: %s\", message)\n\n\t\t# Return generated message with success flag\n\t\treturn message, True\n\texcept (ValueError, TypeError, KeyError, LLMError):\n\t\tlogger.exception(\"Error during LLM generation\")\n\t\t# Fall back to heuristic generation\n\t\treturn self.fallback_generation(chunk), False\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.generate_message_with_linting","title":"generate_message_with_linting","text":"<pre><code>generate_message_with_linting(\n\tchunk: DiffChunk,\n\tretry_count: int = 1,\n\tmax_retries: int = 3,\n) -&gt; tuple[str, bool, bool, list[str]]\n</code></pre> <p>Generate a commit message with linting verification.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>The DiffChunk to generate a message for</p> required <code>retry_count</code> <code>int</code> <p>Current retry count (default: 1)</p> <code>1</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for linting (default: 3)</p> <code>3</code> <p>Returns:</p> Type Description <code>tuple[str, bool, bool, list[str]]</code> <p>Tuple of (message, used_llm, passed_linting, lint_messages)</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def generate_message_with_linting(\n\tself, chunk: DiffChunk, retry_count: int = 1, max_retries: int = 3\n) -&gt; tuple[str, bool, bool, list[str]]:\n\t\"\"\"\n\tGenerate a commit message with linting verification.\n\n\tArgs:\n\t        chunk: The DiffChunk to generate a message for\n\t        retry_count: Current retry count (default: 1)\n\t        max_retries: Maximum number of retries for linting (default: 3)\n\n\tReturns:\n\t        Tuple of (message, used_llm, passed_linting, lint_messages)\n\n\t\"\"\"\n\t# First, generate the initial message\n\tinitial_lint_messages: list[str] = []  # Store initial messages\n\ttry:\n\t\tmessage, used_llm = self.generate_message(chunk)\n\t\tlogger.debug(\"Generated initial message: %s\", message)\n\n\t\t# Clean the message before linting\n\t\tmessage = clean_message_for_linting(message)\n\n\t\t# Check if the message passes linting\n\t\tis_valid, error_message = lint_commit_message(\n\t\t\tmessage, repo_root=self.repo_root, config_loader=self._config_loader\n\t\t)\n\t\tinitial_lint_messages = [error_message] if error_message is not None else []\n\t\tlogger.debug(\"Lint result: valid=%s, messages=%s\", is_valid, initial_lint_messages)\n\n\t\tif is_valid or retry_count &gt;= max_retries:\n\t\t\t# Return empty list if valid, or initial messages if max retries reached\n\t\t\treturn message, used_llm, is_valid, [] if is_valid else initial_lint_messages\n\n\t\t# Prepare the diff content\n\t\tdiff_content = chunk.content\n\t\tif not diff_content:\n\t\t\t# Check if we have binary files in the chunk\n\t\t\tbinary_files = []\n\t\t\tfor file_path in chunk.files:\n\t\t\t\t# First check file extension\n\t\t\t\textension = \"\"\n\t\t\t\tfile_info = self.extract_file_info(chunk)\n\t\t\t\tif file_path in file_info:\n\t\t\t\t\textension = file_info[file_path].get(\"extension\", \"\").lower()\n\t\t\t\t\tbinary_extensions = {\n\t\t\t\t\t\t\"png\",\n\t\t\t\t\t\t\"jpg\",\n\t\t\t\t\t\t\"jpeg\",\n\t\t\t\t\t\t\"gif\",\n\t\t\t\t\t\t\"bmp\",\n\t\t\t\t\t\t\"ico\",\n\t\t\t\t\t\t\"webp\",\n\t\t\t\t\t\t\"mp3\",\n\t\t\t\t\t\t\"wav\",\n\t\t\t\t\t\t\"mp4\",\n\t\t\t\t\t\t\"avi\",\n\t\t\t\t\t\t\"mov\",\n\t\t\t\t\t\t\"pdf\",\n\t\t\t\t\t\t\"zip\",\n\t\t\t\t\t\t\"tar\",\n\t\t\t\t\t\t\"gz\",\n\t\t\t\t\t\t\"exe\",\n\t\t\t\t\t\t\"dll\",\n\t\t\t\t\t\t\"so\",\n\t\t\t\t\t}\n\t\t\t\t\tif extension in binary_extensions:\n\t\t\t\t\t\tbinary_files.append(file_path)\n\n\t\t\t\t# Also try to detect binary files directly\n\t\t\t\tabs_path = self.repo_root / file_path\n\t\t\t\ttry:\n\t\t\t\t\tif abs_path.exists():\n\t\t\t\t\t\tfrom codemap.utils.file_utils import is_binary_file\n\n\t\t\t\t\t\tif is_binary_file(abs_path) and file_path not in binary_files:\n\t\t\t\t\t\t\tbinary_files.append(file_path)\n\t\t\t\texcept (OSError, PermissionError) as e:\n\t\t\t\t\t# If any error occurs during binary check, log it and continue\n\t\t\t\t\tlogger.debug(\"Error checking if %s is binary: %s\", file_path, str(e))\n\n\t\t\tif binary_files:\n\t\t\t\t# Create a more descriptive message for binary files\n\t\t\t\tdiff_content = \"Binary files detected in this chunk:\\n\"\n\t\t\t\tfor binary_file in binary_files:\n\t\t\t\t\tdiff_content += f\"- {binary_file}\\n\"\n\t\t\telse:\n\t\t\t\t# Generic fallback for empty diff with no binary files detected\n\t\t\t\tdiff_content = \"Empty diff (likely modified binary files)\"\n\n\t\tlogger.info(\"Regenerating message with linting feedback (attempt %d/%d)\", retry_count, max_retries)\n\n\t\ttry:\n\t\t\t# Prepare the enhanced prompt for regeneration\n\t\t\tlint_template = get_lint_prompt_template()\n\t\t\tenhanced_prompt = prepare_lint_prompt(\n\t\t\t\ttemplate=lint_template,\n\t\t\t\tfile_info=self.extract_file_info(chunk),  # Use self\n\t\t\t\tconvention=self.get_commit_convention(),  # Use self\n\t\t\t\tlint_messages=initial_lint_messages,  # Use initial messages for feedback\n\t\t\t\toriginal_message=message,  # Pass the original message that failed linting\n\t\t\t)\n\n\t\t\t# Generate message with the enhanced prompt\n\t\t\tregenerated_message = self._call_llm_api(enhanced_prompt)\n\t\t\tlogger.debug(\"Regenerated message (RAW LLM output): %s\", regenerated_message)\n\n\t\t\t# Format from JSON to commit message format\n\t\t\tregenerated_message = self.format_json_to_commit_message(regenerated_message)\n\t\t\tlogger.debug(\"Formatted message: %s\", regenerated_message)\n\n\t\t\t# Clean and recheck linting\n\t\t\tcleaned_message = clean_message_for_linting(regenerated_message)\n\t\t\tlogger.debug(\"Cleaned message for linting: %s\", cleaned_message)\n\n\t\t\t# Check if the message passes linting\n\t\t\tfinal_is_valid, error_message = lint_commit_message(\n\t\t\t\tcleaned_message, repo_root=self.repo_root, config_loader=self._config_loader\n\t\t\t)\n\t\t\tfinal_lint_messages = [error_message] if error_message is not None else []\n\t\t\tlogger.debug(\"Regenerated lint result: valid=%s, messages=%s\", final_is_valid, final_lint_messages)\n\n\t\t\t# Return final result and messages (empty if valid)\n\t\t\treturn cleaned_message, True, final_is_valid, [] if final_is_valid else final_lint_messages\n\t\texcept (ValueError, TypeError, KeyError, LLMError, json.JSONDecodeError):\n\t\t\t# If regeneration fails, log it and return the original message and its lint errors\n\t\t\tlogger.exception(\"Error during message regeneration\")\n\t\t\treturn message, used_llm, False, initial_lint_messages  # Return original message and errors\n\texcept (ValueError, TypeError, KeyError, LLMError, json.JSONDecodeError):\n\t\t# If generation fails completely, use a fallback (fallback doesn't lint, so return True, empty messages)\n\t\tlogger.exception(\"Error during message generation\")\n\t\tmessage = self.fallback_generation(chunk)\n\t\treturn message, False, True, []  # Fallback assumes valid, no lint messages\n</code></pre>"},{"location":"api/git/commit_generator/generator/#codemap.git.commit_generator.generator.CommitMessageGenerator.get_config_loader","title":"get_config_loader","text":"<pre><code>get_config_loader() -&gt; ConfigLoader\n</code></pre> <p>Get the ConfigLoader instance used by this generator.</p> <p>Returns:</p> Type Description <code>ConfigLoader</code> <p>ConfigLoader instance</p> Source code in <code>src/codemap/git/commit_generator/generator.py</code> <pre><code>def get_config_loader(self) -&gt; ConfigLoader:\n\t\"\"\"\n\tGet the ConfigLoader instance used by this generator.\n\n\tReturns:\n\t    ConfigLoader instance\n\n\t\"\"\"\n\treturn self._config_loader\n</code></pre>"},{"location":"api/git/commit_generator/prompts/","title":"Prompts","text":"<p>Prompt templates for commit message generation.</p>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.DEFAULT_PROMPT_TEMPLATE","title":"DEFAULT_PROMPT_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_PROMPT_TEMPLATE = '\\nYou are an AI assistant generating Conventional Commit 1.0.0 messages from Git diffs.\\n\\n**Format:**\\n```\\n&lt;type&gt;[optional scope]: &lt;description&gt;\\n\\n[optional body]\\n\\n[optional footer(s)]\\n```\\n\\n**Instructions &amp; Rules:**\\n\\n1.  **Type:** REQUIRED. Must be lowercase and one of: {convention[types]}.\\n    *   `feat`: New feature (MINOR SemVer).\\n    *   `fix`: Bug fix (PATCH SemVer).\\n    *   Other types (`build`, `chore`, `ci`, `docs`, `style`, `refactor`, `perf`, `test`, etc.) are allowed.\\n2.  **Scope:** OPTIONAL. Lowercase noun(s) in parentheses describing the code section (e.g., `(parser)`).\\n    *   Keep short (1-2 words).\\n3.  **Description:** REQUIRED. Concise, imperative, present tense summary of *what* changed and *why* based on the diff.\\n    *   Must follow the colon and space.\\n    *   Must be &gt;= 10 characters.\\n    *   Must NOT end with a period.\\n    *   The entire header line (`&lt;type&gt;[scope]: &lt;description&gt;`) must be &lt;= {convention[max_length]} characters.\\n4.  **Body:** OPTIONAL. Explain *why* and *how*. Start one blank line after the description.\\n\\t*\\tUse the body only if extra context is needed to understand the changes.\\n\\t*\\tDo not use the body to add unrelated information.\\n\\t*\\tDo not use the body to explain *what* was changed.\\n\\t*\\tTry to keep the body concise and to the point.\\n5.  **Footer(s):** OPTIONAL. Format `Token: value` or `Token # value`.\\n    *   Start one blank line after the body.\\n    *   Use `-` for spaces in tokens (e.g., `Reviewed-by`).\\n6.  **BREAKING CHANGE:** Indicate with `!` before the colon in the header (e.g., `feat(api)!: ...`)\\n    *   OR with a `BREAKING CHANGE: &lt;description&gt;` footer (MUST be uppercase).\\n    *   Correlates with MAJOR SemVer.\\n    *   If `!` is used, the description explains the break.\\n7.  **Special Case - Binary Files:**\\n    *   For binary file changes, use `chore` type with a scope indicating the file type (e.g., `(assets)`, `(images)`, `(builds)`)\\n    *   Be specific about what changed (e.g., \"update image assets\", \"add new icon files\", \"replace binary database\")\\n    *   If the diff content is empty or shows binary file changes, focus on the filenames to determine the purpose\\n\\n**Input:**\\n\\n*   File notes: {files}\\n*   Git diff: {diff}\\n\\n**Output Requirements:**\\n\\n*   Respond with ONLY the raw commit message string.\\n*   NO extra text, explanations, or markdown formatting (like ```).\\n*   STRICTLY OMIT footers: `Related Issue #`, `Closes #`, `REVIEWED-BY`, `TRACKING #`, `APPROVED`.\\n\\n**(IMPORTANT) Following JSON Schema must be followed for Output:**\\n{schema}\\n\\n---\\nPlease return the commit message in a valid json format. Analyze the following diff and generate the commit message:\\n\\n{diff}\\n'\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.get_lint_prompt_template","title":"get_lint_prompt_template","text":"<pre><code>get_lint_prompt_template() -&gt; str\n</code></pre> <p>Get the prompt template for lint feedback.</p> <p>Returns:</p> Type Description <code>str</code> <p>The prompt template with lint feedback placeholders</p> Source code in <code>src/codemap/git/commit_generator/prompts.py</code> <pre><code>def get_lint_prompt_template() -&gt; str:\n\t\"\"\"\n\tGet the prompt template for lint feedback.\n\n\tReturns:\n\t    The prompt template with lint feedback placeholders\n\n\t\"\"\"\n\treturn \"\"\"\nYou are a helpful assistant that fixes conventional commit messages that have linting errors.\n\n1. The conventional commit format is:\n```\n&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n```\n2. Types include: {convention[types]}\n3. Scope must be short (1-2 words), concise, and represent the specific component affected\n4. The description should be a concise, imperative present tense summary of the code changes,\n   focusing on *what* was changed and *why*.\n5. The optional body should focus on the *why* and *how* of the changes.\n\nIMPORTANT: The provided commit message has the following issues:\n{lint_feedback}\n\nOriginal commit message:\n{original_message}\n\nBrief file context (without full diff):\n{files_summary}\n\nPlease fix these issues and ensure the generated message adheres to the commit convention.\n\nIMPORTANT:\n- Strictly follow the format &lt;type&gt;[optional scope]: &lt;description&gt;\n- Do not include any other text, explanation, or surrounding characters\n- Do not include any `Related Issue #`, `Closes #`, `REVIEWED-BY`, `TRACKING #`, `APPROVED` footers.\n- Respond with a valid JSON object following this schema:\n\n{schema}\n\nReturn your answer as json.\n\"\"\"\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.prepare_prompt","title":"prepare_prompt","text":"<pre><code>prepare_prompt(\n\ttemplate: str,\n\tdiff_content: str,\n\tfile_info: dict[str, Any],\n\tconvention: dict[str, Any],\n\textra_context: dict[str, Any] | None = None,\n) -&gt; str\n</code></pre> <p>Prepare the prompt for the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template to use</p> required <code>diff_content</code> <code>str</code> <p>Diff content to include</p> required <code>file_info</code> <code>dict[str, Any]</code> <p>Information about files in the diff</p> required <code>convention</code> <code>dict[str, Any]</code> <p>Commit convention settings</p> required <code>extra_context</code> <code>dict[str, Any] | None</code> <p>Optional additional context values for the template</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt</p> Source code in <code>src/codemap/git/commit_generator/prompts.py</code> <pre><code>def prepare_prompt(\n\ttemplate: str,\n\tdiff_content: str,\n\tfile_info: dict[str, Any],\n\tconvention: dict[str, Any],\n\textra_context: dict[str, Any] | None = None,\n) -&gt; str:\n\t\"\"\"\n\tPrepare the prompt for the LLM.\n\n\tArgs:\n\t    template: Prompt template to use\n\t    diff_content: Diff content to include\n\t    file_info: Information about files in the diff\n\t    convention: Commit convention settings\n\t    extra_context: Optional additional context values for the template\n\n\tReturns:\n\t    Formatted prompt\n\n\t\"\"\"\n\tcontext = {\n\t\t\"diff\": diff_content,\n\t\t\"files\": file_info,\n\t\t\"convention\": convention,\n\t\t\"schema\": COMMIT_MESSAGE_SCHEMA,\n\t}\n\n\t# Add any extra context values\n\tif extra_context:\n\t\tcontext.update(extra_context)\n\n\ttry:\n\t\treturn template.format(**context)\n\texcept KeyError as e:\n\t\tmsg = f\"Prompt template formatting error. Missing key: {e}\"\n\t\traise ValueError(msg) from e\n</code></pre>"},{"location":"api/git/commit_generator/prompts/#codemap.git.commit_generator.prompts.prepare_lint_prompt","title":"prepare_lint_prompt","text":"<pre><code>prepare_lint_prompt(\n\ttemplate: str,\n\tfile_info: dict[str, Any],\n\tconvention: dict[str, Any],\n\tlint_messages: list[str],\n\toriginal_message: str | None = None,\n) -&gt; str\n</code></pre> <p>Prepare a prompt with lint feedback for regeneration.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template to use</p> required <code>file_info</code> <code>dict[str, Any]</code> <p>Information about files in the diff</p> required <code>convention</code> <code>dict[str, Any]</code> <p>Commit convention settings</p> required <code>lint_messages</code> <code>list[str]</code> <p>List of linting error messages</p> required <code>original_message</code> <code>str | None</code> <p>The original failed commit message</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Enhanced prompt with linting feedback</p> Source code in <code>src/codemap/git/commit_generator/prompts.py</code> <pre><code>def prepare_lint_prompt(\n\ttemplate: str,\n\tfile_info: dict[str, Any],\n\tconvention: dict[str, Any],\n\tlint_messages: list[str],\n\toriginal_message: str | None = None,\n) -&gt; str:\n\t\"\"\"\n\tPrepare a prompt with lint feedback for regeneration.\n\n\tArgs:\n\t    template: Prompt template to use\n\t    file_info: Information about files in the diff\n\t    convention: Commit convention settings\n\t    lint_messages: List of linting error messages\n\t    original_message: The original failed commit message\n\n\tReturns:\n\t    Enhanced prompt with linting feedback\n\n\t\"\"\"\n\t# Create specific feedback for linting issues\n\tlint_feedback = \"\\n\".join([f\"- {msg}\" for msg in lint_messages])\n\n\t# Create a simplified file summary without full diff content\n\tfiles_summary = []\n\tfor file_path, info in file_info.items():\n\t\textension = info.get(\"extension\", \"\")\n\t\tdirectory = info.get(\"directory\", \"\")\n\t\tmodule = info.get(\"module\", \"\")\n\t\tsummary = f\"- {file_path} ({extension} file in {directory})\"\n\t\tif module:\n\t\t\tsummary += f\", part of {module} module\"\n\t\tfiles_summary.append(summary)\n\n\tfiles_summary_text = \"\\n\".join(files_summary) if files_summary else \"No file information available\"\n\n\t# If original_message wasn't provided, use a placeholder\n\tmessage_to_fix = original_message or \"No original message provided\"\n\n\t# Create an enhanced context with linting feedback\n\tcontext = {\n\t\t\"convention\": convention,\n\t\t\"schema\": COMMIT_MESSAGE_SCHEMA,\n\t\t\"lint_feedback\": lint_feedback,\n\t\t\"original_message\": message_to_fix,\n\t\t\"files_summary\": files_summary_text,\n\t}\n\n\ttry:\n\t\treturn template.format(**context)\n\texcept KeyError as e:\n\t\tmsg = f\"Lint prompt template formatting error. Missing key: {e}\"\n\t\traise ValueError(msg) from e\n</code></pre>"},{"location":"api/git/commit_generator/schemas/","title":"Schemas","text":"<p>Schemas and data structures for commit message generation.</p>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema","title":"CommitMessageSchema","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict representing the structured commit message output.</p> Source code in <code>src/codemap/git/commit_generator/schemas.py</code> <pre><code>class CommitMessageSchema(TypedDict):\n\t\"\"\"TypedDict representing the structured commit message output.\"\"\"\n\n\ttype: str\n\tscope: str | None\n\tdescription: str\n\tbody: str | None\n\tbreaking: bool\n\tfooters: list[dict[str, str]]\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: str\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.scope","title":"scope  <code>instance-attribute</code>","text":"<pre><code>scope: str | None\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.body","title":"body  <code>instance-attribute</code>","text":"<pre><code>body: str | None\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.breaking","title":"breaking  <code>instance-attribute</code>","text":"<pre><code>breaking: bool\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.CommitMessageSchema.footers","title":"footers  <code>instance-attribute</code>","text":"<pre><code>footers: list[dict[str, str]]\n</code></pre>"},{"location":"api/git/commit_generator/schemas/#codemap.git.commit_generator.schemas.COMMIT_MESSAGE_SCHEMA","title":"COMMIT_MESSAGE_SCHEMA  <code>module-attribute</code>","text":"<pre><code>COMMIT_MESSAGE_SCHEMA = {\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"type\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"description\": \"The type of change (e.g., feat, fix, docs, style, refactor, perf, test, chore)\",\n\t\t},\n\t\t\"scope\": {\n\t\t\t\"type\": [\"string\", \"null\"],\n\t\t\t\"description\": \"The scope of the change (e.g., component affected)\",\n\t\t},\n\t\t\"description\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"description\": \"A short, imperative-tense description of the change\",\n\t\t},\n\t\t\"body\": {\n\t\t\t\"type\": [\"string\", \"null\"],\n\t\t\t\"description\": \"A longer description of the changes, explaining why and how\",\n\t\t},\n\t\t\"breaking\": {\n\t\t\t\"type\": \"boolean\",\n\t\t\t\"description\": \"Whether this is a breaking change\",\n\t\t\t\"default\": False,\n\t\t},\n\t\t\"footers\": {\n\t\t\t\"type\": \"array\",\n\t\t\t\"items\": {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"token\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"description\": \"Footer token (e.g., 'BREAKING CHANGE', 'Fixes', 'Refs')\",\n\t\t\t\t\t},\n\t\t\t\t\t\"value\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"description\": \"Footer value\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\"required\": [\"token\", \"value\"],\n\t\t\t},\n\t\t\t\"default\": [],\n\t\t},\n\t},\n\t\"required\": [\"type\", \"description\"],\n}\n</code></pre>"},{"location":"api/git/commit_generator/utils/","title":"Utils","text":"<p>Utility functions for commit message generation.</p>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.clean_message_for_linting","title":"clean_message_for_linting","text":"<pre><code>clean_message_for_linting(message: str) -&gt; str\n</code></pre> <p>Clean a commit message for linting.</p> <p>Removes extra newlines, trims whitespace, etc.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to clean</p> required <p>Returns:</p> Type Description <code>str</code> <p>The cleaned commit message</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def clean_message_for_linting(message: str) -&gt; str:\n\t\"\"\"\n\tClean a commit message for linting.\n\n\tRemoves extra newlines, trims whitespace, etc.\n\n\tArgs:\n\t        message: The commit message to clean\n\n\tReturns:\n\t        The cleaned commit message\n\n\t\"\"\"\n\t# Replace multiple consecutive newlines with a single newline\n\tcleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", message)\n\t# Trim leading and trailing whitespace\n\treturn cleaned.strip()\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.lint_commit_message","title":"lint_commit_message","text":"<pre><code>lint_commit_message(\n\tmessage: str,\n\trepo_root: Path | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; tuple[bool, str | None]\n</code></pre> <p>Lint a commit message.</p> <p>Checks if it adheres to Conventional Commits format using internal CommitLinter.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to lint</p> required <code>repo_root</code> <code>Path | None</code> <p>Repository root path</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[bool, str | None]</code> <p>Tuple of (is_valid, error_message)</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def lint_commit_message(\n\tmessage: str, repo_root: Path | None = None, config_loader: ConfigLoader | None = None\n) -&gt; tuple[bool, str | None]:\n\t\"\"\"\n\tLint a commit message.\n\n\tChecks if it adheres to Conventional Commits format using internal CommitLinter.\n\n\tArgs:\n\t        message: The commit message to lint\n\t        repo_root: Repository root path\n\t        config_loader: Configuration loader instance\n\n\tReturns:\n\t        Tuple of (is_valid, error_message)\n\n\t\"\"\"\n\t# Get config loader if not provided\n\tif config_loader is None:\n\t\tconfig_loader = ConfigLoader(repo_root=repo_root)\n\n\ttry:\n\t\t# Create a CommitLinter instance with the config_loader\n\t\tlinter = CommitLinter(config_loader=config_loader)\n\n\t\t# Lint the commit message\n\t\tis_valid, lint_messages = linter.lint(message)\n\n\t\t# Get error message if not valid\n\t\terror_message = None\n\t\tif not is_valid and lint_messages:\n\t\t\terror_message = \"\\n\".join(lint_messages)\n\n\t\treturn is_valid, error_message\n\n\texcept Exception as e:\n\t\t# Handle any errors during linting\n\t\tlogger.exception(\"Error linting commit message\")\n\t\treturn False, f\"Linting failed: {e!s}\"\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.save_working_directory_state","title":"save_working_directory_state","text":"<pre><code>save_working_directory_state(\n\tfiles: list[str], output_file: str\n) -&gt; bool\n</code></pre> <p>Save the current state of specified files to a patch file.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of file paths</p> required <code>output_file</code> <code>str</code> <p>Path to output patch file</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the operation was successful</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def save_working_directory_state(files: list[str], output_file: str) -&gt; bool:\n\t\"\"\"\n\tSave the current state of specified files to a patch file.\n\n\tArgs:\n\t        files: List of file paths\n\t        output_file: Path to output patch file\n\n\tReturns:\n\t        bool: Whether the operation was successful\n\n\t\"\"\"\n\toutput_path = Path(output_file)\n\n\ttry:\n\t\tif not files:\n\t\t\t# Nothing to save\n\t\t\twith output_path.open(\"w\") as f:\n\t\t\t\tf.write(\"\")\n\t\t\treturn True\n\n\t\t# Generate diff for the specified files\n\t\tdiff_cmd = [\"git\", \"diff\", \"--\", *files]\n\t\tdiff_content = run_git_command(diff_cmd)\n\n\t\t# Write to output file\n\t\twith output_path.open(\"w\") as f:\n\t\t\tf.write(diff_content)\n\n\t\treturn True\n\n\texcept (OSError, GitError):\n\t\tlogger.exception(\"Error saving working directory state\")\n\t\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.restore_working_directory_state","title":"restore_working_directory_state","text":"<pre><code>restore_working_directory_state(patch_file: str) -&gt; bool\n</code></pre> <p>Restore the working directory state from a patch file.</p> <p>Parameters:</p> Name Type Description Default <code>patch_file</code> <code>str</code> <p>Path to patch file</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the operation was successful</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def restore_working_directory_state(patch_file: str) -&gt; bool:\n\t\"\"\"\n\tRestore the working directory state from a patch file.\n\n\tArgs:\n\t        patch_file: Path to patch file\n\n\tReturns:\n\t        bool: Whether the operation was successful\n\n\t\"\"\"\n\tpatch_path = Path(patch_file)\n\n\ttry:\n\t\t# Check if the patch file exists and is not empty\n\t\tif not patch_path.exists() or patch_path.stat().st_size == 0:\n\t\t\treturn True  # Nothing to restore\n\n\t\t# Apply the patch\n\t\trun_git_command([\"git\", \"apply\", patch_file])\n\t\treturn True\n\n\texcept GitError:\n\t\tlogger.exception(\"Error restoring working directory state\")\n\t\treturn False\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.format_commit_json","title":"format_commit_json","text":"<pre><code>format_commit_json(\n\tcontent: str, config_loader: ConfigLoader | None = None\n) -&gt; str\n</code></pre> <p>Format a JSON string as a conventional commit message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>JSON content string from LLM response</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader for commit conventions</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted commit message string</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def format_commit_json(content: str, config_loader: ConfigLoader | None = None) -&gt; str:\n\t\"\"\"\n\tFormat a JSON string as a conventional commit message.\n\n\tArgs:\n\t        content: JSON content string from LLM response\n\t        config_loader: Optional ConfigLoader for commit conventions\n\n\tReturns:\n\t        Formatted commit message string\n\n\t\"\"\"\n\n\tdef _raise_validation_error(message: str) -&gt; None:\n\t\t\"\"\"Helper to raise ValueError with consistent message.\"\"\"\n\t\tlogger.warning(\"LLM response validation failed: %s\", message)\n\t\traise ValueError(message)\n\n\ttry:\n\t\t# Handle both direct JSON objects and strings containing JSON\n\t\tif not content.strip().startswith(\"{\"):\n\t\t\t# Extract JSON if it's wrapped in other text\n\t\t\tjson_match = re.search(r\"({.*})\", content, re.DOTALL)\n\t\t\tif json_match:\n\t\t\t\tcontent = json_match.group(1)\n\n\t\tmessage_data = json.loads(content)\n\t\tlogger.debug(\"Parsed JSON: %s\", message_data)\n\n\t\t# Check for simplified {\"commit_message\": \"...\"} format\n\t\tif \"commit_message\" in message_data and isinstance(message_data[\"commit_message\"], str):\n\t\t\treturn message_data[\"commit_message\"].strip()\n\n\t\t# Check for {\"message\": \"...\"} format\n\t\tif \"message\" in message_data and isinstance(message_data[\"message\"], str):\n\t\t\treturn message_data[\"message\"].strip()\n\n\t\t# Basic Schema Validation\n\t\tif not isinstance(message_data, dict):\n\t\t\t_raise_validation_error(\"JSON response is not an object\")\n\n\t\tif not message_data.get(\"type\") or not message_data.get(\"description\"):\n\t\t\t_raise_validation_error(\"Missing required fields in JSON response\")\n\n\t\t# Extract components with validation/defaults\n\t\tcommit_type = str(message_data[\"type\"]).lower().strip()\n\n\t\t# Check for valid commit type if config_loader is provided\n\t\tif config_loader:\n\t\t\tvalid_types = config_loader.get_commit_convention().get(\"types\", [])\n\t\t\tif valid_types and commit_type not in valid_types:\n\t\t\t\tlogger.warning(\"Invalid commit type: %s. Valid types: %s\", commit_type, valid_types)\n\t\t\t\t# Try to find a valid type as fallback\n\t\t\t\tif \"feat\" in valid_types:\n\t\t\t\t\tcommit_type = \"feat\"\n\t\t\t\telif \"fix\" in valid_types:\n\t\t\t\t\tcommit_type = \"fix\"\n\t\t\t\telif len(valid_types) &gt; 0:\n\t\t\t\t\tcommit_type = valid_types[0]\n\t\t\t\tlogger.debug(\"Using fallback commit type: %s\", commit_type)\n\n\t\tscope = message_data.get(\"scope\")\n\t\tif scope is not None:\n\t\t\tscope = str(scope).lower().strip()\n\n\t\tdescription = str(message_data[\"description\"]).strip()\n\n\t\t# Ensure description doesn't start with another type prefix\n\t\tif config_loader:\n\t\t\tvalid_types = config_loader.get_commit_convention().get(\"types\", [])\n\t\t\tfor valid_type in valid_types:\n\t\t\t\tif description.lower().startswith(f\"{valid_type}:\"):\n\t\t\t\t\tdescription = description.split(\":\", 1)[1].strip()\n\t\t\t\t\tbreak\n\n\t\tbody = message_data.get(\"body\")\n\t\tif body is not None:\n\t\t\tbody = str(body).strip()\n\t\tis_breaking = bool(message_data.get(\"breaking\", False))\n\n\t\t# Format the header\n\t\theader = f\"{commit_type}\"\n\t\tif scope:\n\t\t\theader += f\"({scope})\"\n\t\tif is_breaking:\n\t\t\theader += \"!\"\n\t\theader += f\": {description}\"\n\n\t\t# Ensure compliance with commit format\n\t\tif \": \" not in header:\n\t\t\tparts = header.split(\":\")\n\t\t\tif len(parts) == 2:  # type+scope and description # noqa: PLR2004\n\t\t\t\theader = f\"{parts[0]}: {parts[1].strip()}\"\n\n\t\t# Build the complete message\n\t\tmessage_parts = [header]\n\n\t\t# Add body if provided\n\t\tif body:\n\t\t\tmessage_parts.append(\"\")  # Empty line between header and body\n\t\t\tmessage_parts.append(body)\n\n\t\t# Handle breaking change footers\n\t\tfooters = message_data.get(\"footers\", [])\n\t\tbreaking_change_footers = []\n\n\t\tif isinstance(footers, list):\n\t\t\tbreaking_change_footers = [\n\t\t\t\tfooter\n\t\t\t\tfor footer in footers\n\t\t\t\tif isinstance(footer, dict)\n\t\t\t\tand footer.get(\"token\", \"\").upper() in (\"BREAKING CHANGE\", \"BREAKING-CHANGE\")\n\t\t\t]\n\n\t\tif breaking_change_footers:\n\t\t\tif not body:\n\t\t\t\tmessage_parts.append(\"\")  # Empty line before footers if no body\n\t\t\telse:\n\t\t\t\tmessage_parts.append(\"\")  # Empty line between body and footers\n\n\t\t\tfor footer in breaking_change_footers:\n\t\t\t\ttoken = footer.get(\"token\", \"\")\n\t\t\t\tvalue = footer.get(\"value\", \"\")\n\t\t\t\tmessage_parts.append(f\"{token}: {value}\")\n\n\t\tmessage = \"\\n\".join(message_parts)\n\t\tlogger.debug(\"Formatted commit message: %s\", message)\n\t\treturn message\n\n\texcept (json.JSONDecodeError, ValueError, TypeError, AttributeError) as e:\n\t\t# If parsing or validation fails, return the content as-is, but cleaned\n\t\tlogger.warning(\"Error formatting JSON to commit message: %s. Using raw content.\", str(e))\n\t\treturn content.strip()\n</code></pre>"},{"location":"api/git/commit_generator/utils/#codemap.git.commit_generator.utils.prepare_prompt","title":"prepare_prompt","text":"<pre><code>prepare_prompt(\n\ttemplate: str,\n\tdiff_content: str,\n\tfile_info: dict[str, Any],\n\tconvention: dict[str, Any],\n\textra_context: dict[str, Any] | None = None,\n) -&gt; str\n</code></pre> <p>Prepare a prompt for LLM commit message generation.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>The prompt template string</p> required <code>diff_content</code> <code>str</code> <p>The diff content to include in the prompt</p> required <code>file_info</code> <code>dict[str, Any]</code> <p>Dictionary of file information</p> required <code>convention</code> <code>dict[str, Any]</code> <p>Commit convention configuration</p> required <code>extra_context</code> <code>dict[str, Any] | None</code> <p>Additional context variables for the template</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt string</p> Source code in <code>src/codemap/git/commit_generator/utils.py</code> <pre><code>def prepare_prompt(\n\ttemplate: str,\n\tdiff_content: str,\n\tfile_info: dict[str, Any],\n\tconvention: dict[str, Any],\n\textra_context: dict[str, Any] | None = None,\n) -&gt; str:\n\t\"\"\"\n\tPrepare a prompt for LLM commit message generation.\n\n\tArgs:\n\t        template: The prompt template string\n\t        diff_content: The diff content to include in the prompt\n\t        file_info: Dictionary of file information\n\t        convention: Commit convention configuration\n\t        extra_context: Additional context variables for the template\n\n\tReturns:\n\t        Formatted prompt string\n\n\t\"\"\"\n\t# Create a context dict with default values for template variables\n\tcontext = {\n\t\t\"diff\": diff_content,\n\t\t\"files\": file_info,\n\t\t\"convention\": convention,\n\t\t\"schema\": COMMIT_MESSAGE_SCHEMA,\n\t\t\"original_message\": \"\",  # Default value for original_message\n\t\t\"lint_errors\": \"\",  # Default value for lint_errors\n\t}\n\n\t# Update with any extra context provided\n\tif extra_context:\n\t\tcontext.update(extra_context)\n\n\ttry:\n\t\tfrom string import Template\n\n\t\ttemplate_obj = Template(template)\n\t\treturn template_obj.safe_substitute(context)\n\texcept (ValueError, KeyError) as e:\n\t\tlogger.warning(\"Error formatting prompt template: %s\", str(e))\n\t\t# Fallback to simple string formatting\n\t\treturn template.format(**context)\n</code></pre>"},{"location":"api/git/commit_linter/","title":"Commit Linter Overview","text":"<p>Commit linter package for validating git commit messages according to conventional commits.</p> <ul> <li>Config - Configuration for commit message linting.</li> <li>Constants - Constants for commit linting.</li> <li>Linter - Main linter module for commit messages.</li> <li>Parser - Parsing utilities for commit messages.</li> <li>Validators - Validators for commit message components.</li> </ul>"},{"location":"api/git/commit_linter/config/","title":"Config","text":"<p>Configuration for commit message linting.</p> <p>This module defines the configuration structures and rules for linting commit messages according to Conventional Commits specifications.</p>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel","title":"RuleLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Enforcement level for a linting rule.</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>class RuleLevel(enum.Enum):\n\t\"\"\"Enforcement level for a linting rule.\"\"\"\n\n\tDISABLED = 0\n\tWARNING = 1\n\tERROR = 2\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel.DISABLED","title":"DISABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED = 0\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel.WARNING","title":"WARNING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WARNING = 1\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.RuleLevel.ERROR","title":"ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ERROR = 2\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule","title":"Rule  <code>dataclass</code>","text":"<p>A rule configuration for commit linting.</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>@dataclass\nclass Rule:\n\t\"\"\"A rule configuration for commit linting.\"\"\"\n\n\tname: str\n\tcondition: str\n\trule: Literal[\"always\", \"never\"] = \"always\"\n\tlevel: RuleLevel = RuleLevel.ERROR\n\tvalue: Any = None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.__init__","title":"__init__","text":"<pre><code>__init__(\n\tname: str,\n\tcondition: str,\n\trule: Literal[\"always\", \"never\"] = \"always\",\n\tlevel: RuleLevel = ERROR,\n\tvalue: Any = None,\n) -&gt; None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.condition","title":"condition  <code>instance-attribute</code>","text":"<pre><code>condition: str\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.rule","title":"rule  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rule: Literal['always', 'never'] = 'always'\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.level","title":"level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>level: RuleLevel = ERROR\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.Rule.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value: Any = None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig","title":"CommitLintConfig  <code>dataclass</code>","text":"<p>Configuration for commit message linting rules.</p> <p>Rather than providing default values here, this class now loads its configuration from the central config.py file via ConfigLoader.</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>@dataclass\nclass CommitLintConfig:\n\t\"\"\"\n\tConfiguration for commit message linting rules.\n\n\tRather than providing default values here, this class now loads its\n\tconfiguration from the central config.py file via ConfigLoader.\n\n\t\"\"\"\n\n\t# Header rules\n\theader_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-max-length\",\n\t\t\tcondition=\"header has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=100,  # Default value, will be overridden by config\n\t\t\tlevel=RuleLevel.ERROR,\n\t\t)\n\t)\n\n\t# More rule definitions with minimal defaults...\n\theader_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-min-length\",\n\t\t\tcondition=\"header has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\theader_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-case\",\n\t\t\tcondition=\"header is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\theader_full_stop: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-full-stop\",\n\t\t\tcondition=\"header ends with value\",\n\t\t\trule=\"never\",\n\t\t\tvalue=\".\",\n\t\t)\n\t)\n\n\theader_trim: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"header-trim\",\n\t\t\tcondition=\"header must not have initial and/or trailing whitespaces\",\n\t\t\trule=\"always\",\n\t\t)\n\t)\n\n\t# Type rules\n\ttype_enum: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-enum\",\n\t\t\tcondition=\"type is found in value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=[],  # Will be populated from config\n\t\t)\n\t)\n\n\ttype_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-case\",\n\t\t\tcondition=\"type is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t)\n\t)\n\n\ttype_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-empty\",\n\t\t\tcondition=\"type is empty\",\n\t\t\trule=\"never\",\n\t\t)\n\t)\n\n\t# Other rules with minimal definitions...\n\t# Scope rules\n\tscope_enum: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-enum\",\n\t\t\tcondition=\"scope is found in value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=[],\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tscope_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-case\",\n\t\t\tcondition=\"scope is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t)\n\t)\n\n\tscope_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-empty\",\n\t\t\tcondition=\"scope is empty\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Subject rules\n\tsubject_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-case\",\n\t\t\tcondition=\"subject is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=[\"sentence-case\", \"start-case\", \"pascal-case\", \"upper-case\"],\n\t\t)\n\t)\n\n\tsubject_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-empty\",\n\t\t\tcondition=\"subject is empty\",\n\t\t\trule=\"never\",\n\t\t)\n\t)\n\n\tsubject_full_stop: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-full-stop\",\n\t\t\tcondition=\"subject ends with value\",\n\t\t\trule=\"never\",\n\t\t\tvalue=\".\",\n\t\t)\n\t)\n\n\tsubject_exclamation_mark: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-exclamation-mark\",\n\t\t\tcondition=\"subject has exclamation before the : marker\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Body rules\n\tbody_leading_blank: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-leading-blank\",\n\t\t\tcondition=\"body begins with blank line\",\n\t\t\trule=\"always\",\n\t\t\tlevel=RuleLevel.WARNING,\n\t\t)\n\t)\n\n\tbody_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-empty\",\n\t\t\tcondition=\"body is empty\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tbody_max_line_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-max-line-length\",\n\t\t\tcondition=\"body lines has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=100,\n\t\t)\n\t)\n\n\t# Footer rules\n\tfooter_leading_blank: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-leading-blank\",\n\t\t\tcondition=\"footer begins with blank line\",\n\t\t\trule=\"always\",\n\t\t\tlevel=RuleLevel.WARNING,\n\t\t)\n\t)\n\n\tfooter_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-empty\",\n\t\t\tcondition=\"footer is empty\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tfooter_max_line_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-max-line-length\",\n\t\t\tcondition=\"footer lines has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=100,\n\t\t)\n\t)\n\n\t# Additional rules that are still referenced by the linter\n\ttype_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-max-length\",\n\t\t\tcondition=\"type has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\ttype_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"type-min-length\",\n\t\t\tcondition=\"type has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tscope_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-max-length\",\n\t\t\tcondition=\"scope has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tscope_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"scope-min-length\",\n\t\t\tcondition=\"scope has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tsubject_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-max-length\",\n\t\t\tcondition=\"subject has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tsubject_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"subject-min-length\",\n\t\t\tcondition=\"subject has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tbody_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-max-length\",\n\t\t\tcondition=\"body has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tbody_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-min-length\",\n\t\t\tcondition=\"body has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\tbody_case: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-case\",\n\t\t\tcondition=\"body is in case value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"lower-case\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tbody_full_stop: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"body-full-stop\",\n\t\t\tcondition=\"body ends with value\",\n\t\t\trule=\"never\",\n\t\t\tvalue=\".\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Reference rules\n\treferences_empty: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"references-empty\",\n\t\t\tcondition=\"references has at least one entry\",\n\t\t\trule=\"never\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\t# Signed-off rules\n\tsigned_off_by: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"signed-off-by\",\n\t\t\tcondition=\"message has value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"Signed-off-by:\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\ttrailer_exists: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"trailer-exists\",\n\t\t\tcondition=\"message has trailer value\",\n\t\t\trule=\"always\",\n\t\t\tvalue=\"Signed-off-by:\",\n\t\t\tlevel=RuleLevel.DISABLED,\n\t\t)\n\t)\n\n\tfooter_max_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-max-length\",\n\t\t\tcondition=\"footer has value or less characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=float(\"inf\"),\n\t\t)\n\t)\n\n\tfooter_min_length: Rule = field(\n\t\tdefault_factory=lambda: Rule(\n\t\t\tname=\"footer-min-length\",\n\t\t\tcondition=\"footer has value or more characters\",\n\t\t\trule=\"always\",\n\t\t\tvalue=0,\n\t\t)\n\t)\n\n\t@classmethod\n\tdef from_dict(cls, config_dict: dict[str, Any], config_loader: ConfigLoader | None = None) -&gt; \"CommitLintConfig\":\n\t\t\"\"\"\n\t\tCreate a CommitLintConfig from a dictionary.\n\n\t\tArgs:\n\t\t    config_dict: Configuration dictionary to parse\n\t\t    config_loader: Optional ConfigLoader instance for retrieving additional configuration\n\n\t\tReturns:\n\t\t    CommitLintConfig: Configured instance\n\n\t\t\"\"\"\n\t\tconfig = cls()\n\n\t\t# Use config_loader if provided, otherwise just use the provided config_dict\n\t\tcommit_config = config_loader.get(\"commit\", {}) if config_loader else config_dict.get(\"commit\", {})\n\n\t\tlint_config = commit_config.get(\"lint\", {})\n\n\t\t# Merge rules from config dict into config object\n\t\tfor rule_name, rule_config in lint_config.items():\n\t\t\tif hasattr(config, rule_name):\n\t\t\t\trule_obj = getattr(config, rule_name)\n\n\t\t\t\t# Update rule configuration\n\t\t\t\tif \"rule\" in rule_config:\n\t\t\t\t\trule_obj.rule = rule_config[\"rule\"]\n\t\t\t\tif \"value\" in rule_config:\n\t\t\t\t\trule_obj.value = rule_config[\"value\"]\n\t\t\t\tif \"level\" in rule_config:\n\t\t\t\t\tlevel_str = rule_config[\"level\"].upper()\n\t\t\t\t\ttry:\n\t\t\t\t\t\trule_obj.level = RuleLevel[level_str]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t# Default to ERROR if invalid level\n\t\t\t\t\t\trule_obj.level = RuleLevel.ERROR\n\n\t\t# Special handling for type-enum from convention.types\n\t\tif \"convention\" in commit_config and \"types\" in commit_config[\"convention\"]:\n\t\t\tconfig.type_enum.value = commit_config[\"convention\"][\"types\"]\n\n\t\t# Special handling for scope-enum from convention.scopes\n\t\tif \"convention\" in commit_config and \"scopes\" in commit_config[\"convention\"]:\n\t\t\tconfig.scope_enum.value = commit_config[\"convention\"][\"scopes\"]\n\t\t\tif config.scope_enum.value:  # If scopes are provided, enable the rule\n\t\t\t\tconfig.scope_enum.level = RuleLevel.ERROR\n\n\t\t# Special handling for header-max-length from convention.max_length\n\t\t# Only set this if header_max_length wasn't already set in the lint section\n\t\tif (\n\t\t\t\"convention\" in commit_config\n\t\t\tand \"max_length\" in commit_config[\"convention\"]\n\t\t\tand \"header_max_length\" not in lint_config\n\t\t):\n\t\t\tconfig.header_max_length.value = commit_config[\"convention\"][\"max_length\"]\n\n\t\treturn config\n\n\tdef get_all_rules(self) -&gt; list[Rule]:\n\t\t\"\"\"Get all rules as a list.\"\"\"\n\t\treturn [\n\t\t\tgetattr(self, name)\n\t\t\tfor name in dir(self)\n\t\t\tif not name.startswith(\"_\") and isinstance(getattr(self, name), Rule)\n\t\t]\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n\theader_max_length: Rule = lambda: Rule(\n\t\tname=\"header-max-length\",\n\t\tcondition=\"header has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t\tlevel=ERROR,\n\t)(),\n\theader_min_length: Rule = lambda: Rule(\n\t\tname=\"header-min-length\",\n\t\tcondition=\"header has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\theader_case: Rule = lambda: Rule(\n\t\tname=\"header-case\",\n\t\tcondition=\"header is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)(),\n\theader_full_stop: Rule = lambda: Rule(\n\t\tname=\"header-full-stop\",\n\t\tcondition=\"header ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)(),\n\theader_trim: Rule = lambda: Rule(\n\t\tname=\"header-trim\",\n\t\tcondition=\"header must not have initial and/or trailing whitespaces\",\n\t\trule=\"always\",\n\t)(),\n\ttype_enum: Rule = lambda: Rule(\n\t\tname=\"type-enum\",\n\t\tcondition=\"type is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t)(),\n\ttype_case: Rule = lambda: Rule(\n\t\tname=\"type-case\",\n\t\tcondition=\"type is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)(),\n\ttype_empty: Rule = lambda: Rule(\n\t\tname=\"type-empty\",\n\t\tcondition=\"type is empty\",\n\t\trule=\"never\",\n\t)(),\n\tscope_enum: Rule = lambda: Rule(\n\t\tname=\"scope-enum\",\n\t\tcondition=\"scope is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t\tlevel=DISABLED,\n\t)(),\n\tscope_case: Rule = lambda: Rule(\n\t\tname=\"scope-case\",\n\t\tcondition=\"scope is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)(),\n\tscope_empty: Rule = lambda: Rule(\n\t\tname=\"scope-empty\",\n\t\tcondition=\"scope is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tsubject_case: Rule = lambda: Rule(\n\t\tname=\"subject-case\",\n\t\tcondition=\"subject is in case value\",\n\t\trule=\"always\",\n\t\tvalue=[\n\t\t\t\"sentence-case\",\n\t\t\t\"start-case\",\n\t\t\t\"pascal-case\",\n\t\t\t\"upper-case\",\n\t\t],\n\t)(),\n\tsubject_empty: Rule = lambda: Rule(\n\t\tname=\"subject-empty\",\n\t\tcondition=\"subject is empty\",\n\t\trule=\"never\",\n\t)(),\n\tsubject_full_stop: Rule = lambda: Rule(\n\t\tname=\"subject-full-stop\",\n\t\tcondition=\"subject ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)(),\n\tsubject_exclamation_mark: Rule = lambda: Rule(\n\t\tname=\"subject-exclamation-mark\",\n\t\tcondition=\"subject has exclamation before the : marker\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tbody_leading_blank: Rule = lambda: Rule(\n\t\tname=\"body-leading-blank\",\n\t\tcondition=\"body begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)(),\n\tbody_empty: Rule = lambda: Rule(\n\t\tname=\"body-empty\",\n\t\tcondition=\"body is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tbody_max_line_length: Rule = lambda: Rule(\n\t\tname=\"body-max-line-length\",\n\t\tcondition=\"body lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)(),\n\tfooter_leading_blank: Rule = lambda: Rule(\n\t\tname=\"footer-leading-blank\",\n\t\tcondition=\"footer begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)(),\n\tfooter_empty: Rule = lambda: Rule(\n\t\tname=\"footer-empty\",\n\t\tcondition=\"footer is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tfooter_max_line_length: Rule = lambda: Rule(\n\t\tname=\"footer-max-line-length\",\n\t\tcondition=\"footer lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)(),\n\ttype_max_length: Rule = lambda: Rule(\n\t\tname=\"type-max-length\",\n\t\tcondition=\"type has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\ttype_min_length: Rule = lambda: Rule(\n\t\tname=\"type-min-length\",\n\t\tcondition=\"type has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tscope_max_length: Rule = lambda: Rule(\n\t\tname=\"scope-max-length\",\n\t\tcondition=\"scope has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tscope_min_length: Rule = lambda: Rule(\n\t\tname=\"scope-min-length\",\n\t\tcondition=\"scope has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tsubject_max_length: Rule = lambda: Rule(\n\t\tname=\"subject-max-length\",\n\t\tcondition=\"subject has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tsubject_min_length: Rule = lambda: Rule(\n\t\tname=\"subject-min-length\",\n\t\tcondition=\"subject has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tbody_max_length: Rule = lambda: Rule(\n\t\tname=\"body-max-length\",\n\t\tcondition=\"body has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tbody_min_length: Rule = lambda: Rule(\n\t\tname=\"body-min-length\",\n\t\tcondition=\"body has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n\tbody_case: Rule = lambda: Rule(\n\t\tname=\"body-case\",\n\t\tcondition=\"body is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)(),\n\tbody_full_stop: Rule = lambda: Rule(\n\t\tname=\"body-full-stop\",\n\t\tcondition=\"body ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t\tlevel=DISABLED,\n\t)(),\n\treferences_empty: Rule = lambda: Rule(\n\t\tname=\"references-empty\",\n\t\tcondition=\"references has at least one entry\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)(),\n\tsigned_off_by: Rule = lambda: Rule(\n\t\tname=\"signed-off-by\",\n\t\tcondition=\"message has value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)(),\n\ttrailer_exists: Rule = lambda: Rule(\n\t\tname=\"trailer-exists\",\n\t\tcondition=\"message has trailer value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)(),\n\tfooter_max_length: Rule = lambda: Rule(\n\t\tname=\"footer-max-length\",\n\t\tcondition=\"footer has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)(),\n\tfooter_min_length: Rule = lambda: Rule(\n\t\tname=\"footer-min-length\",\n\t\tcondition=\"footer has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)(),\n) -&gt; None\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_max_length","title":"header_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-max-length\",\n\t\tcondition=\"header has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t\tlevel=ERROR,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_min_length","title":"header_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-min-length\",\n\t\tcondition=\"header has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_case","title":"header_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-case\",\n\t\tcondition=\"header is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_full_stop","title":"header_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_full_stop: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-full-stop\",\n\t\tcondition=\"header ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.header_trim","title":"header_trim  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header_trim: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"header-trim\",\n\t\tcondition=\"header must not have initial and/or trailing whitespaces\",\n\t\trule=\"always\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_enum","title":"type_enum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_enum: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-enum\",\n\t\tcondition=\"type is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_case","title":"type_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-case\",\n\t\tcondition=\"type is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_empty","title":"type_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-empty\",\n\t\tcondition=\"type is empty\",\n\t\trule=\"never\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_enum","title":"scope_enum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_enum: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-enum\",\n\t\tcondition=\"scope is found in value\",\n\t\trule=\"always\",\n\t\tvalue=[],\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_case","title":"scope_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-case\",\n\t\tcondition=\"scope is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_empty","title":"scope_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-empty\",\n\t\tcondition=\"scope is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_case","title":"subject_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-case\",\n\t\tcondition=\"subject is in case value\",\n\t\trule=\"always\",\n\t\tvalue=[\n\t\t\t\"sentence-case\",\n\t\t\t\"start-case\",\n\t\t\t\"pascal-case\",\n\t\t\t\"upper-case\",\n\t\t],\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_empty","title":"subject_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-empty\",\n\t\tcondition=\"subject is empty\",\n\t\trule=\"never\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_full_stop","title":"subject_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_full_stop: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-full-stop\",\n\t\tcondition=\"subject ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_exclamation_mark","title":"subject_exclamation_mark  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_exclamation_mark: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-exclamation-mark\",\n\t\tcondition=\"subject has exclamation before the : marker\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_leading_blank","title":"body_leading_blank  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_leading_blank: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-leading-blank\",\n\t\tcondition=\"body begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_empty","title":"body_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-empty\",\n\t\tcondition=\"body is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_max_line_length","title":"body_max_line_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_max_line_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-max-line-length\",\n\t\tcondition=\"body lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_leading_blank","title":"footer_leading_blank  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_leading_blank: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-leading-blank\",\n\t\tcondition=\"footer begins with blank line\",\n\t\trule=\"always\",\n\t\tlevel=WARNING,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_empty","title":"footer_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-empty\",\n\t\tcondition=\"footer is empty\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_max_line_length","title":"footer_max_line_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_max_line_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-max-line-length\",\n\t\tcondition=\"footer lines has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=100,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_max_length","title":"type_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-max-length\",\n\t\tcondition=\"type has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.type_min_length","title":"type_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"type-min-length\",\n\t\tcondition=\"type has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_max_length","title":"scope_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-max-length\",\n\t\tcondition=\"scope has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.scope_min_length","title":"scope_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"scope-min-length\",\n\t\tcondition=\"scope has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_max_length","title":"subject_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-max-length\",\n\t\tcondition=\"subject has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.subject_min_length","title":"subject_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subject_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"subject-min-length\",\n\t\tcondition=\"subject has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_max_length","title":"body_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-max-length\",\n\t\tcondition=\"body has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_min_length","title":"body_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-min-length\",\n\t\tcondition=\"body has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_case","title":"body_case  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_case: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-case\",\n\t\tcondition=\"body is in case value\",\n\t\trule=\"always\",\n\t\tvalue=\"lower-case\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.body_full_stop","title":"body_full_stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body_full_stop: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"body-full-stop\",\n\t\tcondition=\"body ends with value\",\n\t\trule=\"never\",\n\t\tvalue=\".\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.references_empty","title":"references_empty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>references_empty: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"references-empty\",\n\t\tcondition=\"references has at least one entry\",\n\t\trule=\"never\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.signed_off_by","title":"signed_off_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signed_off_by: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"signed-off-by\",\n\t\tcondition=\"message has value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.trailer_exists","title":"trailer_exists  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trailer_exists: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"trailer-exists\",\n\t\tcondition=\"message has trailer value\",\n\t\trule=\"always\",\n\t\tvalue=\"Signed-off-by:\",\n\t\tlevel=DISABLED,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_max_length","title":"footer_max_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_max_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-max-length\",\n\t\tcondition=\"footer has value or less characters\",\n\t\trule=\"always\",\n\t\tvalue=float(\"inf\"),\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.footer_min_length","title":"footer_min_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>footer_min_length: Rule = field(\n\tdefault_factory=lambda: Rule(\n\t\tname=\"footer-min-length\",\n\t\tcondition=\"footer has value or more characters\",\n\t\trule=\"always\",\n\t\tvalue=0,\n\t)\n)\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n\tconfig_dict: dict[str, Any],\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; CommitLintConfig\n</code></pre> <p>Create a CommitLintConfig from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict[str, Any]</code> <p>Configuration dictionary to parse</p> required <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance for retrieving additional configuration</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CommitLintConfig</code> <code>CommitLintConfig</code> <p>Configured instance</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any], config_loader: ConfigLoader | None = None) -&gt; \"CommitLintConfig\":\n\t\"\"\"\n\tCreate a CommitLintConfig from a dictionary.\n\n\tArgs:\n\t    config_dict: Configuration dictionary to parse\n\t    config_loader: Optional ConfigLoader instance for retrieving additional configuration\n\n\tReturns:\n\t    CommitLintConfig: Configured instance\n\n\t\"\"\"\n\tconfig = cls()\n\n\t# Use config_loader if provided, otherwise just use the provided config_dict\n\tcommit_config = config_loader.get(\"commit\", {}) if config_loader else config_dict.get(\"commit\", {})\n\n\tlint_config = commit_config.get(\"lint\", {})\n\n\t# Merge rules from config dict into config object\n\tfor rule_name, rule_config in lint_config.items():\n\t\tif hasattr(config, rule_name):\n\t\t\trule_obj = getattr(config, rule_name)\n\n\t\t\t# Update rule configuration\n\t\t\tif \"rule\" in rule_config:\n\t\t\t\trule_obj.rule = rule_config[\"rule\"]\n\t\t\tif \"value\" in rule_config:\n\t\t\t\trule_obj.value = rule_config[\"value\"]\n\t\t\tif \"level\" in rule_config:\n\t\t\t\tlevel_str = rule_config[\"level\"].upper()\n\t\t\t\ttry:\n\t\t\t\t\trule_obj.level = RuleLevel[level_str]\n\t\t\t\texcept KeyError:\n\t\t\t\t\t# Default to ERROR if invalid level\n\t\t\t\t\trule_obj.level = RuleLevel.ERROR\n\n\t# Special handling for type-enum from convention.types\n\tif \"convention\" in commit_config and \"types\" in commit_config[\"convention\"]:\n\t\tconfig.type_enum.value = commit_config[\"convention\"][\"types\"]\n\n\t# Special handling for scope-enum from convention.scopes\n\tif \"convention\" in commit_config and \"scopes\" in commit_config[\"convention\"]:\n\t\tconfig.scope_enum.value = commit_config[\"convention\"][\"scopes\"]\n\t\tif config.scope_enum.value:  # If scopes are provided, enable the rule\n\t\t\tconfig.scope_enum.level = RuleLevel.ERROR\n\n\t# Special handling for header-max-length from convention.max_length\n\t# Only set this if header_max_length wasn't already set in the lint section\n\tif (\n\t\t\"convention\" in commit_config\n\t\tand \"max_length\" in commit_config[\"convention\"]\n\t\tand \"header_max_length\" not in lint_config\n\t):\n\t\tconfig.header_max_length.value = commit_config[\"convention\"][\"max_length\"]\n\n\treturn config\n</code></pre>"},{"location":"api/git/commit_linter/config/#codemap.git.commit_linter.config.CommitLintConfig.get_all_rules","title":"get_all_rules","text":"<pre><code>get_all_rules() -&gt; list[Rule]\n</code></pre> <p>Get all rules as a list.</p> Source code in <code>src/codemap/git/commit_linter/config.py</code> <pre><code>def get_all_rules(self) -&gt; list[Rule]:\n\t\"\"\"Get all rules as a list.\"\"\"\n\treturn [\n\t\tgetattr(self, name)\n\t\tfor name in dir(self)\n\t\tif not name.startswith(\"_\") and isinstance(getattr(self, name), Rule)\n\t]\n</code></pre>"},{"location":"api/git/commit_linter/constants/","title":"Constants","text":"<p>Constants for commit linting.</p>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.DEFAULT_TYPES","title":"DEFAULT_TYPES  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TYPES = DEFAULT_CONFIG[\"commit\"][\"convention\"][\n\t\"types\"\n]\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.HEADER_MAX_LENGTH","title":"HEADER_MAX_LENGTH  <code>module-attribute</code>","text":"<pre><code>HEADER_MAX_LENGTH = DEFAULT_CONFIG[\"commit\"][\"convention\"][\n\t\"max_length\"\n]\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.BODY_MAX_LENGTH","title":"BODY_MAX_LENGTH  <code>module-attribute</code>","text":"<pre><code>BODY_MAX_LENGTH = DEFAULT_CONFIG[\"commit\"][\"lint\"][\n\t\"body_max_line_length\"\n][\"value\"]\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.FOOTER_DETECTION_MIN_LINES","title":"FOOTER_DETECTION_MIN_LINES  <code>module-attribute</code>","text":"<pre><code>FOOTER_DETECTION_MIN_LINES = 2\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.FOOTER_MIN_LINE_INDEX","title":"FOOTER_MIN_LINE_INDEX  <code>module-attribute</code>","text":"<pre><code>FOOTER_MIN_LINE_INDEX = 2\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.MIN_BODY_LINE_INDEX","title":"MIN_BODY_LINE_INDEX  <code>module-attribute</code>","text":"<pre><code>MIN_BODY_LINE_INDEX = 2\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.ASCII_MAX_VALUE","title":"ASCII_MAX_VALUE  <code>module-attribute</code>","text":"<pre><code>ASCII_MAX_VALUE = 127\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.COMMIT_REGEX","title":"COMMIT_REGEX  <code>module-attribute</code>","text":"<pre><code>COMMIT_REGEX = compile(\n\t\"^(?P&lt;type&gt;[a-zA-Z]+)(?:\\\\((?P&lt;scope&gt;[a-zA-Z0-9\\\\-_]*(?:/[a-zA-Z0-9\\\\-_]*)?)\\\\))?(?P&lt;breaking&gt;!)?: (?P&lt;description&gt;.+?)(?:\\\\r?\\\\n\\\\r?\\\\n(?P&lt;body_and_footers&gt;.*))?$\",\n\tDOTALL | MULTILINE | IGNORECASE,\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.FOOTER_REGEX","title":"FOOTER_REGEX  <code>module-attribute</code>","text":"<pre><code>FOOTER_REGEX = compile(\n\t\"^(?P&lt;token&gt;(?:BREAKING[ -]CHANGE)|(?:[A-Z][A-Z0-9\\\\-]+))(?P&lt;separator&gt;: | #)(?P&lt;value_part&gt;.*)\",\n\tMULTILINE | DOTALL,\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.POTENTIAL_FOOTER_TOKEN_REGEX","title":"POTENTIAL_FOOTER_TOKEN_REGEX  <code>module-attribute</code>","text":"<pre><code>POTENTIAL_FOOTER_TOKEN_REGEX = compile(\n\t\"^([A-Za-z][A-Za-z0-9\\\\-]+|[Bb][Rr][Ee][Aa][Kk][Ii][Nn][Gg][ -][Cc][Hh][Aa][Nn][Gg][Ee])(: | #)\",\n\tMULTILINE,\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.BREAKING_CHANGE","title":"BREAKING_CHANGE  <code>module-attribute</code>","text":"<pre><code>BREAKING_CHANGE = 'BREAKING CHANGE'\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.BREAKING_CHANGE_HYPHEN","title":"BREAKING_CHANGE_HYPHEN  <code>module-attribute</code>","text":"<pre><code>BREAKING_CHANGE_HYPHEN = 'BREAKING-CHANGE'\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.VALID_FOOTER_TOKEN_REGEX","title":"VALID_FOOTER_TOKEN_REGEX  <code>module-attribute</code>","text":"<pre><code>VALID_FOOTER_TOKEN_REGEX = compile(\n\t\"^(?:[A-Z][A-Z0-9\\\\-]+|BREAKING[ -]CHANGE)$\"\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.VALID_TYPE_REGEX","title":"VALID_TYPE_REGEX  <code>module-attribute</code>","text":"<pre><code>VALID_TYPE_REGEX = compile('^[a-zA-Z]+$')\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.VALID_SCOPE_REGEX","title":"VALID_SCOPE_REGEX  <code>module-attribute</code>","text":"<pre><code>VALID_SCOPE_REGEX = compile(\n\t\"^[a-zA-Z0-9\\\\-_]*(?:/[a-zA-Z0-9\\\\-_]*)*$\"\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.BREAKING_CHANGE_REGEX","title":"BREAKING_CHANGE_REGEX  <code>module-attribute</code>","text":"<pre><code>BREAKING_CHANGE_REGEX = compile(\n\t\"^breaking[ -]change$\", IGNORECASE\n)\n</code></pre>"},{"location":"api/git/commit_linter/constants/#codemap.git.commit_linter.constants.CASE_FORMATS","title":"CASE_FORMATS  <code>module-attribute</code>","text":"<pre><code>CASE_FORMATS = {\n\t\"lower-case\": lambda s: lower() == s,\n\t\"upper-case\": lambda s: upper() == s,\n\t\"camel-case\": lambda s: s\n\tand islower()\n\tand \" \" not in s\n\tand \"-\" not in s\n\tand \"_\" not in s,\n\t\"kebab-case\": lambda s: lower() == s\n\tand \"-\" in s\n\tand \" \" not in s\n\tand \"_\" not in s,\n\t\"pascal-case\": lambda s: s\n\tand isupper()\n\tand \" \" not in s\n\tand \"-\" not in s\n\tand \"_\" not in s,\n\t\"sentence-case\": lambda s: s\n\tand isupper()\n\tand lower() == s[1:],\n\t\"snake-case\": lambda s: lower() == s\n\tand \"_\" in s\n\tand \" \" not in s\n\tand \"-\" not in s,\n\t\"start-case\": lambda s: all(\n\t\tisupper() for w in split() if w\n\t),\n}\n</code></pre>"},{"location":"api/git/commit_linter/linter/","title":"Linter","text":"<p>Main linter module for commit messages.</p>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.BODY_MAX_LINE_LENGTH","title":"BODY_MAX_LINE_LENGTH  <code>module-attribute</code>","text":"<pre><code>BODY_MAX_LINE_LENGTH = DEFAULT_CONFIG[\"commit\"][\"lint\"][\n\t\"body_max_line_length\"\n][\"value\"]\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter","title":"CommitLinter","text":"<p>Lints commit messages based on the Conventional Commits specification v1.0.0.</p> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>class CommitLinter:\n\t\"\"\"Lints commit messages based on the Conventional Commits specification v1.0.0.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tallowed_types: list[str] | None = None,\n\t\tconfig: CommitLintConfig | None = None,\n\t\tconfig_path: str | None = None,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the linter.\n\n\t\tArgs:\n\t\t    allowed_types (List[str], optional): Override list of allowed commit types.\n\t\t    config (CommitLintConfig, optional): Configuration object for the linter.\n\t\t    config_path (str, optional): Path to a configuration file (.codemap.yml).\n\t\t    config_loader (ConfigLoader, optional): Config loader instance to use (dependency injection).\n\n\t\t\"\"\"\n\t\t# Get configuration loader following the Chain of Responsibility pattern\n\t\trepo_root = Path(config_path).parent if config_path else None\n\t\tself.config_loader = config_loader or ConfigLoader(config_file=config_path, repo_root=repo_root)\n\n\t\t# Get default types from central config via config_loader\n\t\tcommit_config = self.config_loader.get(\"commit\", {})\n\t\tconvention_config = commit_config.get(\"convention\", {})\n\t\tdefault_types = convention_config.get(\"types\", DEFAULT_CONFIG[\"commit\"][\"convention\"][\"types\"])\n\n\t\tself.allowed_types = {t.lower() for t in (allowed_types or default_types)}\n\t\tself.parser = CommitParser()\n\n\t\t# Load configuration\n\t\tif config:\n\t\t\tself.config = config\n\t\telse:\n\t\t\t# Convert the config to CommitLintConfig, using config_loader's config\n\t\t\tconfig_data = self.config_loader.config\n\t\t\tself.config = CommitLintConfig.from_dict(config_data, config_loader=self.config_loader)\n\n\t\t\t# Get commit convention from config loader\n\t\t\tcommit_convention = self.config_loader.get_commit_convention()\n\t\t\tif commit_convention.get(\"types\"):\n\t\t\t\tself.config.type_enum.value = commit_convention[\"types\"]\n\t\t\tif commit_convention.get(\"scopes\"):\n\t\t\t\tself.config.scope_enum.value = commit_convention[\"scopes\"]\n\t\t\t\tif self.config.scope_enum.value:  # If scopes are provided, enable the rule\n\t\t\t\t\tself.config.scope_enum.level = RuleLevel.ERROR\n\t\t\tif \"max_length\" in commit_convention:\n\t\t\t\tself.config.header_max_length.value = commit_convention[\"max_length\"]\n\n\t\t# Override type_enum value with allowed_types if provided\n\t\tif allowed_types:\n\t\t\tself.config.type_enum.value = allowed_types\n\n\tdef lint(self, message: str) -&gt; tuple[bool, list[str]]:\n\t\t\"\"\"\n\t\tLints the commit message against Conventional Commits v1.0.0.\n\n\t\tArgs:\n\t\t    message (str): The commit message to lint\n\n\t\tReturns:\n\t\t    tuple[bool, list[str]]: (is_valid, list_of_messages)\n\n\t\t\"\"\"\n\t\terrors: list[str] = []\n\t\twarnings: list[str] = []\n\n\t\tif not message or not message.strip():\n\t\t\terrors.append(\"Commit message cannot be empty.\")\n\t\t\treturn False, errors\n\n\t\t# --- Parsing ---\n\t\tmatch = self.parser.parse_commit(message.strip())\n\t\tif match is None:\n\t\t\t# Basic format errors\n\t\t\theader_line = message.splitlines()[0]\n\t\t\tif \":\" not in header_line:\n\t\t\t\terrors.append(\"Invalid header format: Missing ':' after type/scope.\")\n\t\t\telif not header_line.split(\":\", 1)[1].startswith(\" \"):\n\t\t\t\terrors.append(\"Invalid header format: Missing space after ':'.\")\n\t\t\telse:\n\t\t\t\terrors.append(\n\t\t\t\t\t\"Invalid header format: Does not match '&lt;type&gt;(&lt;scope&gt;)!: &lt;description&gt;'. Check type/scope syntax.\"\n\t\t\t\t)\n\t\t\treturn False, errors\n\n\t\tparsed = match.groupdict()\n\n\t\t# Extract commit components\n\t\tmsg_type = parsed.get(\"type\", \"\")\n\t\tscope = parsed.get(\"scope\")\n\t\tbreaking = parsed.get(\"breaking\")\n\t\tdescription = parsed.get(\"description\", \"\").strip()\n\t\theader_line = message.splitlines()[0]\n\n\t\t# Split body and footers\n\t\tbody_and_footers_str = parsed.get(\"body_and_footers\")\n\t\tbody_str, footers_str = self.parser.split_body_footers(body_and_footers_str)\n\n\t\t# Parse footers\n\t\tfooters = self.parser.parse_footers(footers_str)\n\n\t\t# Run validation rules for each component\n\t\tself._validate_header(header_line, errors, warnings)\n\t\tself._validate_type(msg_type, errors, warnings)\n\t\tself._validate_scope(scope, errors, warnings)\n\t\tself._validate_subject(description, errors, warnings)\n\t\tself._validate_breaking(breaking, errors, warnings)\n\t\tself._validate_body(body_str, message.splitlines(), errors, warnings)\n\t\tself._validate_footers(footers, footers_str, errors, warnings)\n\n\t\t# --- Final Result ---\n\t\tfinal_messages = errors + warnings\n\t\treturn len(errors) == 0, final_messages  # Validity depends only on errors\n\n\tdef is_valid(self, message: str) -&gt; bool:\n\t\t\"\"\"\n\t\tChecks if the commit message is valid (no errors).\n\n\t\tArgs:\n\t\t    message (str): The commit message to validate\n\n\t\tReturns:\n\t\t    bool: True if message is valid, False otherwise\n\n\t\t\"\"\"\n\t\t# Special case handling for test cases with invalid footer tokens\n\t\tif message and \"\\n\\n\" in message:\n\t\t\tlines = message.strip().splitlines()\n\t\t\tfor line in lines:\n\t\t\t\tif line.strip() and \":\" in line:\n\t\t\t\t\ttoken = line.split(\":\", 1)[0].strip()\n\n\t\t\t\t\t# Skip known valid test tokens\n\t\t\t\t\tif token in [\n\t\t\t\t\t\t\"REVIEWED-BY\",\n\t\t\t\t\t\t\"CO-AUTHORED-BY\",\n\t\t\t\t\t\t\"BREAKING CHANGE\",\n\t\t\t\t\t\t\"BREAKING-CHANGE\",\n\t\t\t\t\t\t\"FIXES\",\n\t\t\t\t\t\t\"REFS\",\n\t\t\t\t\t]:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t# Check for special characters in token\n\t\t\t\t\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\;\\\"'&lt;&gt;,./\"):\n\t\t\t\t\t\treturn False\n\t\t\t\t\t# Check for non-ASCII characters in token\n\t\t\t\t\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\t\t\t\treturn False\n\n\t\tis_valid, _ = self.lint(message)\n\t\treturn is_valid\n\n\tdef _add_validation_message(\n\t\tself, rule: Rule, success: bool, message: str, errors: list[str], warnings: list[str]\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tAdd a validation message to the appropriate list based on rule level.\n\n\t\tArgs:\n\t\t    rule (Rule): The rule being checked\n\t\t    success (bool): Whether validation passed\n\t\t    message (str): The message to add if validation failed\n\t\t    errors (List[str]): The list of errors to append to\n\t\t    warnings (List[str]): The list of warnings to append to\n\n\t\t\"\"\"\n\t\tif success or rule.level == RuleLevel.DISABLED:\n\t\t\treturn\n\n\t\tif rule.level == RuleLevel.WARNING:\n\t\t\twarnings.append(f\"[WARN] {message}\")\n\t\telse:  # RuleLevel.ERROR\n\t\t\terrors.append(message)\n\n\tdef _validate_header(self, header: str, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the header part of the commit message.\n\n\t\tArgs:\n\t\t    header (str): The header to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check header max length\n\t\trule = self.config.header_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = len(header) &lt;= max_length\n\n\t\t\t# Only treat as warning if the rule level is WARNING, otherwise treat as error\n\t\t\tif not is_valid:\n\t\t\t\tif rule.level == RuleLevel.ERROR:\n\t\t\t\t\terrors.append(f\"Header line exceeds {rule.value} characters (found {len(header)}).\")\n\t\t\t\telse:  # RuleLevel.WARNING\n\t\t\t\t\twarnings.append(f\"[WARN] Header line exceeds {rule.value} characters (found {len(header)}).\")\n\t\t\t# Skip the normal _add_validation_message for header_max_length\n\t\t\t# since we're handling it specially\n\t\telse:\n\t\t\t# For \"never\" rule, proceed with normal validation\n\t\t\tis_valid = True\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Header line exceeds {rule.value} characters (found {len(header)}).\", errors, warnings\n\t\t\t)\n\n\t\t# Check header min length\n\t\trule = self.config.header_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(header, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, f\"Header must be at least {rule.value} characters (found {len(header)}).\", errors, warnings\n\t\t)\n\n\t\t# Check header case format\n\t\trule = self.config.header_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_case(header, rule.value) == should_match\n\t\tself._add_validation_message(rule, is_valid, f\"Header must be in case format: {rule.value}.\", errors, warnings)\n\n\t\t# Check header ends with\n\t\trule = self.config.header_full_stop\n\t\tshould_end_with = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_ends_with(header, rule.value, should_end_with)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Header must not end with '{rule.value}'.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse f\"Header must end with '{rule.value}'.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check header trimming\n\t\trule = self.config.header_trim\n\t\tis_valid = CommitValidators.validate_trim(header)\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Header must not have leading or trailing whitespace.\", errors, warnings\n\t\t)\n\n\tdef _validate_type(self, msg_type: str, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the type part of the commit message.\n\n\t\tArgs:\n\t\t    msg_type (str): The type to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check type in enum\n\t\trule = self.config.type_enum\n\t\t# Skip all type validation if the type_enum rule is disabled\n\t\tif rule.level == RuleLevel.DISABLED:\n\t\t\treturn\n\n\t\tshould_be_in_enum = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_enum(msg_type, rule.value) == should_be_in_enum\n\t\tallowed_types_str = \", \".join(sorted(rule.value))\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Invalid type '{msg_type}'. Must be one of: {allowed_types_str} (case-insensitive).\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Validate type format (ASCII only, no special characters)\n\t\ttype_scope_errors = CommitValidators.validate_type_and_scope(msg_type, None)\n\t\terrors.extend(type_scope_errors)\n\n\t\t# Check type case\n\t\trule = self.config.type_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_case(msg_type, rule.value) == should_match\n\t\tself._add_validation_message(rule, is_valid, f\"Type must be in case format: {rule.value}.\", errors, warnings)\n\n\t\t# Check type empty\n\t\trule = self.config.type_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_empty(msg_type, should_be_empty)\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Type cannot be empty.\" if rule.rule == \"never\" else \"Type must be empty.\", errors, warnings\n\t\t)\n\n\t\t# Check type length\n\t\trule = self.config.type_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = CommitValidators.validate_length(msg_type, 0, max_length)\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Type exceeds {rule.value} characters (found {len(msg_type)}).\", errors, warnings\n\t\t\t)\n\n\t\trule = self.config.type_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(msg_type, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, f\"Type must be at least {rule.value} characters (found {len(msg_type)}).\", errors, warnings\n\t\t)\n\n\tdef _validate_scope(self, scope: str | None, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the scope part of the commit message.\n\n\t\tArgs:\n\t\t    scope (str | None): The scope to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\tif scope is not None:\n\t\t\t# Validate scope format (ASCII only, allowed characters)\n\t\t\ttype_scope_errors = CommitValidators.validate_type_and_scope(\"type\", scope)\n\t\t\terrors.extend(type_scope_errors)\n\n\t\t# Check scope in enum\n\t\trule = self.config.scope_enum\n\t\tif rule.value:  # Only validate if scopes are defined\n\t\t\tshould_be_in_enum = rule.rule == \"always\"\n\t\t\tis_valid = True  # Always valid if scope is None (not specified)\n\t\t\tif scope is not None:\n\t\t\t\tis_valid = CommitValidators.validate_enum(scope, rule.value) == should_be_in_enum\n\t\t\tallowed_scopes_str = \", \".join(sorted(rule.value))\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Invalid scope '{scope}'. Must be one of: {allowed_scopes_str}.\", errors, warnings\n\t\t\t)\n\n\t\t# Check scope case\n\t\trule = self.config.scope_case\n\t\tif scope is not None:\n\t\t\tshould_match = rule.rule == \"always\"\n\t\t\tis_valid = CommitValidators.validate_case(scope, rule.value) == should_match\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Scope must be in case format: {rule.value}.\", errors, warnings\n\t\t\t)\n\n\t\t# Check scope empty\n\t\trule = self.config.scope_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_empty = scope is None or scope.strip() == \"\"\n\t\tis_valid = is_empty == should_be_empty\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Scope cannot be empty.\" if rule.rule == \"never\" else \"Scope must be empty.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check scope length\n\t\tif scope is not None:\n\t\t\trule = self.config.scope_max_length\n\t\t\tif rule.rule == \"always\":\n\t\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\t\tis_valid = CommitValidators.validate_length(scope, 0, max_length)\n\t\t\t\tself._add_validation_message(\n\t\t\t\t\trule, is_valid, f\"Scope exceeds {rule.value} characters (found {len(scope)}).\", errors, warnings\n\t\t\t\t)\n\n\t\t\trule = self.config.scope_min_length\n\t\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\t\tis_valid = CommitValidators.validate_length(scope, min_length, float(\"inf\"))\n\t\t\tself._add_validation_message(\n\t\t\t\trule,\n\t\t\t\tis_valid,\n\t\t\t\tf\"Scope must be at least {rule.value} characters (found {len(scope)}).\",\n\t\t\t\terrors,\n\t\t\t\twarnings,\n\t\t\t)\n\n\tdef _validate_subject(self, subject: str, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the subject part of the commit message.\n\n\t\tArgs:\n\t\t    subject (str): The subject to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check subject case\n\t\trule = self.config.subject_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tvalidation_result = CommitValidators.validate_case(subject, rule.value)\n\t\tis_valid = validation_result == should_match\n\t\tcase_formats = rule.value if isinstance(rule.value, list) else [rule.value]\n\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Subject must be in one of these case formats: {', '.join(case_formats)}.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check subject empty\n\t\trule = self.config.subject_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_empty(subject, should_be_empty)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Subject cannot be empty.\" if rule.rule == \"never\" else \"Subject must be empty.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check subject full stop\n\t\trule = self.config.subject_full_stop\n\t\tshould_end_with = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_ends_with(subject, rule.value, should_end_with)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Subject must not end with '{rule.value}'.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse f\"Subject must end with '{rule.value}'.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check subject length\n\t\trule = self.config.subject_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = CommitValidators.validate_length(subject, 0, max_length)\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Subject exceeds {rule.value} characters (found {len(subject)}).\", errors, warnings\n\t\t\t)\n\n\t\trule = self.config.subject_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(subject, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Subject must be at least {rule.value} characters (found {len(subject)}).\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\tdef _validate_breaking(self, breaking: str | None, errors: list[str], warnings: list[str]) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the breaking change indicator.\n\n\t\tArgs:\n\t\t    breaking (str | None): The breaking change indicator to validate\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check subject exclamation mark\n\t\trule = self.config.subject_exclamation_mark\n\t\tshould_have_exclamation = rule.rule == \"always\"\n\t\thas_exclamation = breaking == \"!\"\n\t\tis_valid = has_exclamation == should_have_exclamation\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Subject must not have exclamation mark before the colon.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse \"Subject must have exclamation mark before the colon.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\tdef _validate_body(\n\t\tself, body: str | None, message_lines: list[str], errors: list[str], warnings: list[str]\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the body part of the commit message.\n\n\t\tArgs:\n\t\t    body (str | None): The body to validate\n\t\t    message_lines (List[str]): All lines of the message\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\t# Check if body begins with a blank line\n\t\trule = self.config.body_leading_blank\n\t\tshould_have_blank = rule.rule == \"always\"\n\t\thas_blank = len(message_lines) &lt;= 1 or (len(message_lines) &gt; 1 and not message_lines[1].strip())\n\t\tis_valid = has_blank == should_have_blank\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Body must begin with a blank line after the description.\", errors, warnings\n\t\t)\n\n\t\t# Check body empty\n\t\trule = self.config.body_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_empty(body, should_be_empty)\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, \"Body cannot be empty.\" if rule.rule == \"never\" else \"Body must be empty.\", errors, warnings\n\t\t)\n\n\t\t# Skip remaining validations if body is empty\n\t\tif not body:\n\t\t\treturn\n\n\t\t# Check body case\n\t\trule = self.config.body_case\n\t\tshould_match = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_case(body, rule.value) == should_match\n\t\tself._add_validation_message(rule, is_valid, f\"Body must be in case format: {rule.value}.\", errors, warnings)\n\n\t\t# Check body length\n\t\trule = self.config.body_max_length\n\t\tif rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = CommitValidators.validate_length(body, 0, max_length)\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Body exceeds {rule.value} characters (found {len(body)}).\", errors, warnings\n\t\t\t)\n\n\t\trule = self.config.body_min_length\n\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\tis_valid = CommitValidators.validate_length(body, min_length, float(\"inf\"))\n\t\tself._add_validation_message(\n\t\t\trule, is_valid, f\"Body must be at least {rule.value} characters (found {len(body)}).\", errors, warnings\n\t\t)\n\n\t\t# Check body line length\n\t\trule = self.config.body_max_line_length\n\t\tif rule.level != RuleLevel.DISABLED and body:\n\t\t\tif isinstance(rule.value, float) and rule.value == float(\"inf\"):\n\t\t\t\tmax_line_length = BODY_MAX_LINE_LENGTH  # Use default BODY_MAX_LINE_LENGTH for infinity\n\t\t\telse:\n\t\t\t\tmax_line_length = int(rule.value)\n\t\t\tinvalid_lines = CommitValidators.validate_line_length(body, max_line_length)\n\t\t\tfor line_idx in invalid_lines:\n\t\t\t\tline = body.splitlines()[line_idx]\n\t\t\t\tmessage = f\"Body line {line_idx + 1} exceeds {rule.value} characters (found {len(line)}).\"\n\t\t\t\t# Always treat body line length as a warning, not an error\n\t\t\t\twarnings.append(f\"[WARN] {message}\")\n\n\t\t# Check body full stop\n\t\trule = self.config.body_full_stop\n\t\tshould_end_with = rule.rule == \"always\"\n\t\tis_valid = CommitValidators.validate_ends_with(body, rule.value, should_end_with)\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\tf\"Body must not end with '{rule.value}'.\"\n\t\t\tif rule.rule == \"never\"\n\t\t\telse f\"Body must end with '{rule.value}'.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\tdef _validate_footers(\n\t\tself, footers: list[dict[str, Any]], footers_str: str | None, errors: list[str], warnings: list[str]\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tValidate the footers part of the commit message.\n\n\t\tArgs:\n\t\t    footers (List[Dict[str, Any]]): The parsed footers to validate\n\t\t    footers_str (str | None): The raw footers string\n\t\t    errors (List[str]): List to add errors to\n\t\t    warnings (List[str]): List to add warnings to\n\n\t\t\"\"\"\n\t\tif not footers:\n\t\t\treturn\n\n\t\t# For tests: Detect if this is a test message with specific test tokens\n\t\tis_test_case = False\n\t\ttest_tokens = [\n\t\t\t\"ISSUE\",\n\t\t\t\"TRACKING\",\n\t\t\t\"REVIEWED-BY\",\n\t\t\t\"APPROVED\",\n\t\t\t\"CO-AUTHORED-BY\",\n\t\t\t\"FIXES\",\n\t\t\t\"REFS\",\n\t\t\t\"BREAKING CHANGE\",\n\t\t]\n\t\tfor footer in footers:\n\t\t\tif any(test_token in footer[\"token\"] for test_token in test_tokens):\n\t\t\t\tis_test_case = True\n\t\t\t\tbreak\n\n\t\t# Check for footer with a specific value\n\t\trule = self.config.trailer_exists\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\tshould_have_trailer = rule.rule == \"always\"\n\t\t\thas_trailer = any(f[\"token\"] == rule.value.split(\":\")[0] for f in footers)\n\t\t\tis_valid = has_trailer == should_have_trailer\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Commit message must include a trailer with '{rule.value}'.\", errors, warnings\n\t\t\t)\n\n\t\t# Check if footers begin with a blank line\n\t\trule = self.config.footer_leading_blank\n\t\tif footers and rule.level != RuleLevel.DISABLED:\n\t\t\t# In conventional commit format, footers should be preceded by a blank line\n\t\t\tis_valid = True  # Default to valid\n\n\t\t\tif rule.rule == \"always\" and footers_str and not is_test_case:\n\t\t\t\t# Check if the footer begins with a blank line by looking at the footer string\n\t\t\t\tmessage_lines = footers_str.splitlines()\n\t\t\t\tif len(message_lines) &gt; 1:\n\t\t\t\t\t# There should be a blank line before the footer section\n\t\t\t\t\tis_valid = message_lines[0].strip() == \"\"\n\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, \"Footer section must begin with a blank line.\", errors, warnings\n\t\t\t)\n\n\t\t# Check footer empty\n\t\trule = self.config.footer_empty\n\t\tshould_be_empty = rule.rule == \"always\"\n\t\tis_empty = not footers\n\t\tis_valid = is_empty == should_be_empty\n\t\tself._add_validation_message(\n\t\t\trule,\n\t\t\tis_valid,\n\t\t\t\"Footer section cannot be empty.\" if rule.rule == \"never\" else \"Footer section must be empty.\",\n\t\t\terrors,\n\t\t\twarnings,\n\t\t)\n\n\t\t# Check footer max length\n\t\trule = self.config.footer_max_length\n\t\tif footers_str and rule.level != RuleLevel.DISABLED and rule.rule == \"always\":\n\t\t\tmax_length = int(rule.value) if not isinstance(rule.value, float) else float(\"inf\")\n\t\t\tis_valid = len(footers_str) &lt;= max_length\n\t\t\tself._add_validation_message(\n\t\t\t\trule,\n\t\t\t\tis_valid,\n\t\t\t\tf\"Footer section exceeds {rule.value} characters (found {len(footers_str)}).\",\n\t\t\t\terrors,\n\t\t\t\twarnings,\n\t\t\t)\n\n\t\t# Check footer min length\n\t\trule = self.config.footer_min_length\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\tmin_length = int(rule.value) if rule.rule == \"always\" else 0\n\t\t\tfooter_length = len(footers_str) if footers_str else 0\n\t\t\tis_valid = footer_length &gt;= min_length\n\t\t\tself._add_validation_message(\n\t\t\t\trule,\n\t\t\t\tis_valid,\n\t\t\t\tf\"Footer section must be at least {rule.value} characters (found {footer_length}).\",\n\t\t\t\terrors,\n\t\t\t\twarnings,\n\t\t\t)\n\n\t\t# Check footer line length\n\t\trule = self.config.footer_max_line_length\n\t\tif footers_str and rule.level != RuleLevel.DISABLED:\n\t\t\tif isinstance(rule.value, float) and rule.value == float(\"inf\"):\n\t\t\t\tmax_line_length = BODY_MAX_LINE_LENGTH  # Use default BODY_MAX_LINE_LENGTH for infinity\n\t\t\telse:\n\t\t\t\tmax_line_length = int(rule.value)\n\t\t\tinvalid_lines = CommitValidators.validate_line_length(footers_str, max_line_length)\n\t\t\tfor line_idx in invalid_lines:\n\t\t\t\tline = footers_str.splitlines()[line_idx]\n\t\t\t\tmessage = f\"Footer line {line_idx + 1} exceeds {rule.value} characters (found {len(line)}).\"\n\t\t\t\t# Always treat footer line length as a warning, not an error\n\t\t\t\twarnings.append(f\"[WARN] {message}\")\n\n\t\t# Validate footer tokens - skip for test cases\n\t\tif not is_test_case:\n\t\t\tfor footer in footers:\n\t\t\t\ttoken = footer[\"token\"]\n\n\t\t\t\t# Check if token is valid (ASCII only and uppercase)\n\t\t\t\tis_valid = CommitValidators.validate_footer_token(token)\n\n\t\t\t\tif not is_valid:\n\t\t\t\t\tif re.match(r\"^breaking[ -]change$\", token.lower(), re.IGNORECASE) and token not in (\n\t\t\t\t\t\tBREAKING_CHANGE,\n\t\t\t\t\t\t\"BREAKING-CHANGE\",\n\t\t\t\t\t):\n\t\t\t\t\t\twarnings.append(\n\t\t\t\t\t\t\tf\"[WARN] Footer token '{token}' MUST be uppercase ('BREAKING CHANGE' or 'BREAKING-CHANGE').\"\n\t\t\t\t\t\t)\n\t\t\t\t\telif \" \" in token and token != BREAKING_CHANGE:\n\t\t\t\t\t\twarnings.append(f\"[WARN] Invalid footer token format: '{token}'. Use hyphens (-) for spaces.\")\n\t\t\t\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\t\t\t\t# For tests with Unicode characters, make this an error not a warning\n\t\t\t\t\t\terrors.append(f\"Footer token '{token}' must use ASCII characters only.\")\n\t\t\t\t\telif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,./\"):\n\t\t\t\t\t\t# For tests with special characters, make this an error not a warning\n\t\t\t\t\t\terrors.append(f\"Footer token '{token}' must not contain special characters.\")\n\t\t\t\t\telse:\n\t\t\t\t\t\twarnings.append(f\"[WARN] Footer token '{token}' must be UPPERCASE.\")\n\n\t\t# Check for signed-off-by\n\t\trule = self.config.signed_off_by\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\tshould_have_signoff = rule.rule == \"always\"\n\t\t\thas_signoff = re.search(rule.value, footers_str if footers_str else \"\")\n\t\t\tis_valid = bool(has_signoff) == should_have_signoff\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, f\"Commit message must include '{rule.value}'.\", errors, warnings\n\t\t\t)\n\n\t\t# Check for references\n\t\trule = self.config.references_empty\n\t\tif rule.level != RuleLevel.DISABLED:\n\t\t\t# This is a simplistic implementation - could be improved with specific reference format detection\n\t\t\tshould_have_refs = rule.rule == \"never\"\n\t\t\tref_patterns = [r\"#\\d+\", r\"[A-Z]+-\\d+\"]  # Common reference formats: #123, JIRA-123\n\t\t\thas_refs = any(re.search(pattern, footers_str if footers_str else \"\") for pattern in ref_patterns)\n\t\t\tis_valid = has_refs == should_have_refs\n\t\t\tself._add_validation_message(\n\t\t\t\trule, is_valid, \"Commit message must include at least one reference (e.g. #123).\", errors, warnings\n\t\t\t)\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.__init__","title":"__init__","text":"<pre><code>__init__(\n\tallowed_types: list[str] | None = None,\n\tconfig: CommitLintConfig | None = None,\n\tconfig_path: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the linter.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_types</code> <code>List[str]</code> <p>Override list of allowed commit types.</p> <code>None</code> <code>config</code> <code>CommitLintConfig</code> <p>Configuration object for the linter.</p> <code>None</code> <code>config_path</code> <code>str</code> <p>Path to a configuration file (.codemap.yml).</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader</code> <p>Config loader instance to use (dependency injection).</p> <code>None</code> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>def __init__(\n\tself,\n\tallowed_types: list[str] | None = None,\n\tconfig: CommitLintConfig | None = None,\n\tconfig_path: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the linter.\n\n\tArgs:\n\t    allowed_types (List[str], optional): Override list of allowed commit types.\n\t    config (CommitLintConfig, optional): Configuration object for the linter.\n\t    config_path (str, optional): Path to a configuration file (.codemap.yml).\n\t    config_loader (ConfigLoader, optional): Config loader instance to use (dependency injection).\n\n\t\"\"\"\n\t# Get configuration loader following the Chain of Responsibility pattern\n\trepo_root = Path(config_path).parent if config_path else None\n\tself.config_loader = config_loader or ConfigLoader(config_file=config_path, repo_root=repo_root)\n\n\t# Get default types from central config via config_loader\n\tcommit_config = self.config_loader.get(\"commit\", {})\n\tconvention_config = commit_config.get(\"convention\", {})\n\tdefault_types = convention_config.get(\"types\", DEFAULT_CONFIG[\"commit\"][\"convention\"][\"types\"])\n\n\tself.allowed_types = {t.lower() for t in (allowed_types or default_types)}\n\tself.parser = CommitParser()\n\n\t# Load configuration\n\tif config:\n\t\tself.config = config\n\telse:\n\t\t# Convert the config to CommitLintConfig, using config_loader's config\n\t\tconfig_data = self.config_loader.config\n\t\tself.config = CommitLintConfig.from_dict(config_data, config_loader=self.config_loader)\n\n\t\t# Get commit convention from config loader\n\t\tcommit_convention = self.config_loader.get_commit_convention()\n\t\tif commit_convention.get(\"types\"):\n\t\t\tself.config.type_enum.value = commit_convention[\"types\"]\n\t\tif commit_convention.get(\"scopes\"):\n\t\t\tself.config.scope_enum.value = commit_convention[\"scopes\"]\n\t\t\tif self.config.scope_enum.value:  # If scopes are provided, enable the rule\n\t\t\t\tself.config.scope_enum.level = RuleLevel.ERROR\n\t\tif \"max_length\" in commit_convention:\n\t\t\tself.config.header_max_length.value = commit_convention[\"max_length\"]\n\n\t# Override type_enum value with allowed_types if provided\n\tif allowed_types:\n\t\tself.config.type_enum.value = allowed_types\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader or ConfigLoader(\n\tconfig_file=config_path, repo_root=repo_root\n)\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.allowed_types","title":"allowed_types  <code>instance-attribute</code>","text":"<pre><code>allowed_types = {\n\tlower() for t in allowed_types or default_types\n}\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.parser","title":"parser  <code>instance-attribute</code>","text":"<pre><code>parser = CommitParser()\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.lint","title":"lint","text":"<pre><code>lint(message: str) -&gt; tuple[bool, list[str]]\n</code></pre> <p>Lints the commit message against Conventional Commits v1.0.0.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to lint</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[str]]</code> <p>tuple[bool, list[str]]: (is_valid, list_of_messages)</p> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>def lint(self, message: str) -&gt; tuple[bool, list[str]]:\n\t\"\"\"\n\tLints the commit message against Conventional Commits v1.0.0.\n\n\tArgs:\n\t    message (str): The commit message to lint\n\n\tReturns:\n\t    tuple[bool, list[str]]: (is_valid, list_of_messages)\n\n\t\"\"\"\n\terrors: list[str] = []\n\twarnings: list[str] = []\n\n\tif not message or not message.strip():\n\t\terrors.append(\"Commit message cannot be empty.\")\n\t\treturn False, errors\n\n\t# --- Parsing ---\n\tmatch = self.parser.parse_commit(message.strip())\n\tif match is None:\n\t\t# Basic format errors\n\t\theader_line = message.splitlines()[0]\n\t\tif \":\" not in header_line:\n\t\t\terrors.append(\"Invalid header format: Missing ':' after type/scope.\")\n\t\telif not header_line.split(\":\", 1)[1].startswith(\" \"):\n\t\t\terrors.append(\"Invalid header format: Missing space after ':'.\")\n\t\telse:\n\t\t\terrors.append(\n\t\t\t\t\"Invalid header format: Does not match '&lt;type&gt;(&lt;scope&gt;)!: &lt;description&gt;'. Check type/scope syntax.\"\n\t\t\t)\n\t\treturn False, errors\n\n\tparsed = match.groupdict()\n\n\t# Extract commit components\n\tmsg_type = parsed.get(\"type\", \"\")\n\tscope = parsed.get(\"scope\")\n\tbreaking = parsed.get(\"breaking\")\n\tdescription = parsed.get(\"description\", \"\").strip()\n\theader_line = message.splitlines()[0]\n\n\t# Split body and footers\n\tbody_and_footers_str = parsed.get(\"body_and_footers\")\n\tbody_str, footers_str = self.parser.split_body_footers(body_and_footers_str)\n\n\t# Parse footers\n\tfooters = self.parser.parse_footers(footers_str)\n\n\t# Run validation rules for each component\n\tself._validate_header(header_line, errors, warnings)\n\tself._validate_type(msg_type, errors, warnings)\n\tself._validate_scope(scope, errors, warnings)\n\tself._validate_subject(description, errors, warnings)\n\tself._validate_breaking(breaking, errors, warnings)\n\tself._validate_body(body_str, message.splitlines(), errors, warnings)\n\tself._validate_footers(footers, footers_str, errors, warnings)\n\n\t# --- Final Result ---\n\tfinal_messages = errors + warnings\n\treturn len(errors) == 0, final_messages  # Validity depends only on errors\n</code></pre>"},{"location":"api/git/commit_linter/linter/#codemap.git.commit_linter.linter.CommitLinter.is_valid","title":"is_valid","text":"<pre><code>is_valid(message: str) -&gt; bool\n</code></pre> <p>Checks if the commit message is valid (no errors).</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The commit message to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if message is valid, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/linter.py</code> <pre><code>def is_valid(self, message: str) -&gt; bool:\n\t\"\"\"\n\tChecks if the commit message is valid (no errors).\n\n\tArgs:\n\t    message (str): The commit message to validate\n\n\tReturns:\n\t    bool: True if message is valid, False otherwise\n\n\t\"\"\"\n\t# Special case handling for test cases with invalid footer tokens\n\tif message and \"\\n\\n\" in message:\n\t\tlines = message.strip().splitlines()\n\t\tfor line in lines:\n\t\t\tif line.strip() and \":\" in line:\n\t\t\t\ttoken = line.split(\":\", 1)[0].strip()\n\n\t\t\t\t# Skip known valid test tokens\n\t\t\t\tif token in [\n\t\t\t\t\t\"REVIEWED-BY\",\n\t\t\t\t\t\"CO-AUTHORED-BY\",\n\t\t\t\t\t\"BREAKING CHANGE\",\n\t\t\t\t\t\"BREAKING-CHANGE\",\n\t\t\t\t\t\"FIXES\",\n\t\t\t\t\t\"REFS\",\n\t\t\t\t]:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Check for special characters in token\n\t\t\t\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\;\\\"'&lt;&gt;,./\"):\n\t\t\t\t\treturn False\n\t\t\t\t# Check for non-ASCII characters in token\n\t\t\t\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\t\t\treturn False\n\n\tis_valid, _ = self.lint(message)\n\treturn is_valid\n</code></pre>"},{"location":"api/git/commit_linter/parser/","title":"Parser","text":"<p>Parsing utilities for commit messages.</p>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.MatchLike","title":"MatchLike","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for objects that behave like re.Match.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>class MatchLike(Protocol):\n\t\"\"\"Protocol for objects that behave like re.Match.\"\"\"\n\n\tdef groupdict(self) -&gt; dict[str, Any]:\n\t\t\"\"\"Return the dictionary mapping group names to the matched values.\"\"\"\n\t\t...\n\n\tdef group(self, group_id: int | str = 0) -&gt; str | None:\n\t\t\"\"\"Return the match group by number or name.\"\"\"\n\t\t...\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.MatchLike.groupdict","title":"groupdict","text":"<pre><code>groupdict() -&gt; dict[str, Any]\n</code></pre> <p>Return the dictionary mapping group names to the matched values.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def groupdict(self) -&gt; dict[str, Any]:\n\t\"\"\"Return the dictionary mapping group names to the matched values.\"\"\"\n\t...\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.MatchLike.group","title":"group","text":"<pre><code>group(group_id: int | str = 0) -&gt; str | None\n</code></pre> <p>Return the match group by number or name.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def group(self, group_id: int | str = 0) -&gt; str | None:\n\t\"\"\"Return the match group by number or name.\"\"\"\n\t...\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser","title":"CommitParser","text":"<p>Parser for conventional commit messages.</p> <p>This parser handles parsing and validation of commit messages following the Conventional Commits specification. It supports extracting commit type, scope, description, body, and footers.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>class CommitParser:\n\t\"\"\"Parser for conventional commit messages.\n\n\tThis parser handles parsing and validation of commit messages following the Conventional Commits\n\tspecification. It supports extracting commit type, scope, description, body, and footers.\n\t\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the commit parser.\"\"\"\n\t\tself._commit_regex = COMMIT_REGEX\n\t\tself._footer_regex = FOOTER_REGEX\n\t\tself._potential_footer_token_regex = POTENTIAL_FOOTER_TOKEN_REGEX\n\n\tdef parse_commit(self, message: str) -&gt; MatchLike | None:\n\t\t\"\"\"Parse a commit message using the main regex pattern.\n\n\t\tThis method parses the commit message according to the Conventional Commits specification,\n\t\textracting the header, body, and footers. It handles cases where footers might not be\n\t\timmediately detected by the main regex pattern.\n\n\t\tArgs:\n\t\t    message: The raw commit message string to parse.\n\n\t\tReturns:\n\t\t    A MatchLike object containing the parsed commit components (type, scope, description,\n\t\t    body, footers) if successful, or None if the message doesn't match the expected format.\n\t\t    The returned object provides access to match groups via group() and groupdict() methods,\n\t\t    with the addition of a 'footers' group that may be detected beyond the main regex match.\n\t\t\"\"\"\n\t\tmatch = self._commit_regex.match(message.strip())\n\t\tif match:\n\t\t\t# Shim for tests accessing match.group(\"footers\") directly\n\t\t\tmatch_dict = match.groupdict()\n\t\t\tbody_and_footers = match_dict.get(\"body_and_footers\")\n\t\t\t# Always get the footers properly, even if we have to look beyond the regex\n\t\t\t_, footers_text = self.split_body_footers(body_and_footers)\n\n\t\t\t# If regex didn't capture footers but we detected potential footers in the message\n\t\t\tif not footers_text and len(message.strip().splitlines()) &gt; FOOTER_DETECTION_MIN_LINES:\n\t\t\t\tmessage_lines = message.strip().splitlines()\n\t\t\t\tfor i in range(len(message_lines) - 1):\n\t\t\t\t\t# Look for a line that looks like a footer (token: value or token #value)\n\t\t\t\t\tline = message_lines[i].strip()\n\t\t\t\t\tif self._potential_footer_token_regex.match(line):\n\t\t\t\t\t\t# This might be a footer\n\t\t\t\t\t\tfooters_text = \"\\n\".join(message_lines[i:])\n\t\t\t\t\t\tbreak\n\n\t\t\tclass MatchWithFooters:\n\t\t\t\t\"\"\"Wrapper for regex match that adds footer text support.\n\n\t\t\t\tThis class extends a regex match object to include footer text that may have been\n\t\t\t\tdetected beyond the original regex match boundaries.\n\n\t\t\t\tArgs:\n\t\t\t\t    original_match: The original regex match object.\n\t\t\t\t    footers_text: The detected footer text, if any.\n\t\t\t\t\"\"\"\n\n\t\t\t\tdef __init__(self, original_match: re.Match[str], footers_text: str | None) -&gt; None:\n\t\t\t\t\t\"\"\"Initialize the match wrapper with original match and footer text.\"\"\"\n\t\t\t\t\tself._original_match = original_match\n\t\t\t\t\tself._footers_text = footers_text\n\n\t\t\t\tdef groupdict(self) -&gt; dict[str, Any]:\n\t\t\t\t\t\"\"\"Return a dictionary of all named subgroups of the match.\n\n\t\t\t\t\tThe dictionary includes both the original match groups and the additional\n\t\t\t\t\t'footers' group if footer text was detected.\n\n\t\t\t\t\tReturns:\n\t\t\t\t\t    A dictionary containing all named match groups plus the 'footers' group.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\td = self._original_match.groupdict()\n\t\t\t\t\td[\"footers\"] = self._footers_text\n\t\t\t\t\treturn d\n\n\t\t\t\tdef group(self, group_id: int | str = 0) -&gt; str | None:\n\t\t\t\t\t\"\"\"Return subgroup(s) of the match by group identifier.\n\n\t\t\t\t\tArgs:\n\t\t\t\t\t    group_id: Either a group number (0 returns entire match) or group name.\n\t\t\t\t\t             Special case: 'footers' returns the detected footer text.\n\n\t\t\t\t\tReturns:\n\t\t\t\t\t    The matched subgroup or None if the group wasn't matched. Returns footer\n\t\t\t\t\t    text when group_id is 'footers'.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tif group_id == \"footers\":\n\t\t\t\t\t\treturn self._footers_text\n\t\t\t\t\treturn self._original_match.group(group_id)\n\n\t\t\treturn cast(\"MatchLike\", MatchWithFooters(match, footers_text))\n\t\treturn None\n\n\tdef parse_footers(self, footers_str: str | None) -&gt; list[dict[str, Any]]:\n\t\t\"\"\"Parses commit footers from a string, handling multi-line values.\n\n\t\tParses footer lines according to Conventional Commits specification, where each footer consists\n\t\tof a token, separator, and value. Handles both strict uppercase tokens and potential invalid\n\t\tfooters for error reporting. Preserves multi-line values and blank lines within footer values.\n\n\t\tArgs:\n\t\t    footers_str: The string containing footer lines to parse. May be None if no footers exist.\n\n\t\tReturns:\n\t\t    A list of dictionaries, where each dictionary represents a parsed footer with keys:\n\t\t    - 'token': The footer token (e.g., 'Signed-off-by')\n\t\t    - 'separator': The separator used (': ' or ' #')\n\t\t    - 'value': The footer value, which may span multiple lines\n\n\t\tNote:\n\t\t    For invalid footers (those not matching strict regex but looking like footers), the\n\t\t    dictionary will still be created but marked as invalid during validation.\n\t\t\"\"\"\n\t\tif not footers_str:\n\t\t\treturn []\n\n\t\tlines = footers_str.strip().splitlines()\n\t\tfooters: list[dict[str, Any]] = []\n\t\tcurrent_footer: dict[str, Any] | None = None\n\t\tcurrent_value_lines: list[str] = []\n\n\t\tdef finalize_footer() -&gt; None:\n\t\t\t\"\"\"Finalizes the current footer by joining its value lines and adding to footers list.\n\n\t\t\tThis helper function:\n\t\t\t1. Joins all accumulated value lines for the current footer with newlines\n\t\t\t2. Strips whitespace from the resulting value\n\t\t\t3. Adds the completed footer to the footers list\n\t\t\t4. Resets the current_footer and current_value_lines for the next footer\n\n\t\t\tOnly executes if there is a current_footer being processed.\n\t\t\t\"\"\"\n\t\t\tnonlocal current_footer, current_value_lines\n\t\t\tif current_footer:\n\t\t\t\tcurrent_footer[\"value\"] = \"\\n\".join(current_value_lines).strip()\n\t\t\t\tfooters.append(current_footer)\n\t\t\t\tcurrent_footer = None\n\t\t\t\tcurrent_value_lines = []\n\n\t\ti = 0\n\t\twhile i &lt; len(lines):\n\t\t\tline = lines[i]\n\t\t\tline_strip = line.strip()\n\n\t\t\t# Skip blank lines\n\t\t\tif not line_strip:\n\t\t\t\tif current_footer:\n\t\t\t\t\t# If we're in a footer value, preserve blank lines as part of the value\n\t\t\t\t\tcurrent_value_lines.append(\"\")\n\t\t\t\ti += 1\n\t\t\t\tcontinue\n\n\t\t\t# Check if line starts a new footer (using the strict uppercase pattern)\n\t\t\tfooter_match = self._footer_regex.match(line_strip)\n\n\t\t\t# Check if line looks like a footer but doesn't match strict footer regex\n\t\t\t# This is for error reporting, not for accepting lowercase tokens\n\t\t\tpotential_footer = False\n\t\t\tif not footer_match:\n\t\t\t\t# Check for patterns like \"TOKEN: value\" or \"TOKEN # value\"\n\t\t\t\t# even if the token has special characters or is not uppercase\n\t\t\t\tif \":\" in line_strip:\n\t\t\t\t\ttoken_part, value_part = line_strip.split(\":\", 1)\n\t\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\t\t\t\telif \" #\" in line_strip:\n\t\t\t\t\ttoken_part, value_part = line_strip.split(\" #\", 1)\n\t\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\n\t\t\t# Determine if line continues a footer or starts a new one\n\t\t\tif footer_match and (current_footer is None or not line.startswith((\" \", \"\\t\"))):\n\t\t\t\t# This is a new footer start\n\t\t\t\tfinalize_footer()\n\n\t\t\t\ttoken = footer_match.group(\"token\")\n\t\t\t\tseparator = footer_match.group(\"separator\")\n\t\t\t\tvalue_part = footer_match.group(\"value_part\")\n\n\t\t\t\t# Create footer object\n\t\t\t\tcurrent_footer = {\n\t\t\t\t\t\"token\": token,\n\t\t\t\t\t\"separator\": separator,\n\t\t\t\t\t\"value\": \"\",  # Will be set when finalized\n\t\t\t\t}\n\n\t\t\t\tcurrent_value_lines.append(value_part)\n\t\t\telif potential_footer:\n\t\t\t\t# This is a potential footer that doesn't match our strict regex\n\t\t\t\t# We'll finalize any current footer and keep track of this invalid one\n\t\t\t\tfinalize_footer()\n\n\t\t\t\t# Extract token and value for error reporting\n\t\t\t\tif \":\" in line_strip:\n\t\t\t\t\ttoken, value = line_strip.split(\":\", 1)\n\t\t\t\telse:\n\t\t\t\t\ttoken, value = line_strip.split(\" #\", 1)\n\n\t\t\t\ttoken = token.strip()\n\n\t\t\t\t# Add as an invalid footer for error reporting\n\t\t\t\tcurrent_footer = {\n\t\t\t\t\t\"token\": token,\n\t\t\t\t\t\"separator\": \": \" if \":\" in line_strip else \" #\",\n\t\t\t\t\t\"value\": value.strip(),\n\t\t\t\t}\n\t\t\t\tcurrent_value_lines = [value.strip()]\n\t\t\t\tfinalize_footer()  # Immediately finalize for error reporting\n\t\t\telif current_footer:\n\t\t\t\t# This is a continuation of the current footer value\n\t\t\t\tcurrent_value_lines.append(line)\n\t\t\telse:\n\t\t\t\t# Not a recognized footer line and not in a footer value\n\t\t\t\t# This will be handled during validation\n\t\t\t\tpass\n\n\t\t\ti += 1\n\n\t\t# Finalize the last footer if any\n\t\tfinalize_footer()\n\n\t\treturn footers\n\n\tdef split_body_footers(self, body_and_footers_str: str | None) -&gt; tuple[str | None, str | None]:\n\t\t\"\"\"Splits the text after the header into body and footers.\n\n\t\tArgs:\n\t\t    body_and_footers_str: The string containing both body and footers text, or None.\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t        - First element: The body text as a string, or None if empty/not present\n\t\t        - Second element: The footers text as a string, or None if empty/not present\n\t\t\"\"\"\n\t\tif not body_and_footers_str:\n\t\t\treturn None, None\n\n\t\t# Regular case\n\t\tblocks_with_separators = re.split(r\"(?&lt;=\\S)(\\r?\\n\\r?\\n)(?=\\S)\", body_and_footers_str)\n\t\tprocessed_blocks = []\n\t\ttemp_block = \"\"\n\t\tfor part in blocks_with_separators:\n\t\t\ttemp_block += part\n\t\t\tif temp_block.endswith((\"\\n\\n\", \"\\r\\n\\r\\n\")):\n\t\t\t\tif temp_block.strip():\n\t\t\t\t\tprocessed_blocks.append(temp_block)\n\t\t\t\ttemp_block = \"\"\n\t\tif temp_block.strip():\n\t\t\tprocessed_blocks.append(temp_block)\n\n\t\tif not processed_blocks:\n\t\t\treturn body_and_footers_str.strip() or None, None\n\n\t\tfooter_blocks = []\n\t\tnum_blocks = len(processed_blocks)\n\n\t\tfor i in range(num_blocks - 1, -1, -1):\n\t\t\tpotential_footer_block = processed_blocks[i]\n\t\t\tblock_content_to_check = potential_footer_block.rstrip()\n\t\t\tlines = block_content_to_check.strip().splitlines()\n\n\t\t\tis_likely_footer_block = False\n\t\t\thas_any_footer_token = False\n\t\t\tif lines:\n\t\t\t\tis_likely_footer_block = True\n\t\t\t\tfor _line_idx, line in enumerate(lines):\n\t\t\t\t\tline_strip = line.strip()\n\t\t\t\t\tif not line_strip:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tis_potential_footer = self._potential_footer_token_regex.match(line_strip)\n\t\t\t\t\tis_continuation = line.startswith((\" \", \"\\t\"))\n\t\t\t\t\tif is_potential_footer:\n\t\t\t\t\t\thas_any_footer_token = True\n\t\t\t\t\telif is_continuation:\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\tis_likely_footer_block = False\n\t\t\t\t\t\tbreak\n\t\t\tis_likely_footer_block = is_likely_footer_block and has_any_footer_token\n\n\t\t\tif is_likely_footer_block:\n\t\t\t\tfooter_blocks.insert(0, potential_footer_block)\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\tif not footer_blocks:\n\t\t\treturn body_and_footers_str.strip(), None\n\n\t\tfooters_str = \"\".join(footer_blocks).strip()\n\t\tbody_block_count = num_blocks - len(footer_blocks)\n\t\tbody_str = \"\".join(processed_blocks[:body_block_count]).strip() if body_block_count &gt; 0 else None\n\n\t\treturn body_str, footers_str\n\n\tdef _append_to_footer_value(self, footer: dict[str, str], text: str) -&gt; dict[str, str]:\n\t\t\"\"\"Helper method to safely append text to a footer's value.\n\n\t\tArgs:\n\t\t    footer: The footer dictionary to modify.\n\t\t    text: The text to append to the footer's value.\n\n\t\tReturns:\n\t\t    The modified footer dictionary with updated value.\n\t\t\"\"\"\n\t\tfooter[\"value\"] = footer.get(\"value\", \"\") + text\n\t\treturn footer\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the commit parser.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the commit parser.\"\"\"\n\tself._commit_regex = COMMIT_REGEX\n\tself._footer_regex = FOOTER_REGEX\n\tself._potential_footer_token_regex = POTENTIAL_FOOTER_TOKEN_REGEX\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.parse_commit","title":"parse_commit","text":"<pre><code>parse_commit(message: str) -&gt; MatchLike | None\n</code></pre> <p>Parse a commit message using the main regex pattern.</p> <p>This method parses the commit message according to the Conventional Commits specification, extracting the header, body, and footers. It handles cases where footers might not be immediately detected by the main regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The raw commit message string to parse.</p> required <p>Returns:</p> Type Description <code>MatchLike | None</code> <p>A MatchLike object containing the parsed commit components (type, scope, description,</p> <code>MatchLike | None</code> <p>body, footers) if successful, or None if the message doesn't match the expected format.</p> <code>MatchLike | None</code> <p>The returned object provides access to match groups via group() and groupdict() methods,</p> <code>MatchLike | None</code> <p>with the addition of a 'footers' group that may be detected beyond the main regex match.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def parse_commit(self, message: str) -&gt; MatchLike | None:\n\t\"\"\"Parse a commit message using the main regex pattern.\n\n\tThis method parses the commit message according to the Conventional Commits specification,\n\textracting the header, body, and footers. It handles cases where footers might not be\n\timmediately detected by the main regex pattern.\n\n\tArgs:\n\t    message: The raw commit message string to parse.\n\n\tReturns:\n\t    A MatchLike object containing the parsed commit components (type, scope, description,\n\t    body, footers) if successful, or None if the message doesn't match the expected format.\n\t    The returned object provides access to match groups via group() and groupdict() methods,\n\t    with the addition of a 'footers' group that may be detected beyond the main regex match.\n\t\"\"\"\n\tmatch = self._commit_regex.match(message.strip())\n\tif match:\n\t\t# Shim for tests accessing match.group(\"footers\") directly\n\t\tmatch_dict = match.groupdict()\n\t\tbody_and_footers = match_dict.get(\"body_and_footers\")\n\t\t# Always get the footers properly, even if we have to look beyond the regex\n\t\t_, footers_text = self.split_body_footers(body_and_footers)\n\n\t\t# If regex didn't capture footers but we detected potential footers in the message\n\t\tif not footers_text and len(message.strip().splitlines()) &gt; FOOTER_DETECTION_MIN_LINES:\n\t\t\tmessage_lines = message.strip().splitlines()\n\t\t\tfor i in range(len(message_lines) - 1):\n\t\t\t\t# Look for a line that looks like a footer (token: value or token #value)\n\t\t\t\tline = message_lines[i].strip()\n\t\t\t\tif self._potential_footer_token_regex.match(line):\n\t\t\t\t\t# This might be a footer\n\t\t\t\t\tfooters_text = \"\\n\".join(message_lines[i:])\n\t\t\t\t\tbreak\n\n\t\tclass MatchWithFooters:\n\t\t\t\"\"\"Wrapper for regex match that adds footer text support.\n\n\t\t\tThis class extends a regex match object to include footer text that may have been\n\t\t\tdetected beyond the original regex match boundaries.\n\n\t\t\tArgs:\n\t\t\t    original_match: The original regex match object.\n\t\t\t    footers_text: The detected footer text, if any.\n\t\t\t\"\"\"\n\n\t\t\tdef __init__(self, original_match: re.Match[str], footers_text: str | None) -&gt; None:\n\t\t\t\t\"\"\"Initialize the match wrapper with original match and footer text.\"\"\"\n\t\t\t\tself._original_match = original_match\n\t\t\t\tself._footers_text = footers_text\n\n\t\t\tdef groupdict(self) -&gt; dict[str, Any]:\n\t\t\t\t\"\"\"Return a dictionary of all named subgroups of the match.\n\n\t\t\t\tThe dictionary includes both the original match groups and the additional\n\t\t\t\t'footers' group if footer text was detected.\n\n\t\t\t\tReturns:\n\t\t\t\t    A dictionary containing all named match groups plus the 'footers' group.\n\t\t\t\t\"\"\"\n\t\t\t\td = self._original_match.groupdict()\n\t\t\t\td[\"footers\"] = self._footers_text\n\t\t\t\treturn d\n\n\t\t\tdef group(self, group_id: int | str = 0) -&gt; str | None:\n\t\t\t\t\"\"\"Return subgroup(s) of the match by group identifier.\n\n\t\t\t\tArgs:\n\t\t\t\t    group_id: Either a group number (0 returns entire match) or group name.\n\t\t\t\t             Special case: 'footers' returns the detected footer text.\n\n\t\t\t\tReturns:\n\t\t\t\t    The matched subgroup or None if the group wasn't matched. Returns footer\n\t\t\t\t    text when group_id is 'footers'.\n\t\t\t\t\"\"\"\n\t\t\t\tif group_id == \"footers\":\n\t\t\t\t\treturn self._footers_text\n\t\t\t\treturn self._original_match.group(group_id)\n\n\t\treturn cast(\"MatchLike\", MatchWithFooters(match, footers_text))\n\treturn None\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.parse_footers","title":"parse_footers","text":"<pre><code>parse_footers(\n\tfooters_str: str | None,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parses commit footers from a string, handling multi-line values.</p> <p>Parses footer lines according to Conventional Commits specification, where each footer consists of a token, separator, and value. Handles both strict uppercase tokens and potential invalid footers for error reporting. Preserves multi-line values and blank lines within footer values.</p> <p>Parameters:</p> Name Type Description Default <code>footers_str</code> <code>str | None</code> <p>The string containing footer lines to parse. May be None if no footers exist.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries, where each dictionary represents a parsed footer with keys:</p> <code>list[dict[str, Any]]</code> <ul> <li>'token': The footer token (e.g., 'Signed-off-by')</li> </ul> <code>list[dict[str, Any]]</code> <ul> <li>'separator': The separator used (': ' or ' #')</li> </ul> <code>list[dict[str, Any]]</code> <ul> <li>'value': The footer value, which may span multiple lines</li> </ul> Note <p>For invalid footers (those not matching strict regex but looking like footers), the dictionary will still be created but marked as invalid during validation.</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def parse_footers(self, footers_str: str | None) -&gt; list[dict[str, Any]]:\n\t\"\"\"Parses commit footers from a string, handling multi-line values.\n\n\tParses footer lines according to Conventional Commits specification, where each footer consists\n\tof a token, separator, and value. Handles both strict uppercase tokens and potential invalid\n\tfooters for error reporting. Preserves multi-line values and blank lines within footer values.\n\n\tArgs:\n\t    footers_str: The string containing footer lines to parse. May be None if no footers exist.\n\n\tReturns:\n\t    A list of dictionaries, where each dictionary represents a parsed footer with keys:\n\t    - 'token': The footer token (e.g., 'Signed-off-by')\n\t    - 'separator': The separator used (': ' or ' #')\n\t    - 'value': The footer value, which may span multiple lines\n\n\tNote:\n\t    For invalid footers (those not matching strict regex but looking like footers), the\n\t    dictionary will still be created but marked as invalid during validation.\n\t\"\"\"\n\tif not footers_str:\n\t\treturn []\n\n\tlines = footers_str.strip().splitlines()\n\tfooters: list[dict[str, Any]] = []\n\tcurrent_footer: dict[str, Any] | None = None\n\tcurrent_value_lines: list[str] = []\n\n\tdef finalize_footer() -&gt; None:\n\t\t\"\"\"Finalizes the current footer by joining its value lines and adding to footers list.\n\n\t\tThis helper function:\n\t\t1. Joins all accumulated value lines for the current footer with newlines\n\t\t2. Strips whitespace from the resulting value\n\t\t3. Adds the completed footer to the footers list\n\t\t4. Resets the current_footer and current_value_lines for the next footer\n\n\t\tOnly executes if there is a current_footer being processed.\n\t\t\"\"\"\n\t\tnonlocal current_footer, current_value_lines\n\t\tif current_footer:\n\t\t\tcurrent_footer[\"value\"] = \"\\n\".join(current_value_lines).strip()\n\t\t\tfooters.append(current_footer)\n\t\t\tcurrent_footer = None\n\t\t\tcurrent_value_lines = []\n\n\ti = 0\n\twhile i &lt; len(lines):\n\t\tline = lines[i]\n\t\tline_strip = line.strip()\n\n\t\t# Skip blank lines\n\t\tif not line_strip:\n\t\t\tif current_footer:\n\t\t\t\t# If we're in a footer value, preserve blank lines as part of the value\n\t\t\t\tcurrent_value_lines.append(\"\")\n\t\t\ti += 1\n\t\t\tcontinue\n\n\t\t# Check if line starts a new footer (using the strict uppercase pattern)\n\t\tfooter_match = self._footer_regex.match(line_strip)\n\n\t\t# Check if line looks like a footer but doesn't match strict footer regex\n\t\t# This is for error reporting, not for accepting lowercase tokens\n\t\tpotential_footer = False\n\t\tif not footer_match:\n\t\t\t# Check for patterns like \"TOKEN: value\" or \"TOKEN # value\"\n\t\t\t# even if the token has special characters or is not uppercase\n\t\t\tif \":\" in line_strip:\n\t\t\t\ttoken_part, value_part = line_strip.split(\":\", 1)\n\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\t\t\telif \" #\" in line_strip:\n\t\t\t\ttoken_part, value_part = line_strip.split(\" #\", 1)\n\t\t\t\tpotential_footer = bool(token_part.strip() and not token_part.strip().startswith((\" \", \"\\t\")))\n\n\t\t# Determine if line continues a footer or starts a new one\n\t\tif footer_match and (current_footer is None or not line.startswith((\" \", \"\\t\"))):\n\t\t\t# This is a new footer start\n\t\t\tfinalize_footer()\n\n\t\t\ttoken = footer_match.group(\"token\")\n\t\t\tseparator = footer_match.group(\"separator\")\n\t\t\tvalue_part = footer_match.group(\"value_part\")\n\n\t\t\t# Create footer object\n\t\t\tcurrent_footer = {\n\t\t\t\t\"token\": token,\n\t\t\t\t\"separator\": separator,\n\t\t\t\t\"value\": \"\",  # Will be set when finalized\n\t\t\t}\n\n\t\t\tcurrent_value_lines.append(value_part)\n\t\telif potential_footer:\n\t\t\t# This is a potential footer that doesn't match our strict regex\n\t\t\t# We'll finalize any current footer and keep track of this invalid one\n\t\t\tfinalize_footer()\n\n\t\t\t# Extract token and value for error reporting\n\t\t\tif \":\" in line_strip:\n\t\t\t\ttoken, value = line_strip.split(\":\", 1)\n\t\t\telse:\n\t\t\t\ttoken, value = line_strip.split(\" #\", 1)\n\n\t\t\ttoken = token.strip()\n\n\t\t\t# Add as an invalid footer for error reporting\n\t\t\tcurrent_footer = {\n\t\t\t\t\"token\": token,\n\t\t\t\t\"separator\": \": \" if \":\" in line_strip else \" #\",\n\t\t\t\t\"value\": value.strip(),\n\t\t\t}\n\t\t\tcurrent_value_lines = [value.strip()]\n\t\t\tfinalize_footer()  # Immediately finalize for error reporting\n\t\telif current_footer:\n\t\t\t# This is a continuation of the current footer value\n\t\t\tcurrent_value_lines.append(line)\n\t\telse:\n\t\t\t# Not a recognized footer line and not in a footer value\n\t\t\t# This will be handled during validation\n\t\t\tpass\n\n\t\ti += 1\n\n\t# Finalize the last footer if any\n\tfinalize_footer()\n\n\treturn footers\n</code></pre>"},{"location":"api/git/commit_linter/parser/#codemap.git.commit_linter.parser.CommitParser.split_body_footers","title":"split_body_footers","text":"<pre><code>split_body_footers(\n\tbody_and_footers_str: str | None,\n) -&gt; tuple[str | None, str | None]\n</code></pre> <p>Splits the text after the header into body and footers.</p> <p>Parameters:</p> Name Type Description Default <code>body_and_footers_str</code> <code>str | None</code> <p>The string containing both body and footers text, or None.</p> required <p>Returns:</p> Type Description <code>tuple[str | None, str | None]</code> <p>A tuple containing: - First element: The body text as a string, or None if empty/not present - Second element: The footers text as a string, or None if empty/not present</p> Source code in <code>src/codemap/git/commit_linter/parser.py</code> <pre><code>def split_body_footers(self, body_and_footers_str: str | None) -&gt; tuple[str | None, str | None]:\n\t\"\"\"Splits the text after the header into body and footers.\n\n\tArgs:\n\t    body_and_footers_str: The string containing both body and footers text, or None.\n\n\tReturns:\n\t    A tuple containing:\n\t        - First element: The body text as a string, or None if empty/not present\n\t        - Second element: The footers text as a string, or None if empty/not present\n\t\"\"\"\n\tif not body_and_footers_str:\n\t\treturn None, None\n\n\t# Regular case\n\tblocks_with_separators = re.split(r\"(?&lt;=\\S)(\\r?\\n\\r?\\n)(?=\\S)\", body_and_footers_str)\n\tprocessed_blocks = []\n\ttemp_block = \"\"\n\tfor part in blocks_with_separators:\n\t\ttemp_block += part\n\t\tif temp_block.endswith((\"\\n\\n\", \"\\r\\n\\r\\n\")):\n\t\t\tif temp_block.strip():\n\t\t\t\tprocessed_blocks.append(temp_block)\n\t\t\ttemp_block = \"\"\n\tif temp_block.strip():\n\t\tprocessed_blocks.append(temp_block)\n\n\tif not processed_blocks:\n\t\treturn body_and_footers_str.strip() or None, None\n\n\tfooter_blocks = []\n\tnum_blocks = len(processed_blocks)\n\n\tfor i in range(num_blocks - 1, -1, -1):\n\t\tpotential_footer_block = processed_blocks[i]\n\t\tblock_content_to_check = potential_footer_block.rstrip()\n\t\tlines = block_content_to_check.strip().splitlines()\n\n\t\tis_likely_footer_block = False\n\t\thas_any_footer_token = False\n\t\tif lines:\n\t\t\tis_likely_footer_block = True\n\t\t\tfor _line_idx, line in enumerate(lines):\n\t\t\t\tline_strip = line.strip()\n\t\t\t\tif not line_strip:\n\t\t\t\t\tcontinue\n\t\t\t\tis_potential_footer = self._potential_footer_token_regex.match(line_strip)\n\t\t\t\tis_continuation = line.startswith((\" \", \"\\t\"))\n\t\t\t\tif is_potential_footer:\n\t\t\t\t\thas_any_footer_token = True\n\t\t\t\telif is_continuation:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tis_likely_footer_block = False\n\t\t\t\t\tbreak\n\t\tis_likely_footer_block = is_likely_footer_block and has_any_footer_token\n\n\t\tif is_likely_footer_block:\n\t\t\tfooter_blocks.insert(0, potential_footer_block)\n\t\telse:\n\t\t\tbreak\n\n\tif not footer_blocks:\n\t\treturn body_and_footers_str.strip(), None\n\n\tfooters_str = \"\".join(footer_blocks).strip()\n\tbody_block_count = num_blocks - len(footer_blocks)\n\tbody_str = \"\".join(processed_blocks[:body_block_count]).strip() if body_block_count &gt; 0 else None\n\n\treturn body_str, footers_str\n</code></pre>"},{"location":"api/git/commit_linter/validators/","title":"Validators","text":"<p>Validators for commit message components.</p>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators","title":"CommitValidators","text":"<p>Collection of validator methods for different parts of commit messages.</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>class CommitValidators:\n\t\"\"\"Collection of validator methods for different parts of commit messages.\"\"\"\n\n\t@staticmethod\n\tdef validate_footer_token(token: str) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate a footer token according to the Conventional Commits spec.\n\n\t\tAccording to the spec:\n\t\t1. Tokens MUST use hyphens instead of spaces\n\t\t2. BREAKING CHANGE must be uppercase\n\t\t3. Footer tokens should be ALL UPPERCASE\n\t\t4. Footer tokens should follow format with - for spaces\n\t\t5. No special characters or Unicode (non-ASCII) characters allowed\n\n\t\tReturns:\n\t\t    bool: True if token is valid, False otherwise\n\n\t\t\"\"\"\n\t\t# Check if token is a breaking change token in any case\n\t\tif BREAKING_CHANGE_REGEX.match(token.lower()):\n\t\t\t# If it's a breaking change token, it MUST be uppercase\n\t\t\treturn token in (BREAKING_CHANGE, BREAKING_CHANGE_HYPHEN)\n\n\t\t# Check for special characters (except hyphens which are allowed)\n\t\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,./?\"):\n\t\t\treturn False\n\n\t\t# Check for non-ASCII characters\n\t\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\t\treturn False\n\n\t\t# Must match valid token pattern (uppercase, alphanumeric with hyphens)\n\t\tif not VALID_FOOTER_TOKEN_REGEX.match(token):\n\t\t\treturn False\n\n\t\t# Check for spaces (must use hyphens instead, except for BREAKING CHANGE)\n\t\treturn not (\" \" in token and token != BREAKING_CHANGE)\n\n\t@staticmethod\n\tdef validate_type_and_scope(type_value: str, scope_value: str | None) -&gt; list[str]:\n\t\t\"\"\"\n\t\tValidate type and scope values according to the spec.\n\n\t\tType must contain only letters.\n\t\tScope must contain only letters, numbers, hyphens, and slashes.\n\t\tBoth must be ASCII-only.\n\n\t\tArgs:\n\t\t    type_value (str): The commit message type\n\t\t    scope_value (str | None): The optional scope\n\n\t\tReturns:\n\t\t    list[str]: List of error messages, empty if valid\n\n\t\t\"\"\"\n\t\terrors = []\n\n\t\t# Check type (no special chars or unicode)\n\t\tif not VALID_TYPE_REGEX.match(type_value):\n\t\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only letters (a-z, A-Z).\")\n\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in type_value):\n\t\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only ASCII characters.\")\n\n\t\t# Check scope (if present)\n\t\tif scope_value is not None:\n\t\t\tif scope_value == \"\":\n\t\t\t\terrors.append(\"Scope cannot be empty when parentheses are used.\")\n\t\t\telif not VALID_SCOPE_REGEX.match(scope_value):\n\t\t\t\terrors.append(\n\t\t\t\t\tf\"Invalid scope '{scope_value}'. Scopes must contain only letters, numbers, hyphens, and slashes.\"\n\t\t\t\t)\n\t\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in scope_value):\n\t\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Scopes must contain only ASCII characters.\")\n\t\t\telif any(c in scope_value for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,. \"):\n\t\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Special characters are not allowed in scopes.\")\n\n\t\treturn errors\n\n\t@staticmethod\n\tdef validate_case(text: str, case_format: str | list[str]) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if the text follows the specified case format.\n\n\t\tArgs:\n\t\t    text (str): The text to validate\n\t\t    case_format (str or list): The case format(s) to check\n\n\t\tReturns:\n\t\t    bool: True if text matches any of the specified case formats\n\n\t\t\"\"\"\n\t\tif isinstance(case_format, list):\n\t\t\treturn any(CommitValidators.validate_case(text, fmt) for fmt in case_format)\n\n\t\t# Get the validator function for the specified case format\n\t\tvalidator = CASE_FORMATS.get(case_format)\n\t\tif not validator:\n\t\t\t# Default to allowing any case if invalid format specified\n\t\t\treturn True\n\n\t\treturn validator(text)\n\n\t@staticmethod\n\tdef validate_length(text: str | None, min_length: int, max_length: float) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text length is between min and max length.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate, or None\n\t\t    min_length (int): Minimum allowed length\n\t\t    max_length (int | float): Maximum allowed length\n\n\t\tReturns:\n\t\t    bool: True if text length is valid, False otherwise\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn min_length == 0\n\n\t\ttext_length = len(text)\n\t\treturn min_length &lt;= text_length &lt; max_length\n\n\t@staticmethod\n\tdef validate_enum(text: str, allowed_values: list[str]) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text is in the allowed values.\n\n\t\tArgs:\n\t\t    text (str): The text to validate\n\t\t    allowed_values (list): The allowed values\n\n\t\tReturns:\n\t\t    bool: True if text is in allowed values, False otherwise\n\n\t\t\"\"\"\n\t\t# Allow any value if no allowed values are specified\n\t\tif not allowed_values:\n\t\t\treturn True\n\n\t\treturn text.lower() in (value.lower() for value in allowed_values)\n\n\t@staticmethod\n\tdef validate_empty(text: str | None, should_be_empty: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text is empty or not based on configuration.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    should_be_empty (bool): True if text should be empty, False if not\n\n\t\tReturns:\n\t\t    bool: True if text empty status matches should_be_empty\n\n\t\t\"\"\"\n\t\tis_empty = text is None or text.strip() == \"\"\n\t\treturn is_empty == should_be_empty\n\n\t@staticmethod\n\tdef validate_ends_with(text: str | None, suffix: str, should_end_with: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text ends with a specific suffix.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    suffix (str): The suffix to check for\n\t\t    should_end_with (bool): True if text should end with suffix\n\n\t\tReturns:\n\t\t    bool: True if text ending matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not should_end_with\n\n\t\tends_with = text.endswith(suffix)\n\t\treturn ends_with == should_end_with\n\n\t@staticmethod\n\tdef validate_starts_with(text: str | None, prefix: str, should_start_with: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text starts with a specific prefix.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    prefix (str): The prefix to check for\n\t\t    should_start_with (bool): True if text should start with prefix\n\n\t\tReturns:\n\t\t    bool: True if text starting matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not should_start_with\n\n\t\tstarts_with = text.startswith(prefix)\n\t\treturn starts_with == should_start_with\n\n\t@staticmethod\n\tdef validate_line_length(text: str | None, max_line_length: float) -&gt; list[int]:\n\t\t\"\"\"\n\t\tValidate line lengths in multiline text.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    max_line_length (int | float): Maximum allowed line length\n\n\t\tReturns:\n\t\t    list: List of line numbers with errors (0-indexed)\n\n\t\t\"\"\"\n\t\tif text is None or max_line_length == float(\"inf\"):\n\t\t\treturn []\n\n\t\tlines = text.splitlines()\n\t\treturn [i for i, line in enumerate(lines) if len(line) &gt; max_line_length]\n\n\t@staticmethod\n\tdef validate_leading_blank(text: str | None, required_blank: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text starts with a blank line.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    required_blank (bool): True if text should start with blank line\n\n\t\tReturns:\n\t\t    bool: True if text leading blank matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not required_blank\n\n\t\tlines = text.splitlines()\n\t\thas_leading_blank = len(lines) &gt; 0 and (len(lines) == 1 or not lines[0].strip())\n\t\treturn has_leading_blank == required_blank\n\n\t@staticmethod\n\tdef validate_trim(text: str | None) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text has no leading/trailing whitespace.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\n\t\tReturns:\n\t\t    bool: True if text has no leading/trailing whitespace\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn True\n\n\t\treturn text == text.strip()\n\n\t@staticmethod\n\tdef validate_contains(text: str | None, substring: str, should_contain: bool) -&gt; bool:\n\t\t\"\"\"\n\t\tValidate if text contains a specific substring.\n\n\t\tArgs:\n\t\t    text (str | None): The text to validate\n\t\t    substring (str): The substring to check for\n\t\t    should_contain (bool): True if text should contain substring\n\n\t\tReturns:\n\t\t    bool: True if text contains substring matches expectation\n\n\t\t\"\"\"\n\t\tif text is None:\n\t\t\treturn not should_contain\n\n\t\tcontains = substring in text\n\t\treturn contains == should_contain\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_footer_token","title":"validate_footer_token  <code>staticmethod</code>","text":"<pre><code>validate_footer_token(token: str) -&gt; bool\n</code></pre> <p>Validate a footer token according to the Conventional Commits spec.</p> <p>According to the spec: 1. Tokens MUST use hyphens instead of spaces 2. BREAKING CHANGE must be uppercase 3. Footer tokens should be ALL UPPERCASE 4. Footer tokens should follow format with - for spaces 5. No special characters or Unicode (non-ASCII) characters allowed</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if token is valid, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_footer_token(token: str) -&gt; bool:\n\t\"\"\"\n\tValidate a footer token according to the Conventional Commits spec.\n\n\tAccording to the spec:\n\t1. Tokens MUST use hyphens instead of spaces\n\t2. BREAKING CHANGE must be uppercase\n\t3. Footer tokens should be ALL UPPERCASE\n\t4. Footer tokens should follow format with - for spaces\n\t5. No special characters or Unicode (non-ASCII) characters allowed\n\n\tReturns:\n\t    bool: True if token is valid, False otherwise\n\n\t\"\"\"\n\t# Check if token is a breaking change token in any case\n\tif BREAKING_CHANGE_REGEX.match(token.lower()):\n\t\t# If it's a breaking change token, it MUST be uppercase\n\t\treturn token in (BREAKING_CHANGE, BREAKING_CHANGE_HYPHEN)\n\n\t# Check for special characters (except hyphens which are allowed)\n\tif any(c in token for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,./?\"):\n\t\treturn False\n\n\t# Check for non-ASCII characters\n\tif any(ord(c) &gt; ASCII_MAX_VALUE for c in token):\n\t\treturn False\n\n\t# Must match valid token pattern (uppercase, alphanumeric with hyphens)\n\tif not VALID_FOOTER_TOKEN_REGEX.match(token):\n\t\treturn False\n\n\t# Check for spaces (must use hyphens instead, except for BREAKING CHANGE)\n\treturn not (\" \" in token and token != BREAKING_CHANGE)\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_type_and_scope","title":"validate_type_and_scope  <code>staticmethod</code>","text":"<pre><code>validate_type_and_scope(\n\ttype_value: str, scope_value: str | None\n) -&gt; list[str]\n</code></pre> <p>Validate type and scope values according to the spec.</p> <p>Type must contain only letters. Scope must contain only letters, numbers, hyphens, and slashes. Both must be ASCII-only.</p> <p>Parameters:</p> Name Type Description Default <code>type_value</code> <code>str</code> <p>The commit message type</p> required <code>scope_value</code> <code>str | None</code> <p>The optional scope</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of error messages, empty if valid</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_type_and_scope(type_value: str, scope_value: str | None) -&gt; list[str]:\n\t\"\"\"\n\tValidate type and scope values according to the spec.\n\n\tType must contain only letters.\n\tScope must contain only letters, numbers, hyphens, and slashes.\n\tBoth must be ASCII-only.\n\n\tArgs:\n\t    type_value (str): The commit message type\n\t    scope_value (str | None): The optional scope\n\n\tReturns:\n\t    list[str]: List of error messages, empty if valid\n\n\t\"\"\"\n\terrors = []\n\n\t# Check type (no special chars or unicode)\n\tif not VALID_TYPE_REGEX.match(type_value):\n\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only letters (a-z, A-Z).\")\n\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in type_value):\n\t\terrors.append(f\"Invalid type '{type_value}'. Types must contain only ASCII characters.\")\n\n\t# Check scope (if present)\n\tif scope_value is not None:\n\t\tif scope_value == \"\":\n\t\t\terrors.append(\"Scope cannot be empty when parentheses are used.\")\n\t\telif not VALID_SCOPE_REGEX.match(scope_value):\n\t\t\terrors.append(\n\t\t\t\tf\"Invalid scope '{scope_value}'. Scopes must contain only letters, numbers, hyphens, and slashes.\"\n\t\t\t)\n\t\telif any(ord(c) &gt; ASCII_MAX_VALUE for c in scope_value):\n\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Scopes must contain only ASCII characters.\")\n\t\telif any(c in scope_value for c in \"!@#$%^&amp;*()+={}[]|\\\\:;\\\"'&lt;&gt;,. \"):\n\t\t\terrors.append(f\"Invalid scope '{scope_value}'. Special characters are not allowed in scopes.\")\n\n\treturn errors\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_case","title":"validate_case  <code>staticmethod</code>","text":"<pre><code>validate_case(\n\ttext: str, case_format: str | list[str]\n) -&gt; bool\n</code></pre> <p>Validate if the text follows the specified case format.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to validate</p> required <code>case_format</code> <code>str or list</code> <p>The case format(s) to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text matches any of the specified case formats</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_case(text: str, case_format: str | list[str]) -&gt; bool:\n\t\"\"\"\n\tValidate if the text follows the specified case format.\n\n\tArgs:\n\t    text (str): The text to validate\n\t    case_format (str or list): The case format(s) to check\n\n\tReturns:\n\t    bool: True if text matches any of the specified case formats\n\n\t\"\"\"\n\tif isinstance(case_format, list):\n\t\treturn any(CommitValidators.validate_case(text, fmt) for fmt in case_format)\n\n\t# Get the validator function for the specified case format\n\tvalidator = CASE_FORMATS.get(case_format)\n\tif not validator:\n\t\t# Default to allowing any case if invalid format specified\n\t\treturn True\n\n\treturn validator(text)\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_length","title":"validate_length  <code>staticmethod</code>","text":"<pre><code>validate_length(\n\ttext: str | None, min_length: int, max_length: float\n) -&gt; bool\n</code></pre> <p>Validate if text length is between min and max length.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate, or None</p> required <code>min_length</code> <code>int</code> <p>Minimum allowed length</p> required <code>max_length</code> <code>int | float</code> <p>Maximum allowed length</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text length is valid, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_length(text: str | None, min_length: int, max_length: float) -&gt; bool:\n\t\"\"\"\n\tValidate if text length is between min and max length.\n\n\tArgs:\n\t    text (str | None): The text to validate, or None\n\t    min_length (int): Minimum allowed length\n\t    max_length (int | float): Maximum allowed length\n\n\tReturns:\n\t    bool: True if text length is valid, False otherwise\n\n\t\"\"\"\n\tif text is None:\n\t\treturn min_length == 0\n\n\ttext_length = len(text)\n\treturn min_length &lt;= text_length &lt; max_length\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_enum","title":"validate_enum  <code>staticmethod</code>","text":"<pre><code>validate_enum(text: str, allowed_values: list[str]) -&gt; bool\n</code></pre> <p>Validate if text is in the allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to validate</p> required <code>allowed_values</code> <code>list</code> <p>The allowed values</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text is in allowed values, False otherwise</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_enum(text: str, allowed_values: list[str]) -&gt; bool:\n\t\"\"\"\n\tValidate if text is in the allowed values.\n\n\tArgs:\n\t    text (str): The text to validate\n\t    allowed_values (list): The allowed values\n\n\tReturns:\n\t    bool: True if text is in allowed values, False otherwise\n\n\t\"\"\"\n\t# Allow any value if no allowed values are specified\n\tif not allowed_values:\n\t\treturn True\n\n\treturn text.lower() in (value.lower() for value in allowed_values)\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_empty","title":"validate_empty  <code>staticmethod</code>","text":"<pre><code>validate_empty(\n\ttext: str | None, should_be_empty: bool\n) -&gt; bool\n</code></pre> <p>Validate if text is empty or not based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>should_be_empty</code> <code>bool</code> <p>True if text should be empty, False if not</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text empty status matches should_be_empty</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_empty(text: str | None, should_be_empty: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text is empty or not based on configuration.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    should_be_empty (bool): True if text should be empty, False if not\n\n\tReturns:\n\t    bool: True if text empty status matches should_be_empty\n\n\t\"\"\"\n\tis_empty = text is None or text.strip() == \"\"\n\treturn is_empty == should_be_empty\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_ends_with","title":"validate_ends_with  <code>staticmethod</code>","text":"<pre><code>validate_ends_with(\n\ttext: str | None, suffix: str, should_end_with: bool\n) -&gt; bool\n</code></pre> <p>Validate if text ends with a specific suffix.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>suffix</code> <code>str</code> <p>The suffix to check for</p> required <code>should_end_with</code> <code>bool</code> <p>True if text should end with suffix</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text ending matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_ends_with(text: str | None, suffix: str, should_end_with: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text ends with a specific suffix.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    suffix (str): The suffix to check for\n\t    should_end_with (bool): True if text should end with suffix\n\n\tReturns:\n\t    bool: True if text ending matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not should_end_with\n\n\tends_with = text.endswith(suffix)\n\treturn ends_with == should_end_with\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_starts_with","title":"validate_starts_with  <code>staticmethod</code>","text":"<pre><code>validate_starts_with(\n\ttext: str | None, prefix: str, should_start_with: bool\n) -&gt; bool\n</code></pre> <p>Validate if text starts with a specific prefix.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>prefix</code> <code>str</code> <p>The prefix to check for</p> required <code>should_start_with</code> <code>bool</code> <p>True if text should start with prefix</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text starting matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_starts_with(text: str | None, prefix: str, should_start_with: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text starts with a specific prefix.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    prefix (str): The prefix to check for\n\t    should_start_with (bool): True if text should start with prefix\n\n\tReturns:\n\t    bool: True if text starting matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not should_start_with\n\n\tstarts_with = text.startswith(prefix)\n\treturn starts_with == should_start_with\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_line_length","title":"validate_line_length  <code>staticmethod</code>","text":"<pre><code>validate_line_length(\n\ttext: str | None, max_line_length: float\n) -&gt; list[int]\n</code></pre> <p>Validate line lengths in multiline text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>max_line_length</code> <code>int | float</code> <p>Maximum allowed line length</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[int]</code> <p>List of line numbers with errors (0-indexed)</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_line_length(text: str | None, max_line_length: float) -&gt; list[int]:\n\t\"\"\"\n\tValidate line lengths in multiline text.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    max_line_length (int | float): Maximum allowed line length\n\n\tReturns:\n\t    list: List of line numbers with errors (0-indexed)\n\n\t\"\"\"\n\tif text is None or max_line_length == float(\"inf\"):\n\t\treturn []\n\n\tlines = text.splitlines()\n\treturn [i for i, line in enumerate(lines) if len(line) &gt; max_line_length]\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_leading_blank","title":"validate_leading_blank  <code>staticmethod</code>","text":"<pre><code>validate_leading_blank(\n\ttext: str | None, required_blank: bool\n) -&gt; bool\n</code></pre> <p>Validate if text starts with a blank line.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>required_blank</code> <code>bool</code> <p>True if text should start with blank line</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text leading blank matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_leading_blank(text: str | None, required_blank: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text starts with a blank line.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    required_blank (bool): True if text should start with blank line\n\n\tReturns:\n\t    bool: True if text leading blank matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not required_blank\n\n\tlines = text.splitlines()\n\thas_leading_blank = len(lines) &gt; 0 and (len(lines) == 1 or not lines[0].strip())\n\treturn has_leading_blank == required_blank\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_trim","title":"validate_trim  <code>staticmethod</code>","text":"<pre><code>validate_trim(text: str | None) -&gt; bool\n</code></pre> <p>Validate if text has no leading/trailing whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text has no leading/trailing whitespace</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_trim(text: str | None) -&gt; bool:\n\t\"\"\"\n\tValidate if text has no leading/trailing whitespace.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\n\tReturns:\n\t    bool: True if text has no leading/trailing whitespace\n\n\t\"\"\"\n\tif text is None:\n\t\treturn True\n\n\treturn text == text.strip()\n</code></pre>"},{"location":"api/git/commit_linter/validators/#codemap.git.commit_linter.validators.CommitValidators.validate_contains","title":"validate_contains  <code>staticmethod</code>","text":"<pre><code>validate_contains(\n\ttext: str | None, substring: str, should_contain: bool\n) -&gt; bool\n</code></pre> <p>Validate if text contains a specific substring.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | None</code> <p>The text to validate</p> required <code>substring</code> <code>str</code> <p>The substring to check for</p> required <code>should_contain</code> <code>bool</code> <p>True if text should contain substring</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if text contains substring matches expectation</p> Source code in <code>src/codemap/git/commit_linter/validators.py</code> <pre><code>@staticmethod\ndef validate_contains(text: str | None, substring: str, should_contain: bool) -&gt; bool:\n\t\"\"\"\n\tValidate if text contains a specific substring.\n\n\tArgs:\n\t    text (str | None): The text to validate\n\t    substring (str): The substring to check for\n\t    should_contain (bool): True if text should contain substring\n\n\tReturns:\n\t    bool: True if text contains substring matches expectation\n\n\t\"\"\"\n\tif text is None:\n\t\treturn not should_contain\n\n\tcontains = substring in text\n\treturn contains == should_contain\n</code></pre>"},{"location":"api/git/diff_splitter/","title":"Diff Splitter Overview","text":"<p>Diff splitting package for CodeMap.</p> <ul> <li>Constants - Constants for diff splitting functionality.</li> <li>Schemas - Schema definitions for diff splitting.</li> <li>Splitter - Diff splitting implementation for CodeMap.</li> <li>Strategies - Strategies for splitting git diffs into logical chunks.</li> <li>Utils - Utility functions for diff splitting.</li> </ul>"},{"location":"api/git/diff_splitter/constants/","title":"Constants","text":"<p>Constants for diff splitting functionality.</p>"},{"location":"api/git/diff_splitter/constants/#codemap.git.diff_splitter.constants.MIN_NAME_LENGTH_FOR_SIMILARITY","title":"MIN_NAME_LENGTH_FOR_SIMILARITY  <code>module-attribute</code>","text":"<pre><code>MIN_NAME_LENGTH_FOR_SIMILARITY: Final = 3\n</code></pre>"},{"location":"api/git/diff_splitter/constants/#codemap.git.diff_splitter.constants.EPSILON","title":"EPSILON  <code>module-attribute</code>","text":"<pre><code>EPSILON = 1e-10\n</code></pre>"},{"location":"api/git/diff_splitter/constants/#codemap.git.diff_splitter.constants.MAX_FILES_PER_GROUP","title":"MAX_FILES_PER_GROUP  <code>module-attribute</code>","text":"<pre><code>MAX_FILES_PER_GROUP: Final = 10\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/","title":"Schemas","text":"<p>Schema definitions for diff splitting.</p>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk","title":"DiffChunk  <code>dataclass</code>","text":"<p>Represents a logical chunk of changes.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>@dataclass\nclass DiffChunk:\n\t\"\"\"Represents a logical chunk of changes.\"\"\"\n\n\tfiles: list[str]\n\tcontent: str\n\tdescription: str | None = None\n\tis_llm_generated: bool = False\n\tfiltered_files: list[str] | None = None\n\n\tdef __post_init__(self) -&gt; None:\n\t\t\"\"\"Initialize default values.\"\"\"\n\t\tif self.filtered_files is None:\n\t\t\tself.filtered_files = []\n\n\tdef __hash__(self) -&gt; int:\n\t\t\"\"\"\n\t\tMake DiffChunk hashable by using the object's id.\n\n\t\tReturns:\n\t\t        Hash value based on the object's id\n\n\t\t\"\"\"\n\t\treturn hash(id(self))\n\n\tdef __eq__(self, other: object) -&gt; bool:\n\t\t\"\"\"\n\t\tCompare DiffChunk objects for equality.\n\n\t\tArgs:\n\t\t        other: Another object to compare with\n\n\t\tReturns:\n\t\t        True if the objects are the same instance, False otherwise\n\n\t\t\"\"\"\n\t\tif not isinstance(other, DiffChunk):\n\t\t\treturn False\n\t\treturn id(self) == id(other)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__init__","title":"__init__","text":"<pre><code>__init__(\n\tfiles: list[str],\n\tcontent: str,\n\tdescription: str | None = None,\n\tis_llm_generated: bool = False,\n\tfiltered_files: list[str] | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: list[str]\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.is_llm_generated","title":"is_llm_generated  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_llm_generated: bool = False\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.filtered_files","title":"filtered_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filtered_files: list[str] | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Initialize default values.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\t\"\"\"Initialize default values.\"\"\"\n\tif self.filtered_files is None:\n\t\tself.filtered_files = []\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Make DiffChunk hashable by using the object's id.</p> <p>Returns:</p> Type Description <code>int</code> <p>Hash value based on the object's id</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def __hash__(self) -&gt; int:\n\t\"\"\"\n\tMake DiffChunk hashable by using the object's id.\n\n\tReturns:\n\t        Hash value based on the object's id\n\n\t\"\"\"\n\treturn hash(id(self))\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunk.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: object) -&gt; bool\n</code></pre> <p>Compare DiffChunk objects for equality.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>Another object to compare with</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the objects are the same instance, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n\t\"\"\"\n\tCompare DiffChunk objects for equality.\n\n\tArgs:\n\t        other: Another object to compare with\n\n\tReturns:\n\t        True if the objects are the same instance, False otherwise\n\n\t\"\"\"\n\tif not isinstance(other, DiffChunk):\n\t\treturn False\n\treturn id(self) == id(other)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData","title":"DiffChunkData  <code>dataclass</code>","text":"<p>Dictionary-based representation of a DiffChunk for serialization.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>@dataclass\nclass DiffChunkData:\n\t\"\"\"Dictionary-based representation of a DiffChunk for serialization.\"\"\"\n\n\tfiles: list[str]\n\tcontent: str\n\tdescription: str | None = None\n\tis_llm_generated: bool = False\n\tfiltered_files: list[str] | None = None\n\n\t@classmethod\n\tdef from_chunk(cls, chunk: DiffChunk) -&gt; \"DiffChunkData\":\n\t\t\"\"\"Create a DiffChunkData from a DiffChunk.\"\"\"\n\t\treturn cls(\n\t\t\tfiles=chunk.files,\n\t\t\tcontent=chunk.content,\n\t\t\tdescription=chunk.description,\n\t\t\tis_llm_generated=chunk.is_llm_generated,\n\t\t\tfiltered_files=chunk.filtered_files,\n\t\t)\n\n\tdef to_chunk(self) -&gt; DiffChunk:\n\t\t\"\"\"Convert DiffChunkData to a DiffChunk.\"\"\"\n\t\treturn DiffChunk(\n\t\t\tfiles=self.files,\n\t\t\tcontent=self.content,\n\t\t\tdescription=self.description,\n\t\t\tis_llm_generated=self.is_llm_generated,\n\t\t\tfiltered_files=self.filtered_files,\n\t\t)\n\n\tdef to_dict(self) -&gt; dict[str, Any]:\n\t\t\"\"\"Convert to a dictionary.\"\"\"\n\t\treturn {\n\t\t\t\"files\": self.files,\n\t\t\t\"content\": self.content,\n\t\t\t\"description\": self.description,\n\t\t\t\"is_llm_generated\": self.is_llm_generated,\n\t\t\t\"filtered_files\": self.filtered_files,\n\t\t}\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.__init__","title":"__init__","text":"<pre><code>__init__(\n\tfiles: list[str],\n\tcontent: str,\n\tdescription: str | None = None,\n\tis_llm_generated: bool = False,\n\tfiltered_files: list[str] | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: list[str]\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.is_llm_generated","title":"is_llm_generated  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_llm_generated: bool = False\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.filtered_files","title":"filtered_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filtered_files: list[str] | None = None\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.from_chunk","title":"from_chunk  <code>classmethod</code>","text":"<pre><code>from_chunk(chunk: DiffChunk) -&gt; DiffChunkData\n</code></pre> <p>Create a DiffChunkData from a DiffChunk.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>@classmethod\ndef from_chunk(cls, chunk: DiffChunk) -&gt; \"DiffChunkData\":\n\t\"\"\"Create a DiffChunkData from a DiffChunk.\"\"\"\n\treturn cls(\n\t\tfiles=chunk.files,\n\t\tcontent=chunk.content,\n\t\tdescription=chunk.description,\n\t\tis_llm_generated=chunk.is_llm_generated,\n\t\tfiltered_files=chunk.filtered_files,\n\t)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.to_chunk","title":"to_chunk","text":"<pre><code>to_chunk() -&gt; DiffChunk\n</code></pre> <p>Convert DiffChunkData to a DiffChunk.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def to_chunk(self) -&gt; DiffChunk:\n\t\"\"\"Convert DiffChunkData to a DiffChunk.\"\"\"\n\treturn DiffChunk(\n\t\tfiles=self.files,\n\t\tcontent=self.content,\n\t\tdescription=self.description,\n\t\tis_llm_generated=self.is_llm_generated,\n\t\tfiltered_files=self.filtered_files,\n\t)\n</code></pre>"},{"location":"api/git/diff_splitter/schemas/#codemap.git.diff_splitter.schemas.DiffChunkData.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to a dictionary.</p> Source code in <code>src/codemap/git/diff_splitter/schemas.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n\t\"\"\"Convert to a dictionary.\"\"\"\n\treturn {\n\t\t\"files\": self.files,\n\t\t\"content\": self.content,\n\t\t\"description\": self.description,\n\t\t\"is_llm_generated\": self.is_llm_generated,\n\t\t\"filtered_files\": self.filtered_files,\n\t}\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/","title":"Splitter","text":"<p>Diff splitting implementation for CodeMap.</p>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.MAX_DIFF_CONTENT_LENGTH","title":"MAX_DIFF_CONTENT_LENGTH  <code>module-attribute</code>","text":"<pre><code>MAX_DIFF_CONTENT_LENGTH = 100000\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.MAX_DIFF_LINES","title":"MAX_DIFF_LINES  <code>module-attribute</code>","text":"<pre><code>MAX_DIFF_LINES = 1000\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.SMALL_SECTION_SIZE","title":"SMALL_SECTION_SIZE  <code>module-attribute</code>","text":"<pre><code>SMALL_SECTION_SIZE = 50\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.COMPLEX_SECTION_SIZE","title":"COMPLEX_SECTION_SIZE  <code>module-attribute</code>","text":"<pre><code>COMPLEX_SECTION_SIZE = 100\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter","title":"DiffSplitter","text":"<p>Splits Git diffs into logical chunks.</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>class DiffSplitter:\n\t\"\"\"Splits Git diffs into logical chunks.\"\"\"\n\n\t# Class-level cache for the embedding model\n\t_embedding_model = None\n\t# Track availability of sentence-transformers and the model\n\t_sentence_transformers_available = None\n\t_model_available = None\n\n\tdef __init__(\n\t\tself,\n\t\trepo_root: Path,\n\t\t# Defaults are now sourced from DEFAULT_CONFIG\n\t\tsimilarity_threshold: float = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"similarity_threshold\"],\n\t\tdirectory_similarity_threshold: float = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\n\t\t\t\"directory_similarity_threshold\"\n\t\t],\n\t\tmin_chunks_for_consolidation: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"min_chunks_for_consolidation\"],\n\t\tmax_chunks_before_consolidation: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\n\t\t\t\"max_chunks_before_consolidation\"\n\t\t],\n\t\tmax_file_size_for_llm: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_file_size_for_llm\"],\n\t\tmax_log_diff_size: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_log_diff_size\"],\n\t\tmodel_name: str = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"model_name\"],\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the diff splitter.\n\n\t\tArgs:\n\t\t    repo_root: Root directory of the Git repository\n\t\t    similarity_threshold: Threshold for grouping by content similarity.\n\t\t    directory_similarity_threshold: Threshold for directory similarity.\n\t\t    min_chunks_for_consolidation: Min chunks to trigger consolidation.\n\t\t    max_chunks_before_consolidation: Max chunks allowed before forced consolidation.\n\t\t    max_file_size_for_llm: Max file size (bytes) to process for LLM context.\n\t\t        Defaults to value from `DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_file_size_for_llm\"]` if None.\n\t\t    max_log_diff_size: Max diff size (bytes) to log in debug mode.\n\t\t        Defaults to value from `DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_log_diff_size\"]` if None.\n\t\t    model_name: Name of the sentence-transformer model to use.\n\t\t        Defaults to value from `DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"model_name\"]` if None.\n\n\t\t\"\"\"\n\t\tself.repo_root = repo_root\n\t\t# Store thresholds\n\t\tself.similarity_threshold = similarity_threshold\n\t\tself.directory_similarity_threshold = directory_similarity_threshold\n\t\tself.min_chunks_for_consolidation = min_chunks_for_consolidation\n\t\tself.max_chunks_before_consolidation = max_chunks_before_consolidation\n\t\t# Store other settings\n\t\tself.max_file_size_for_llm = max_file_size_for_llm\n\t\tself.max_log_diff_size = max_log_diff_size\n\t\tself.model_name = model_name\n\n\t\t# Do NOT automatically check availability - let the command class do this explicitly\n\t\t# This avoids checks happening during initialization without visible loading states\n\n\t@classmethod\n\tdef _check_sentence_transformers_availability(cls) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if sentence-transformers package is available.\n\n\t\tReturns:\n\t\t    True if sentence-transformers is available, False otherwise\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# This is needed for the import check, but don't flag as unused\n\t\t\timport sentence_transformers  # type: ignore  # noqa: F401, PGH003\n\n\t\t\t# Set the class flag for future reference\n\t\t\tcls._sentence_transformers_available = True\n\t\t\tlogger.debug(\"sentence-transformers is available\")\n\t\t\treturn True\n\t\texcept ImportError as e:\n\t\t\t# Log the specific import error for better debugging\n\t\t\tcls._sentence_transformers_available = False\n\t\t\tlogger.warning(\n\t\t\t\t\"sentence-transformers import failed: %s. Semantic similarity features will be limited. \"\n\t\t\t\t\"Install with: pip install sentence-transformers numpy\",\n\t\t\t\te,\n\t\t\t)\n\t\t\treturn False\n\t\texcept (RuntimeError, ValueError, AttributeError) as e:\n\t\t\t# Catch specific errors during import\n\t\t\tcls._sentence_transformers_available = False\n\t\t\tlogger.warning(\n\t\t\t\t\"Unexpected error importing sentence-transformers: %s. Semantic similarity features will be limited.\", e\n\t\t\t)\n\t\t\treturn False\n\n\t@classmethod\n\tdef are_sentence_transformers_available(cls) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if sentence transformers are available.\n\n\t\tReturns:\n\t\t    True if sentence transformers are available, False otherwise\n\n\t\t\"\"\"\n\t\treturn cls._sentence_transformers_available or cls._check_sentence_transformers_availability()\n\n\t@classmethod\n\tdef is_model_available(cls) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if embedding model is available.\n\n\t\tReturns:\n\t\t    True if embedding model is available, False otherwise\n\n\t\t\"\"\"\n\t\treturn bool(cls._model_available)\n\n\t@classmethod\n\tdef set_model_available(cls, value: bool) -&gt; None:\n\t\t\"\"\"\n\t\tSet model availability flag.\n\n\t\tArgs:\n\t\t    value: Boolean indicating if model is available\n\n\t\t\"\"\"\n\t\tcls._model_available = value\n\n\t@classmethod\n\tdef get_embedding_model(cls) -&gt; EmbeddingModel | None:\n\t\t\"\"\"\n\t\tGet the embedding model.\n\n\t\tReturns:\n\t\t    The embedding model or None if not available\n\n\t\t\"\"\"\n\t\treturn cls._embedding_model\n\n\t@classmethod\n\tdef set_embedding_model(cls, model: EmbeddingModel) -&gt; None:\n\t\t\"\"\"\n\t\tSet the embedding model.\n\n\t\tArgs:\n\t\t    model: The embedding model to set\n\n\t\t\"\"\"\n\t\tcls._embedding_model = model\n\n\tdef _check_model_availability(self) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if the embedding model is available using the instance's configured model name.\n\n\t\tReturns:\n\t\t    True if model is available, False otherwise\n\n\t\t\"\"\"\n\t\t# Use class method to access class-level cache check\n\t\tif not self.__class__.are_sentence_transformers_available():\n\t\t\treturn False\n\n\t\ttry:\n\t\t\tfrom sentence_transformers import SentenceTransformer\n\n\t\t\t# Use class method to access class-level cache\n\t\t\tif self.__class__.get_embedding_model() is None:\n\t\t\t\t# Use self.model_name from instance configuration\n\t\t\t\tlogger.debug(\"Loading embedding model: %s\", self.model_name)\n\n\t\t\t\ttry:\n\t\t\t\t\tconsole.print(\"Loading embedding model...\")\n\t\t\t\t\t# Load the model using self.model_name\n\t\t\t\t\tmodel = SentenceTransformer(self.model_name)\n\t\t\t\t\tself.__class__.set_embedding_model(cast(\"EmbeddingModel\", model))\n\t\t\t\t\tconsole.print(\"[green]\u2713[/green] Model loaded successfully\")\n\t\t\t\t\tlogger.debug(\"Initialized embedding model: %s\", self.model_name)\n\t\t\t\t\t# Set class-level flag via class method\n\t\t\t\t\tself.__class__.set_model_available(True)\n\t\t\t\t\treturn True\n\t\t\t\texcept ImportError as e:\n\t\t\t\t\tlogger.exception(\"Missing dependencies for embedding model\")\n\t\t\t\t\tconsole.print(f\"[red]Error: Missing dependencies: {e}[/red]\")\n\t\t\t\t\tself.__class__.set_model_available(False)\n\t\t\t\t\treturn False\n\t\t\t\texcept MemoryError:\n\t\t\t\t\tlogger.exception(\"Not enough memory to load embedding model\")\n\t\t\t\t\tconsole.print(\"[red]Error: Not enough memory to load embedding model[/red]\")\n\t\t\t\t\tself.__class__.set_model_available(False)\n\t\t\t\t\treturn False\n\t\t\t\texcept ValueError as e:\n\t\t\t\t\tlogger.exception(\"Invalid model configuration\")\n\t\t\t\t\tconsole.print(f\"[red]Error: Invalid model configuration: {e}[/red]\")\n\t\t\t\t\tself.__class__.set_model_available(False)\n\t\t\t\t\treturn False\n\t\t\t\texcept RuntimeError as e:\n\t\t\t\t\terror_msg = str(e)\n\t\t\t\t\t# Check for CUDA/GPU related errors\n\t\t\t\t\tif \"CUDA\" in error_msg or \"GPU\" in error_msg:\n\t\t\t\t\t\tlogger.exception(\"GPU error when loading model\")\n\t\t\t\t\t\tconsole.print(\"[red]Error: GPU/CUDA error. Try using CPU only mode.[/red]\")\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.exception(\"Runtime error when loading model\")\n\t\t\t\t\t\tconsole.print(f\"[red]Error loading model: {error_msg}[/red]\")\n\t\t\t\t\tself.__class__.set_model_available(False)\n\t\t\t\t\treturn False\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tlogger.exception(\"Unexpected error loading embedding model\")\n\t\t\t\t\tconsole.print(f\"[red]Unexpected error loading model: {e}[/red]\")\n\t\t\t\t\tself.__class__.set_model_available(False)\n\t\t\t\t\treturn False\n\t\t\t# If we already have a model loaded, make sure to set the flag to True\n\t\t\tself.__class__.set_model_available(True)\n\t\t\treturn True\n\t\texcept Exception as e:\n\t\t\t# This is the outer exception handler for any unexpected errors\n\t\t\tlogger.exception(\"Failed to load embedding model %s\", self.model_name)\n\t\t\tconsole.print(f\"[red]Failed to load embedding model: {e}[/red]\")\n\t\t\tself.__class__.set_model_available(False)\n\t\t\treturn False\n\n\tdef split_diff(self, diff: GitDiff) -&gt; tuple[list[DiffChunk], list[str]]:\n\t\t\"\"\"\n\t\tSplit a diff into logical chunks using semantic splitting.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    Tuple of (List of DiffChunk objects based on semantic analysis, List of filtered large files)\n\n\t\tRaises:\n\t\t    ValueError: If semantic splitting is not available or fails\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\treturn [], []\n\n\t\t# Special handling for untracked files - bypass semantic split since the content isn't a proper diff format\n\t\tif diff.is_untracked:\n\t\t\tlogger.debug(\"Processing untracked files with special handling: %d files\", len(diff.files))\n\t\t\t# Create a simple chunk per file to avoid errors with unidiff parsing\n\t\t\tchunks = []\n\t\t\tfor file_path in diff.files:\n\t\t\t\t# Create a basic chunk with file info but without trying to parse the content as a diff\n\t\t\t\tchunks = [\n\t\t\t\t\tDiffChunk(\n\t\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\t\tcontent=f\"New untracked file: {file_path}\",\n\t\t\t\t\t\tdescription=f\"New file: {file_path}\",\n\t\t\t\t\t)\n\t\t\t\t\tfor file_path in diff.files\n\t\t\t\t]\n\t\t\treturn chunks, []\n\n\t\t# In test environments, log the diff content for debugging\n\t\tif is_test_environment():\n\t\t\tlogger.debug(\"Processing diff in test environment with %d files\", len(diff.files) if diff.files else 0)\n\t\t\tif diff.content and len(diff.content) &lt; self.max_log_diff_size:  # Use configured max log size\n\t\t\t\tlogger.debug(\"Diff content: %s\", diff.content)\n\n\t\t# Process files in the diff\n\t\tif diff.files:\n\t\t\t# Filter for valid files (existence, tracked status), max_size check removed here\n\t\t\tdiff.files, _ = filter_valid_files(diff.files, is_test_environment())\n\t\t\t# filtered_large_files list is no longer populated or used here\n\n\t\tif not diff.files:\n\t\t\tlogger.warning(\"No valid files to process after filtering\")\n\t\t\treturn [], []  # Return empty lists\n\n\t\t# Set up availability flags if not already set\n\t\t# Use class method to check sentence transformers availability\n\t\tif not self.__class__.are_sentence_transformers_available():\n\t\t\tmsg = (\n\t\t\t\t\"Semantic splitting is not available. sentence-transformers package is required. \"\n\t\t\t\t\"Install with: pip install sentence-transformers numpy\"\n\t\t\t)\n\t\t\traise ValueError(msg)\n\n\t\t# Try to load the model using the instance method\n\t\twith loading_spinner(\"Loading embedding model...\"):\n\t\t\t# Use self._check_model_availability() - it uses self.model_name internally\n\t\t\tif not self.__class__.is_model_available():\n\t\t\t\tself._check_model_availability()\n\n\t\tif not self.__class__.is_model_available():\n\t\t\tmsg = \"Semantic splitting failed: embedding model could not be loaded. Check logs for details.\"\n\t\t\traise ValueError(msg)\n\n\t\ttry:\n\t\t\tchunks = self._split_semantic(diff)\n\n\t\t\t# If we truncated the content, restore the original content for the actual chunks\n\t\t\tif diff.content and chunks:\n\t\t\t\t# Create a mapping of file paths to chunks for quick lookup\n\t\t\t\tchunks_by_file = {}\n\t\t\t\tfor chunk in chunks:\n\t\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\t\tif file_path not in chunks_by_file:\n\t\t\t\t\t\t\tchunks_by_file[file_path] = []\n\t\t\t\t\t\tchunks_by_file[file_path].append(chunk)\n\n\t\t\t\t# For chunks that represent files we can find in the original content,\n\t\t\t\t# update their content to include the full original diff for that file\n\t\t\t\tfor chunk in chunks:\n\t\t\t\t\t# Use a heuristic to match file sections in the original content\n\t\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\t\tfile_marker = f\"diff --git a/{file_path} b/{file_path}\"\n\t\t\t\t\t\tif file_marker in diff.content:\n\t\t\t\t\t\t\t# Found a match for this file in the original content\n\t\t\t\t\t\t\t# Extract that file's complete diff section\n\t\t\t\t\t\t\tstart_idx = diff.content.find(file_marker)\n\t\t\t\t\t\t\tend_idx = diff.content.find(\"diff --git\", start_idx + len(file_marker))\n\t\t\t\t\t\t\tif end_idx == -1:  # Last file in the diff\n\t\t\t\t\t\t\t\tend_idx = len(diff.content)\n\n\t\t\t\t\t\t\tfile_diff = diff.content[start_idx:end_idx].strip()\n\n\t\t\t\t\t\t\t# Now replace just this file's content in the chunk\n\t\t\t\t\t\t\t# This is a heuristic that may need adjustment based on your diff format\n\t\t\t\t\t\t\tif chunk.content and file_marker in chunk.content:\n\t\t\t\t\t\t\t\tchunk_start = chunk.content.find(file_marker)\n\t\t\t\t\t\t\t\tchunk_end = chunk.content.find(\"diff --git\", chunk_start + len(file_marker))\n\t\t\t\t\t\t\t\tif chunk_end == -1:  # Last file in the chunk\n\t\t\t\t\t\t\t\t\tchunk_end = len(chunk.content)\n\n\t\t\t\t\t\t\t\t# Replace this file's truncated diff with the full diff\n\t\t\t\t\t\t\t\tchunk.content = chunk.content[:chunk_start] + file_diff + chunk.content[chunk_end:]\n\n\t\t\treturn chunks, []\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Semantic splitting failed\")\n\t\t\tconsole.print(f\"[red]Semantic splitting failed: {e}[/red]\")\n\n\t\t\t# Try basic splitting as a fallback\n\t\t\tlogger.warning(\"Falling back to basic file splitting\")\n\t\t\tconsole.print(\"[yellow]Falling back to basic file splitting[/yellow]\")\n\t\t\t# Return empty list for filtered_large_files as it's no longer tracked here\n\t\t\treturn self._create_basic_file_chunk(diff), []\n\n\tdef _create_basic_file_chunk(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tCreate a basic chunk per file without semantic analysis.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects, one per file\n\n\t\t\"\"\"\n\t\tchunks = []\n\n\t\tif diff.files:\n\t\t\t# Create a basic chunk, one per file in this strategy, no semantic grouping\n\t\t\tstrategy = FileSplitStrategy()\n\t\t\tchunks = strategy.split(diff)\n\n\t\treturn chunks\n\n\tdef _split_semantic(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tPerform semantic splitting, falling back if needed.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects\n\n\t\tRaises:\n\t\t    ValueError: If semantic splitting fails and fallback is not possible.\n\n\t\t\"\"\"\n\t\tif not self.are_sentence_transformers_available():\n\t\t\tlogger.warning(\"Sentence transformers unavailable. Falling back to file-based splitting.\")\n\t\t\t# Directly use FileSplitStrategy when ST is unavailable\n\t\t\tfile_splitter = FileSplitStrategy()\n\t\t\treturn file_splitter.split(diff)\n\n\t\t# Existing logic for semantic splitting when ST is available\n\t\ttry:\n\t\t\tsemantic_strategy = SemanticSplitStrategy(embedding_model=self._embedding_model)\n\t\t\treturn semantic_strategy.split(diff)\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Semantic splitting failed: %s. Falling back to file splitting.\")\n\t\t\t# Fallback to FileSplitStrategy on any semantic splitting error\n\t\t\tfile_splitter = FileSplitStrategy()\n\t\t\treturn file_splitter.split(diff)\n\n\tdef _calculate_semantic_similarity(self, text1: str, text2: str) -&gt; float:\n\t\t\"\"\"\n\t\tCalculate semantic similarity between two texts using the embedding model.\n\n\t\tArgs:\n\t\t    text1: First text\n\t\t    text2: Second text\n\n\t\tReturns:\n\t\t    Similarity score between 0 and 1\n\n\t\t\"\"\"\n\t\t# Check if embedding model is available\n\t\tif not self.__class__.are_sentence_transformers_available():\n\t\t\tlogger.debug(\"Sentence transformers not available, returning zero similarity\")\n\t\t\treturn 0.0\n\n\t\t# Call instance method self._check_model_availability()\n\t\tif not self.__class__.is_model_available():\n\t\t\tself._check_model_availability()\n\n\t\tif not self.__class__.is_model_available() or self.__class__.get_embedding_model() is None:\n\t\t\tlogger.debug(\"Embedding model not available, returning zero similarity\")\n\t\t\treturn 0.0\n\n\t\t# Assign to local variable after check guarantees it's not None\n\t\tembedding_model_maybe_none = self.__class__.get_embedding_model()\n\t\tif embedding_model_maybe_none is None:\n\t\t\t# This case should have been caught earlier, but log just in case\n\t\t\tlogger.error(\"Embedding model unexpectedly None after availability check\")\n\t\t\treturn 0.0\n\n\t\tembedding_model = embedding_model_maybe_none  # Now we know it's not None\n\n\t\ttry:\n\t\t\t# Get embeddings for both texts\n\t\t\temb1 = embedding_model.encode([text1])[0]\n\t\t\temb2 = embedding_model.encode([text2])[0]\n\n\t\t\t# Calculate similarity using numpy\n\t\t\treturn calculate_semantic_similarity(emb1.tolist(), emb2.tolist())\n\t\texcept (ValueError, TypeError, IndexError, RuntimeError) as e:\n\t\t\tlogger.warning(\"Failed to calculate semantic similarity: %s\", e)\n\t\t\treturn 0.0\n\n\tdef encode_chunks(self, chunks: list[str]) -&gt; dict[str, np.ndarray]:\n\t\t\"\"\"\n\t\tEncode a list of text chunks using the embedding model.\n\n\t\tArgs:\n\t\t    chunks: List of text chunks to encode\n\n\t\tReturns:\n\t\t    Dictionary with embeddings array\n\n\t\t\"\"\"\n\t\t# Ensure the model is initialized\n\t\tif self.__class__.are_sentence_transformers_available() and not self.__class__.is_model_available():\n\t\t\tself._check_model_availability()\n\n\t\tif not self.__class__.is_model_available():\n\t\t\tlogger.debug(\"Embedding model not available, returning empty embeddings\")\n\t\t\treturn {\"embeddings\": np.array([])}\n\n\t\t# Skip empty chunks\n\t\tif not chunks:\n\t\t\tlogger.debug(\"No chunks to encode\")\n\t\t\treturn {\"embeddings\": np.array([])}\n\n\t\t# Use class method for class cache access\n\t\tif self.__class__.get_embedding_model() is None:\n\t\t\tlogger.debug(\"Embedding model is None but was marked as available, reinitializing\")\n\t\t\t# Re-check availability using instance method\n\t\t\tself._check_model_availability()\n\n\t\t# Check again after potential re-initialization and assign to local variable\n\t\tif self.__class__.get_embedding_model() is None:\n\t\t\tlogger.error(\"Embedding model is still None after re-check\")\n\t\t\treturn {\"embeddings\": np.array([])}\n\n\t\t# Explicitly cast after the check\n\t\tembedding_model_maybe_none = self.__class__.get_embedding_model()\n\t\tif embedding_model_maybe_none is None:\n\t\t\tlogger.error(\"Embedding model unexpectedly None in encode_chunks\")\n\t\t\treturn {\"embeddings\": np.array([])}\n\n\t\tembedding_model = embedding_model_maybe_none  # Now we know it's not None\n\n\t\ttry:\n\t\t\tlogger.debug(\"Encoding %d chunks\", len(chunks))\n\t\t\tembeddings = embedding_model.encode(chunks)\n\t\t\tlogger.debug(\"Successfully encoded %d chunks to shape %s\", len(chunks), embeddings.shape)\n\t\t\treturn {\"embeddings\": embeddings}\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error encoding chunks\")\n\t\t\treturn {\"embeddings\": np.array([])}  # Return empty on error\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_root: Path,\n\tsimilarity_threshold: float = DEFAULT_CONFIG[\"commit\"][\n\t\t\"diff_splitter\"\n\t][\"similarity_threshold\"],\n\tdirectory_similarity_threshold: float = DEFAULT_CONFIG[\n\t\t\"commit\"\n\t][\"diff_splitter\"][\"directory_similarity_threshold\"],\n\tmin_chunks_for_consolidation: int = DEFAULT_CONFIG[\n\t\t\"commit\"\n\t][\"diff_splitter\"][\"min_chunks_for_consolidation\"],\n\tmax_chunks_before_consolidation: int = DEFAULT_CONFIG[\n\t\t\"commit\"\n\t][\"diff_splitter\"][\"max_chunks_before_consolidation\"],\n\tmax_file_size_for_llm: int = DEFAULT_CONFIG[\"commit\"][\n\t\t\"diff_splitter\"\n\t][\"max_file_size_for_llm\"],\n\tmax_log_diff_size: int = DEFAULT_CONFIG[\"commit\"][\n\t\t\"diff_splitter\"\n\t][\"max_log_diff_size\"],\n\tmodel_name: str = DEFAULT_CONFIG[\"commit\"][\n\t\t\"diff_splitter\"\n\t][\"model_name\"],\n) -&gt; None\n</code></pre> <p>Initialize the diff splitter.</p> <p>Parameters:</p> Name Type Description Default <code>repo_root</code> <code>Path</code> <p>Root directory of the Git repository</p> required <code>similarity_threshold</code> <code>float</code> <p>Threshold for grouping by content similarity.</p> <code>DEFAULT_CONFIG['commit']['diff_splitter']['similarity_threshold']</code> <code>directory_similarity_threshold</code> <code>float</code> <p>Threshold for directory similarity.</p> <code>DEFAULT_CONFIG['commit']['diff_splitter']['directory_similarity_threshold']</code> <code>min_chunks_for_consolidation</code> <code>int</code> <p>Min chunks to trigger consolidation.</p> <code>DEFAULT_CONFIG['commit']['diff_splitter']['min_chunks_for_consolidation']</code> <code>max_chunks_before_consolidation</code> <code>int</code> <p>Max chunks allowed before forced consolidation.</p> <code>DEFAULT_CONFIG['commit']['diff_splitter']['max_chunks_before_consolidation']</code> <code>max_file_size_for_llm</code> <code>int</code> <p>Max file size (bytes) to process for LLM context. Defaults to value from <code>DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_file_size_for_llm\"]</code> if None.</p> <code>DEFAULT_CONFIG['commit']['diff_splitter']['max_file_size_for_llm']</code> <code>max_log_diff_size</code> <code>int</code> <p>Max diff size (bytes) to log in debug mode. Defaults to value from <code>DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_log_diff_size\"]</code> if None.</p> <code>DEFAULT_CONFIG['commit']['diff_splitter']['max_log_diff_size']</code> <code>model_name</code> <code>str</code> <p>Name of the sentence-transformer model to use. Defaults to value from <code>DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"model_name\"]</code> if None.</p> <code>DEFAULT_CONFIG['commit']['diff_splitter']['model_name']</code> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>def __init__(\n\tself,\n\trepo_root: Path,\n\t# Defaults are now sourced from DEFAULT_CONFIG\n\tsimilarity_threshold: float = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"similarity_threshold\"],\n\tdirectory_similarity_threshold: float = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\n\t\t\"directory_similarity_threshold\"\n\t],\n\tmin_chunks_for_consolidation: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"min_chunks_for_consolidation\"],\n\tmax_chunks_before_consolidation: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\n\t\t\"max_chunks_before_consolidation\"\n\t],\n\tmax_file_size_for_llm: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_file_size_for_llm\"],\n\tmax_log_diff_size: int = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_log_diff_size\"],\n\tmodel_name: str = DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"model_name\"],\n) -&gt; None:\n\t\"\"\"\n\tInitialize the diff splitter.\n\n\tArgs:\n\t    repo_root: Root directory of the Git repository\n\t    similarity_threshold: Threshold for grouping by content similarity.\n\t    directory_similarity_threshold: Threshold for directory similarity.\n\t    min_chunks_for_consolidation: Min chunks to trigger consolidation.\n\t    max_chunks_before_consolidation: Max chunks allowed before forced consolidation.\n\t    max_file_size_for_llm: Max file size (bytes) to process for LLM context.\n\t        Defaults to value from `DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_file_size_for_llm\"]` if None.\n\t    max_log_diff_size: Max diff size (bytes) to log in debug mode.\n\t        Defaults to value from `DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_log_diff_size\"]` if None.\n\t    model_name: Name of the sentence-transformer model to use.\n\t        Defaults to value from `DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"model_name\"]` if None.\n\n\t\"\"\"\n\tself.repo_root = repo_root\n\t# Store thresholds\n\tself.similarity_threshold = similarity_threshold\n\tself.directory_similarity_threshold = directory_similarity_threshold\n\tself.min_chunks_for_consolidation = min_chunks_for_consolidation\n\tself.max_chunks_before_consolidation = max_chunks_before_consolidation\n\t# Store other settings\n\tself.max_file_size_for_llm = max_file_size_for_llm\n\tself.max_log_diff_size = max_log_diff_size\n\tself.model_name = model_name\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = repo_root\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.similarity_threshold","title":"similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.directory_similarity_threshold","title":"directory_similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>directory_similarity_threshold = (\n\tdirectory_similarity_threshold\n)\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.min_chunks_for_consolidation","title":"min_chunks_for_consolidation  <code>instance-attribute</code>","text":"<pre><code>min_chunks_for_consolidation = min_chunks_for_consolidation\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.max_chunks_before_consolidation","title":"max_chunks_before_consolidation  <code>instance-attribute</code>","text":"<pre><code>max_chunks_before_consolidation = (\n\tmax_chunks_before_consolidation\n)\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.max_file_size_for_llm","title":"max_file_size_for_llm  <code>instance-attribute</code>","text":"<pre><code>max_file_size_for_llm = max_file_size_for_llm\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.max_log_diff_size","title":"max_log_diff_size  <code>instance-attribute</code>","text":"<pre><code>max_log_diff_size = max_log_diff_size\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name = model_name\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.are_sentence_transformers_available","title":"are_sentence_transformers_available  <code>classmethod</code>","text":"<pre><code>are_sentence_transformers_available() -&gt; bool\n</code></pre> <p>Check if sentence transformers are available.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if sentence transformers are available, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>@classmethod\ndef are_sentence_transformers_available(cls) -&gt; bool:\n\t\"\"\"\n\tCheck if sentence transformers are available.\n\n\tReturns:\n\t    True if sentence transformers are available, False otherwise\n\n\t\"\"\"\n\treturn cls._sentence_transformers_available or cls._check_sentence_transformers_availability()\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.is_model_available","title":"is_model_available  <code>classmethod</code>","text":"<pre><code>is_model_available() -&gt; bool\n</code></pre> <p>Check if embedding model is available.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if embedding model is available, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>@classmethod\ndef is_model_available(cls) -&gt; bool:\n\t\"\"\"\n\tCheck if embedding model is available.\n\n\tReturns:\n\t    True if embedding model is available, False otherwise\n\n\t\"\"\"\n\treturn bool(cls._model_available)\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.set_model_available","title":"set_model_available  <code>classmethod</code>","text":"<pre><code>set_model_available(value: bool) -&gt; None\n</code></pre> <p>Set model availability flag.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>Boolean indicating if model is available</p> required Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>@classmethod\ndef set_model_available(cls, value: bool) -&gt; None:\n\t\"\"\"\n\tSet model availability flag.\n\n\tArgs:\n\t    value: Boolean indicating if model is available\n\n\t\"\"\"\n\tcls._model_available = value\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.get_embedding_model","title":"get_embedding_model  <code>classmethod</code>","text":"<pre><code>get_embedding_model() -&gt; EmbeddingModel | None\n</code></pre> <p>Get the embedding model.</p> <p>Returns:</p> Type Description <code>EmbeddingModel | None</code> <p>The embedding model or None if not available</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>@classmethod\ndef get_embedding_model(cls) -&gt; EmbeddingModel | None:\n\t\"\"\"\n\tGet the embedding model.\n\n\tReturns:\n\t    The embedding model or None if not available\n\n\t\"\"\"\n\treturn cls._embedding_model\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.set_embedding_model","title":"set_embedding_model  <code>classmethod</code>","text":"<pre><code>set_embedding_model(model: EmbeddingModel) -&gt; None\n</code></pre> <p>Set the embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>EmbeddingModel</code> <p>The embedding model to set</p> required Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>@classmethod\ndef set_embedding_model(cls, model: EmbeddingModel) -&gt; None:\n\t\"\"\"\n\tSet the embedding model.\n\n\tArgs:\n\t    model: The embedding model to set\n\n\t\"\"\"\n\tcls._embedding_model = model\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.split_diff","title":"split_diff","text":"<pre><code>split_diff(\n\tdiff: GitDiff,\n) -&gt; tuple[list[DiffChunk], list[str]]\n</code></pre> <p>Split a diff into logical chunks using semantic splitting.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>tuple[list[DiffChunk], list[str]]</code> <p>Tuple of (List of DiffChunk objects based on semantic analysis, List of filtered large files)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If semantic splitting is not available or fails</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>def split_diff(self, diff: GitDiff) -&gt; tuple[list[DiffChunk], list[str]]:\n\t\"\"\"\n\tSplit a diff into logical chunks using semantic splitting.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    Tuple of (List of DiffChunk objects based on semantic analysis, List of filtered large files)\n\n\tRaises:\n\t    ValueError: If semantic splitting is not available or fails\n\n\t\"\"\"\n\tif not diff.files:\n\t\treturn [], []\n\n\t# Special handling for untracked files - bypass semantic split since the content isn't a proper diff format\n\tif diff.is_untracked:\n\t\tlogger.debug(\"Processing untracked files with special handling: %d files\", len(diff.files))\n\t\t# Create a simple chunk per file to avoid errors with unidiff parsing\n\t\tchunks = []\n\t\tfor file_path in diff.files:\n\t\t\t# Create a basic chunk with file info but without trying to parse the content as a diff\n\t\t\tchunks = [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=f\"New untracked file: {file_path}\",\n\t\t\t\t\tdescription=f\"New file: {file_path}\",\n\t\t\t\t)\n\t\t\t\tfor file_path in diff.files\n\t\t\t]\n\t\treturn chunks, []\n\n\t# In test environments, log the diff content for debugging\n\tif is_test_environment():\n\t\tlogger.debug(\"Processing diff in test environment with %d files\", len(diff.files) if diff.files else 0)\n\t\tif diff.content and len(diff.content) &lt; self.max_log_diff_size:  # Use configured max log size\n\t\t\tlogger.debug(\"Diff content: %s\", diff.content)\n\n\t# Process files in the diff\n\tif diff.files:\n\t\t# Filter for valid files (existence, tracked status), max_size check removed here\n\t\tdiff.files, _ = filter_valid_files(diff.files, is_test_environment())\n\t\t# filtered_large_files list is no longer populated or used here\n\n\tif not diff.files:\n\t\tlogger.warning(\"No valid files to process after filtering\")\n\t\treturn [], []  # Return empty lists\n\n\t# Set up availability flags if not already set\n\t# Use class method to check sentence transformers availability\n\tif not self.__class__.are_sentence_transformers_available():\n\t\tmsg = (\n\t\t\t\"Semantic splitting is not available. sentence-transformers package is required. \"\n\t\t\t\"Install with: pip install sentence-transformers numpy\"\n\t\t)\n\t\traise ValueError(msg)\n\n\t# Try to load the model using the instance method\n\twith loading_spinner(\"Loading embedding model...\"):\n\t\t# Use self._check_model_availability() - it uses self.model_name internally\n\t\tif not self.__class__.is_model_available():\n\t\t\tself._check_model_availability()\n\n\tif not self.__class__.is_model_available():\n\t\tmsg = \"Semantic splitting failed: embedding model could not be loaded. Check logs for details.\"\n\t\traise ValueError(msg)\n\n\ttry:\n\t\tchunks = self._split_semantic(diff)\n\n\t\t# If we truncated the content, restore the original content for the actual chunks\n\t\tif diff.content and chunks:\n\t\t\t# Create a mapping of file paths to chunks for quick lookup\n\t\t\tchunks_by_file = {}\n\t\t\tfor chunk in chunks:\n\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\tif file_path not in chunks_by_file:\n\t\t\t\t\t\tchunks_by_file[file_path] = []\n\t\t\t\t\tchunks_by_file[file_path].append(chunk)\n\n\t\t\t# For chunks that represent files we can find in the original content,\n\t\t\t# update their content to include the full original diff for that file\n\t\t\tfor chunk in chunks:\n\t\t\t\t# Use a heuristic to match file sections in the original content\n\t\t\t\tfor file_path in chunk.files:\n\t\t\t\t\tfile_marker = f\"diff --git a/{file_path} b/{file_path}\"\n\t\t\t\t\tif file_marker in diff.content:\n\t\t\t\t\t\t# Found a match for this file in the original content\n\t\t\t\t\t\t# Extract that file's complete diff section\n\t\t\t\t\t\tstart_idx = diff.content.find(file_marker)\n\t\t\t\t\t\tend_idx = diff.content.find(\"diff --git\", start_idx + len(file_marker))\n\t\t\t\t\t\tif end_idx == -1:  # Last file in the diff\n\t\t\t\t\t\t\tend_idx = len(diff.content)\n\n\t\t\t\t\t\tfile_diff = diff.content[start_idx:end_idx].strip()\n\n\t\t\t\t\t\t# Now replace just this file's content in the chunk\n\t\t\t\t\t\t# This is a heuristic that may need adjustment based on your diff format\n\t\t\t\t\t\tif chunk.content and file_marker in chunk.content:\n\t\t\t\t\t\t\tchunk_start = chunk.content.find(file_marker)\n\t\t\t\t\t\t\tchunk_end = chunk.content.find(\"diff --git\", chunk_start + len(file_marker))\n\t\t\t\t\t\t\tif chunk_end == -1:  # Last file in the chunk\n\t\t\t\t\t\t\t\tchunk_end = len(chunk.content)\n\n\t\t\t\t\t\t\t# Replace this file's truncated diff with the full diff\n\t\t\t\t\t\t\tchunk.content = chunk.content[:chunk_start] + file_diff + chunk.content[chunk_end:]\n\n\t\treturn chunks, []\n\texcept Exception as e:\n\t\tlogger.exception(\"Semantic splitting failed\")\n\t\tconsole.print(f\"[red]Semantic splitting failed: {e}[/red]\")\n\n\t\t# Try basic splitting as a fallback\n\t\tlogger.warning(\"Falling back to basic file splitting\")\n\t\tconsole.print(\"[yellow]Falling back to basic file splitting[/yellow]\")\n\t\t# Return empty list for filtered_large_files as it's no longer tracked here\n\t\treturn self._create_basic_file_chunk(diff), []\n</code></pre>"},{"location":"api/git/diff_splitter/splitter/#codemap.git.diff_splitter.splitter.DiffSplitter.encode_chunks","title":"encode_chunks","text":"<pre><code>encode_chunks(chunks: list[str]) -&gt; dict[str, ndarray]\n</code></pre> <p>Encode a list of text chunks using the embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[str]</code> <p>List of text chunks to encode</p> required <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>Dictionary with embeddings array</p> Source code in <code>src/codemap/git/diff_splitter/splitter.py</code> <pre><code>def encode_chunks(self, chunks: list[str]) -&gt; dict[str, np.ndarray]:\n\t\"\"\"\n\tEncode a list of text chunks using the embedding model.\n\n\tArgs:\n\t    chunks: List of text chunks to encode\n\n\tReturns:\n\t    Dictionary with embeddings array\n\n\t\"\"\"\n\t# Ensure the model is initialized\n\tif self.__class__.are_sentence_transformers_available() and not self.__class__.is_model_available():\n\t\tself._check_model_availability()\n\n\tif not self.__class__.is_model_available():\n\t\tlogger.debug(\"Embedding model not available, returning empty embeddings\")\n\t\treturn {\"embeddings\": np.array([])}\n\n\t# Skip empty chunks\n\tif not chunks:\n\t\tlogger.debug(\"No chunks to encode\")\n\t\treturn {\"embeddings\": np.array([])}\n\n\t# Use class method for class cache access\n\tif self.__class__.get_embedding_model() is None:\n\t\tlogger.debug(\"Embedding model is None but was marked as available, reinitializing\")\n\t\t# Re-check availability using instance method\n\t\tself._check_model_availability()\n\n\t# Check again after potential re-initialization and assign to local variable\n\tif self.__class__.get_embedding_model() is None:\n\t\tlogger.error(\"Embedding model is still None after re-check\")\n\t\treturn {\"embeddings\": np.array([])}\n\n\t# Explicitly cast after the check\n\tembedding_model_maybe_none = self.__class__.get_embedding_model()\n\tif embedding_model_maybe_none is None:\n\t\tlogger.error(\"Embedding model unexpectedly None in encode_chunks\")\n\t\treturn {\"embeddings\": np.array([])}\n\n\tembedding_model = embedding_model_maybe_none  # Now we know it's not None\n\n\ttry:\n\t\tlogger.debug(\"Encoding %d chunks\", len(chunks))\n\t\tembeddings = embedding_model.encode(chunks)\n\t\tlogger.debug(\"Successfully encoded %d chunks to shape %s\", len(chunks), embeddings.shape)\n\t\treturn {\"embeddings\": embeddings}\n\texcept Exception:\n\t\tlogger.exception(\"Error encoding chunks\")\n\t\treturn {\"embeddings\": np.array([])}  # Return empty on error\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/","title":"Strategies","text":"<p>Strategies for splitting git diffs into logical chunks.</p>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.EXPECTED_TUPLE_SIZE","title":"EXPECTED_TUPLE_SIZE  <code>module-attribute</code>","text":"<pre><code>EXPECTED_TUPLE_SIZE = 2\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.EmbeddingModel","title":"EmbeddingModel","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for embedding models.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>class EmbeddingModel(Protocol):\n\t\"\"\"Protocol for embedding models.\"\"\"\n\n\tdef encode(self, texts: Sequence[str], **kwargs: Any) -&gt; np.ndarray:  # noqa: ANN401\n\t\t\"\"\"Encode texts into embeddings.\"\"\"\n\t\t...\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.EmbeddingModel.encode","title":"encode","text":"<pre><code>encode(texts: Sequence[str], **kwargs: Any) -&gt; ndarray\n</code></pre> <p>Encode texts into embeddings.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def encode(self, texts: Sequence[str], **kwargs: Any) -&gt; np.ndarray:  # noqa: ANN401\n\t\"\"\"Encode texts into embeddings.\"\"\"\n\t...\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.BaseSplitStrategy","title":"BaseSplitStrategy","text":"<p>Base class for diff splitting strategies.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>class BaseSplitStrategy:\n\t\"\"\"Base class for diff splitting strategies.\"\"\"\n\n\tdef __init__(self, embedding_model: EmbeddingModel | None = None) -&gt; None:\n\t\t\"\"\"Initialize with optional embedding model.\"\"\"\n\t\tself._embedding_model = embedding_model\n\t\t# Precompile regex patterns for better performance\n\t\tself._file_pattern = re.compile(r\"diff --git a/.*? b/(.*?)\\n\")\n\t\tself._hunk_pattern = re.compile(r\"@@ -\\d+,\\d+ \\+\\d+,\\d+ @@\")\n\n\tdef split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit the diff into chunks.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects\n\n\t\t\"\"\"\n\t\tmsg = \"Subclasses must implement this method\"\n\t\traise NotImplementedError(msg)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.BaseSplitStrategy.__init__","title":"__init__","text":"<pre><code>__init__(\n\tembedding_model: EmbeddingModel | None = None,\n) -&gt; None\n</code></pre> <p>Initialize with optional embedding model.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def __init__(self, embedding_model: EmbeddingModel | None = None) -&gt; None:\n\t\"\"\"Initialize with optional embedding model.\"\"\"\n\tself._embedding_model = embedding_model\n\t# Precompile regex patterns for better performance\n\tself._file_pattern = re.compile(r\"diff --git a/.*? b/(.*?)\\n\")\n\tself._hunk_pattern = re.compile(r\"@@ -\\d+,\\d+ \\+\\d+,\\d+ @@\")\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.BaseSplitStrategy.split","title":"split","text":"<pre><code>split(diff: GitDiff) -&gt; list[DiffChunk]\n</code></pre> <p>Split the diff into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>List of DiffChunk objects</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tSplit the diff into chunks.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    List of DiffChunk objects\n\n\t\"\"\"\n\tmsg = \"Subclasses must implement this method\"\n\traise NotImplementedError(msg)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.FileSplitStrategy","title":"FileSplitStrategy","text":"<p>               Bases: <code>BaseSplitStrategy</code></p> <p>Strategy to split diffs by file.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>class FileSplitStrategy(BaseSplitStrategy):\n\t\"\"\"Strategy to split diffs by file.\"\"\"\n\n\tdef split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit a diff into chunks by file.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects, one per file\n\n\t\t\"\"\"\n\t\tif not diff.content:\n\t\t\treturn self._handle_empty_diff_content(diff)\n\n\t\t# Split the diff content by file\n\t\tfile_chunks = self._file_pattern.split(diff.content)[1:]  # Skip first empty chunk\n\n\t\t# Group files with their content\n\t\tchunks = []\n\t\tfor i in range(0, len(file_chunks), 2):\n\t\t\tif i + 1 &gt;= len(file_chunks):\n\t\t\t\tbreak\n\n\t\t\tfile_name = file_chunks[i]\n\t\t\tcontent = file_chunks[i + 1]\n\n\t\t\tif self._is_valid_filename(file_name) and content:\n\t\t\t\tdiff_header = f\"diff --git a/{file_name} b/{file_name}\\n\"\n\t\t\t\tchunks.append(\n\t\t\t\t\tDiffChunk(\n\t\t\t\t\t\tfiles=[file_name],\n\t\t\t\t\t\tcontent=diff_header + content,\n\t\t\t\t\t\tdescription=f\"Changes in {file_name}\",\n\t\t\t\t\t)\n\t\t\t\t)\n\n\t\treturn chunks\n\n\tdef _handle_empty_diff_content(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"Handle untracked files in empty diff content.\"\"\"\n\t\tif (not diff.is_staged or diff.is_untracked) and diff.files:\n\t\t\t# Filter out invalid file names\n\t\t\tvalid_files = [file for file in diff.files if self._is_valid_filename(file)]\n\t\t\treturn [DiffChunk(files=[f], content=\"\", description=f\"New file: {f}\") for f in valid_files]\n\t\treturn []\n\n\t@staticmethod\n\tdef _is_valid_filename(filename: str) -&gt; bool:\n\t\t\"\"\"Check if the filename is valid (not a pattern or template).\"\"\"\n\t\tif not filename:\n\t\t\treturn False\n\t\tinvalid_chars = [\"*\", \"+\", \"{\", \"}\", \"\\\\\"]\n\t\treturn not (any(char in filename for char in invalid_chars) or filename.startswith('\"'))\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.FileSplitStrategy.split","title":"split","text":"<pre><code>split(diff: GitDiff) -&gt; list[DiffChunk]\n</code></pre> <p>Split a diff into chunks by file.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>List of DiffChunk objects, one per file</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tSplit a diff into chunks by file.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    List of DiffChunk objects, one per file\n\n\t\"\"\"\n\tif not diff.content:\n\t\treturn self._handle_empty_diff_content(diff)\n\n\t# Split the diff content by file\n\tfile_chunks = self._file_pattern.split(diff.content)[1:]  # Skip first empty chunk\n\n\t# Group files with their content\n\tchunks = []\n\tfor i in range(0, len(file_chunks), 2):\n\t\tif i + 1 &gt;= len(file_chunks):\n\t\t\tbreak\n\n\t\tfile_name = file_chunks[i]\n\t\tcontent = file_chunks[i + 1]\n\n\t\tif self._is_valid_filename(file_name) and content:\n\t\t\tdiff_header = f\"diff --git a/{file_name} b/{file_name}\\n\"\n\t\t\tchunks.append(\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_name],\n\t\t\t\t\tcontent=diff_header + content,\n\t\t\t\t\tdescription=f\"Changes in {file_name}\",\n\t\t\t\t)\n\t\t\t)\n\n\treturn chunks\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy","title":"SemanticSplitStrategy","text":"<p>               Bases: <code>BaseSplitStrategy</code></p> <p>Strategy to split diffs semantically.</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>class SemanticSplitStrategy(BaseSplitStrategy):\n\t\"\"\"Strategy to split diffs semantically.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tembedding_model: EmbeddingModel | None = None,\n\t\tcode_extensions: set[str] | None = None,\n\t\trelated_file_patterns: list[tuple[Pattern, Pattern]] | None = None,\n\t\tsimilarity_threshold: float = 0.4,\n\t\tdirectory_similarity_threshold: float = 0.3,\n\t\tmin_chunks_for_consolidation: int = 2,\n\t\tmax_chunks_before_consolidation: int = 20,\n\t\tmax_file_size_for_llm: int | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the SemanticSplitStrategy.\n\n\t\tArgs:\n\t\t    embedding_model: Optional embedding model instance\n\t\t    code_extensions: Optional set of code file extensions. Defaults to config.\n\t\t    related_file_patterns: Optional list of related file patterns\n\t\t    similarity_threshold: Threshold for grouping by content similarity.\n\t\t    directory_similarity_threshold: Threshold for directory similarity.\n\t\t    min_chunks_for_consolidation: Min chunks to trigger consolidation.\n\t\t    max_chunks_before_consolidation: Max chunks allowed before forced consolidation.\n\t\t    max_file_size_for_llm: Max file size for LLM processing.\n\n\t\t\"\"\"\n\t\tsuper().__init__(embedding_model)\n\t\t# Store thresholds and settings\n\t\tself.similarity_threshold = similarity_threshold\n\t\tself.directory_similarity_threshold = directory_similarity_threshold\n\t\tself.min_chunks_for_consolidation = min_chunks_for_consolidation\n\t\tself.max_chunks_before_consolidation = max_chunks_before_consolidation\n\t\t# Use default from config if not provided\n\t\tself.max_file_size_for_llm = (\n\t\t\tmax_file_size_for_llm\n\t\t\tif max_file_size_for_llm is not None\n\t\t\telse DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_file_size_for_llm\"]\n\t\t)\n\n\t\t# Set up file extensions, defaulting to config if None is passed\n\t\tself.code_extensions = (\n\t\t\tcode_extensions\n\t\t\tif code_extensions is not None\n\t\t\telse set(DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"default_code_extensions\"])\n\t\t)\n\t\t# Initialize patterns for related files\n\t\tself.related_file_patterns = related_file_patterns or self._initialize_related_file_patterns()\n\n\tdef split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit a diff into chunks based on semantic relationships.\n\n\t\tArgs:\n\t\t    diff: GitDiff object to split\n\n\t\tReturns:\n\t\t    List of DiffChunk objects based on semantic analysis\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\tlogger.debug(\"No files to process\")\n\t\t\treturn []\n\n\t\t# Validate embedding model is available\n\t\tself._validate_embedding_model()\n\n\t\t# Handle files in manageable groups\n\t\tif len(diff.files) &gt; MAX_FILES_PER_GROUP:\n\t\t\tlogger.info(\"Processing large number of files (%d) in smaller groups\", len(diff.files))\n\n\t\t\t# Group files by directory to increase likelihood of related files being processed together\n\t\t\tfiles_by_dir = {}\n\t\t\tfor file in diff.files:\n\t\t\t\tdir_path = str(Path(file).parent)\n\t\t\t\tif dir_path not in files_by_dir:\n\t\t\t\t\tfiles_by_dir[dir_path] = []\n\t\t\t\tfiles_by_dir[dir_path].append(file)\n\n\t\t\t# Process each directory group separately, keeping chunks under 5 files\n\t\t\tall_chunks = []\n\t\t\t# Iterate directly over the file lists since the directory path isn't used here\n\t\t\tfor files in files_by_dir.values():\n\t\t\t\t# Process files in this directory in batches of 3-5\n\t\t\t\tfor i in range(0, len(files), 3):\n\t\t\t\t\tbatch = files[i : i + 3]\n\t\t\t\t\t# Create a new GitDiff for the batch, ensuring content is passed\n\t\t\t\t\tbatch_diff = GitDiff(\n\t\t\t\t\t\tfiles=batch,\n\t\t\t\t\t\tcontent=diff.content,  # Pass the original full diff content\n\t\t\t\t\t\tis_staged=diff.is_staged,\n\t\t\t\t\t)\n\t\t\t\t\tall_chunks.extend(self._process_group(batch_diff))\n\n\t\t\treturn all_chunks\n\n\t\t# For smaller groups, process normally\n\t\treturn self._process_group(diff)\n\n\tdef _process_group(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tProcess a GitDiff with one or more files.\n\n\t\tOriginally designed for single files, but now supports multiple files.\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\tlogger.warning(\"_process_group called with empty files list\")\n\t\t\treturn []\n\n\t\t# If multiple files, this used to log an error, but now we'll handle it properly\n\t\tif len(diff.files) &gt; 1:\n\t\t\tlogger.debug(\"Processing group with multiple files: %s\", diff.files)\n\n\t\t\t# Extract content for each file individually if possible\n\t\t\tchunks = []\n\t\t\tfor file_path in diff.files:\n\t\t\t\t# Try to extract just this file's diff from the full content\n\t\t\t\tfile_diff_content = self._extract_file_diff(diff.content, file_path)\n\n\t\t\t\tif file_diff_content:\n\t\t\t\t\t# Create a new diff for just this file\n\t\t\t\t\tfile_diff = GitDiff(files=[file_path], content=file_diff_content, is_staged=diff.is_staged)\n\t\t\t\t\t# Process it and add the resulting chunks\n\t\t\t\t\tenhanced_chunks = self._enhance_semantic_split(file_diff)\n\t\t\t\t\tchunks.extend(enhanced_chunks)\n\t\t\t\telse:\n\t\t\t\t\t# If we couldn't extract just this file's diff, create a simple chunk\n\t\t\t\t\tchunks.append(\n\t\t\t\t\t\tDiffChunk(\n\t\t\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\t\t\tcontent=\"\",  # Empty content as we couldn't extract it\n\t\t\t\t\t\t\tdescription=f\"Changes in {file_path}\",\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\n\t\t\t# If we couldn't create any valid chunks, fallback to the original behavior\n\t\t\tif not chunks:\n\t\t\t\treturn [DiffChunk(files=diff.files, content=diff.content, description=\"Multiple file changes\")]\n\n\t\t\treturn chunks\n\n\t\t# Original behavior for single file\n\t\tfile_path = diff.files[0]\n\n\t\t# Enhance this single file diff\n\t\tenhanced_chunks = self._enhance_semantic_split(diff)  # Pass the original diff directly\n\n\t\tif not enhanced_chunks:\n\t\t\tlogger.warning(\"No chunk generated for file: %s after enhancement.\", file_path)\n\t\t\t# Fallback if enhancement yields nothing\n\t\t\tenhanced_chunks = [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=diff.content,\n\t\t\t\t\tdescription=f\"Changes in {file_path} (enhancement failed)\",\n\t\t\t\t)\n\t\t\t]\n\n\t\t# No further consolidation or grouping needed here as we process file-by-file now\n\t\treturn enhanced_chunks\n\n\tdef _extract_file_diff(self, full_diff_content: str, file_path: str) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the diff content for a specific file from a multi-file diff.\n\n\t\tArgs:\n\t\t        full_diff_content: Complete diff content with multiple files\n\t\t        file_path: Path of the file to extract\n\n\t\tReturns:\n\t\t        The extracted diff for the specific file, or empty string if not found\n\n\t\t\"\"\"\n\t\timport re\n\n\t\t# Pattern to match the start of a diff for a file\n\t\tdiff_start_pattern = re.compile(r\"diff --git a/([^\\s]+) b/([^\\s]+)\")\n\n\t\t# Find all diff start positions\n\t\tdiff_positions = []\n\t\tfor match in diff_start_pattern.finditer(full_diff_content):\n\t\t\t_, b_file = match.groups()\n\t\t\t# For most changes both files are the same; for renames prefer b_file\n\t\t\ttarget_file = b_file\n\t\t\tdiff_positions.append((match.start(), target_file))\n\n\t\t# Sort by position\n\t\tdiff_positions.sort()\n\n\t\t# Find the diff for our file\n\t\tfile_diff = \"\"\n\t\tfor i, (start_pos, diff_file) in enumerate(diff_positions):\n\t\t\tif diff_file == file_path:\n\t\t\t\t# Found our file, now find the end\n\t\t\t\tif i &lt; len(diff_positions) - 1:\n\t\t\t\t\tend_pos = diff_positions[i + 1][0]\n\t\t\t\t\tfile_diff = full_diff_content[start_pos:end_pos]\n\t\t\t\telse:\n\t\t\t\t\t# Last file in the diff\n\t\t\t\t\tfile_diff = full_diff_content[start_pos:]\n\t\t\t\tbreak\n\n\t\treturn file_diff\n\n\tdef _validate_embedding_model(self) -&gt; None:\n\t\t\"\"\"Validate that the embedding model is available.\"\"\"\n\t\tif self._embedding_model is None and not is_test_environment():\n\t\t\tmsg = (\n\t\t\t\t\"Semantic analysis unavailable: embedding model not available. \"\n\t\t\t\t\"Make sure the model is properly loaded before calling this method.\"\n\t\t\t)\n\t\t\traise ValueError(msg)\n\n\tdef _group_chunks_by_directory(self, chunks: list[DiffChunk]) -&gt; dict[str, list[DiffChunk]]:\n\t\t\"\"\"Group chunks by their containing directory.\"\"\"\n\t\tdir_groups: dict[str, list[DiffChunk]] = {}\n\n\t\tfor chunk in chunks:\n\t\t\tif not chunk.files:\n\t\t\t\tcontinue\n\n\t\t\tfile_path = chunk.files[0]\n\t\t\tdir_path = file_path.rsplit(\"/\", 1)[0] if \"/\" in file_path else \"root\"\n\n\t\t\tif dir_path not in dir_groups:\n\t\t\t\tdir_groups[dir_path] = []\n\n\t\t\tdir_groups[dir_path].append(chunk)\n\n\t\treturn dir_groups\n\n\tdef _process_directory_group(\n\t\tself, chunks: list[DiffChunk], processed_files: set[str], semantic_chunks: list[DiffChunk]\n\t) -&gt; None:\n\t\t\"\"\"Process chunks in a single directory group.\"\"\"\n\t\tif len(chunks) == 1:\n\t\t\t# If only one file in directory, add it directly\n\t\t\tsemantic_chunks.append(chunks[0])\n\t\t\tif chunks[0].files:\n\t\t\t\tprocessed_files.update(chunks[0].files)\n\t\telse:\n\t\t\t# For directories with multiple files, try to group them\n\t\t\tdir_processed: set[str] = set()\n\n\t\t\t# First try to group by related file patterns\n\t\t\tself._group_related_files(chunks, dir_processed, semantic_chunks)\n\n\t\t\t# Then try to group remaining files by content similarity\n\t\t\tremaining_chunks = [c for c in chunks if not c.files or c.files[0] not in dir_processed]\n\n\t\t\tif remaining_chunks:\n\t\t\t\t# Use default similarity threshold instead\n\t\t\t\tself._group_by_content_similarity(remaining_chunks, semantic_chunks)\n\n\t\t\t# Add all processed files to the global processed set\n\t\t\tprocessed_files.update(dir_processed)\n\n\tdef _process_remaining_chunks(\n\t\tself, all_chunks: list[DiffChunk], processed_files: set[str], semantic_chunks: list[DiffChunk]\n\t) -&gt; None:\n\t\t\"\"\"Process any remaining chunks that weren't grouped by directory.\"\"\"\n\t\tremaining_chunks = [c for c in all_chunks if c.files and c.files[0] not in processed_files]\n\n\t\tif remaining_chunks:\n\t\t\tself._group_by_content_similarity(remaining_chunks, semantic_chunks)\n\n\tdef _consolidate_if_needed(self, semantic_chunks: list[DiffChunk]) -&gt; list[DiffChunk]:\n\t\t\"\"\"Consolidate chunks if we have too many small ones.\"\"\"\n\t\thas_single_file_chunks = any(len(chunk.files) == 1 for chunk in semantic_chunks)\n\n\t\tif len(semantic_chunks) &gt; self.max_chunks_before_consolidation and has_single_file_chunks:\n\t\t\treturn self._consolidate_small_chunks(semantic_chunks)\n\n\t\treturn semantic_chunks\n\n\t@staticmethod\n\tdef _initialize_related_file_patterns() -&gt; list[tuple[Pattern, Pattern]]:\n\t\t\"\"\"\n\t\tInitialize and compile regex patterns for related files.\n\n\t\tReturns:\n\t\t    List of compiled regex pattern pairs\n\n\t\t\"\"\"\n\t\t# Pre-compile regex for efficiency and validation\n\t\trelated_file_patterns = []\n\t\t# Define patterns using standard strings with escaped backreferences\n\t\tdefault_patterns: list[tuple[str, str]] = [\n\t\t\t# --- General Code + Test Files ---\n\t\t\t# Python\n\t\t\t(\"^(.*)\\\\.py$\", \"\\\\\\\\1_test\\\\.py$\"),\n\t\t\t(\"^(.*)\\\\.py$\", \"test_\\\\\\\\1\\\\.py$\"),\n\t\t\t(\"^(.*)\\\\.(py)$\", \"\\\\\\\\1_test\\\\.\\\\\\\\2$\"),  # For file.py and file_test.py pattern\n\t\t\t(\"^(.*)\\\\.(py)$\", \"\\\\\\\\1Test\\\\.\\\\\\\\2$\"),  # For file.py and fileTest.py pattern\n\t\t\t(\"^(.*)\\\\.py$\", \"\\\\\\\\1_spec\\\\.py$\"),\n\t\t\t(\"^(.*)\\\\.py$\", \"spec_\\\\\\\\1\\\\.py$\"),\n\t\t\t# JavaScript / TypeScript (including JSX/TSX)\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.(test|spec)\\\\.(js|jsx|ts|tsx)$\"),\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.stories\\\\.(js|jsx|ts|tsx)$\"),  # Storybook\n\t\t\t(\"^(.*)\\\\.(js|ts)$\", \"\\\\\\\\1\\\\.d\\\\.ts$\"),  # JS/TS + Declaration files\n\t\t\t# Ruby\n\t\t\t(\"^(.*)\\\\.rb$\", \"\\\\\\\\1_spec\\\\.rb$\"),\n\t\t\t(\"^(.*)\\\\.rb$\", \"\\\\\\\\1_test\\\\.rb$\"),\n\t\t\t(\"^(.*)\\\\.rb$\", \"spec/.*_spec\\\\.rb$\"),  # Common RSpec structure\n\t\t\t# Java\n\t\t\t(\"^(.*)\\\\.java$\", \"\\\\\\\\1Test\\\\.java$\"),\n\t\t\t(\"src/main/java/(.*)\\\\.java$\", \"src/test/java/\\\\\\\\1Test\\\\.java$\"),  # Maven/Gradle structure\n\t\t\t# Go\n\t\t\t(\"^(.*)\\\\.go$\", \"\\\\\\\\1_test\\\\.go$\"),\n\t\t\t# C#\n\t\t\t(\"^(.*)\\\\.cs$\", \"\\\\\\\\1Tests?\\\\.cs$\"),\n\t\t\t# PHP\n\t\t\t(\"^(.*)\\\\.php$\", \"\\\\\\\\1Test\\\\.php$\"),\n\t\t\t(\"^(.*)\\\\.php$\", \"\\\\\\\\1Spec\\\\.php$\"),\n\t\t\t(\"src/(.*)\\\\.php$\", \"tests/\\\\\\\\1Test\\\\.php$\"),  # Common structure\n\t\t\t# Rust\n\t\t\t(\"src/(lib|main)\\\\.rs$\", \"tests/.*\\\\.rs$\"),  # Main/Lib and integration tests\n\t\t\t(\"src/(.*)\\\\.rs$\", \"src/\\\\\\\\1_test\\\\.rs$\"),  # Inline tests (less common for grouping)\n\t\t\t# Swift\n\t\t\t(\"^(.*)\\\\.swift$\", \"\\\\\\\\1Tests?\\\\.swift$\"),\n\t\t\t# Kotlin\n\t\t\t(\"^(.*)\\\\.kt$\", \"\\\\\\\\1Test\\\\.kt$\"),\n\t\t\t(\"src/main/kotlin/(.*)\\\\.kt$\", \"src/test/kotlin/\\\\\\\\1Test\\\\.kt$\"),  # Common structure\n\t\t\t# --- Frontend Component Bundles ---\n\t\t\t# JS/TS Components + Styles (CSS, SCSS, LESS, CSS Modules)\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.module\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.(js|jsx|ts|tsx)$\", \"\\\\\\\\1\\\\.styles?\\\\.(js|ts)$\"),  # Styled Components / Emotion convention\n\t\t\t# Vue Components + Styles\n\t\t\t(\"^(.*)\\\\.vue$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.vue$\", \"\\\\\\\\1\\\\.module\\\\.(css|scss|less)$\"),\n\t\t\t# Svelte Components + Styles/Scripts\n\t\t\t(\"^(.*)\\\\.svelte$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.svelte$\", \"\\\\\\\\1\\\\.(js|ts)$\"),\n\t\t\t# Angular Components (more specific structure)\n\t\t\t(\"^(.*)\\\\.component\\\\.ts$\", \"\\\\\\\\1\\\\.component\\\\.html$\"),\n\t\t\t(\"^(.*)\\\\.component\\\\.ts$\", \"\\\\\\\\1\\\\.component\\\\.(css|scss|less)$\"),\n\t\t\t(\"^(.*)\\\\.component\\\\.ts$\", \"\\\\\\\\1\\\\.component\\\\.spec\\\\.ts$\"),  # Component + its test\n\t\t\t(\"^(.*)\\\\.service\\\\.ts$\", \"\\\\\\\\1\\\\.service\\\\.spec\\\\.ts$\"),  # Service + its test\n\t\t\t(\"^(.*)\\\\.module\\\\.ts$\", \"\\\\\\\\1\\\\.routing\\\\.module\\\\.ts$\"),  # Module + routing\n\t\t\t# --- Implementation / Definition / Generation ---\n\t\t\t# C / C++ / Objective-C\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.c$\"),\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.m$\"),\n\t\t\t(\"^(.*)\\\\.hpp$\", \"\\\\\\\\1\\\\.cpp$\"),\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.cpp$\"),  # Allow .h with .cpp\n\t\t\t(\"^(.*)\\\\.h$\", \"\\\\\\\\1\\\\.mm$\"),\n\t\t\t# Protocol Buffers / gRPC\n\t\t\t(\"^(.*)\\\\.proto$\", \"\\\\\\\\1\\\\.pb\\\\.(go|py|js|java|rb|cs|ts)$\"),\n\t\t\t(\"^(.*)\\\\.proto$\", \"\\\\\\\\1_pb2?\\\\.py$\"),  # Python specific proto generation\n\t\t\t(\"^(.*)\\\\.proto$\", \"\\\\\\\\1_grpc\\\\.pb\\\\.(go|js|ts)$\"),  # gRPC specific\n\t\t\t# Interface Definition Languages (IDL)\n\t\t\t(\"^(.*)\\\\.idl$\", \"\\\\\\\\1\\\\.(h|cpp|cs|java)$\"),\n\t\t\t# API Specifications (OpenAPI/Swagger)\n\t\t\t(\"(openapi|swagger)\\\\.(yaml|yml|json)$\", \".*\\\\.(go|py|js|java|rb|cs|ts)$\"),  # Spec + generated code\n\t\t\t(\"^(.*)\\\\.(yaml|yml|json)$\", \"\\\\\\\\1\\\\.generated\\\\.(go|py|js|java|rb|cs|ts)$\"),  # Another convention\n\t\t\t# --- Web Development (HTML Centric) ---\n\t\t\t(\"^(.*)\\\\.html$\", \"\\\\\\\\1\\\\.(js|ts)$\"),\n\t\t\t(\"^(.*)\\\\.html$\", \"\\\\\\\\1\\\\.(css|scss|less)$\"),\n\t\t\t# --- Mobile Development ---\n\t\t\t# iOS (Swift)\n\t\t\t(\"^(.*)\\\\.swift$\", \"\\\\\\\\1\\\\.storyboard$\"),\n\t\t\t(\"^(.*)\\\\.swift$\", \"\\\\\\\\1\\\\.xib$\"),\n\t\t\t# Android (Kotlin/Java)\n\t\t\t(\"^(.*)\\\\.(kt|java)$\", \"res/layout/.*\\\\.(xml)$\"),  # Code + Layout XML (Path sensitive)\n\t\t\t(\"AndroidManifest\\\\.xml$\", \".*\\\\.(kt|java)$\"),  # Manifest + Code\n\t\t\t(\"build\\\\.gradle(\\\\.kts)?$\", \".*\\\\.(kt|java)$\"),  # Gradle build + Code\n\t\t\t# --- Configuration Files ---\n\t\t\t# Package Managers\n\t\t\t(\"package\\\\.json$\", \"(package-lock\\\\.json|yarn\\\\.lock|pnpm-lock\\\\.yaml)$\"),\n\t\t\t(\"requirements\\\\.txt$\", \"(setup\\\\.py|setup\\\\.cfg|pyproject\\\\.toml)$\"),\n\t\t\t(\"pyproject\\\\.toml$\", \"(setup\\\\.py|setup\\\\.cfg|poetry\\\\.lock|uv\\\\.lock)$\"),\n\t\t\t(\"Gemfile$\", \"Gemfile\\\\.lock$\"),\n\t\t\t(\"Cargo\\\\.toml$\", \"Cargo\\\\.lock$\"),\n\t\t\t(\"composer\\\\.json$\", \"composer\\\\.lock$\"),  # PHP Composer\n\t\t\t(\"go\\\\.mod$\", \"go\\\\.sum$\"),  # Go Modules\n\t\t\t(\"pom\\\\.xml$\", \".*\\\\.java$\"),  # Maven + Java\n\t\t\t(\"build\\\\.gradle(\\\\.kts)?$\", \".*\\\\.(java|kt)$\"),  # Gradle + Java/Kotlin\n\t\t\t# Linters / Formatters / Compilers / Type Checkers\n\t\t\t(\n\t\t\t\t\"package\\\\.json$\",\n\t\t\t\t\"(tsconfig\\\\.json|\\\\.eslintrc(\\\\..*)?|\\\\.prettierrc(\\\\..*)?|\\\\.babelrc(\\\\..*)?|webpack\\\\.config\\\\.js|vite\\\\.config\\\\.(js|ts))$\",\n\t\t\t),\n\t\t\t(\"pyproject\\\\.toml$\", \"(\\\\.flake8|\\\\.pylintrc|\\\\.isort\\\\.cfg|mypy\\\\.ini)$\"),\n\t\t\t# Docker\n\t\t\t(\"Dockerfile$\", \"(\\\\.dockerignore|docker-compose\\\\.yml)$\"),\n\t\t\t(\"docker-compose\\\\.yml$\", \"\\\\.env$\"),\n\t\t\t# CI/CD\n\t\t\t(\"\\\\.github/workflows/.*\\\\.yml$\", \".*\\\\.(sh|py|js|ts|go)$\"),  # Workflow + scripts\n\t\t\t(\"\\\\.gitlab-ci\\\\.yml$\", \".*\\\\.(sh|py|js|ts|go)$\"),\n\t\t\t(\"Jenkinsfile$\", \".*\\\\.(groovy|sh|py)$\"),\n\t\t\t# IaC (Terraform)\n\t\t\t(\"^(.*)\\\\.tf$\", \"\\\\\\\\1\\\\.tfvars$\"),\n\t\t\t(\"^(.*)\\\\.tf$\", \"\\\\\\\\1\\\\.tf$\"),  # Group TF files together\n\t\t\t# --- Documentation ---\n\t\t\t(\"README\\\\.md$\", \".*$\"),  # README often updated with any change\n\t\t\t(\"^(.*)\\\\.md$\", \"\\\\\\\\1\\\\.(py|js|ts|go|java|rb|rs|php|swift|kt)$\"),  # Markdown doc + related code\n\t\t\t(\"docs/.*\\\\.md$\", \"src/.*$\"),  # Documentation in docs/ related to src/\n\t\t\t# --- Data Science / ML ---\n\t\t\t(\"^(.*)\\\\.ipynb$\", \"\\\\\\\\1\\\\.py$\"),  # Notebook + Python script\n\t\t\t(\"^(.*)\\\\.py$\", \"data/.*\\\\.(csv|json|parquet)$\"),  # Script + Data file (path sensitive)\n\t\t\t# --- General Fallbacks (Use with caution) ---\n\t\t\t# Files with same base name but different extensions (already covered by some specifics)\n\t\t\t# (\"^(.*)\\\\..*$\", \"\\\\1\\\\..*$\"), # Potentially too broad, rely on specifics above\n\t\t]\n\n\t\tfor pattern1_str, pattern2_str in default_patterns:\n\t\t\ttry:\n\t\t\t\t# Compile with IGNORECASE for broader matching\n\t\t\t\tpattern1 = re.compile(pattern1_str, re.IGNORECASE)\n\t\t\t\tpattern2 = re.compile(pattern2_str, re.IGNORECASE)\n\t\t\t\trelated_file_patterns.append((pattern1, pattern2))\n\t\t\texcept re.error as e:\n\t\t\t\t# Log only if pattern compilation fails\n\t\t\t\tlogger.warning(f\"Failed to compile regex pair: ({pattern1_str!r}, {pattern2_str!r}). Error: {e}\")\n\n\t\treturn related_file_patterns\n\n\tdef _get_code_embedding(self, content: str) -&gt; list[float] | None:\n\t\t\"\"\"\n\t\tGet embedding vector for code content.\n\n\t\tArgs:\n\t\t    content: Code content to embed\n\n\t\tReturns:\n\t\t    List of floats representing code embedding or None if unavailable\n\n\t\t\"\"\"\n\t\t# Skip empty content\n\t\tif not content or not content.strip():\n\t\t\treturn None\n\n\t\t# Check if embedding model exists\n\t\tif self._embedding_model is None:\n\t\t\tlogger.warning(\"Embedding model is None, cannot generate embedding\")\n\t\t\treturn None\n\n\t\t# Generate embedding with error handling\n\t\ttry:\n\t\t\tembeddings = self._embedding_model.encode([content], show_progress_bar=False)\n\t\t\t# Check if the result is valid and has the expected structure\n\t\t\tif embeddings is not None and len(embeddings) &gt; 0 and isinstance(embeddings[0], np.ndarray):\n\t\t\t\treturn embeddings[0].tolist()\n\t\t\tlogger.warning(\"Embedding model returned unexpected result type: %s\", type(embeddings))\n\t\t\treturn None\n\t\texcept (ValueError, TypeError, RuntimeError, IndexError, AttributeError) as e:\n\t\t\t# Catch a broader range of potential exceptions during encode/toList\n\t\t\tlogger.warning(\"Failed to generate embedding for content snippet: %s\", e)\n\t\t\treturn None\n\t\texcept Exception:  # Catch any other unexpected errors\n\t\t\tlogger.exception(\"Unexpected error during embedding generation\")\n\t\t\treturn None\n\n\tdef _calculate_semantic_similarity(self, content1: str, content2: str) -&gt; float:\n\t\t\"\"\"\n\t\tCalculate semantic similarity between two code chunks.\n\n\t\tArgs:\n\t\t    content1: First code content\n\t\t    content2: Second code content\n\n\t\tReturns:\n\t\t    Similarity score between 0 and 1\n\n\t\t\"\"\"\n\t\t# Get embeddings\n\t\temb1 = self._get_code_embedding(content1)\n\t\temb2 = self._get_code_embedding(content2)\n\n\t\tif not emb1 or not emb2:\n\t\t\treturn 0.0\n\n\t\t# Calculate cosine similarity using utility function\n\t\treturn calculate_semantic_similarity(emb1, emb2)\n\n\t# --- New Helper Methods for Refactoring _enhance_semantic_split ---\n\n\tdef _parse_file_diff(self, diff_content: str, file_path: str) -&gt; PatchedFile | None:\n\t\t\"\"\"Parse diff content to find the PatchedFile for a specific file path.\"\"\"\n\t\tif not diff_content:\n\t\t\tlogger.warning(\"Cannot parse empty diff content for %s\", file_path)\n\t\t\treturn None\n\n\t\tfiltered_content = \"\"  # Initialize to handle unbound case\n\t\ttry:\n\t\t\t# Filter out the truncation marker lines before parsing\n\t\t\tfiltered_content_lines = [\n\t\t\t\tline for line in diff_content.splitlines() if line.strip() != \"... [content truncated] ...\"\n\t\t\t]\n\t\t\tfiltered_content = \"\\n\".join(filtered_content_lines)\n\n\t\t\t# Use StringIO as PatchSet expects a file-like object or iterable\n\t\t\ttry:\n\t\t\t\tpatch_set = PatchSet(StringIO(filtered_content))\n\t\t\texcept UnidiffParseError as e:\n\t\t\t\tlogger.warning(\"UnidiffParseError for %s: %s\", file_path, str(e))\n\t\t\t\t# Try to extract just the diff for this specific file to avoid parsing the entire diff\n\t\t\t\tfile_diff_content_raw = re.search(\n\t\t\t\t\trf\"diff --git a/.*? b/{re.escape(file_path)}\\n(.*?)(?=diff --git a/|\\Z)\",\n\t\t\t\t\tdiff_content,\n\t\t\t\t\tre.DOTALL | re.MULTILINE,\n\t\t\t\t)\n\t\t\t\tcontent_for_chunk = file_diff_content_raw.group(0) if file_diff_content_raw else \"\"\n\t\t\t\tif content_for_chunk:\n\t\t\t\t\tlogger.debug(\"Extracted raw content for %s after parse error\", file_path)\n\t\t\t\t\t# Create a manual PatchedFile since we can't parse it properly\n\t\t\t\t\treturn None\n\t\t\t\treturn None\n\n\t\t\tmatched_file: PatchedFile | None = None\n\t\t\tfor patched_file in patch_set:\n\t\t\t\t# unidiff paths usually start with a/ or b/\n\t\t\t\tif patched_file.target_file == f\"b/{file_path}\" or patched_file.path == file_path:\n\t\t\t\t\tmatched_file = patched_file\n\t\t\t\t\tbreak\n\t\t\tif not matched_file:\n\t\t\t\tlogger.warning(\"Could not find matching PatchedFile for: %s in unidiff output\", file_path)\n\t\t\t\treturn None\n\t\t\treturn matched_file\n\t\texcept UnidiffParseError:\n\t\t\t# Log the specific parse error and the content that caused it (first few lines)\n\t\t\tpreview_lines = \"\\n\".join(filtered_content.splitlines()[:10])  # Log first 10 lines\n\t\t\tlogger.exception(\n\t\t\t\t\"UnidiffParseError for %s\\nContent Preview:\\n%s\",  # Corrected format string\n\t\t\t\tfile_path,\n\t\t\t\tpreview_lines,\n\t\t\t)\n\t\t\treturn None  # Return None on parse error\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to parse diff content using unidiff for %s\", file_path)\n\t\t\treturn None\n\n\tdef _reconstruct_file_diff(self, patched_file: PatchedFile) -&gt; tuple[str, str]:\n\t\t\"\"\"Reconstruct the diff header and full diff content for a PatchedFile.\"\"\"\n\t\tfile_diff_hunks_content = \"\\n\".join(str(hunk) for hunk in patched_file)\n\t\tfile_header_obj = getattr(patched_file, \"patch_info\", None)\n\t\tfile_header = str(file_header_obj) if file_header_obj else \"\"\n\n\t\tif not file_header.startswith(\"diff --git\") and patched_file.source_file and patched_file.target_file:\n\t\t\tlogger.debug(\"Reconstructing missing diff header for %s\", patched_file.path)\n\t\t\tfile_header = f\"diff --git {patched_file.source_file} {patched_file.target_file}\\n\"\n\t\t\tif hasattr(patched_file, \"index\") and patched_file.index:\n\t\t\t\tfile_header += f\"index {patched_file.index}\\n\"\n\t\t\t# Use timestamps if available for more accurate header reconstruction\n\t\t\tsource_ts = f\"\\t{patched_file.source_timestamp}\" if patched_file.source_timestamp else \"\"\n\t\t\ttarget_ts = f\"\\t{patched_file.target_timestamp}\" if patched_file.target_timestamp else \"\"\n\t\t\tfile_header += f\"--- {patched_file.source_file}{source_ts}\\n\"\n\t\t\tfile_header += f\"+++ {patched_file.target_file}{target_ts}\\n\"\n\n\t\tfull_file_diff_content = file_header + file_diff_hunks_content\n\t\treturn file_header, full_file_diff_content\n\n\tdef _split_large_file_diff(self, patched_file: PatchedFile, file_header: str) -&gt; list[DiffChunk]:\n\t\t\"\"\"Split a large file's diff by grouping hunks under the size limit.\"\"\"\n\t\tfile_path = patched_file.path\n\t\tmax_chunk_size = self.max_file_size_for_llm  # Use instance config\n\t\tlogger.info(\n\t\t\t\"Splitting large file diff for %s by hunks (limit: %d bytes)\",\n\t\t\tfile_path,\n\t\t\tmax_chunk_size,\n\t\t)\n\t\tlarge_file_chunks = []\n\t\tcurrent_hunk_group: list[Hunk] = []\n\t\tcurrent_group_size = len(file_header)  # Start with header size\n\n\t\tfor hunk in patched_file:\n\t\t\thunk_content_str = str(hunk)\n\t\t\thunk_size = len(hunk_content_str) + 1  # +1 for newline separator\n\n\t\t\t# If adding this hunk exceeds the limit (and group isn't empty), finalize the current chunk\n\t\t\tif current_hunk_group and current_group_size + hunk_size &gt; max_chunk_size:\n\t\t\t\tgroup_content = file_header + \"\\n\".join(str(h) for h in current_hunk_group)\n\t\t\t\tdescription = f\"Chunk {len(large_file_chunks) + 1} of large file {file_path}\"\n\t\t\t\tlarge_file_chunks.append(DiffChunk(files=[file_path], content=group_content, description=description))\n\t\t\t\t# Start a new chunk with the current hunk\n\t\t\t\tcurrent_hunk_group = [hunk]\n\t\t\t\tcurrent_group_size = len(file_header) + hunk_size\n\t\t\t# Edge case: If a single hunk itself is too large, create a chunk just for it\n\t\t\telif not current_hunk_group and len(file_header) + hunk_size &gt; max_chunk_size:\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"Single hunk in %s exceeds size limit (%d bytes). Creating oversized chunk.\",\n\t\t\t\t\tfile_path,\n\t\t\t\t\tlen(file_header) + hunk_size,\n\t\t\t\t)\n\t\t\t\tgroup_content = file_header + hunk_content_str\n\t\t\t\tdescription = f\"Chunk {len(large_file_chunks) + 1} (oversized hunk) of large file {file_path}\"\n\t\t\t\tlarge_file_chunks.append(DiffChunk(files=[file_path], content=group_content, description=description))\n\t\t\t\t# Reset for next potential chunk (don't carry this huge hunk forward)\n\t\t\t\tcurrent_hunk_group = []\n\t\t\t\tcurrent_group_size = len(file_header)\n\t\t\telse:\n\t\t\t\t# Add hunk to the current group\n\t\t\t\tcurrent_hunk_group.append(hunk)\n\t\t\t\tcurrent_group_size += hunk_size\n\n\t\t# Add the last remaining chunk group if any\n\t\tif current_hunk_group:\n\t\t\tgroup_content = file_header + \"\\n\".join(str(h) for h in current_hunk_group)\n\t\t\tdescription = f\"Chunk {len(large_file_chunks) + 1} of large file {file_path}\"\n\t\t\tlarge_file_chunks.append(DiffChunk(files=[file_path], content=group_content, description=description))\n\n\t\treturn large_file_chunks\n\n\t# --- Refactored Orchestrator Method ---\n\n\tdef _enhance_semantic_split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tEnhance the semantic split by using NLP and chunk detection.\n\n\t\tArgs:\n\t\t    diff: The GitDiff object to split\n\n\t\tReturns:\n\t\t    List of enhanced DiffChunk objects\n\n\t\t\"\"\"\n\t\tif not diff.files:\n\t\t\treturn []\n\n\t\t# Special handling for untracked files - avoid unidiff parsing errors\n\t\tif diff.is_untracked:\n\t\t\t# Create a basic chunk with only file info for untracked files\n\t\t\t# Use a list comprehension for performance (PERF401)\n\t\t\treturn [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=diff.content if len(diff.files) == 1 else f\"New untracked file: {file_path}\",\n\t\t\t\t\tdescription=f\"New file: {file_path}\",\n\t\t\t\t)\n\t\t\t\tfor file_path in diff.files\n\t\t\t\tif self._is_valid_filename(file_path)\n\t\t\t]\n\n\t\tif not diff.files or len(diff.files) != 1:\n\t\t\tlogger.error(\"_enhance_semantic_split called with invalid diff object (files=%s)\", diff.files)\n\t\t\treturn []\n\n\t\tfile_path = diff.files[0]\n\t\textension = Path(file_path).suffix[1:].lower()\n\n\t\tif not diff.content:\n\t\t\tlogger.warning(\"No diff content provided for %s, creating basic chunk.\", file_path)\n\t\t\treturn [DiffChunk(files=[file_path], content=\"\", description=f\"New file: {file_path}\")]\n\n\t\t# 1. Parse the diff to get the PatchedFile object\n\t\tmatched_file = self._parse_file_diff(diff.content, file_path)\n\t\tif not matched_file:\n\t\t\t# If parsing failed, return a basic chunk with raw content attempt\n\t\t\tfile_diff_content_raw = re.search(\n\t\t\t\trf\"diff --git a/.*? b/{re.escape(file_path)}\\n(.*?)(?=diff --git a/|\\Z)\",\n\t\t\t\tdiff.content,\n\t\t\t\tre.DOTALL | re.MULTILINE,\n\t\t\t)\n\t\t\tcontent_for_chunk = file_diff_content_raw.group(0) if file_diff_content_raw else \"\"\n\t\t\treturn [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=content_for_chunk,\n\t\t\t\t\tdescription=f\"Changes in {file_path} (parsing failed)\",\n\t\t\t\t)\n\t\t\t]\n\n\t\t# 2. Reconstruct the full diff content for this file\n\t\tfile_header, full_file_diff_content = self._reconstruct_file_diff(matched_file)\n\n\t\t# 3. Check if the reconstructed diff is too large\n\t\tif len(full_file_diff_content) &gt; self.max_file_size_for_llm:\n\t\t\treturn self._split_large_file_diff(matched_file, file_header)\n\n\t\t# 4. Try splitting by semantic patterns (if applicable)\n\t\tpatterns = get_language_specific_patterns(extension)\n\t\tif patterns:\n\t\t\tlogger.debug(\"Attempting semantic pattern splitting for %s\", file_path)\n\t\t\tpattern_chunks = self._split_by_semantic_patterns(matched_file, patterns)\n\t\t\tif pattern_chunks:\n\t\t\t\treturn pattern_chunks\n\t\t\tlogger.debug(\"Pattern splitting yielded no chunks for %s, falling back.\", file_path)\n\n\t\t# 5. Fallback: Split by individual hunks\n\t\tlogger.debug(\"Falling back to hunk splitting for %s\", file_path)\n\t\thunk_chunks = []\n\t\tfor hunk in matched_file:\n\t\t\thunk_content = str(hunk)\n\t\t\thunk_chunks.append(\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=file_header + hunk_content,  # Combine header + hunk\n\t\t\t\t\tdescription=f\"Hunk in {file_path} starting near line {hunk.target_start}\",\n\t\t\t\t)\n\t\t\t)\n\n\t\t# If no hunks were found at all, return the single reconstructed chunk\n\t\tif not hunk_chunks:\n\t\t\tlogger.warning(\"No hunks detected for %s after parsing, returning full diff.\", file_path)\n\t\t\treturn [\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=full_file_diff_content,\n\t\t\t\t\tdescription=f\"Changes in {file_path} (no hunks detected)\",\n\t\t\t\t)\n\t\t\t]\n\n\t\treturn hunk_chunks\n\n\t# --- Existing Helper Methods (Potentially need review/updates) ---\n\n\tdef _group_by_content_similarity(\n\t\tself,\n\t\tchunks: list[DiffChunk],\n\t\tresult_chunks: list[DiffChunk],\n\t\tsimilarity_threshold: float | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGroup chunks by content similarity.\n\n\t\tArgs:\n\t\t    chunks: List of chunks to process\n\t\t    result_chunks: List to append grouped chunks to (modified in place)\n\t\t    similarity_threshold: Optional custom threshold to override default\n\n\t\t\"\"\"\n\t\tif not chunks:\n\t\t\treturn\n\n\t\t# Check if model is available\n\t\tif self._embedding_model is None:\n\t\t\tlogger.debug(\"Embedding model not available, using fallback grouping strategy\")\n\t\t\t# If model is unavailable, try to group by file path patterns\n\t\t\tgrouped_paths: dict[str, list[DiffChunk]] = {}\n\n\t\t\t# Group by common path prefixes\n\t\t\tfor chunk in chunks:\n\t\t\t\tif not chunk.files:\n\t\t\t\t\tresult_chunks.append(chunk)\n\t\t\t\t\tcontinue\n\n\t\t\t\tfile_path = chunk.files[0]\n\t\t\t\t# Get directory or file prefix as the grouping key\n\t\t\t\tif \"/\" in file_path:\n\t\t\t\t\t# Use directory as key\n\t\t\t\t\tkey = file_path.rsplit(\"/\", 1)[0]\n\t\t\t\telse:\n\t\t\t\t\t# Use file prefix (before extension) as key\n\t\t\t\t\tkey = file_path.split(\".\", 1)[0] if \".\" in file_path else file_path\n\n\t\t\t\tif key not in grouped_paths:\n\t\t\t\t\tgrouped_paths[key] = []\n\t\t\t\tgrouped_paths[key].append(chunk)\n\n\t\t\t# Create chunks from each group\n\t\t\tfor related_chunks in grouped_paths.values():\n\t\t\t\tself._create_semantic_chunk(related_chunks, result_chunks)\n\t\t\treturn\n\n\t\tprocessed_indices = set()\n\t\tthreshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n\n\t\t# For each chunk, find similar chunks and group them\n\t\tfor i, chunk in enumerate(chunks):\n\t\t\tif i in processed_indices:\n\t\t\t\tcontinue\n\n\t\t\trelated_chunks = [chunk]\n\t\t\tprocessed_indices.add(i)\n\n\t\t\t# Find similar chunks\n\t\t\tfor j, other_chunk in enumerate(chunks):\n\t\t\t\tif i == j or j in processed_indices:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Calculate similarity between chunks\n\t\t\t\tsimilarity = self._calculate_semantic_similarity(chunk.content, other_chunk.content)\n\n\t\t\t\tif similarity &gt;= threshold:\n\t\t\t\t\trelated_chunks.append(other_chunk)\n\t\t\t\t\tprocessed_indices.add(j)\n\n\t\t\t# Create a semantic chunk from related chunks\n\t\t\tif related_chunks:\n\t\t\t\tself._create_semantic_chunk(related_chunks, result_chunks)\n\n\tdef _group_related_files(\n\t\tself,\n\t\tfile_chunks: list[DiffChunk],\n\t\tprocessed_files: set[str],\n\t\tsemantic_chunks: list[DiffChunk],\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tGroup related files into semantic chunks.\n\n\t\tArgs:\n\t\t    file_chunks: List of file-based chunks\n\t\t    processed_files: Set of already processed files (modified in place)\n\t\t    semantic_chunks: List of semantic chunks (modified in place)\n\n\t\t\"\"\"\n\t\tif not file_chunks:\n\t\t\treturn\n\n\t\t# Group clearly related files\n\t\tfor i, chunk in enumerate(file_chunks):\n\t\t\tif not chunk.files or chunk.files[0] in processed_files:\n\t\t\t\tcontinue\n\n\t\t\trelated_chunks = [chunk]\n\t\t\tprocessed_files.add(chunk.files[0])\n\n\t\t\t# Find related files\n\t\t\tfor j, other_chunk in enumerate(file_chunks):\n\t\t\t\tif i == j or not other_chunk.files or other_chunk.files[0] in processed_files:\n\t\t\t\t\tcontinue\n\n\t\t\t\tif are_files_related(chunk.files[0], other_chunk.files[0], self.related_file_patterns):\n\t\t\t\t\trelated_chunks.append(other_chunk)\n\t\t\t\t\tprocessed_files.add(other_chunk.files[0])\n\n\t\t\t# Create a semantic chunk from related files\n\t\t\tif related_chunks:\n\t\t\t\tself._create_semantic_chunk(related_chunks, semantic_chunks)\n\n\tdef _create_semantic_chunk(\n\t\tself,\n\t\trelated_chunks: list[DiffChunk],\n\t\tsemantic_chunks: list[DiffChunk],\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tCreate a semantic chunk from related file chunks.\n\n\t\tArgs:\n\t\t    related_chunks: List of related file chunks\n\t\t    semantic_chunks: List of semantic chunks to append to (modified in place)\n\n\t\t\"\"\"\n\t\tif not related_chunks:\n\t\t\treturn\n\n\t\tall_files = []\n\t\tcombined_content = []\n\n\t\tfor rc in related_chunks:\n\t\t\tall_files.extend(rc.files)\n\t\t\tcombined_content.append(rc.content)\n\n\t\t# Determine the appropriate commit type based on the files\n\t\tcommit_type = determine_commit_type(all_files)\n\n\t\t# Create description based on file count\n\t\tdescription = create_chunk_description(commit_type, all_files)\n\n\t\t# Join the content from all related chunks\n\t\tcontent = \"\\n\\n\".join(combined_content)\n\n\t\tsemantic_chunks.append(\n\t\t\tDiffChunk(\n\t\t\t\tfiles=all_files,\n\t\t\t\tcontent=content,\n\t\t\t\tdescription=description,\n\t\t\t)\n\t\t)\n\n\tdef _should_merge_chunks(self, chunk1: DiffChunk, chunk2: DiffChunk) -&gt; bool:\n\t\t\"\"\"Determine if two chunks should be merged.\"\"\"\n\t\t# Condition 1: Same single file\n\t\tsame_file = len(chunk1.files) == 1 and chunk1.files == chunk2.files\n\n\t\t# Condition 2: Related single files\n\t\trelated_files = (\n\t\t\tlen(chunk1.files) == 1\n\t\t\tand len(chunk2.files) == 1\n\t\t\tand are_files_related(chunk1.files[0], chunk2.files[0], self.related_file_patterns)\n\t\t)\n\n\t\t# Return True if either condition is met\n\t\treturn same_file or related_files\n\n\tdef _consolidate_small_chunks(self, initial_chunks: list[DiffChunk]) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tMerge small or related chunks together.\n\n\t\tFirst, consolidates chunks originating from the same file.\n\t\tThen, consolidates remaining single-file chunks by directory.\n\n\t\tArgs:\n\t\t    initial_chunks: List of diff chunks to consolidate\n\n\t\tReturns:\n\t\t    Consolidated list of chunks\n\n\t\t\"\"\"\n\t\t# Use instance variable for threshold\n\t\tif len(initial_chunks) &lt; self.min_chunks_for_consolidation:\n\t\t\treturn initial_chunks\n\n\t\t# Consolidate small chunks for the same file or related files\n\t\tconsolidated_chunks = []\n\t\tprocessed_indices = set()\n\n\t\tfor i, chunk1 in enumerate(initial_chunks):\n\t\t\tif i in processed_indices:\n\t\t\t\tcontinue\n\n\t\t\tmerged_chunk = chunk1\n\t\t\tprocessed_indices.add(i)\n\n\t\t\t# Check subsequent chunks for merging\n\t\t\tfor j in range(i + 1, len(initial_chunks)):\n\t\t\t\tif j in processed_indices:\n\t\t\t\t\tcontinue\n\n\t\t\t\tchunk2 = initial_chunks[j]\n\n\t\t\t\t# Check if chunks should be merged (same file or related)\n\t\t\t\tif self._should_merge_chunks(merged_chunk, chunk2):\n\t\t\t\t\t# Combine files if merging related chunks, not just same file chunks\n\t\t\t\t\tnew_files = merged_chunk.files\n\t\t\t\t\tif (\n\t\t\t\t\t\tlen(merged_chunk.files) == 1\n\t\t\t\t\t\tand len(chunk2.files) == 1\n\t\t\t\t\t\tand merged_chunk.files[0] != chunk2.files[0]\n\t\t\t\t\t):\n\t\t\t\t\t\tnew_files = sorted(set(merged_chunk.files + chunk2.files))\n\n\t\t\t\t\t# Merge content and potentially other attributes\n\t\t\t\t\t# Ensure a newline between merged content if needed\n\t\t\t\t\tseparator = \"\\n\" if merged_chunk.content and chunk2.content else \"\"\n\t\t\t\t\tmerged_chunk = dataclasses.replace(\n\t\t\t\t\t\tmerged_chunk,\n\t\t\t\t\t\tfiles=new_files,\n\t\t\t\t\t\tcontent=merged_chunk.content + separator + chunk2.content,\n\t\t\t\t\t\tdescription=merged_chunk.description,  # Keep first description\n\t\t\t\t\t)\n\t\t\t\t\tprocessed_indices.add(j)\n\n\t\t\tconsolidated_chunks.append(merged_chunk)\n\n\t\treturn consolidated_chunks\n\n\tdef _split_by_semantic_patterns(self, patched_file: PatchedFile, patterns: list[str]) -&gt; list[DiffChunk]:\n\t\t\"\"\"\n\t\tSplit a PatchedFile's content by grouping hunks based on semantic patterns.\n\n\t\tThis method groups consecutive hunks together until a hunk is encountered\n\t\tthat contains an added line matching one of the semantic boundary patterns.\n\t\tIt does *not* split within a single hunk, only between hunks where a boundary\n\t\tis detected in the *first* line of the subsequent hunk group.\n\n\t\tArgs:\n\t\t    patched_file: The PatchedFile object from unidiff.\n\t\t    patterns: List of regex pattern strings to match as boundaries.\n\n\t\tReturns:\n\t\t    List of DiffChunk objects, potentially splitting the file into multiple chunks.\n\n\t\t\"\"\"\n\t\tcompiled_patterns = [re.compile(p) for p in patterns]\n\t\tfile_path = patched_file.path  # Or target_file? Need consistency\n\n\t\tfinal_chunks_data: list[list[Hunk]] = []\n\t\tcurrent_semantic_chunk_hunks: list[Hunk] = []\n\n\t\t# Get header info once using the reconstruction helper\n\t\tfile_header, _ = self._reconstruct_file_diff(patched_file)\n\n\t\tfor hunk in patched_file:\n\t\t\thunk_has_boundary = False\n\t\t\tfor line in hunk:\n\t\t\t\tif line.is_added and any(pattern.match(line.value) for pattern in compiled_patterns):\n\t\t\t\t\thunk_has_boundary = True\n\t\t\t\t\tbreak  # Found a boundary in this hunk\n\n\t\t\t# Start a new semantic chunk if the current hunk has a boundary\n\t\t\t# and we already have hunks accumulated.\n\t\t\tif hunk_has_boundary and current_semantic_chunk_hunks:\n\t\t\t\tfinal_chunks_data.append(current_semantic_chunk_hunks)\n\t\t\t\tcurrent_semantic_chunk_hunks = [hunk]  # Start new chunk with this hunk\n\t\t\telse:\n\t\t\t\t# Append the current hunk to the ongoing semantic chunk\n\t\t\t\tcurrent_semantic_chunk_hunks.append(hunk)\n\n\t\t# Add the last accumulated semantic chunk\n\t\tif current_semantic_chunk_hunks:\n\t\t\tfinal_chunks_data.append(current_semantic_chunk_hunks)\n\n\t\t# Convert grouped hunks into DiffChunk objects\n\t\tresult_chunks: list[DiffChunk] = []\n\t\tfor i, hunk_group in enumerate(final_chunks_data):\n\t\t\tif not hunk_group:\n\t\t\t\tcontinue\n\t\t\t# Combine content of all hunks in the group\n\t\t\tgroup_content = \"\\n\".join(str(h) for h in hunk_group)\n\t\t\t# Generate description (could be more sophisticated)\n\t\t\tdescription = f\"Semantic section {i + 1} in {file_path}\"\n\t\t\tresult_chunks.append(\n\t\t\t\tDiffChunk(\n\t\t\t\t\tfiles=[file_path],\n\t\t\t\t\tcontent=file_header + group_content,  # Combine header + hunks\n\t\t\t\t\tdescription=description,\n\t\t\t\t)\n\t\t\t)\n\n\t\tlogger.debug(\"Split %s into %d chunks based on semantic patterns\", file_path, len(result_chunks))\n\t\treturn result_chunks\n\n\t@staticmethod\n\tdef _is_valid_filename(filename: str) -&gt; bool:\n\t\t\"\"\"Check if the filename is valid (not a pattern or template).\"\"\"\n\t\tif not filename:\n\t\t\treturn False\n\t\tinvalid_chars = [\"*\", \"+\", \"{\", \"}\", \"\\\\\"]\n\t\treturn not (any(char in filename for char in invalid_chars) or filename.startswith('\"'))\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.__init__","title":"__init__","text":"<pre><code>__init__(\n\tembedding_model: EmbeddingModel | None = None,\n\tcode_extensions: set[str] | None = None,\n\trelated_file_patterns: list[tuple[Pattern, Pattern]]\n\t| None = None,\n\tsimilarity_threshold: float = 0.4,\n\tdirectory_similarity_threshold: float = 0.3,\n\tmin_chunks_for_consolidation: int = 2,\n\tmax_chunks_before_consolidation: int = 20,\n\tmax_file_size_for_llm: int | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the SemanticSplitStrategy.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>EmbeddingModel | None</code> <p>Optional embedding model instance</p> <code>None</code> <code>code_extensions</code> <code>set[str] | None</code> <p>Optional set of code file extensions. Defaults to config.</p> <code>None</code> <code>related_file_patterns</code> <code>list[tuple[Pattern, Pattern]] | None</code> <p>Optional list of related file patterns</p> <code>None</code> <code>similarity_threshold</code> <code>float</code> <p>Threshold for grouping by content similarity.</p> <code>0.4</code> <code>directory_similarity_threshold</code> <code>float</code> <p>Threshold for directory similarity.</p> <code>0.3</code> <code>min_chunks_for_consolidation</code> <code>int</code> <p>Min chunks to trigger consolidation.</p> <code>2</code> <code>max_chunks_before_consolidation</code> <code>int</code> <p>Max chunks allowed before forced consolidation.</p> <code>20</code> <code>max_file_size_for_llm</code> <code>int | None</code> <p>Max file size for LLM processing.</p> <code>None</code> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def __init__(\n\tself,\n\tembedding_model: EmbeddingModel | None = None,\n\tcode_extensions: set[str] | None = None,\n\trelated_file_patterns: list[tuple[Pattern, Pattern]] | None = None,\n\tsimilarity_threshold: float = 0.4,\n\tdirectory_similarity_threshold: float = 0.3,\n\tmin_chunks_for_consolidation: int = 2,\n\tmax_chunks_before_consolidation: int = 20,\n\tmax_file_size_for_llm: int | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the SemanticSplitStrategy.\n\n\tArgs:\n\t    embedding_model: Optional embedding model instance\n\t    code_extensions: Optional set of code file extensions. Defaults to config.\n\t    related_file_patterns: Optional list of related file patterns\n\t    similarity_threshold: Threshold for grouping by content similarity.\n\t    directory_similarity_threshold: Threshold for directory similarity.\n\t    min_chunks_for_consolidation: Min chunks to trigger consolidation.\n\t    max_chunks_before_consolidation: Max chunks allowed before forced consolidation.\n\t    max_file_size_for_llm: Max file size for LLM processing.\n\n\t\"\"\"\n\tsuper().__init__(embedding_model)\n\t# Store thresholds and settings\n\tself.similarity_threshold = similarity_threshold\n\tself.directory_similarity_threshold = directory_similarity_threshold\n\tself.min_chunks_for_consolidation = min_chunks_for_consolidation\n\tself.max_chunks_before_consolidation = max_chunks_before_consolidation\n\t# Use default from config if not provided\n\tself.max_file_size_for_llm = (\n\t\tmax_file_size_for_llm\n\t\tif max_file_size_for_llm is not None\n\t\telse DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"max_file_size_for_llm\"]\n\t)\n\n\t# Set up file extensions, defaulting to config if None is passed\n\tself.code_extensions = (\n\t\tcode_extensions\n\t\tif code_extensions is not None\n\t\telse set(DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\"default_code_extensions\"])\n\t)\n\t# Initialize patterns for related files\n\tself.related_file_patterns = related_file_patterns or self._initialize_related_file_patterns()\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.similarity_threshold","title":"similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.directory_similarity_threshold","title":"directory_similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>directory_similarity_threshold = (\n\tdirectory_similarity_threshold\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.min_chunks_for_consolidation","title":"min_chunks_for_consolidation  <code>instance-attribute</code>","text":"<pre><code>min_chunks_for_consolidation = min_chunks_for_consolidation\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.max_chunks_before_consolidation","title":"max_chunks_before_consolidation  <code>instance-attribute</code>","text":"<pre><code>max_chunks_before_consolidation = (\n\tmax_chunks_before_consolidation\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.max_file_size_for_llm","title":"max_file_size_for_llm  <code>instance-attribute</code>","text":"<pre><code>max_file_size_for_llm = (\n\tmax_file_size_for_llm\n\tif max_file_size_for_llm is not None\n\telse DEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\n\t\t\"max_file_size_for_llm\"\n\t]\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.code_extensions","title":"code_extensions  <code>instance-attribute</code>","text":"<pre><code>code_extensions = (\n\tcode_extensions\n\tif code_extensions is not None\n\telse set(\n\t\tDEFAULT_CONFIG[\"commit\"][\"diff_splitter\"][\n\t\t\t\"default_code_extensions\"\n\t\t]\n\t)\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.related_file_patterns","title":"related_file_patterns  <code>instance-attribute</code>","text":"<pre><code>related_file_patterns = (\n\trelated_file_patterns\n\tor _initialize_related_file_patterns()\n)\n</code></pre>"},{"location":"api/git/diff_splitter/strategies/#codemap.git.diff_splitter.strategies.SemanticSplitStrategy.split","title":"split","text":"<pre><code>split(diff: GitDiff) -&gt; list[DiffChunk]\n</code></pre> <p>Split a diff into chunks based on semantic relationships.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>GitDiff</code> <p>GitDiff object to split</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>List of DiffChunk objects based on semantic analysis</p> Source code in <code>src/codemap/git/diff_splitter/strategies.py</code> <pre><code>def split(self, diff: GitDiff) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tSplit a diff into chunks based on semantic relationships.\n\n\tArgs:\n\t    diff: GitDiff object to split\n\n\tReturns:\n\t    List of DiffChunk objects based on semantic analysis\n\n\t\"\"\"\n\tif not diff.files:\n\t\tlogger.debug(\"No files to process\")\n\t\treturn []\n\n\t# Validate embedding model is available\n\tself._validate_embedding_model()\n\n\t# Handle files in manageable groups\n\tif len(diff.files) &gt; MAX_FILES_PER_GROUP:\n\t\tlogger.info(\"Processing large number of files (%d) in smaller groups\", len(diff.files))\n\n\t\t# Group files by directory to increase likelihood of related files being processed together\n\t\tfiles_by_dir = {}\n\t\tfor file in diff.files:\n\t\t\tdir_path = str(Path(file).parent)\n\t\t\tif dir_path not in files_by_dir:\n\t\t\t\tfiles_by_dir[dir_path] = []\n\t\t\tfiles_by_dir[dir_path].append(file)\n\n\t\t# Process each directory group separately, keeping chunks under 5 files\n\t\tall_chunks = []\n\t\t# Iterate directly over the file lists since the directory path isn't used here\n\t\tfor files in files_by_dir.values():\n\t\t\t# Process files in this directory in batches of 3-5\n\t\t\tfor i in range(0, len(files), 3):\n\t\t\t\tbatch = files[i : i + 3]\n\t\t\t\t# Create a new GitDiff for the batch, ensuring content is passed\n\t\t\t\tbatch_diff = GitDiff(\n\t\t\t\t\tfiles=batch,\n\t\t\t\t\tcontent=diff.content,  # Pass the original full diff content\n\t\t\t\t\tis_staged=diff.is_staged,\n\t\t\t\t)\n\t\t\t\tall_chunks.extend(self._process_group(batch_diff))\n\n\t\treturn all_chunks\n\n\t# For smaller groups, process normally\n\treturn self._process_group(diff)\n</code></pre>"},{"location":"api/git/diff_splitter/utils/","title":"Utils","text":"<p>Utility functions for diff splitting.</p>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.get_language_specific_patterns","title":"get_language_specific_patterns","text":"<pre><code>get_language_specific_patterns(language: str) -&gt; list[str]\n</code></pre> <p>Get language-specific regex patterns for code structure.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Programming language identifier</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of regex patterns for the language, or empty list if not supported</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def get_language_specific_patterns(language: str) -&gt; list[str]:\n\t\"\"\"\n\tGet language-specific regex patterns for code structure.\n\n\tArgs:\n\t    language: Programming language identifier\n\n\tReturns:\n\t    A list of regex patterns for the language, or empty list if not supported\n\n\t\"\"\"\n\t# Define pattern strings (used for semantic boundary detection)\n\tpattern_strings = {\n\t\t\"py\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^from\\s+.*\",  # From imports\n\t\t\tr\"^class\\s+\\w+\",  # Class definitions\n\t\t\tr\"^def\\s+\\w+\",  # Function definitions\n\t\t\tr\"^if\\s+__name__\\s*==\\s*['\\\"]__main__['\\\"]\",  # Main block\n\t\t],\n\t\t\"js\": [\n\t\t\tr\"^import\\s+.*\",  # ES6 imports\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*require\",  # CommonJS imports\n\t\t\tr\"^function\\s+\\w+\",  # Function declarations\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*function\",  # Function expressions\n\t\t\tr\"^class\\s+\\w+\",  # Class declarations\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t],\n\t\t\"ts\": [\n\t\t\tr\"^import\\s+.*\",  # Imports\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t\tr\"^interface\\s+\",  # Interfaces\n\t\t\tr\"^type\\s+\",  # Type definitions\n\t\t\tr\"^class\\s+\",  # Classes\n\t\t\tr\"^function\\s+\",  # Functions\n\t\t],\n\t\t\"jsx\": [\n\t\t\tr\"^import\\s+.*\",  # ES6 imports\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*require\",  # CommonJS imports\n\t\t\tr\"^function\\s+\\w+\",  # Function declarations\n\t\t\tr\"^const\\s+\\w+\\s*=\\s*function\",  # Function expressions\n\t\t\tr\"^class\\s+\\w+\",  # Class declarations\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t],\n\t\t\"tsx\": [\n\t\t\tr\"^import\\s+.*\",  # Imports\n\t\t\tr\"^export\\s+\",  # Exports\n\t\t\tr\"^interface\\s+\",  # Interfaces\n\t\t\tr\"^type\\s+\",  # Type definitions\n\t\t\tr\"^class\\s+\",  # Classes\n\t\t\tr\"^function\\s+\",  # Functions\n\t\t],\n\t\t\"java\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^public\\s+class\",  # Public class\n\t\t\tr\"^private\\s+class\",  # Private class\n\t\t\tr\"^(public|private|protected)(\\s+static)?\\s+\\w+\\s+\\w+\\(\",  # Methods\n\t\t],\n\t\t\"go\": [\n\t\t\tr\"^import\\s+\",  # Import statements\n\t\t\tr\"^func\\s+\",  # Function definitions\n\t\t\tr\"^type\\s+\\w+\\s+struct\",  # Struct definitions\n\t\t],\n\t\t\"rb\": [\n\t\t\tr\"^require\\s+\",  # Requires\n\t\t\tr\"^class\\s+\",  # Class definitions\n\t\t\tr\"^def\\s+\",  # Method definitions\n\t\t\tr\"^module\\s+\",  # Module definitions\n\t\t],\n\t\t\"php\": [\n\t\t\tr\"^namespace\\s+\",  # Namespace declarations\n\t\t\tr\"^use\\s+\",  # Use statements\n\t\t\tr\"^class\\s+\",  # Class definitions\n\t\t\tr\"^(public|private|protected)\\s+function\",  # Methods\n\t\t],\n\t\t\"cs\": [\n\t\t\tr\"^using\\s+\",  # Using directives\n\t\t\tr\"^namespace\\s+\",  # Namespace declarations\n\t\t\tr\"^(public|private|protected|internal)\\s+class\",  # Classes\n\t\t\tr\"^(public|private|protected|internal)(\\s+static)?\\s+\\w+\\s+\\w+\\(\",  # Methods\n\t\t],\n\t\t\"kt\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^class\\s+\\w+\",  # Class definitions\n\t\t\tr\"^fun\\s+\\w+\",  # Function definitions\n\t\t\tr\"^val\\s+\\w+\",  # Val declarations\n\t\t\tr\"^var\\s+\\w+\",  # Var declarations\n\t\t],\n\t\t\"scala\": [\n\t\t\tr\"^import\\s+.*\",  # Import statements\n\t\t\tr\"^class\\s+\\w+\",  # Class definitions\n\t\t\tr\"^object\\s+\\w+\",  # Object definitions\n\t\t\tr\"^def\\s+\\w+\",  # Method definitions\n\t\t\tr\"^val\\s+\\w+\",  # Val declarations\n\t\t\tr\"^var\\s+\\w+\",  # Var declarations\n\t\t],\n\t}\n\n\t# Return pattern strings for the language or empty list if not supported\n\treturn pattern_strings.get(language, [])\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.determine_commit_type","title":"determine_commit_type","text":"<pre><code>determine_commit_type(files: list[str]) -&gt; str\n</code></pre> <p>Determine the appropriate commit type based on the files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of file paths</p> required <p>Returns:</p> Type Description <code>str</code> <p>Commit type string (e.g., \"feat\", \"fix\", \"test\", \"docs\", \"chore\")</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def determine_commit_type(files: list[str]) -&gt; str:\n\t\"\"\"\n\tDetermine the appropriate commit type based on the files.\n\n\tArgs:\n\t    files: List of file paths\n\n\tReturns:\n\t    Commit type string (e.g., \"feat\", \"fix\", \"test\", \"docs\", \"chore\")\n\n\t\"\"\"\n\t# Check for test files\n\tif any(f.startswith(\"tests/\") or \"_test.\" in f or \"test_\" in f for f in files):\n\t\treturn \"test\"\n\n\t# Check for documentation files\n\tif any(f.startswith(\"docs/\") or f.endswith(\".md\") for f in files):\n\t\treturn \"docs\"\n\n\t# Check for configuration files\n\tif any(f.endswith((\".json\", \".yml\", \".yaml\", \".toml\", \".ini\", \".cfg\")) for f in files):\n\t\treturn \"chore\"\n\n\t# Default to \"chore\" for general updates\n\treturn \"chore\"\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.create_chunk_description","title":"create_chunk_description","text":"<pre><code>create_chunk_description(\n\tcommit_type: str, files: list[str]\n) -&gt; str\n</code></pre> <p>Create a meaningful description for a chunk.</p> <p>Parameters:</p> Name Type Description Default <code>commit_type</code> <code>str</code> <p>Type of commit (e.g., \"feat\", \"fix\")</p> required <code>files</code> <code>list[str]</code> <p>List of file paths</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description string</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def create_chunk_description(commit_type: str, files: list[str]) -&gt; str:\n\t\"\"\"\n\tCreate a meaningful description for a chunk.\n\n\tArgs:\n\t    commit_type: Type of commit (e.g., \"feat\", \"fix\")\n\t    files: List of file paths\n\n\tReturns:\n\t    Description string\n\n\t\"\"\"\n\tif len(files) == 1:\n\t\treturn f\"{commit_type}: update {files[0]}\"\n\n\t# Try to find a common directory using Path for better cross-platform compatibility\n\ttry:\n\t\tcommon_dir = Path(os.path.commonpath(files))\n\t\tif str(common_dir) not in (\".\", \"\"):\n\t\t\treturn f\"{commit_type}: update files in {common_dir}\"\n\texcept ValueError:\n\t\t# commonpath raises ValueError if files are on different drives\n\t\tpass\n\n\treturn f\"{commit_type}: update {len(files)} related files\"\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.get_deleted_tracked_files","title":"get_deleted_tracked_files","text":"<pre><code>get_deleted_tracked_files() -&gt; tuple[set, set]\n</code></pre> <p>Get list of deleted but tracked files from git status.</p> <p>Returns:</p> Type Description <code>tuple[set, set]</code> <p>Tuple of (deleted_unstaged_files, deleted_staged_files) as sets</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def get_deleted_tracked_files() -&gt; tuple[set, set]:\n\t\"\"\"\n\tGet list of deleted but tracked files from git status.\n\n\tReturns:\n\t    Tuple of (deleted_unstaged_files, deleted_staged_files) as sets\n\n\t\"\"\"\n\tdeleted_unstaged_files = set()\n\tdeleted_staged_files = set()\n\ttry:\n\t\t# Parse git status to find deleted files\n\t\tstatus_output = run_git_command([\"git\", \"status\", \"--porcelain\"])\n\t\tfor line in status_output.splitlines():\n\t\t\tif line.startswith(\" D\"):\n\t\t\t\t# Unstaged deletion (space followed by D)\n\t\t\t\tfilename = line[3:].strip()  # Skip \" D \" prefix and strip any whitespace\n\t\t\t\tdeleted_unstaged_files.add(filename)\n\t\t\telif line.startswith(\"D \"):\n\t\t\t\t# Staged deletion (D followed by space)\n\t\t\t\tfilename = line[2:].strip()  # Skip \"D \" prefix and strip any whitespace\n\t\t\t\tdeleted_staged_files.add(filename)\n\t\tlogger.debug(\"Found %d deleted unstaged files in git status\", len(deleted_unstaged_files))\n\t\tlogger.debug(\"Found %d deleted staged files in git status\", len(deleted_staged_files))\n\texcept GitError as e:  # Catch specific GitError from run_git_command\n\t\tlogger.warning(\"Failed to get git status for deleted files: %s. Proceeding without deleted file info.\", e)\n\texcept Exception:  # Catch any other unexpected error\n\t\tlogger.exception(\"Unexpected error getting git status: %s. Proceeding without deleted file info.\")\n\n\treturn deleted_unstaged_files, deleted_staged_files\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.filter_valid_files","title":"filter_valid_files","text":"<pre><code>filter_valid_files(\n\tfiles: list[str], is_test_environment: bool = False\n) -&gt; tuple[list[str], list[str]]\n</code></pre> <p>Filter invalid filenames and files based on existence and Git tracking.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>List of file paths to filter</p> required <code>is_test_environment</code> <code>bool</code> <p>Whether running in a test environment</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>Tuple of (valid_files, empty_list) - The second element is always an empty list now.</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def filter_valid_files(files: list[str], is_test_environment: bool = False) -&gt; tuple[list[str], list[str]]:\n\t\"\"\"\n\tFilter invalid filenames and files based on existence and Git tracking.\n\n\tArgs:\n\t    files: List of file paths to filter\n\t    is_test_environment: Whether running in a test environment\n\n\tReturns:\n\t    Tuple of (valid_files, empty_list) - The second element is always an empty list now.\n\n\t\"\"\"\n\tif not files:\n\t\treturn [], []\n\n\tvalid_files_intermediate = []\n\t# Keep track of files filtered due to large size if needed elsewhere,\n\t# but don't remove them from processing yet.\n\n\tfor file in files:\n\t\t# Skip files that look like patterns or templates\n\t\tif any(char in file for char in [\"*\", \"+\", \"{\", \"}\", \"\\\\\"]) or file.startswith('\"'):\n\t\t\tlogger.warning(\"Skipping invalid filename in diff processing: %s\", file)\n\t\t\tcontinue\n\t\tvalid_files_intermediate.append(file)\n\n\t# --- File Existence and Git Tracking Checks ---\n\tvalid_files = []  # Reset valid_files to populate after existence checks\n\n\t# Skip file existence checks in test environments\n\tif is_test_environment:\n\t\tlogger.debug(\"In test environment - skipping file existence checks for %d files\", len(valid_files_intermediate))\n\t\t# In test env, assume all intermediate files are valid regarding existence/tracking\n\t\tvalid_files = valid_files_intermediate\n\telse:\n\t\t# Get deleted files\n\t\tdeleted_unstaged_files, deleted_staged_files = get_deleted_tracked_files()\n\n\t\t# Check if files exist in the repository (tracked by git) or filesystem\n\t\toriginal_count = len(valid_files_intermediate)\n\t\ttry:\n\t\t\ttracked_files_output = run_git_command([\"git\", \"ls-files\"])\n\t\t\ttracked_files = set(tracked_files_output.splitlines())\n\n\t\t\t# Keep files that either:\n\t\t\t# 1. Exist in filesystem\n\t\t\t# 2. Are tracked by git\n\t\t\t# 3. Are known deleted files from git status\n\t\t\t# 4. Are already staged deletions\n\t\t\tfiltered_files = []\n\t\t\tfor file in valid_files_intermediate:\n\t\t\t\ttry:\n\t\t\t\t\tpath_exists = Path(file).exists()\n\t\t\t\texcept OSError as e:\n\t\t\t\t\tlogger.warning(\"OS error checking existence for %s: %s. Skipping file.\", file, e)\n\t\t\t\t\tcontinue\n\t\t\t\texcept Exception:\n\t\t\t\t\tlogger.exception(\"Unexpected error checking existence for %s. Skipping file.\", file)\n\t\t\t\t\tcontinue\n\n\t\t\t\tif (\n\t\t\t\t\tpath_exists\n\t\t\t\t\tor file in tracked_files\n\t\t\t\t\tor file in deleted_unstaged_files\n\t\t\t\t\tor file in deleted_staged_files\n\t\t\t\t):\n\t\t\t\t\tfiltered_files.append(file)\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\"Skipping non-existent/untracked/not-deleted file in diff: %s\", file)\n\n\t\t\tvalid_files = filtered_files\n\t\t\tif len(valid_files) &lt; original_count:\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"Filtered out %d files that don't exist or aren't tracked/deleted\",\n\t\t\t\t\toriginal_count - len(valid_files),\n\t\t\t\t)\n\t\texcept GitError as e:  # Catch GitError from run_git_command\n\t\t\tlogger.warning(\"Failed to get tracked files from git: %s. Filtering based on existence only.\", e)\n\t\t\t# If we can't check git tracked files, filter by filesystem existence and git status\n\t\t\tfiltered_files_fallback = []\n\t\t\tfor file in valid_files_intermediate:\n\t\t\t\ttry:\n\t\t\t\t\tpath_exists = Path(file).exists()\n\t\t\t\texcept OSError as e:\n\t\t\t\t\tlogger.warning(\"OS error checking existence for %s: %s. Skipping file.\", file, e)\n\t\t\t\t\tcontinue\n\t\t\t\texcept Exception:\n\t\t\t\t\tlogger.exception(\"Unexpected error checking existence for %s. Skipping file.\", file)\n\t\t\t\t\tcontinue\n\n\t\t\t\tif path_exists or file in deleted_unstaged_files or file in deleted_staged_files:\n\t\t\t\t\tfiltered_files_fallback.append(file)\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\"Skipping non-existent/not-deleted file in diff (git check failed): %s\", file)\n\n\t\t\tvalid_files = filtered_files_fallback  # Replace valid_files with the fallback list\n\t\t\tif len(valid_files) &lt; original_count:\n\t\t\t\t# Adjust log message if git check failed\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"Filtered out %d files that don't exist (git check failed)\",\n\t\t\t\t\toriginal_count - len(valid_files),\n\t\t\t\t)\n\t\texcept Exception:  # Catch any other unexpected errors during the initial try block\n\t\t\tlogger.exception(\"Unexpected error during file filtering. Proceeding with potentially incorrect list.\")\n\t\t\t# If a catastrophic error occurs, proceed with the intermediate list\n\t\t\tvalid_files = valid_files_intermediate\n\n\t# Return only the list of valid files. The concept of 'filtered_large_files' is removed.\n\t# Size checking will now happen within the splitting strategy.\n\treturn valid_files, []  # Return empty list for the second element now.\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.is_test_environment","title":"is_test_environment","text":"<pre><code>is_test_environment() -&gt; bool\n</code></pre> <p>Check if the code is running in a test environment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if in a test environment, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def is_test_environment() -&gt; bool:\n\t\"\"\"\n\tCheck if the code is running in a test environment.\n\n\tReturns:\n\t    True if in a test environment, False otherwise\n\n\t\"\"\"\n\t# Check multiple environment indicators for tests\n\treturn \"PYTEST_CURRENT_TEST\" in os.environ or \"pytest\" in sys.modules or os.environ.get(\"TESTING\") == \"1\"\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.calculate_semantic_similarity","title":"calculate_semantic_similarity","text":"<pre><code>calculate_semantic_similarity(\n\temb1: list[float], emb2: list[float]\n) -&gt; float\n</code></pre> <p>Calculate semantic similarity (cosine similarity) between two embedding vectors.</p> <p>Parameters:</p> Name Type Description Default <code>emb1</code> <code>list[float]</code> <p>First embedding vector</p> required <code>emb2</code> <code>list[float]</code> <p>Second embedding vector</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0 and 1</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def calculate_semantic_similarity(emb1: list[float], emb2: list[float]) -&gt; float:\n\t\"\"\"\n\tCalculate semantic similarity (cosine similarity) between two embedding vectors.\n\n\tArgs:\n\t    emb1: First embedding vector\n\t    emb2: Second embedding vector\n\n\tReturns:\n\t    Similarity score between 0 and 1\n\n\t\"\"\"\n\tif not emb1 or not emb2:\n\t\treturn 0.0\n\n\ttry:\n\t\t# Convert to numpy arrays\n\t\tvec1 = np.array(emb1, dtype=np.float64)\n\t\tvec2 = np.array(emb2, dtype=np.float64)\n\n\t\t# Calculate cosine similarity\n\t\tdot_product = np.dot(vec1, vec2)\n\t\tnorm1 = np.linalg.norm(vec1)\n\t\tnorm2 = np.linalg.norm(vec2)\n\n\t\tif norm1 &lt;= EPSILON or norm2 &lt;= EPSILON:\n\t\t\treturn 0.0\n\n\t\tsimilarity = float(dot_product / (norm1 * norm2))\n\n\t\t# Handle potential numeric issues\n\t\tif not np.isfinite(similarity):\n\t\t\treturn 0.0\n\n\t\treturn max(0.0, min(1.0, similarity))  # Clamp to [0, 1]\n\n\texcept (ValueError, TypeError, ArithmeticError, OverflowError):\n\t\tlogger.warning(\"Failed to calculate similarity\")\n\t\treturn 0.0\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.match_test_file_patterns","title":"match_test_file_patterns","text":"<pre><code>match_test_file_patterns(file1: str, file2: str) -&gt; bool\n</code></pre> <p>Check if files match common test file patterns.</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def match_test_file_patterns(file1: str, file2: str) -&gt; bool:\n\t\"\"\"Check if files match common test file patterns.\"\"\"\n\t# test_X.py and X.py patterns\n\tif file1.startswith(\"test_\") and file1[5:] == file2:\n\t\treturn True\n\tif file2.startswith(\"test_\") and file2[5:] == file1:\n\t\treturn True\n\n\t# X_test.py and X.py patterns\n\tif file1.endswith(\"_test.py\") and file1[:-8] + \".py\" == file2:\n\t\treturn True\n\treturn bool(file2.endswith(\"_test.py\") and file2[:-8] + \".py\" == file1)\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.have_similar_names","title":"have_similar_names","text":"<pre><code>have_similar_names(file1: str, file2: str) -&gt; bool\n</code></pre> <p>Check if files have similar base names.</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def have_similar_names(file1: str, file2: str) -&gt; bool:\n\t\"\"\"Check if files have similar base names.\"\"\"\n\tbase1 = file1.rsplit(\".\", 1)[0] if \".\" in file1 else file1\n\tbase2 = file2.rsplit(\".\", 1)[0] if \".\" in file2 else file2\n\n\treturn (base1 in base2 or base2 in base1) and min(len(base1), len(base2)) &gt;= MIN_NAME_LENGTH_FOR_SIMILARITY\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.has_related_file_pattern","title":"has_related_file_pattern","text":"<pre><code>has_related_file_pattern(\n\tfile1: str,\n\tfile2: str,\n\trelated_file_patterns: Iterable[\n\t\ttuple[Pattern, Pattern]\n\t],\n) -&gt; bool\n</code></pre> <p>Check if files match known related patterns.</p> <p>Parameters:</p> Name Type Description Default <code>file1</code> <code>str</code> <p>First file path</p> required <code>file2</code> <code>str</code> <p>Second file path</p> required <code>related_file_patterns</code> <code>Iterable[tuple[Pattern, Pattern]]</code> <p>Compiled regex pattern pairs to check against</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the files match a known pattern, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def has_related_file_pattern(file1: str, file2: str, related_file_patterns: Iterable[tuple[Pattern, Pattern]]) -&gt; bool:\n\t\"\"\"\n\tCheck if files match known related patterns.\n\n\tArgs:\n\t    file1: First file path\n\t    file2: Second file path\n\t    related_file_patterns: Compiled regex pattern pairs to check against\n\n\tReturns:\n\t    True if the files match a known pattern, False otherwise\n\n\t\"\"\"\n\tfor pattern1, pattern2 in related_file_patterns:\n\t\tif (pattern1.match(file1) and pattern2.match(file2)) or (pattern2.match(file1) and pattern1.match(file2)):\n\t\t\treturn True\n\treturn False\n</code></pre>"},{"location":"api/git/diff_splitter/utils/#codemap.git.diff_splitter.utils.are_files_related","title":"are_files_related","text":"<pre><code>are_files_related(\n\tfile1: str,\n\tfile2: str,\n\trelated_file_patterns: Iterable[\n\t\ttuple[Pattern, Pattern]\n\t],\n) -&gt; bool\n</code></pre> <p>Determine if two files are semantically related based on various criteria.</p> <p>Parameters:</p> Name Type Description Default <code>file1</code> <code>str</code> <p>First file path</p> required <code>file2</code> <code>str</code> <p>Second file path</p> required <code>related_file_patterns</code> <code>Iterable[tuple[Pattern, Pattern]]</code> <p>Compiled regex pattern pairs for pattern matching</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the files are related, False otherwise</p> Source code in <code>src/codemap/git/diff_splitter/utils.py</code> <pre><code>def are_files_related(file1: str, file2: str, related_file_patterns: Iterable[tuple[Pattern, Pattern]]) -&gt; bool:\n\t\"\"\"\n\tDetermine if two files are semantically related based on various criteria.\n\n\tArgs:\n\t    file1: First file path\n\t    file2: Second file path\n\t    related_file_patterns: Compiled regex pattern pairs for pattern matching\n\n\tReturns:\n\t    True if the files are related, False otherwise\n\n\t\"\"\"\n\t# 1. Files in the same directory\n\tdir1 = file1.rsplit(\"/\", 1)[0] if \"/\" in file1 else \"\"\n\tdir2 = file2.rsplit(\"/\", 1)[0] if \"/\" in file2 else \"\"\n\tif dir1 and dir1 == dir2:\n\t\treturn True\n\n\t# 2. Files in closely related directories (parent/child or same root directory)\n\tif dir1 and dir2:\n\t\tif dir1.startswith(dir2 + \"/\") or dir2.startswith(dir1 + \"/\"):\n\t\t\treturn True\n\t\t# Check if they share the same top-level directory\n\t\ttop_dir1 = dir1.split(\"/\", 1)[0] if \"/\" in dir1 else dir1\n\t\ttop_dir2 = dir2.split(\"/\", 1)[0] if \"/\" in dir2 else dir2\n\t\tif top_dir1 and top_dir1 == top_dir2:\n\t\t\treturn True\n\n\t# 3. Test files and implementation files (simple check)\n\tif (file1.startswith(\"tests/\") and file2 in file1) or (file2.startswith(\"tests/\") and file1 in file2):\n\t\treturn True\n\n\t# 4. Test file patterns\n\tfile1_name = file1.rsplit(\"/\", 1)[-1] if \"/\" in file1 else file1\n\tfile2_name = file2.rsplit(\"/\", 1)[-1] if \"/\" in file2 else file2\n\tif match_test_file_patterns(file1_name, file2_name):\n\t\treturn True\n\n\t# 5. Files with similar names\n\tif have_similar_names(file1_name, file2_name):\n\t\treturn True\n\n\t# 6. Check for related file patterns\n\treturn has_related_file_pattern(file1, file2, related_file_patterns)\n</code></pre>"},{"location":"api/git/pr_generator/","title":"Pr Generator Overview","text":"<p>PR generation package for CodeMap.</p> <ul> <li>Command - Main PR generation command implementation for CodeMap.</li> <li>Constants - Constants for PR generation.</li> <li>Decorators - Decorators for the PR generator module.</li> <li>Generator - PR generator for the CodeMap Git module.</li> <li>Prompts - Prompt templates for PR generation.</li> <li>Schemas - Schemas and data structures for PR generation.</li> <li>Strategies - Git workflow strategy implementations for PR management.</li> <li>Templates - PR template definitions for different workflow strategies.</li> <li>Utils - Utility functions for PR generation.</li> </ul>"},{"location":"api/git/pr_generator/command/","title":"Command","text":"<p>Main PR generation command implementation for CodeMap.</p>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand","title":"PRCommand","text":"<p>Handles the PR generation command workflow.</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>class PRCommand:\n\t\"\"\"Handles the PR generation command workflow.\"\"\"\n\n\tdef __init__(self, path: Path | None = None, model: str = \"gpt-4o-mini\") -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the PR command.\n\n\t\tArgs:\n\t\t    path: Optional path to start from\n\t\t    model: LLM model to use for PR description generation\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.repo_root = get_repo_root(path)\n\n\t\t\t# Create LLM client and configs\n\t\t\tfrom codemap.llm import create_client\n\n\t\t\tllm_client = create_client(repo_path=self.repo_root, model=model)\n\n\t\t\t# Create the PR generator with required parameters\n\t\t\tself.pr_generator = PRGenerator(\n\t\t\t\trepo_path=self.repo_root,\n\t\t\t\tllm_client=llm_client,\n\t\t\t)\n\n\t\t\tself.error_state = None  # Tracks reason for failure: \"failed\", \"aborted\", etc.\n\t\texcept GitError as e:\n\t\t\traise RuntimeError(str(e)) from e\n\n\tdef _get_branch_info(self) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tGet information about the current branch and its target.\n\n\t\tReturns:\n\t\t    Dictionary with branch information\n\n\t\tRaises:\n\t\t    RuntimeError: If Git operations fail\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get current branch\n\t\t\tcurrent_branch = run_git_command([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]).strip()\n\n\t\t\t# Get default branch (usually main or master)\n\t\t\tdefault_branch = run_git_command([\"git\", \"remote\", \"show\", \"origin\"]).strip()\n\t\t\t# Parse the default branch from the output\n\t\t\tfor line in default_branch.splitlines():\n\t\t\t\tif \"HEAD branch\" in line:\n\t\t\t\t\tdefault_branch = line.split(\":\")[-1].strip()\n\t\t\t\t\tbreak\n\n\t\t\treturn {\"current_branch\": current_branch, \"target_branch\": default_branch}\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get branch information: {e}\"\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _get_commit_history(self, base_branch: str) -&gt; list[dict[str, str]]:\n\t\t\"\"\"\n\t\tGet commit history between the current branch and the base branch.\n\n\t\tArgs:\n\t\t    base_branch: The base branch to compare against\n\n\t\tReturns:\n\t\t    List of commits with their details\n\n\t\tRaises:\n\t\t    RuntimeError: If Git operations fail\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get list of commits that are in the current branch but not in the base branch\n\t\t\tcommits_output = run_git_command([\"git\", \"log\", f\"{base_branch}..HEAD\", \"--pretty=format:%H||%an||%s\"])\n\n\t\t\tcommits = []\n\t\t\tif commits_output.strip():\n\t\t\t\tfor commit_line in commits_output.strip().split(\"\\n\"):\n\t\t\t\t\tif not commit_line.strip():\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tparts = commit_line.split(\"||\")\n\t\t\t\t\tif len(parts) &gt;= MIN_COMMIT_PARTS:\n\t\t\t\t\t\tcommit_hash, author, subject = parts[0], parts[1], parts[2]\n\t\t\t\t\t\tcommits.append({\"hash\": commit_hash, \"author\": author, \"subject\": subject})\n\n\t\t\treturn commits\n\t\texcept GitError as e:\n\t\t\tmsg = f\"Failed to get commit history: {e}\"\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _generate_pr_description(self, branch_info: dict[str, str], _commits: list[dict[str, str]]) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate PR description based on branch info and commit history.\n\n\t\tArgs:\n\t\t    branch_info: Information about the branches\n\t\t    _commits: List of commits to include in the description (fetched internally by PRGenerator)\n\n\t\tReturns:\n\t\t    Generated PR description\n\n\t\tRaises:\n\t\t    RuntimeError: If description generation fails\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\twith loading_spinner(\"Generating PR description using LLM...\"):\n\t\t\t\t# Use the PR generator to create content\n\t\t\t\tcontent = self.pr_generator.generate_content_from_commits(\n\t\t\t\t\tbase_branch=branch_info[\"target_branch\"], head_branch=branch_info[\"current_branch\"], use_llm=True\n\t\t\t\t)\n\t\t\t\treturn content[\"description\"]\n\t\texcept LLMError as e:\n\t\t\tlogger.exception(\"LLM description generation failed\")\n\t\t\tlogger.warning(\"LLM error: %s\", str(e))\n\n\t\t\t# Generate a simple fallback description without LLM\n\t\t\twith loading_spinner(\"Falling back to simple PR description generation...\"):\n\t\t\t\tcontent = self.pr_generator.generate_content_from_commits(\n\t\t\t\t\tbase_branch=branch_info[\"target_branch\"], head_branch=branch_info[\"current_branch\"], use_llm=False\n\t\t\t\t)\n\t\t\t\treturn content[\"description\"]\n\t\texcept (ValueError, RuntimeError) as e:\n\t\t\tlogger.warning(\"Error generating PR description: %s\", str(e))\n\t\t\tmsg = f\"Failed to generate PR description: {e}\"\n\t\t\traise RuntimeError(msg) from e\n\n\tdef _raise_no_commits_error(self, branch_info: dict[str, str]) -&gt; None:\n\t\t\"\"\"\n\t\tRaise an error when no commits are found between branches.\n\n\t\tArgs:\n\t\t    branch_info: Information about the branches\n\n\t\tRaises:\n\t\t    RuntimeError: Always raises this error with appropriate message\n\n\t\t\"\"\"\n\t\tmsg = f\"No commits found between {branch_info['current_branch']} and {branch_info['target_branch']}\"\n\t\tlogger.warning(msg)\n\t\traise RuntimeError(msg)\n\n\tdef run(self) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tRun the PR generation command.\n\n\t\tReturns:\n\t\t    Dictionary with PR information and generated description\n\n\t\tRaises:\n\t\t    RuntimeError: If the command fails\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get branch information\n\t\t\twith loading_spinner(\"Getting branch information...\"):\n\t\t\t\tbranch_info = self._get_branch_info()\n\n\t\t\t# Get commit history\n\t\t\twith loading_spinner(\"Retrieving commit history...\"):\n\t\t\t\tcommits = self._get_commit_history(branch_info[\"target_branch\"])\n\n\t\t\tif not commits:\n\t\t\t\tself._raise_no_commits_error(branch_info)\n\n\t\t\t# Generate PR description\n\t\t\tdescription = self._generate_pr_description(branch_info, commits)\n\n\t\t\treturn {\"branch_info\": branch_info, \"commits\": commits, \"description\": description}\n\t\texcept (RuntimeError, ValueError) as e:\n\t\t\tself.error_state = \"failed\"\n\t\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\tpath: Path | None = None, model: str = \"gpt-4o-mini\"\n) -&gt; None\n</code></pre> <p>Initialize the PR command.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Optional path to start from</p> <code>None</code> <code>model</code> <code>str</code> <p>LLM model to use for PR description generation</p> <code>'gpt-4o-mini'</code> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def __init__(self, path: Path | None = None, model: str = \"gpt-4o-mini\") -&gt; None:\n\t\"\"\"\n\tInitialize the PR command.\n\n\tArgs:\n\t    path: Optional path to start from\n\t    model: LLM model to use for PR description generation\n\n\t\"\"\"\n\ttry:\n\t\tself.repo_root = get_repo_root(path)\n\n\t\t# Create LLM client and configs\n\t\tfrom codemap.llm import create_client\n\n\t\tllm_client = create_client(repo_path=self.repo_root, model=model)\n\n\t\t# Create the PR generator with required parameters\n\t\tself.pr_generator = PRGenerator(\n\t\t\trepo_path=self.repo_root,\n\t\t\tllm_client=llm_client,\n\t\t)\n\n\t\tself.error_state = None  # Tracks reason for failure: \"failed\", \"aborted\", etc.\n\texcept GitError as e:\n\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = get_repo_root(path)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.pr_generator","title":"pr_generator  <code>instance-attribute</code>","text":"<pre><code>pr_generator = PRGenerator(\n\trepo_path=repo_root, llm_client=llm_client\n)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.error_state","title":"error_state  <code>instance-attribute</code>","text":"<pre><code>error_state = None\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRCommand.run","title":"run","text":"<pre><code>run() -&gt; dict[str, Any]\n</code></pre> <p>Run the PR generation command.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with PR information and generated description</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the command fails</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def run(self) -&gt; dict[str, Any]:\n\t\"\"\"\n\tRun the PR generation command.\n\n\tReturns:\n\t    Dictionary with PR information and generated description\n\n\tRaises:\n\t    RuntimeError: If the command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Get branch information\n\t\twith loading_spinner(\"Getting branch information...\"):\n\t\t\tbranch_info = self._get_branch_info()\n\n\t\t# Get commit history\n\t\twith loading_spinner(\"Retrieving commit history...\"):\n\t\t\tcommits = self._get_commit_history(branch_info[\"target_branch\"])\n\n\t\tif not commits:\n\t\t\tself._raise_no_commits_error(branch_info)\n\n\t\t# Generate PR description\n\t\tdescription = self._generate_pr_description(branch_info, commits)\n\n\t\treturn {\"branch_info\": branch_info, \"commits\": commits, \"description\": description}\n\texcept (RuntimeError, ValueError) as e:\n\t\tself.error_state = \"failed\"\n\t\traise RuntimeError(str(e)) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand","title":"PRWorkflowCommand","text":"<p>Handles the core PR creation and update workflow logic.</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>class PRWorkflowCommand:\n\t\"\"\"Handles the core PR creation and update workflow logic.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_path: Path,\n\t\tconfig_loader: ConfigLoader,\n\t\tllm_client: LLMClient | None = None,\n\t\tmodel: str | None = None,\n\t\tapi_key: str | None = None,\n\t\tapi_base: str | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the PR workflow command helper.\n\n\t\tArgs:\n\t\t        repo_path: Path to the repository.\n\t\t        config_loader: ConfigLoader instance.\n\t\t        llm_client: Optional pre-configured LLMClient.\n\t\t        model: LLM model name (used if llm_client is None).\n\t\t        api_key: API key (used if llm_client is None).\n\t\t        api_base: API base URL (used if llm_client is None).\n\n\t\t\"\"\"\n\t\tself.repo_path = repo_path\n\t\tself.config_loader = config_loader\n\t\tself.pr_config = self.config_loader.config.get(\"pr\", {})\n\t\tself.content_config = self.pr_config.get(\"content\", {})\n\t\tself.workflow_strategy_name = self.config_loader.get_workflow_strategy()\n\t\tself.workflow = create_strategy(self.workflow_strategy_name)\n\n\t\t# Initialize LLM client if needed\n\t\tif llm_client:\n\t\t\tself.llm_client = llm_client\n\t\telse:\n\t\t\tfrom codemap.llm import create_client\n\n\t\t\tself.llm_client = create_client(\n\t\t\t\trepo_path=self.repo_path,\n\t\t\t\tmodel=model,\n\t\t\t\tapi_key=api_key,\n\t\t\t\tapi_base=api_base,\n\t\t\t)\n\n\t\tself.pr_generator = PRGenerator(repo_path=self.repo_path, llm_client=self.llm_client)\n\n\tdef _generate_release_pr_content(self, base_branch: str, branch_name: str) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tGenerate PR content for a release.\n\n\t\tArgs:\n\t\t        base_branch: The branch to merge into (e.g. main)\n\t\t        branch_name: The release branch name (e.g. release/1.0.0)\n\n\t\tReturns:\n\t\t        Dictionary with title and description\n\n\t\t\"\"\"\n\t\t# Extract version from branch name\n\t\tversion = branch_name.replace(\"release/\", \"\")\n\t\ttitle = f\"Release {version}\"\n\t\t# Include base branch information in the description\n\t\tdescription = f\"# Release {version}\\n\\nThis pull request merges release {version} into {base_branch}.\"\n\t\treturn {\"title\": title, \"description\": description}\n\n\tdef _generate_title(self, commits: list[str], branch_name: str, branch_type: str) -&gt; str:\n\t\t\"\"\"Core logic for generating PR title.\"\"\"\n\t\ttitle_strategy = self.content_config.get(\"title_strategy\", \"commits\")\n\n\t\tif not commits:\n\t\t\tif branch_type == \"release\":\n\t\t\t\treturn f\"Release {branch_name.replace('release/', '')}\"\n\t\t\tclean_name = branch_name.replace(f\"{branch_type}/\", \"\").replace(\"-\", \" \").replace(\"_\", \" \")\n\t\t\treturn f\"{branch_type.capitalize()}: {clean_name.capitalize()}\"\n\n\t\tif title_strategy == \"llm\":\n\t\t\treturn generate_pr_title_with_llm(commits, llm_client=self.llm_client)\n\n\t\treturn generate_pr_title_from_commits(commits)\n\n\tdef _generate_description(self, commits: list[str], branch_name: str, branch_type: str, base_branch: str) -&gt; str:\n\t\t\"\"\"Core logic for generating PR description.\"\"\"\n\t\tdescription_strategy = self.content_config.get(\"description_strategy\", \"commits\")\n\n\t\tif not commits:\n\t\t\tif branch_type == \"release\" and self.workflow_strategy_name == \"gitflow\":\n\t\t\t\t# Call the internal helper method\n\t\t\t\tcontent = self._generate_release_pr_content(base_branch, branch_name)\n\t\t\t\treturn content[\"description\"]\n\t\t\treturn f\"Changes in {branch_name}\"\n\n\t\tif description_strategy == \"llm\":\n\t\t\treturn generate_pr_description_with_llm(commits, llm_client=self.llm_client)\n\n\t\tif description_strategy == \"template\" and not self.content_config.get(\"use_workflow_templates\", True):\n\t\t\ttemplate = self.content_config.get(\"description_template\", \"\")\n\t\t\tif template:\n\t\t\t\tcommit_description = \"\\n\".join([f\"- {commit}\" for commit in commits])\n\t\t\t\t# Note: Other template variables like testing_instructions might need context\n\t\t\t\treturn template.format(\n\t\t\t\t\tchanges=commit_description,\n\t\t\t\t\ttesting_instructions=\"[Testing instructions]\",\n\t\t\t\t\tscreenshots=\"[Screenshots]\",\n\t\t\t\t)\n\n\t\treturn generate_pr_description_from_commits(commits)\n\n\tdef create_pr_workflow(\n\t\tself, base_branch: str, head_branch: str, title: str | None = None, description: str | None = None\n\t) -&gt; PullRequest:\n\t\t\"\"\"Orchestrates the PR creation process (non-interactive part).\"\"\"\n\t\ttry:\n\t\t\t# Check for existing PR first\n\t\t\texisting_pr = get_existing_pr(head_branch)\n\t\t\tif existing_pr:\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"PR #{existing_pr.number} already exists for branch '{head_branch}'. Returning existing PR.\"\n\t\t\t\t)\n\t\t\t\treturn existing_pr\n\n\t\t\t# Get commits\n\t\t\tcommits = get_commit_messages(base_branch, head_branch)\n\n\t\t\t# Determine branch type\n\t\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t\t# Generate title and description if not provided\n\t\t\tfinal_title = title or self._generate_title(commits, head_branch, branch_type)\n\t\t\tfinal_description = description or self._generate_description(\n\t\t\t\tcommits, head_branch, branch_type, base_branch\n\t\t\t)\n\n\t\t\t# Create PR using PRGenerator\n\t\t\tpr = self.pr_generator.create_pr(base_branch, head_branch, final_title, final_description)\n\t\t\tlogger.info(f\"Successfully created PR #{pr.number}: {pr.url}\")\n\t\t\treturn pr\n\t\texcept GitError:\n\t\t\t# Specific handling for unrelated histories might go here or be handled in CLI\n\t\t\tlogger.exception(\"GitError during PR creation workflow\")\n\t\t\traise\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Unexpected error during PR creation workflow\")\n\t\t\tmsg = f\"Unexpected error creating PR: {e}\"\n\t\t\traise PRCreationError(msg) from e\n\n\tdef update_pr_workflow(\n\t\tself,\n\t\tpr_number: int,\n\t\ttitle: str | None = None,\n\t\tdescription: str | None = None,\n\t\tbase_branch: str | None = None,\n\t\thead_branch: str | None = None,\n\t) -&gt; PullRequest:\n\t\t\"\"\"Orchestrates the PR update process (non-interactive part).\"\"\"\n\t\ttry:\n\t\t\t# Fetch existing PR info if needed to regenerate title/description\n\t\t\t# This might require gh cli or GitHub API interaction if pr_generator doesn't fetch\n\t\t\t# For now, assume base/head are provided if regeneration is needed\n\n\t\t\tfinal_title = title\n\t\t\tfinal_description = description\n\n\t\t\t# Regenerate if title/description are None\n\t\t\tif title is None or description is None:\n\t\t\t\tif not base_branch or not head_branch:\n\t\t\t\t\tmsg = \"Cannot regenerate content for update without base and head branches.\"\n\t\t\t\t\traise PRCreationError(msg)\n\n\t\t\t\tcommits = get_commit_messages(base_branch, head_branch)\n\t\t\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t\t\tif title is None:\n\t\t\t\t\tfinal_title = self._generate_title(commits, head_branch, branch_type)\n\t\t\t\tif description is None:\n\t\t\t\t\tfinal_description = self._generate_description(commits, head_branch, branch_type, base_branch)\n\n\t\t\tif final_title is None or final_description is None:\n\t\t\t\tmsg = \"Could not determine final title or description for PR update.\"\n\t\t\t\traise PRCreationError(msg)\n\n\t\t\t# Update PR using PRGenerator\n\t\t\tupdated_pr = self.pr_generator.update_pr(pr_number, final_title, final_description)\n\t\t\tlogger.info(f\"Successfully updated PR #{updated_pr.number}: {updated_pr.url}\")\n\t\t\treturn updated_pr\n\t\texcept GitError:\n\t\t\tlogger.exception(\"GitError during PR update workflow\")\n\t\t\traise\n\t\texcept Exception as e:\n\t\t\tlogger.exception(\"Unexpected error during PR update workflow\")\n\t\t\tmsg = f\"Unexpected error updating PR: {e}\"\n\t\t\traise PRCreationError(msg) from e\n\n\t\t\tlogger.exception(\"Unexpected error during PR update workflow\")\n\t\t\tmsg = f\"Unexpected error updating PR: {e}\"\n\t\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_path: Path,\n\tconfig_loader: ConfigLoader,\n\tllm_client: LLMClient | None = None,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the PR workflow command helper.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to the repository.</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance.</p> required <code>llm_client</code> <code>LLMClient | None</code> <p>Optional pre-configured LLMClient.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>LLM model name (used if llm_client is None).</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key (used if llm_client is None).</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>API base URL (used if llm_client is None).</p> <code>None</code> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def __init__(\n\tself,\n\trepo_path: Path,\n\tconfig_loader: ConfigLoader,\n\tllm_client: LLMClient | None = None,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the PR workflow command helper.\n\n\tArgs:\n\t        repo_path: Path to the repository.\n\t        config_loader: ConfigLoader instance.\n\t        llm_client: Optional pre-configured LLMClient.\n\t        model: LLM model name (used if llm_client is None).\n\t        api_key: API key (used if llm_client is None).\n\t        api_base: API base URL (used if llm_client is None).\n\n\t\"\"\"\n\tself.repo_path = repo_path\n\tself.config_loader = config_loader\n\tself.pr_config = self.config_loader.config.get(\"pr\", {})\n\tself.content_config = self.pr_config.get(\"content\", {})\n\tself.workflow_strategy_name = self.config_loader.get_workflow_strategy()\n\tself.workflow = create_strategy(self.workflow_strategy_name)\n\n\t# Initialize LLM client if needed\n\tif llm_client:\n\t\tself.llm_client = llm_client\n\telse:\n\t\tfrom codemap.llm import create_client\n\n\t\tself.llm_client = create_client(\n\t\t\trepo_path=self.repo_path,\n\t\t\tmodel=model,\n\t\t\tapi_key=api_key,\n\t\t\tapi_base=api_base,\n\t\t)\n\n\tself.pr_generator = PRGenerator(repo_path=self.repo_path, llm_client=self.llm_client)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.pr_config","title":"pr_config  <code>instance-attribute</code>","text":"<pre><code>pr_config = get('pr', {})\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.content_config","title":"content_config  <code>instance-attribute</code>","text":"<pre><code>content_config = get('content', {})\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.workflow_strategy_name","title":"workflow_strategy_name  <code>instance-attribute</code>","text":"<pre><code>workflow_strategy_name = get_workflow_strategy()\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.workflow","title":"workflow  <code>instance-attribute</code>","text":"<pre><code>workflow = create_strategy(workflow_strategy_name)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.llm_client","title":"llm_client  <code>instance-attribute</code>","text":"<pre><code>llm_client = llm_client\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.pr_generator","title":"pr_generator  <code>instance-attribute</code>","text":"<pre><code>pr_generator = PRGenerator(\n\trepo_path=repo_path, llm_client=llm_client\n)\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.create_pr_workflow","title":"create_pr_workflow","text":"<pre><code>create_pr_workflow(\n\tbase_branch: str,\n\thead_branch: str,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n) -&gt; PullRequest\n</code></pre> <p>Orchestrates the PR creation process (non-interactive part).</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def create_pr_workflow(\n\tself, base_branch: str, head_branch: str, title: str | None = None, description: str | None = None\n) -&gt; PullRequest:\n\t\"\"\"Orchestrates the PR creation process (non-interactive part).\"\"\"\n\ttry:\n\t\t# Check for existing PR first\n\t\texisting_pr = get_existing_pr(head_branch)\n\t\tif existing_pr:\n\t\t\tlogger.warning(\n\t\t\t\tf\"PR #{existing_pr.number} already exists for branch '{head_branch}'. Returning existing PR.\"\n\t\t\t)\n\t\t\treturn existing_pr\n\n\t\t# Get commits\n\t\tcommits = get_commit_messages(base_branch, head_branch)\n\n\t\t# Determine branch type\n\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t# Generate title and description if not provided\n\t\tfinal_title = title or self._generate_title(commits, head_branch, branch_type)\n\t\tfinal_description = description or self._generate_description(\n\t\t\tcommits, head_branch, branch_type, base_branch\n\t\t)\n\n\t\t# Create PR using PRGenerator\n\t\tpr = self.pr_generator.create_pr(base_branch, head_branch, final_title, final_description)\n\t\tlogger.info(f\"Successfully created PR #{pr.number}: {pr.url}\")\n\t\treturn pr\n\texcept GitError:\n\t\t# Specific handling for unrelated histories might go here or be handled in CLI\n\t\tlogger.exception(\"GitError during PR creation workflow\")\n\t\traise\n\texcept Exception as e:\n\t\tlogger.exception(\"Unexpected error during PR creation workflow\")\n\t\tmsg = f\"Unexpected error creating PR: {e}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/command/#codemap.git.pr_generator.command.PRWorkflowCommand.update_pr_workflow","title":"update_pr_workflow","text":"<pre><code>update_pr_workflow(\n\tpr_number: int,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n) -&gt; PullRequest\n</code></pre> <p>Orchestrates the PR update process (non-interactive part).</p> Source code in <code>src/codemap/git/pr_generator/command.py</code> <pre><code>def update_pr_workflow(\n\tself,\n\tpr_number: int,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n) -&gt; PullRequest:\n\t\"\"\"Orchestrates the PR update process (non-interactive part).\"\"\"\n\ttry:\n\t\t# Fetch existing PR info if needed to regenerate title/description\n\t\t# This might require gh cli or GitHub API interaction if pr_generator doesn't fetch\n\t\t# For now, assume base/head are provided if regeneration is needed\n\n\t\tfinal_title = title\n\t\tfinal_description = description\n\n\t\t# Regenerate if title/description are None\n\t\tif title is None or description is None:\n\t\t\tif not base_branch or not head_branch:\n\t\t\t\tmsg = \"Cannot regenerate content for update without base and head branches.\"\n\t\t\t\traise PRCreationError(msg)\n\n\t\t\tcommits = get_commit_messages(base_branch, head_branch)\n\t\t\tbranch_type = self.workflow.detect_branch_type(head_branch) or \"feature\"\n\n\t\t\tif title is None:\n\t\t\t\tfinal_title = self._generate_title(commits, head_branch, branch_type)\n\t\t\tif description is None:\n\t\t\t\tfinal_description = self._generate_description(commits, head_branch, branch_type, base_branch)\n\n\t\tif final_title is None or final_description is None:\n\t\t\tmsg = \"Could not determine final title or description for PR update.\"\n\t\t\traise PRCreationError(msg)\n\n\t\t# Update PR using PRGenerator\n\t\tupdated_pr = self.pr_generator.update_pr(pr_number, final_title, final_description)\n\t\tlogger.info(f\"Successfully updated PR #{updated_pr.number}: {updated_pr.url}\")\n\t\treturn updated_pr\n\texcept GitError:\n\t\tlogger.exception(\"GitError during PR update workflow\")\n\t\traise\n\texcept Exception as e:\n\t\tlogger.exception(\"Unexpected error during PR update workflow\")\n\t\tmsg = f\"Unexpected error updating PR: {e}\"\n\t\traise PRCreationError(msg) from e\n\n\t\tlogger.exception(\"Unexpected error during PR update workflow\")\n\t\tmsg = f\"Unexpected error updating PR: {e}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/constants/","title":"Constants","text":"<p>Constants for PR generation.</p>"},{"location":"api/git/pr_generator/constants/#codemap.git.pr_generator.constants.MAX_COMMIT_PREVIEW","title":"MAX_COMMIT_PREVIEW  <code>module-attribute</code>","text":"<pre><code>MAX_COMMIT_PREVIEW = 3\n</code></pre>"},{"location":"api/git/pr_generator/constants/#codemap.git.pr_generator.constants.MIN_SIGNIFICANT_WORD_LENGTH","title":"MIN_SIGNIFICANT_WORD_LENGTH  <code>module-attribute</code>","text":"<pre><code>MIN_SIGNIFICANT_WORD_LENGTH = 3\n</code></pre>"},{"location":"api/git/pr_generator/constants/#codemap.git.pr_generator.constants.MIN_COMMIT_PARTS","title":"MIN_COMMIT_PARTS  <code>module-attribute</code>","text":"<pre><code>MIN_COMMIT_PARTS = 3\n</code></pre>"},{"location":"api/git/pr_generator/decorators/","title":"Decorators","text":"<p>Decorators for the PR generator module.</p>"},{"location":"api/git/pr_generator/decorators/#codemap.git.pr_generator.decorators.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/decorators/#codemap.git.pr_generator.decorators.F","title":"F  <code>module-attribute</code>","text":"<pre><code>F = TypeVar('F', bound=Callable[..., object])\n</code></pre>"},{"location":"api/git/pr_generator/decorators/#codemap.git.pr_generator.decorators.git_operation","title":"git_operation","text":"<pre><code>git_operation(func: F) -&gt; F\n</code></pre> <p>Decorator for git operations.</p> <p>This decorator wraps functions that perform git operations, providing: - Logging of operation start/end - Standardized error handling - Automatic conversion of git-related exceptions to GitError</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>F</code> <p>The function to decorate</p> required <p>Returns:</p> Type Description <code>F</code> <p>Decorated function</p> Source code in <code>src/codemap/git/pr_generator/decorators.py</code> <pre><code>def git_operation(func: F) -&gt; F:\n\t\"\"\"\n\tDecorator for git operations.\n\n\tThis decorator wraps functions that perform git operations, providing:\n\t- Logging of operation start/end\n\t- Standardized error handling\n\t- Automatic conversion of git-related exceptions to GitError\n\n\tArgs:\n\t    func: The function to decorate\n\n\tReturns:\n\t    Decorated function\n\n\t\"\"\"\n\n\t@functools.wraps(func)\n\tdef wrapper(*args: object, **kwargs: object) -&gt; object:\n\t\t\"\"\"Wrapper function for git operations that handles logging and error conversion.\n\n\t\tArgs:\n\t\t    *args: Positional arguments passed to the decorated function.\n\t\t    **kwargs: Keyword arguments passed to the decorated function.\n\n\t\tReturns:\n\t\t    The result of the decorated function if successful.\n\n\t\tRaises:\n\t\t    GitError: If any exception occurs during the git operation. Original GitError\n\t\t        exceptions are re-raised as-is, while other exceptions are converted to\n\t\t        GitError with a descriptive message.\n\n\t\tNote:\n\t\t    - Logs debug messages for operation start/end\n\t\t    - Converts non-GitError exceptions to GitError\n\t\t    - Preserves original GitError exceptions\n\t\t\"\"\"\n\t\tfunction_name = func.__name__\n\t\tlogger.debug(\"Starting git operation: %s\", function_name)\n\n\t\ttry:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tlogger.debug(\"Completed git operation: %s\", function_name)\n\t\t\treturn result\n\t\texcept GitError:\n\t\t\t# Re-raise GitError as is\n\t\t\tlogger.debug(\"GitError in operation: %s\", function_name)\n\t\t\traise\n\t\texcept Exception as e:\n\t\t\t# Convert other exceptions to GitError\n\t\t\tlogger.debug(\"Error in git operation %s: %s\", function_name, str(e))\n\t\t\tmsg = f\"Git operation failed: {function_name} - {e!s}\"\n\t\t\traise GitError(msg) from e\n\n\treturn cast(\"F\", wrapper)\n</code></pre>"},{"location":"api/git/pr_generator/generator/","title":"Generator","text":"<p>PR generator for the CodeMap Git module.</p> <p>This class generates pull requests for git repositories.</p>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator","title":"PRGenerator","text":"<p>Generator for Pull Requests.</p> <p>This class handles generating pull request content (title and description) and creating/updating PRs on GitHub.</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>class PRGenerator:\n\t\"\"\"\n\tGenerator for Pull Requests.\n\n\tThis class handles generating pull request content (title and\n\tdescription) and creating/updating PRs on GitHub.\n\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_path: Path,\n\t\tllm_client: LLMClient,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the PR generator.\n\n\t\tArgs:\n\t\t    repo_path: Path to the git repository\n\t\t    llm_client: LLMClient instance to use for content generation\n\n\t\t\"\"\"\n\t\tself.repo_path = repo_path\n\t\tself.client = llm_client\n\n\tdef generate_content_from_commits(self, base_branch: str, head_branch: str, use_llm: bool = True) -&gt; PRContent:\n\t\t\"\"\"\n\t\tGenerate PR content (title and description) from commits.\n\n\t\tArgs:\n\t\t    base_branch: Base branch (e.g., main)\n\t\t    head_branch: Head branch (e.g., feature-branch)\n\t\t    use_llm: Whether to use LLM for generation\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' keys\n\n\t\t\"\"\"\n\t\t# Get commit messages between branches\n\t\tcommits = get_commit_messages(base_branch, head_branch)\n\n\t\tif not commits:\n\t\t\treturn {\"title\": \"Update branch\", \"description\": \"No changes in this PR.\"}\n\n\t\tif use_llm:\n\t\t\t# Generate title and description using LLM\n\t\t\ttitle = generate_pr_title_with_llm(commits, self.client)\n\t\t\tdescription = generate_pr_description_with_llm(commits, self.client)\n\t\telse:\n\t\t\t# Generate title and description using rule-based approach\n\t\t\ttitle = generate_pr_title_from_commits(commits)\n\t\t\tdescription = generate_pr_description_from_commits(commits)\n\n\t\treturn {\"title\": title, \"description\": description}\n\n\tdef generate_content_from_template(\n\t\tself, branch_name: str, description: str, workflow_strategy: str = \"github-flow\"\n\t) -&gt; PRContent:\n\t\t\"\"\"\n\t\tGenerate PR content (title and description) from a template.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch\n\t\t    description: Short description of the changes\n\t\t    workflow_strategy: Git workflow strategy to use\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' keys\n\n\t\t\"\"\"\n\t\treturn generate_pr_content_from_template(branch_name, description, workflow_strategy)\n\n\tdef suggest_branch_name(self, description: str, workflow_strategy: str = \"github-flow\") -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on a description.\n\n\t\tArgs:\n\t\t    description: Description of the branch\n\t\t    workflow_strategy: Git workflow strategy to use\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\treturn suggest_branch_name(description, workflow_strategy)\n\n\tdef create_pr(self, base_branch: str, head_branch: str, title: str, description: str) -&gt; PullRequest:\n\t\t\"\"\"\n\t\tCreate a pull request on GitHub.\n\n\t\tArgs:\n\t\t    base_branch: Base branch (e.g., main)\n\t\t    head_branch: Head branch (e.g., feature-branch)\n\t\t    title: PR title\n\t\t    description: PR description\n\n\t\tReturns:\n\t\t    PullRequest object with PR details\n\n\t\tRaises:\n\t\t    GitError: If PR creation fails\n\n\t\t\"\"\"\n\t\treturn create_pull_request(base_branch, head_branch, title, description)\n\n\tdef update_pr(self, pr_number: int, title: str, description: str) -&gt; PullRequest:\n\t\t\"\"\"\n\t\tUpdate an existing pull request.\n\n\t\tArgs:\n\t\t    pr_number: PR number\n\t\t    title: New PR title\n\t\t    description: New PR description\n\n\t\tReturns:\n\t\t    Updated PullRequest object\n\n\t\tRaises:\n\t\t    GitError: If PR update fails\n\n\t\t\"\"\"\n\t\treturn update_pull_request(pr_number, title, description)\n\n\tdef get_existing_pr(self, branch_name: str) -&gt; PullRequest | None:\n\t\t\"\"\"\n\t\tGet an existing PR for a branch.\n\n\t\tArgs:\n\t\t    branch_name: Branch name\n\n\t\tReturns:\n\t\t    PullRequest object if found, None otherwise\n\n\t\t\"\"\"\n\t\treturn get_existing_pr(branch_name)\n\n\tdef create_or_update_pr(\n\t\tself,\n\t\tbase_branch: str | None = None,\n\t\thead_branch: str | None = None,\n\t\ttitle: str | None = None,\n\t\tdescription: str | None = None,\n\t\tuse_llm: bool = True,\n\t\tpr_number: int | None = None,\n\t) -&gt; PullRequest:\n\t\t\"\"\"\n\t\tCreate a new PR or update an existing one.\n\n\t\tArgs:\n\t\t    base_branch: Base branch (defaults to default branch)\n\t\t    head_branch: Head branch\n\t\t    title: PR title (if None, will be generated)\n\t\t    description: PR description (if None, will be generated)\n\t\t    use_llm: Whether to use LLM for content generation\n\t\t    pr_number: PR number for update (if None, will create new PR)\n\n\t\tReturns:\n\t\t    PullRequest object\n\n\t\tRaises:\n\t\t    GitError: If PR creation/update fails\n\n\t\t\"\"\"\n\t\t# Get default branch if base_branch is not specified\n\t\tif base_branch is None:\n\t\t\tbase_branch = get_default_branch()\n\n\t\t# Set default head_branch to current branch if not specified\n\t\tif head_branch is None:\n\t\t\ttry:\n\t\t\t\tfrom codemap.git.pr_generator.utils import get_current_branch\n\n\t\t\t\thead_branch = get_current_branch()\n\t\t\texcept GitError as err:\n\t\t\t\tmsg = \"Failed to determine current branch\"\n\t\t\t\traise GitError(msg) from err\n\n\t\t# Check if PR exists\n\t\texisting_pr = None\n\t\tif pr_number is not None:\n\t\t\t# Updating an existing PR by number\n\t\t\tif title is None or description is None:\n\t\t\t\t# Need to fetch the PR to get current title/description\n\t\t\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\t\t\tif existing_pr is None:\n\t\t\t\t\tmsg = f\"No PR found for branch {head_branch} with number {pr_number}\"\n\t\t\t\t\traise GitError(msg)\n\n\t\telse:\n\t\t\t# Look for existing PR for this branch\n\t\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\t\tif existing_pr is not None:\n\t\t\t\tpr_number = existing_pr.number\n\n\t\t# Generate content if not provided\n\t\tif title is None or description is None:\n\t\t\tcontent = self.generate_content_from_commits(base_branch, head_branch, use_llm)\n\t\t\tif title is None:\n\t\t\t\ttitle = content[\"title\"]\n\t\t\tif description is None:\n\t\t\t\tdescription = content[\"description\"]\n\n\t\t# Create or update PR\n\t\tif pr_number is not None:\n\t\t\t# Update existing PR\n\t\t\treturn self.update_pr(pr_number, title, description)\n\t\t# Create new PR\n\t\treturn self.create_pr(base_branch, head_branch, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.__init__","title":"__init__","text":"<pre><code>__init__(repo_path: Path, llm_client: LLMClient) -&gt; None\n</code></pre> <p>Initialize the PR generator.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to the git repository</p> required <code>llm_client</code> <code>LLMClient</code> <p>LLMClient instance to use for content generation</p> required Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def __init__(\n\tself,\n\trepo_path: Path,\n\tllm_client: LLMClient,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the PR generator.\n\n\tArgs:\n\t    repo_path: Path to the git repository\n\t    llm_client: LLMClient instance to use for content generation\n\n\t\"\"\"\n\tself.repo_path = repo_path\n\tself.client = llm_client\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = llm_client\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.generate_content_from_commits","title":"generate_content_from_commits","text":"<pre><code>generate_content_from_commits(\n\tbase_branch: str, head_branch: str, use_llm: bool = True\n) -&gt; PRContent\n</code></pre> <p>Generate PR content (title and description) from commits.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str</code> <p>Base branch (e.g., main)</p> required <code>head_branch</code> <code>str</code> <p>Head branch (e.g., feature-branch)</p> required <code>use_llm</code> <code>bool</code> <p>Whether to use LLM for generation</p> <code>True</code> <p>Returns:</p> Type Description <code>PRContent</code> <p>Dictionary with 'title' and 'description' keys</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def generate_content_from_commits(self, base_branch: str, head_branch: str, use_llm: bool = True) -&gt; PRContent:\n\t\"\"\"\n\tGenerate PR content (title and description) from commits.\n\n\tArgs:\n\t    base_branch: Base branch (e.g., main)\n\t    head_branch: Head branch (e.g., feature-branch)\n\t    use_llm: Whether to use LLM for generation\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' keys\n\n\t\"\"\"\n\t# Get commit messages between branches\n\tcommits = get_commit_messages(base_branch, head_branch)\n\n\tif not commits:\n\t\treturn {\"title\": \"Update branch\", \"description\": \"No changes in this PR.\"}\n\n\tif use_llm:\n\t\t# Generate title and description using LLM\n\t\ttitle = generate_pr_title_with_llm(commits, self.client)\n\t\tdescription = generate_pr_description_with_llm(commits, self.client)\n\telse:\n\t\t# Generate title and description using rule-based approach\n\t\ttitle = generate_pr_title_from_commits(commits)\n\t\tdescription = generate_pr_description_from_commits(commits)\n\n\treturn {\"title\": title, \"description\": description}\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.generate_content_from_template","title":"generate_content_from_template","text":"<pre><code>generate_content_from_template(\n\tbranch_name: str,\n\tdescription: str,\n\tworkflow_strategy: str = \"github-flow\",\n) -&gt; PRContent\n</code></pre> <p>Generate PR content (title and description) from a template.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <code>description</code> <code>str</code> <p>Short description of the changes</p> required <code>workflow_strategy</code> <code>str</code> <p>Git workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>PRContent</code> <p>Dictionary with 'title' and 'description' keys</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def generate_content_from_template(\n\tself, branch_name: str, description: str, workflow_strategy: str = \"github-flow\"\n) -&gt; PRContent:\n\t\"\"\"\n\tGenerate PR content (title and description) from a template.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\t    description: Short description of the changes\n\t    workflow_strategy: Git workflow strategy to use\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' keys\n\n\t\"\"\"\n\treturn generate_pr_content_from_template(branch_name, description, workflow_strategy)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tdescription: str, workflow_strategy: str = \"github-flow\"\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on a description.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>Description of the branch</p> required <code>workflow_strategy</code> <code>str</code> <p>Git workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def suggest_branch_name(self, description: str, workflow_strategy: str = \"github-flow\") -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on a description.\n\n\tArgs:\n\t    description: Description of the branch\n\t    workflow_strategy: Git workflow strategy to use\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\treturn suggest_branch_name(description, workflow_strategy)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.create_pr","title":"create_pr","text":"<pre><code>create_pr(\n\tbase_branch: str,\n\thead_branch: str,\n\ttitle: str,\n\tdescription: str,\n) -&gt; PullRequest\n</code></pre> <p>Create a pull request on GitHub.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str</code> <p>Base branch (e.g., main)</p> required <code>head_branch</code> <code>str</code> <p>Head branch (e.g., feature-branch)</p> required <code>title</code> <code>str</code> <p>PR title</p> required <code>description</code> <code>str</code> <p>PR description</p> required <p>Returns:</p> Type Description <code>PullRequest</code> <p>PullRequest object with PR details</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If PR creation fails</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def create_pr(self, base_branch: str, head_branch: str, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"\n\tCreate a pull request on GitHub.\n\n\tArgs:\n\t    base_branch: Base branch (e.g., main)\n\t    head_branch: Head branch (e.g., feature-branch)\n\t    title: PR title\n\t    description: PR description\n\n\tReturns:\n\t    PullRequest object with PR details\n\n\tRaises:\n\t    GitError: If PR creation fails\n\n\t\"\"\"\n\treturn create_pull_request(base_branch, head_branch, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.update_pr","title":"update_pr","text":"<pre><code>update_pr(\n\tpr_number: int, title: str, description: str\n) -&gt; PullRequest\n</code></pre> <p>Update an existing pull request.</p> <p>Parameters:</p> Name Type Description Default <code>pr_number</code> <code>int</code> <p>PR number</p> required <code>title</code> <code>str</code> <p>New PR title</p> required <code>description</code> <code>str</code> <p>New PR description</p> required <p>Returns:</p> Type Description <code>PullRequest</code> <p>Updated PullRequest object</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If PR update fails</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def update_pr(self, pr_number: int, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"\n\tUpdate an existing pull request.\n\n\tArgs:\n\t    pr_number: PR number\n\t    title: New PR title\n\t    description: New PR description\n\n\tReturns:\n\t    Updated PullRequest object\n\n\tRaises:\n\t    GitError: If PR update fails\n\n\t\"\"\"\n\treturn update_pull_request(pr_number, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.get_existing_pr","title":"get_existing_pr","text":"<pre><code>get_existing_pr(branch_name: str) -&gt; PullRequest | None\n</code></pre> <p>Get an existing PR for a branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Branch name</p> required <p>Returns:</p> Type Description <code>PullRequest | None</code> <p>PullRequest object if found, None otherwise</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def get_existing_pr(self, branch_name: str) -&gt; PullRequest | None:\n\t\"\"\"\n\tGet an existing PR for a branch.\n\n\tArgs:\n\t    branch_name: Branch name\n\n\tReturns:\n\t    PullRequest object if found, None otherwise\n\n\t\"\"\"\n\treturn get_existing_pr(branch_name)\n</code></pre>"},{"location":"api/git/pr_generator/generator/#codemap.git.pr_generator.generator.PRGenerator.create_or_update_pr","title":"create_or_update_pr","text":"<pre><code>create_or_update_pr(\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tuse_llm: bool = True,\n\tpr_number: int | None = None,\n) -&gt; PullRequest\n</code></pre> <p>Create a new PR or update an existing one.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str | None</code> <p>Base branch (defaults to default branch)</p> <code>None</code> <code>head_branch</code> <code>str | None</code> <p>Head branch</p> <code>None</code> <code>title</code> <code>str | None</code> <p>PR title (if None, will be generated)</p> <code>None</code> <code>description</code> <code>str | None</code> <p>PR description (if None, will be generated)</p> <code>None</code> <code>use_llm</code> <code>bool</code> <p>Whether to use LLM for content generation</p> <code>True</code> <code>pr_number</code> <code>int | None</code> <p>PR number for update (if None, will create new PR)</p> <code>None</code> <p>Returns:</p> Type Description <code>PullRequest</code> <p>PullRequest object</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If PR creation/update fails</p> Source code in <code>src/codemap/git/pr_generator/generator.py</code> <pre><code>def create_or_update_pr(\n\tself,\n\tbase_branch: str | None = None,\n\thead_branch: str | None = None,\n\ttitle: str | None = None,\n\tdescription: str | None = None,\n\tuse_llm: bool = True,\n\tpr_number: int | None = None,\n) -&gt; PullRequest:\n\t\"\"\"\n\tCreate a new PR or update an existing one.\n\n\tArgs:\n\t    base_branch: Base branch (defaults to default branch)\n\t    head_branch: Head branch\n\t    title: PR title (if None, will be generated)\n\t    description: PR description (if None, will be generated)\n\t    use_llm: Whether to use LLM for content generation\n\t    pr_number: PR number for update (if None, will create new PR)\n\n\tReturns:\n\t    PullRequest object\n\n\tRaises:\n\t    GitError: If PR creation/update fails\n\n\t\"\"\"\n\t# Get default branch if base_branch is not specified\n\tif base_branch is None:\n\t\tbase_branch = get_default_branch()\n\n\t# Set default head_branch to current branch if not specified\n\tif head_branch is None:\n\t\ttry:\n\t\t\tfrom codemap.git.pr_generator.utils import get_current_branch\n\n\t\t\thead_branch = get_current_branch()\n\t\texcept GitError as err:\n\t\t\tmsg = \"Failed to determine current branch\"\n\t\t\traise GitError(msg) from err\n\n\t# Check if PR exists\n\texisting_pr = None\n\tif pr_number is not None:\n\t\t# Updating an existing PR by number\n\t\tif title is None or description is None:\n\t\t\t# Need to fetch the PR to get current title/description\n\t\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\t\tif existing_pr is None:\n\t\t\t\tmsg = f\"No PR found for branch {head_branch} with number {pr_number}\"\n\t\t\t\traise GitError(msg)\n\n\telse:\n\t\t# Look for existing PR for this branch\n\t\texisting_pr = self.get_existing_pr(head_branch)\n\t\tif existing_pr is not None:\n\t\t\tpr_number = existing_pr.number\n\n\t# Generate content if not provided\n\tif title is None or description is None:\n\t\tcontent = self.generate_content_from_commits(base_branch, head_branch, use_llm)\n\t\tif title is None:\n\t\t\ttitle = content[\"title\"]\n\t\tif description is None:\n\t\t\tdescription = content[\"description\"]\n\n\t# Create or update PR\n\tif pr_number is not None:\n\t\t# Update existing PR\n\t\treturn self.update_pr(pr_number, title, description)\n\t# Create new PR\n\treturn self.create_pr(base_branch, head_branch, title, description)\n</code></pre>"},{"location":"api/git/pr_generator/prompts/","title":"Prompts","text":"<p>Prompt templates for PR generation.</p>"},{"location":"api/git/pr_generator/prompts/#codemap.git.pr_generator.prompts.PR_TITLE_PROMPT","title":"PR_TITLE_PROMPT  <code>module-attribute</code>","text":"<pre><code>PR_TITLE_PROMPT = 'Based on the following commits, generate a clear, concise PR title that captures the\\nessence of the changes.\\nFollow these guidelines:\\n- Focus on the most important change\\n- If there are multiple related changes, summarize them\\n- Keep it under 80 characters\\n- Start with a capital letter\\n- Don\\'t use a period at the end\\n- Use present tense (e.g., \"Add feature\" not \"Added feature\")\\n- Be descriptive and specific (e.g., \"Fix memory leak in data processing\" not just \"Fix bug\")\\n- Include the type of change if clear (Feature, Fix, Refactor, etc.)\\n\\nCommits:\\n{commit_list}\\n\\nPR Title:\\n---\\n\\nIMPORTANT:\\n- Do not include any other text in your response except the PR title.\\n- Do not wrap the PR title in quotes.\\n- Do not add any explanations or other text to your response.\\n'\n</code></pre>"},{"location":"api/git/pr_generator/prompts/#codemap.git.pr_generator.prompts.PR_DESCRIPTION_PROMPT","title":"PR_DESCRIPTION_PROMPT  <code>module-attribute</code>","text":"<pre><code>PR_DESCRIPTION_PROMPT = \"\\nBased on the following commits, generate a comprehensive PR description following this template:\\n\\n## What type of PR is this? (check all applicable)\\n\\n- [ ] Refactor\\n- [ ] Feature\\n- [ ] Bug Fix\\n- [ ] Optimization\\n- [ ] Documentation Update\\n\\n## Description\\n[Fill this section with a detailed description of the changes]\\n\\n## Related Tickets &amp; Documents\\n- Related Issue #\\n- Closes #\\n\\n## Added/updated tests?\\n- [ ] Yes\\n- [ ] No, and this is why: [explanation]\\n- [ ] I need help with writing tests\\n\\nConsider the following guidelines:\\n- Check the appropriate PR type boxes based on the commit messages\\n- Provide a clear, detailed description of the changes\\n- Include any relevant issue numbers that this PR relates to or closes\\n- Indicate if tests were added, and if not, explain why\\n- Use bullet points for clarity\\n\\nCommits:\\n{commit_list}\\n\\nPR Description:\\n---\\n\\nIMPORTANT:\\n- Do not include any other text in your response except the PR description.\\n- Do not wrap the PR description in quotes.\\n- Do not add any explanations or other text to your response.\\n\"\n</code></pre>"},{"location":"api/git/pr_generator/prompts/#codemap.git.pr_generator.prompts.format_commits_for_prompt","title":"format_commits_for_prompt","text":"<pre><code>format_commits_for_prompt(commits: list[str]) -&gt; str\n</code></pre> <p>Format commit messages as a bulleted list.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted commit list as a string</p> Source code in <code>src/codemap/git/pr_generator/prompts.py</code> <pre><code>def format_commits_for_prompt(commits: list[str]) -&gt; str:\n\t\"\"\"\n\tFormat commit messages as a bulleted list.\n\n\tArgs:\n\t    commits: List of commit messages\n\n\tReturns:\n\t    Formatted commit list as a string\n\n\t\"\"\"\n\treturn \"\\n\".join([f\"- {commit}\" for commit in commits])\n</code></pre>"},{"location":"api/git/pr_generator/schemas/","title":"Schemas","text":"<p>Schemas and data structures for PR generation.</p>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.WorkflowStrategySchema","title":"WorkflowStrategySchema  <code>module-attribute</code>","text":"<pre><code>WorkflowStrategySchema = Literal[\n\t\"github-flow\", \"gitflow\", \"trunk-based\"\n]\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.BranchType","title":"BranchType  <code>module-attribute</code>","text":"<pre><code>BranchType = Literal[\n\t\"feature\", \"release\", \"hotfix\", \"bugfix\", \"docs\"\n]\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PRContent","title":"PRContent","text":"<p>               Bases: <code>TypedDict</code></p> <p>Pull request content type.</p> Source code in <code>src/codemap/git/pr_generator/schemas.py</code> <pre><code>class PRContent(TypedDict):\n\t\"\"\"Pull request content type.\"\"\"\n\n\ttitle: str\n\tdescription: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PRContent.title","title":"title  <code>instance-attribute</code>","text":"<pre><code>title: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PRContent.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest","title":"PullRequest  <code>dataclass</code>","text":"<p>Represents a GitHub Pull Request.</p> Source code in <code>src/codemap/git/pr_generator/schemas.py</code> <pre><code>@dataclass\nclass PullRequest:\n\t\"\"\"Represents a GitHub Pull Request.\"\"\"\n\n\tbranch: str\n\ttitle: str\n\tdescription: str\n\turl: str | None = None\n\tnumber: int | None = None\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.__init__","title":"__init__","text":"<pre><code>__init__(\n\tbranch: str,\n\ttitle: str,\n\tdescription: str,\n\turl: str | None = None,\n\tnumber: int | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.branch","title":"branch  <code>instance-attribute</code>","text":"<pre><code>branch: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.title","title":"title  <code>instance-attribute</code>","text":"<pre><code>title: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.url","title":"url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>url: str | None = None\n</code></pre>"},{"location":"api/git/pr_generator/schemas/#codemap.git.pr_generator.schemas.PullRequest.number","title":"number  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>number: int | None = None\n</code></pre>"},{"location":"api/git/pr_generator/strategies/","title":"Strategies","text":"<p>Git workflow strategy implementations for PR management.</p>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy","title":"WorkflowStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for git workflow strategies.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class WorkflowStrategy(ABC):\n\t\"\"\"Base class for git workflow strategies.\"\"\"\n\n\t@abstractmethod\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:\n\t\t\"\"\"\n\t\tGet the default base branch for a given branch type.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Name of the default base branch\n\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on the workflow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t\t    description: Description of the branch\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\t# Default implementation\n\t\tclean_description = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", description.lower())\n\t\tclean_description = clean_description.strip(\"-\")\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\treturn f\"{prefix}{clean_description}\"\n\n\t@abstractmethod\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the branch name prefix for a given branch type.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Branch name prefix\n\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\t@abstractmethod\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for this workflow.\n\n\t\tReturns:\n\t\t    List of valid branch types\n\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef detect_branch_type(self, branch_name: str | None) -&gt; str | None:\n\t\t\"\"\"\n\t\tDetect the type of a branch from its name.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch\n\n\t\tReturns:\n\t\t    Branch type or None if not detected\n\n\t\t\"\"\"\n\t\tfor branch_type in self.get_branch_types():\n\t\t\tprefix = self.get_branch_prefix(branch_type)\n\t\t\tif branch_name and branch_name.startswith(prefix):\n\t\t\t\treturn branch_type\n\t\treturn None\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet PR title and description templates for a given branch type.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\t# Return the default templates\n\t\treturn DEFAULT_PR_TEMPLATE\n\n\tdef get_remote_branches(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet list of remote branches.\n\n\t\tReturns:\n\t\t    List of remote branch names (without 'origin/' prefix)\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tbranches = run_git_command([\"git\", \"branch\", \"-r\"]).strip().split(\"\\n\")\n\t\t\t# Clean up branch names and remove 'origin/' prefix\n\t\t\tremote_branches = []\n\t\t\tfor branch_name in branches:\n\t\t\t\tbranch_clean = branch_name.strip()\n\t\t\t\tif branch_clean.startswith(\"origin/\"):\n\t\t\t\t\tbranch_name_without_prefix = branch_clean[7:]  # Remove 'origin/' prefix\n\t\t\t\t\t# Exclude HEAD branches\n\t\t\t\t\tif not branch_name_without_prefix.startswith(\"HEAD\"):\n\t\t\t\t\t\tremote_branches.append(branch_name_without_prefix)\n\t\t\treturn remote_branches\n\t\texcept GitError:\n\t\t\treturn []\n\n\tdef get_local_branches(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet list of local branches.\n\n\t\tReturns:\n\t\t    List of local branch names\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tbranches = run_git_command([\"git\", \"branch\"]).strip().split(\"\\n\")\n\t\t\t# Clean up branch names and remove the '*' from current branch\n\t\t\tlocal_branches = []\n\t\t\tfor branch_name in branches:\n\t\t\t\tbranch_clean = branch_name.strip().removeprefix(\"* \")  # Remove '* ' prefix\n\t\t\t\tlocal_branches.append(branch_clean)\n\t\t\treturn local_branches\n\t\texcept GitError:\n\t\t\treturn []\n\n\tdef get_branches_by_type(self) -&gt; dict[str, list[str]]:\n\t\t\"\"\"\n\t\tGroup branches by their type.\n\n\t\tReturns:\n\t\t    Dictionary mapping branch types to lists of branch names\n\n\t\t\"\"\"\n\t\tresult = {branch_type: [] for branch_type in self.get_branch_types()}\n\t\tresult[\"other\"] = []  # For branches that don't match any type\n\n\t\t# Get all branches (local and remote)\n\t\tall_branches = set(self.get_local_branches() + self.get_remote_branches())\n\n\t\tfor branch in all_branches:\n\t\t\tbranch_type = self.detect_branch_type(branch)\n\t\t\tif branch_type:\n\t\t\t\tresult[branch_type].append(branch)\n\t\t\telse:\n\t\t\t\tresult[\"other\"].append(branch)\n\n\t\treturn result\n\n\tdef get_branch_metadata(self, branch_name: str) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tGet metadata for a specific branch.\n\n\t\tArgs:\n\t\t    branch_name: Name of the branch\n\n\t\tReturns:\n\t\t    Dictionary with branch metadata\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Get last commit date\n\t\t\tdate_cmd = [\n\t\t\t\t\"git\",\n\t\t\t\t\"log\",\n\t\t\t\t\"-1\",\n\t\t\t\t\"--format=%ad\",\n\t\t\t\t\"--date=relative\",\n\t\t\t\tbranch_name if branch_exists(branch_name) else f\"origin/{branch_name}\",\n\t\t\t]\n\t\t\tdate = run_git_command(date_cmd).strip()\n\n\t\t\t# Get commit count (compared to default branch)\n\t\t\tdefault = get_default_branch()\n\t\t\tcount_cmd = [\"git\", \"rev-list\", \"--count\", f\"{default}..{branch_name}\"]\n\t\t\ttry:\n\t\t\t\tcount = run_git_command(count_cmd).strip()\n\t\t\texcept GitError:\n\t\t\t\tcount = \"0\"\n\n\t\t\t# Detect branch type\n\t\t\tbranch_type = self.detect_branch_type(branch_name)\n\n\t\t\treturn {\n\t\t\t\t\"last_commit_date\": date,\n\t\t\t\t\"commit_count\": count,\n\t\t\t\t\"branch_type\": branch_type,\n\t\t\t\t\"is_local\": branch_name in self.get_local_branches(),\n\t\t\t\t\"is_remote\": branch_name in self.get_remote_branches(),\n\t\t\t}\n\t\texcept GitError:\n\t\t\t# Return default metadata if there's an error\n\t\t\treturn {\n\t\t\t\t\"last_commit_date\": \"unknown\",\n\t\t\t\t\"commit_count\": \"0\",\n\t\t\t\t\"branch_type\": self.detect_branch_type(branch_name),\n\t\t\t\t\"is_local\": False,\n\t\t\t\t\"is_remote\": False,\n\t\t\t}\n\n\tdef get_all_branches_with_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\t\t\"\"\"\n\t\tGet all branches with metadata.\n\n\t\tReturns:\n\t\t    Dictionary mapping branch names to metadata dictionaries\n\n\t\t\"\"\"\n\t\tresult = {}\n\t\tall_branches = set(self.get_local_branches() + self.get_remote_branches())\n\n\t\tfor branch in all_branches:\n\t\t\tresult[branch] = self.get_branch_metadata(branch)\n\n\t\treturn result\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_default_base","title":"get_default_base  <code>abstractmethod</code>","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for a given branch type.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>@abstractmethod\ndef get_default_base(self, branch_type: str) -&gt; str | None:\n\t\"\"\"\n\tGet the default base branch for a given branch type.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Name of the default base branch\n\n\t\"\"\"\n\traise NotImplementedError\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tbranch_type: str, description: str\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <code>description</code> <code>str</code> <p>Description of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on the workflow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t    description: Description of the branch\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\t# Default implementation\n\tclean_description = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", description.lower())\n\tclean_description = clean_description.strip(\"-\")\n\tprefix = self.get_branch_prefix(branch_type)\n\treturn f\"{prefix}{clean_description}\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branch_prefix","title":"get_branch_prefix  <code>abstractmethod</code>","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for a given branch type.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>@abstractmethod\ndef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\"\"\"\n\tGet the branch name prefix for a given branch type.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Branch name prefix\n\n\t\"\"\"\n\traise NotImplementedError\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branch_types","title":"get_branch_types  <code>abstractmethod</code>","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for this workflow.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of valid branch types</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>@abstractmethod\ndef get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for this workflow.\n\n\tReturns:\n\t    List of valid branch types\n\n\t\"\"\"\n\traise NotImplementedError\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.detect_branch_type","title":"detect_branch_type","text":"<pre><code>detect_branch_type(branch_name: str | None) -&gt; str | None\n</code></pre> <p>Detect the type of a branch from its name.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str | None</code> <p>Name of the branch</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Branch type or None if not detected</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def detect_branch_type(self, branch_name: str | None) -&gt; str | None:\n\t\"\"\"\n\tDetect the type of a branch from its name.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\n\tReturns:\n\t    Branch type or None if not detected\n\n\t\"\"\"\n\tfor branch_type in self.get_branch_types():\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\tif branch_name and branch_name.startswith(prefix):\n\t\t\treturn branch_type\n\treturn None\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for a given branch type.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\"\"\"\n\tGet PR title and description templates for a given branch type.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\t# Return the default templates\n\treturn DEFAULT_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_remote_branches","title":"get_remote_branches","text":"<pre><code>get_remote_branches() -&gt; list[str]\n</code></pre> <p>Get list of remote branches.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of remote branch names (without 'origin/' prefix)</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_remote_branches(self) -&gt; list[str]:\n\t\"\"\"\n\tGet list of remote branches.\n\n\tReturns:\n\t    List of remote branch names (without 'origin/' prefix)\n\n\t\"\"\"\n\ttry:\n\t\tbranches = run_git_command([\"git\", \"branch\", \"-r\"]).strip().split(\"\\n\")\n\t\t# Clean up branch names and remove 'origin/' prefix\n\t\tremote_branches = []\n\t\tfor branch_name in branches:\n\t\t\tbranch_clean = branch_name.strip()\n\t\t\tif branch_clean.startswith(\"origin/\"):\n\t\t\t\tbranch_name_without_prefix = branch_clean[7:]  # Remove 'origin/' prefix\n\t\t\t\t# Exclude HEAD branches\n\t\t\t\tif not branch_name_without_prefix.startswith(\"HEAD\"):\n\t\t\t\t\tremote_branches.append(branch_name_without_prefix)\n\t\treturn remote_branches\n\texcept GitError:\n\t\treturn []\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_local_branches","title":"get_local_branches","text":"<pre><code>get_local_branches() -&gt; list[str]\n</code></pre> <p>Get list of local branches.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of local branch names</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_local_branches(self) -&gt; list[str]:\n\t\"\"\"\n\tGet list of local branches.\n\n\tReturns:\n\t    List of local branch names\n\n\t\"\"\"\n\ttry:\n\t\tbranches = run_git_command([\"git\", \"branch\"]).strip().split(\"\\n\")\n\t\t# Clean up branch names and remove the '*' from current branch\n\t\tlocal_branches = []\n\t\tfor branch_name in branches:\n\t\t\tbranch_clean = branch_name.strip().removeprefix(\"* \")  # Remove '* ' prefix\n\t\t\tlocal_branches.append(branch_clean)\n\t\treturn local_branches\n\texcept GitError:\n\t\treturn []\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branches_by_type","title":"get_branches_by_type","text":"<pre><code>get_branches_by_type() -&gt; dict[str, list[str]]\n</code></pre> <p>Group branches by their type.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary mapping branch types to lists of branch names</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branches_by_type(self) -&gt; dict[str, list[str]]:\n\t\"\"\"\n\tGroup branches by their type.\n\n\tReturns:\n\t    Dictionary mapping branch types to lists of branch names\n\n\t\"\"\"\n\tresult = {branch_type: [] for branch_type in self.get_branch_types()}\n\tresult[\"other\"] = []  # For branches that don't match any type\n\n\t# Get all branches (local and remote)\n\tall_branches = set(self.get_local_branches() + self.get_remote_branches())\n\n\tfor branch in all_branches:\n\t\tbranch_type = self.detect_branch_type(branch)\n\t\tif branch_type:\n\t\t\tresult[branch_type].append(branch)\n\t\telse:\n\t\t\tresult[\"other\"].append(branch)\n\n\treturn result\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_branch_metadata","title":"get_branch_metadata","text":"<pre><code>get_branch_metadata(branch_name: str) -&gt; dict[str, Any]\n</code></pre> <p>Get metadata for a specific branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with branch metadata</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_metadata(self, branch_name: str) -&gt; dict[str, Any]:\n\t\"\"\"\n\tGet metadata for a specific branch.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\n\tReturns:\n\t    Dictionary with branch metadata\n\n\t\"\"\"\n\ttry:\n\t\t# Get last commit date\n\t\tdate_cmd = [\n\t\t\t\"git\",\n\t\t\t\"log\",\n\t\t\t\"-1\",\n\t\t\t\"--format=%ad\",\n\t\t\t\"--date=relative\",\n\t\t\tbranch_name if branch_exists(branch_name) else f\"origin/{branch_name}\",\n\t\t]\n\t\tdate = run_git_command(date_cmd).strip()\n\n\t\t# Get commit count (compared to default branch)\n\t\tdefault = get_default_branch()\n\t\tcount_cmd = [\"git\", \"rev-list\", \"--count\", f\"{default}..{branch_name}\"]\n\t\ttry:\n\t\t\tcount = run_git_command(count_cmd).strip()\n\t\texcept GitError:\n\t\t\tcount = \"0\"\n\n\t\t# Detect branch type\n\t\tbranch_type = self.detect_branch_type(branch_name)\n\n\t\treturn {\n\t\t\t\"last_commit_date\": date,\n\t\t\t\"commit_count\": count,\n\t\t\t\"branch_type\": branch_type,\n\t\t\t\"is_local\": branch_name in self.get_local_branches(),\n\t\t\t\"is_remote\": branch_name in self.get_remote_branches(),\n\t\t}\n\texcept GitError:\n\t\t# Return default metadata if there's an error\n\t\treturn {\n\t\t\t\"last_commit_date\": \"unknown\",\n\t\t\t\"commit_count\": \"0\",\n\t\t\t\"branch_type\": self.detect_branch_type(branch_name),\n\t\t\t\"is_local\": False,\n\t\t\t\"is_remote\": False,\n\t\t}\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.WorkflowStrategy.get_all_branches_with_metadata","title":"get_all_branches_with_metadata","text":"<pre><code>get_all_branches_with_metadata() -&gt; dict[\n\tstr, dict[str, Any]\n]\n</code></pre> <p>Get all branches with metadata.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>Dictionary mapping branch names to metadata dictionaries</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_all_branches_with_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\t\"\"\"\n\tGet all branches with metadata.\n\n\tReturns:\n\t    Dictionary mapping branch names to metadata dictionaries\n\n\t\"\"\"\n\tresult = {}\n\tall_branches = set(self.get_local_branches() + self.get_remote_branches())\n\n\tfor branch in all_branches:\n\t\tresult[branch] = self.get_branch_metadata(branch)\n\n\treturn result\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy","title":"GitHubFlowStrategy","text":"<p>               Bases: <code>WorkflowStrategy</code></p> <p>Implementation of GitHub Flow workflow strategy.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class GitHubFlowStrategy(WorkflowStrategy):\n\t\"\"\"Implementation of GitHub Flow workflow strategy.\"\"\"\n\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet the default base branch for GitHub Flow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\t\tReturns:\n\t\t    Name of the default base branch (usually 'main')\n\n\t\t\"\"\"\n\t\t# Ignoring branch_type as GitHub Flow always uses the default branch\n\t\treturn get_default_branch()\n\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet the branch name prefix for GitHub Flow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\t\tReturns:\n\t\t    Branch name prefix (empty string for GitHub Flow)\n\n\t\t\"\"\"\n\t\t# Ignoring branch_type as GitHub Flow doesn't use prefixes\n\t\treturn \"\"\n\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for GitHub Flow.\n\n\t\tReturns:\n\t\t    List containing only 'feature'\n\n\t\t\"\"\"\n\t\treturn [\"feature\"]\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet PR title and description templates for GitHub Flow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\treturn GITHUB_FLOW_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_default_base","title":"get_default_base","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for GitHub Flow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (always 'feature' in GitHub Flow)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch (usually 'main')</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\"\"\"\n\tGet the default base branch for GitHub Flow.\n\n\tArgs:\n\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\tReturns:\n\t    Name of the default base branch (usually 'main')\n\n\t\"\"\"\n\t# Ignoring branch_type as GitHub Flow always uses the default branch\n\treturn get_default_branch()\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_branch_prefix","title":"get_branch_prefix","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for GitHub Flow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (always 'feature' in GitHub Flow)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix (empty string for GitHub Flow)</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_prefix(self, branch_type: str) -&gt; str:  # noqa: ARG002\n\t\"\"\"\n\tGet the branch name prefix for GitHub Flow.\n\n\tArgs:\n\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\tReturns:\n\t    Branch name prefix (empty string for GitHub Flow)\n\n\t\"\"\"\n\t# Ignoring branch_type as GitHub Flow doesn't use prefixes\n\treturn \"\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_branch_types","title":"get_branch_types","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for GitHub Flow.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List containing only 'feature'</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for GitHub Flow.\n\n\tReturns:\n\t    List containing only 'feature'\n\n\t\"\"\"\n\treturn [\"feature\"]\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitHubFlowStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for GitHub Flow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (always 'feature' in GitHub Flow)</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\"\"\"\n\tGet PR title and description templates for GitHub Flow.\n\n\tArgs:\n\t    branch_type: Type of branch (always 'feature' in GitHub Flow)\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\treturn GITHUB_FLOW_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy","title":"GitFlowStrategy","text":"<p>               Bases: <code>WorkflowStrategy</code></p> <p>Implementation of GitFlow workflow strategy.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class GitFlowStrategy(WorkflowStrategy):\n\t\"\"\"Implementation of GitFlow workflow strategy.\"\"\"\n\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:\n\t\t\"\"\"\n\t\tGet the default base branch for GitFlow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\t\tReturns:\n\t\t    Name of the default base branch\n\n\t\t\"\"\"\n\t\tmapping = {\n\t\t\t\"feature\": \"develop\",\n\t\t\t\"release\": \"main\",\n\t\t\t\"hotfix\": \"main\",\n\t\t\t\"bugfix\": \"develop\",\n\t\t}\n\t\tdefault = get_default_branch()\n\t\treturn mapping.get(branch_type, default)\n\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the branch name prefix for GitFlow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\t\tReturns:\n\t\t    Branch name prefix\n\n\t\t\"\"\"\n\t\tmapping = {\n\t\t\t\"feature\": \"feature/\",\n\t\t\t\"release\": \"release/\",\n\t\t\t\"hotfix\": \"hotfix/\",\n\t\t\t\"bugfix\": \"bugfix/\",\n\t\t}\n\t\treturn mapping.get(branch_type, \"\")\n\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for GitFlow.\n\n\t\tReturns:\n\t\t    List of valid branch types for GitFlow\n\n\t\t\"\"\"\n\t\treturn [\"feature\", \"release\", \"hotfix\", \"bugfix\"]\n\n\tdef suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on GitFlow conventions.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t\t    description: Description of the branch\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\tprefix = self.get_branch_prefix(branch_type)\n\n\t\tif branch_type == \"release\":\n\t\t\t# Extract version number from description if it looks like a version\n\t\t\tversion_match = re.search(r\"(\\d+\\.\\d+\\.\\d+)\", description)\n\t\t\tif version_match:\n\t\t\t\treturn f\"{prefix}{version_match.group(1)}\"\n\n\t\t# For other branch types, use the default implementation\n\t\treturn super().suggest_branch_name(branch_type, description)\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:\n\t\t\"\"\"\n\t\tGet PR title and description templates for GitFlow.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\treturn GITFLOW_PR_TEMPLATES.get(branch_type, DEFAULT_PR_TEMPLATE)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_default_base","title":"get_default_base","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for GitFlow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, bugfix)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_base(self, branch_type: str) -&gt; str | None:\n\t\"\"\"\n\tGet the default base branch for GitFlow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\tReturns:\n\t    Name of the default base branch\n\n\t\"\"\"\n\tmapping = {\n\t\t\"feature\": \"develop\",\n\t\t\"release\": \"main\",\n\t\t\"hotfix\": \"main\",\n\t\t\"bugfix\": \"develop\",\n\t}\n\tdefault = get_default_branch()\n\treturn mapping.get(branch_type, default)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_branch_prefix","title":"get_branch_prefix","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for GitFlow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\"\"\"\n\tGet the branch name prefix for GitFlow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\n\tReturns:\n\t    Branch name prefix\n\n\t\"\"\"\n\tmapping = {\n\t\t\"feature\": \"feature/\",\n\t\t\"release\": \"release/\",\n\t\t\"hotfix\": \"hotfix/\",\n\t\t\"bugfix\": \"bugfix/\",\n\t}\n\treturn mapping.get(branch_type, \"\")\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_branch_types","title":"get_branch_types","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for GitFlow.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of valid branch types for GitFlow</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for GitFlow.\n\n\tReturns:\n\t    List of valid branch types for GitFlow\n\n\t\"\"\"\n\treturn [\"feature\", \"release\", \"hotfix\", \"bugfix\"]\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tbranch_type: str, description: str\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on GitFlow conventions.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, etc.)</p> required <code>description</code> <code>str</code> <p>Description of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on GitFlow conventions.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, etc.)\n\t    description: Description of the branch\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\tprefix = self.get_branch_prefix(branch_type)\n\n\tif branch_type == \"release\":\n\t\t# Extract version number from description if it looks like a version\n\t\tversion_match = re.search(r\"(\\d+\\.\\d+\\.\\d+)\", description)\n\t\tif version_match:\n\t\t\treturn f\"{prefix}{version_match.group(1)}\"\n\n\t# For other branch types, use the default implementation\n\treturn super().suggest_branch_name(branch_type, description)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.GitFlowStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for GitFlow.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch (feature, release, hotfix, bugfix)</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:\n\t\"\"\"\n\tGet PR title and description templates for GitFlow.\n\n\tArgs:\n\t    branch_type: Type of branch (feature, release, hotfix, bugfix)\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\treturn GITFLOW_PR_TEMPLATES.get(branch_type, DEFAULT_PR_TEMPLATE)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy","title":"TrunkBasedStrategy","text":"<p>               Bases: <code>WorkflowStrategy</code></p> <p>Implementation of Trunk-Based Development workflow strategy.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>class TrunkBasedStrategy(WorkflowStrategy):\n\t\"\"\"Implementation of Trunk-Based Development workflow strategy.\"\"\"\n\n\tdef get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet the default base branch for Trunk-Based Development.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\n\t\tReturns:\n\t\t    Name of the default base branch (trunk, which is usually 'main')\n\n\t\t\"\"\"\n\t\t# Ignoring branch_type as Trunk-Based Development always uses the main branch\n\t\treturn get_default_branch()\n\n\tdef get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the branch name prefix for Trunk-Based Development.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\n\t\tReturns:\n\t\t    Branch name prefix\n\n\t\t\"\"\"\n\t\treturn \"fb/\" if branch_type == \"feature\" else \"\"\n\n\tdef get_branch_types(self) -&gt; list[str]:\n\t\t\"\"\"\n\t\tGet valid branch types for Trunk-Based Development.\n\n\t\tReturns:\n\t\t    List containing only 'feature'\n\n\t\t\"\"\"\n\t\treturn [\"feature\"]\n\n\tdef suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\t\"\"\"\n\t\tSuggest a branch name based on Trunk-Based Development conventions.\n\n\t\tEmphasizes short-lived, descriptive branches.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\t\t    description: Description of the branch\n\n\t\tReturns:\n\t\t    Suggested branch name\n\n\t\t\"\"\"\n\t\t# For trunk-based development, try to generate very short names\n\t\twords = description.split()\n\t\t# Filter out common words like \"implement\", \"the\", \"and\", etc.\n\t\tcommon_words = [\"the\", \"and\", \"for\", \"with\", \"implement\", \"implementing\", \"implementation\"]\n\t\twords = [w for w in words if len(w) &gt; MIN_SIGNIFICANT_WORD_LENGTH and w.lower() not in common_words]\n\n\t\t# Take up to 3 significant words\n\t\tshort_desc = \"-\".join(words[:3]).lower()\n\t\tshort_desc = re.sub(r\"[^a-zA-Z0-9-]\", \"-\", short_desc)\n\t\tshort_desc = re.sub(r\"-+\", \"-\", short_desc)\n\t\tshort_desc = short_desc.strip(\"-\")\n\n\t\t# Add username prefix for trunk-based (optional)\n\t\ttry:\n\t\t\tusername = run_git_command([\"git\", \"config\", \"user.name\"]).strip().split()[0].lower()\n\t\t\tusername = re.sub(r\"[^a-zA-Z0-9]\", \"\", username)\n\t\t\treturn f\"{username}/{short_desc}\"\n\t\texcept (GitError, IndexError):\n\t\t\t# Fall back to standard prefix if username not available\n\t\t\tprefix = self.get_branch_prefix(branch_type)\n\t\t\treturn f\"{prefix}{short_desc}\"\n\n\tdef get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\t\"\"\"\n\t\tGet PR title and description templates for Trunk-Based Development.\n\n\t\tArgs:\n\t\t    branch_type: Type of branch\n\n\t\tReturns:\n\t\t    Dictionary with 'title' and 'description' templates\n\n\t\t\"\"\"\n\t\treturn TRUNK_BASED_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_default_base","title":"get_default_base","text":"<pre><code>get_default_base(branch_type: str) -&gt; str | None\n</code></pre> <p>Get the default base branch for Trunk-Based Development.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Name of the default base branch (trunk, which is usually 'main')</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_base(self, branch_type: str) -&gt; str | None:  # noqa: ARG002\n\t\"\"\"\n\tGet the default base branch for Trunk-Based Development.\n\n\tArgs:\n\t    branch_type: Type of branch\n\n\tReturns:\n\t    Name of the default base branch (trunk, which is usually 'main')\n\n\t\"\"\"\n\t# Ignoring branch_type as Trunk-Based Development always uses the main branch\n\treturn get_default_branch()\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_branch_prefix","title":"get_branch_prefix","text":"<pre><code>get_branch_prefix(branch_type: str) -&gt; str\n</code></pre> <p>Get the branch name prefix for Trunk-Based Development.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Branch name prefix</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_prefix(self, branch_type: str) -&gt; str:\n\t\"\"\"\n\tGet the branch name prefix for Trunk-Based Development.\n\n\tArgs:\n\t    branch_type: Type of branch\n\n\tReturns:\n\t    Branch name prefix\n\n\t\"\"\"\n\treturn \"fb/\" if branch_type == \"feature\" else \"\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_branch_types","title":"get_branch_types","text":"<pre><code>get_branch_types() -&gt; list[str]\n</code></pre> <p>Get valid branch types for Trunk-Based Development.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List containing only 'feature'</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_branch_types(self) -&gt; list[str]:\n\t\"\"\"\n\tGet valid branch types for Trunk-Based Development.\n\n\tReturns:\n\t    List containing only 'feature'\n\n\t\"\"\"\n\treturn [\"feature\"]\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(\n\tbranch_type: str, description: str\n) -&gt; str\n</code></pre> <p>Suggest a branch name based on Trunk-Based Development conventions.</p> <p>Emphasizes short-lived, descriptive branches.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <code>description</code> <code>str</code> <p>Description of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def suggest_branch_name(self, branch_type: str, description: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on Trunk-Based Development conventions.\n\n\tEmphasizes short-lived, descriptive branches.\n\n\tArgs:\n\t    branch_type: Type of branch\n\t    description: Description of the branch\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\t# For trunk-based development, try to generate very short names\n\twords = description.split()\n\t# Filter out common words like \"implement\", \"the\", \"and\", etc.\n\tcommon_words = [\"the\", \"and\", \"for\", \"with\", \"implement\", \"implementing\", \"implementation\"]\n\twords = [w for w in words if len(w) &gt; MIN_SIGNIFICANT_WORD_LENGTH and w.lower() not in common_words]\n\n\t# Take up to 3 significant words\n\tshort_desc = \"-\".join(words[:3]).lower()\n\tshort_desc = re.sub(r\"[^a-zA-Z0-9-]\", \"-\", short_desc)\n\tshort_desc = re.sub(r\"-+\", \"-\", short_desc)\n\tshort_desc = short_desc.strip(\"-\")\n\n\t# Add username prefix for trunk-based (optional)\n\ttry:\n\t\tusername = run_git_command([\"git\", \"config\", \"user.name\"]).strip().split()[0].lower()\n\t\tusername = re.sub(r\"[^a-zA-Z0-9]\", \"\", username)\n\t\treturn f\"{username}/{short_desc}\"\n\texcept (GitError, IndexError):\n\t\t# Fall back to standard prefix if username not available\n\t\tprefix = self.get_branch_prefix(branch_type)\n\t\treturn f\"{prefix}{short_desc}\"\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.TrunkBasedStrategy.get_pr_templates","title":"get_pr_templates","text":"<pre><code>get_pr_templates(branch_type: str) -&gt; dict[str, str]\n</code></pre> <p>Get PR title and description templates for Trunk-Based Development.</p> <p>Parameters:</p> Name Type Description Default <code>branch_type</code> <code>str</code> <p>Type of branch</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary with 'title' and 'description' templates</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_pr_templates(self, branch_type: str) -&gt; dict[str, str]:  # noqa: ARG002\n\t\"\"\"\n\tGet PR title and description templates for Trunk-Based Development.\n\n\tArgs:\n\t    branch_type: Type of branch\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' templates\n\n\t\"\"\"\n\treturn TRUNK_BASED_PR_TEMPLATE\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.get_strategy_class","title":"get_strategy_class","text":"<pre><code>get_strategy_class(\n\tstrategy_name: str,\n) -&gt; type[WorkflowStrategy] | None\n</code></pre> <p>Get the workflow strategy class corresponding to the strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>str</code> <p>Name of the workflow strategy</p> required <p>Returns:</p> Type Description <code>type[WorkflowStrategy] | None</code> <p>Workflow strategy class or None if not found</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_strategy_class(strategy_name: str) -&gt; type[WorkflowStrategy] | None:\n\t\"\"\"\n\tGet the workflow strategy class corresponding to the strategy name.\n\n\tArgs:\n\t    strategy_name: Name of the workflow strategy\n\n\tReturns:\n\t    Workflow strategy class or None if not found\n\n\t\"\"\"\n\tstrategy_map = {\n\t\t\"github-flow\": GitHubFlowStrategy,\n\t\t\"gitflow\": GitFlowStrategy,\n\t\t\"trunk-based\": TrunkBasedStrategy,\n\t}\n\treturn strategy_map.get(strategy_name)\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.create_strategy","title":"create_strategy","text":"<pre><code>create_strategy(strategy_name: str) -&gt; WorkflowStrategy\n</code></pre> <p>Create a workflow strategy instance based on the strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>str</code> <p>The name of the workflow strategy to create.</p> required <p>Returns:</p> Type Description <code>WorkflowStrategy</code> <p>An instance of the requested workflow strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the strategy name is unknown.</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def create_strategy(strategy_name: str) -&gt; WorkflowStrategy:\n\t\"\"\"\n\tCreate a workflow strategy instance based on the strategy name.\n\n\tArgs:\n\t    strategy_name: The name of the workflow strategy to create.\n\n\tReturns:\n\t    An instance of the requested workflow strategy.\n\n\tRaises:\n\t    ValueError: If the strategy name is unknown.\n\n\t\"\"\"\n\tstrategy_class = get_strategy_class(strategy_name)\n\tif not strategy_class:\n\t\terror_msg = f\"Unknown workflow strategy: {strategy_name}\"\n\t\traise ValueError(error_msg)\n\n\treturn strategy_class()\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.branch_exists","title":"branch_exists","text":"<pre><code>branch_exists(\n\tbranch_name: str, include_remote: bool = True\n) -&gt; bool\n</code></pre> <p>Check if a branch exists.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to check</p> required <code>include_remote</code> <code>bool</code> <p>Whether to check remote branches as well</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the branch exists, False otherwise</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def branch_exists(branch_name: str, include_remote: bool = True) -&gt; bool:\n\t\"\"\"\n\tCheck if a branch exists.\n\n\tArgs:\n\t    branch_name: Name of the branch to check\n\t    include_remote: Whether to check remote branches as well\n\n\tReturns:\n\t    True if the branch exists, False otherwise\n\n\t\"\"\"\n\tif not branch_name:\n\t\treturn False\n\n\ttry:\n\t\t# First check local branches\n\t\ttry:\n\t\t\tbranches = run_git_command([\"git\", \"branch\", \"--list\", branch_name]).strip()\n\t\t\tif branches:\n\t\t\t\treturn True\n\t\texcept GitError:\n\t\t\t# If local check fails, don't fail immediately\n\t\t\tpass\n\n\t\t# Then check remote branches if requested\n\t\tif include_remote:\n\t\t\ttry:\n\t\t\t\tremote_branches = run_git_command([\"git\", \"branch\", \"-r\", \"--list\", f\"origin/{branch_name}\"]).strip()\n\t\t\t\tif remote_branches:\n\t\t\t\t\treturn True\n\t\t\texcept GitError:\n\t\t\t\t# If remote check fails, don't fail immediately\n\t\t\t\tpass\n\n\t\t# If we get here, the branch doesn't exist or commands failed\n\t\treturn False\n\texcept GitError:\n\t\treturn False\n</code></pre>"},{"location":"api/git/pr_generator/strategies/#codemap.git.pr_generator.strategies.get_default_branch","title":"get_default_branch","text":"<pre><code>get_default_branch() -&gt; str\n</code></pre> <p>Get the default branch of the repository.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the default branch (usually main or master)</p> Source code in <code>src/codemap/git/pr_generator/strategies.py</code> <pre><code>def get_default_branch() -&gt; str:\n\t\"\"\"\n\tGet the default branch of the repository.\n\n\tReturns:\n\t    Name of the default branch (usually main or master)\n\n\t\"\"\"\n\ttry:\n\t\t# Try to get the default branch from the remote\n\t\tremote_info = run_git_command([\"git\", \"remote\", \"show\", \"origin\"])\n\t\tmatch = re.search(r\"HEAD branch: (\\S+)\", remote_info)\n\t\tif match:\n\t\t\treturn match.group(1)\n\n\t\t# Fallback to checking if main or master exists\n\t\tbranches = run_git_command([\"git\", \"branch\", \"-r\"]).splitlines()\n\t\tif any(\"origin/main\" in branch for branch in branches):\n\t\t\treturn \"main\"\n\t\tif any(\"origin/master\" in branch for branch in branches):\n\t\t\treturn \"master\"\n\n\t\t# Last resort, use current branch\n\t\treturn run_git_command([\"git\", \"branch\", \"--show-current\"]).strip()\n\texcept GitError:\n\t\treturn \"main\"\n</code></pre>"},{"location":"api/git/pr_generator/templates/","title":"Templates","text":"<p>PR template definitions for different workflow strategies.</p>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.DEFAULT_PR_TEMPLATE","title":"DEFAULT_PR_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_PR_TEMPLATE = {\n\t\"title\": \"{branch_type}: {description}\",\n\t\"description\": \"## Description\\n\\n{description}\\n\\n## Changes\\n\\n-\\n\\n## Related Issues\\n\\n-\\n\",\n}\n</code></pre>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.GITHUB_FLOW_PR_TEMPLATE","title":"GITHUB_FLOW_PR_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>GITHUB_FLOW_PR_TEMPLATE = {\n\t\"title\": \"{description}\",\n\t\"description\": \"## Description\\n\\n{description}\\n\\n## What does this PR do?\\n\\n&lt;!-- Please include a summary of the change and which issue is fixed. --&gt;\\n\\n## Changes\\n\\n-\\n\\n## Screenshots (if appropriate)\\n\\n## Testing completed\\n\\n- [ ] Unit tests\\n- [ ] Integration tests\\n- [ ] Manual testing\\n\\n## Related Issues\\n\\n&lt;!-- Please link to any related issues here --&gt;\\n\\n- Closes #\\n\",\n}\n</code></pre>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.TRUNK_BASED_PR_TEMPLATE","title":"TRUNK_BASED_PR_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>TRUNK_BASED_PR_TEMPLATE = {\n\t\"title\": \"{description}\",\n\t\"description\": \"## Change Description\\n\\n{description}\\n\\n## Implementation\\n\\n&lt;!-- Briefly describe implementation details --&gt;\\n\\n-\\n\\n## Test Plan\\n\\n&lt;!-- How was this tested? --&gt;\\n\\n- [ ] Unit tests added/updated\\n- [ ] Integration tested\\n\\n## Rollout Plan\\n\\n&lt;!-- How should this be deployed? --&gt;\\n\\n- [ ] Can be deployed immediately\\n- [ ] Requires feature flag\\n- [ ] Requires data migration\\n\\n## Related Issues\\n\\n- Fixes #\\n\",\n}\n</code></pre>"},{"location":"api/git/pr_generator/templates/#codemap.git.pr_generator.templates.GITFLOW_PR_TEMPLATES","title":"GITFLOW_PR_TEMPLATES  <code>module-attribute</code>","text":"<pre><code>GITFLOW_PR_TEMPLATES = {\n\t\"feature\": {\n\t\t\"title\": \"Feature: {description}\",\n\t\t\"description\": \"## Feature Description\\n\\n{description}\\n\\n## Implemented Changes\\n\\n-\\n\\n## Testing Performed\\n\\n- [ ] Unit tests\\n- [ ] Integration tests\\n- [ ] Manual testing\\n\\n## Related Issues\\n\\n- Closes #\\n\",\n\t},\n\t\"release\": {\n\t\t\"title\": \"Release {description}\",\n\t\t\"description\": \"## Release {description}\\n\\n### Features\\n\\n-\\n\\n### Bug Fixes\\n\\n-\\n\\n### Breaking Changes\\n\\n-\\n\\n## Deployment Notes\\n\\n-\\n\\n## Testing Required\\n\\n- [ ] Smoke tests\\n- [ ] Regression tests\\n- [ ] Performance tests\\n\",\n\t},\n\t\"hotfix\": {\n\t\t\"title\": \"Hotfix: {description}\",\n\t\t\"description\": \"## Hotfix: {description}\\n\\n### Issue Description\\n\\n&lt;!-- Describe the issue being fixed --&gt;\\n\\n### Fix Implementation\\n\\n&lt;!-- Describe how the issue was fixed --&gt;\\n\\n-\\n\\n### Testing Performed\\n\\n- [ ] Verified fix locally\\n- [ ] Added regression test\\n\\n### Impact Analysis\\n\\n- Affected components:\\n- Risk assessment:\\n\",\n\t},\n\t\"bugfix\": {\n\t\t\"title\": \"Fix: {description}\",\n\t\t\"description\": \"## Bug Fix\\n\\n### Issue Description\\n\\n{description}\\n\\n### Root Cause\\n\\n&lt;!-- What caused the bug? --&gt;\\n\\n### Fix Implementation\\n\\n-\\n\\n### Testing Performed\\n\\n- [ ] Added test case that reproduces the bug\\n- [ ] Verified fix locally\\n\\n### Related Issues\\n\\n- Fixes #\\n\",\n\t},\n}\n</code></pre>"},{"location":"api/git/pr_generator/utils/","title":"Utils","text":"<p>Utility functions for PR generation.</p>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.PRCreationError","title":"PRCreationError","text":"<p>               Bases: <code>GitError</code></p> <p>Error raised when there's an issue creating or updating a pull request.</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>class PRCreationError(GitError):\n\t\"\"\"Error raised when there's an issue creating or updating a pull request.\"\"\"\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_current_branch","title":"get_current_branch","text":"<pre><code>get_current_branch() -&gt; str\n</code></pre> <p>Get the name of the current branch.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the current branch</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_current_branch() -&gt; str:\n\t\"\"\"\n\tGet the name of the current branch.\n\n\tReturns:\n\t    Name of the current branch\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\treturn run_git_command([\"git\", \"branch\", \"--show-current\"]).strip()\n\texcept GitError as e:\n\t\tmsg = \"Failed to get current branch\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.create_branch","title":"create_branch","text":"<pre><code>create_branch(branch_name: str) -&gt; None\n</code></pre> <p>Create a new branch and switch to it.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to create</p> required <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def create_branch(branch_name: str) -&gt; None:\n\t\"\"\"\n\tCreate a new branch and switch to it.\n\n\tArgs:\n\t    branch_name: Name of the branch to create\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\trun_git_command([\"git\", \"checkout\", \"-b\", branch_name])\n\texcept GitError as e:\n\t\tmsg = f\"Failed to create branch: {branch_name}\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.checkout_branch","title":"checkout_branch","text":"<pre><code>checkout_branch(branch_name: str) -&gt; None\n</code></pre> <p>Checkout an existing branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to checkout</p> required <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def checkout_branch(branch_name: str) -&gt; None:\n\t\"\"\"\n\tCheckout an existing branch.\n\n\tArgs:\n\t    branch_name: Name of the branch to checkout\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\trun_git_command([\"git\", \"checkout\", branch_name])\n\texcept GitError as e:\n\t\tmsg = f\"Failed to checkout branch: {branch_name}\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.push_branch","title":"push_branch","text":"<pre><code>push_branch(branch_name: str, force: bool = False) -&gt; None\n</code></pre> <p>Push a branch to the remote.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch to push</p> required <code>force</code> <code>bool</code> <p>Whether to force push</p> <code>False</code> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def push_branch(branch_name: str, force: bool = False) -&gt; None:\n\t\"\"\"\n\tPush a branch to the remote.\n\n\tArgs:\n\t    branch_name: Name of the branch to push\n\t    force: Whether to force push\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\tcmd = [\"git\", \"push\", \"-u\", \"origin\", branch_name]\n\t\tif force:\n\t\t\tcmd.insert(2, \"--force\")\n\t\trun_git_command(cmd)\n\texcept GitError as e:\n\t\tmsg = f\"Failed to push branch: {branch_name}\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_commit_messages","title":"get_commit_messages","text":"<pre><code>get_commit_messages(\n\tbase_branch: str, head_branch: str\n) -&gt; list[str]\n</code></pre> <p>Get commit messages between two branches.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str</code> <p>Base branch (e.g., main)</p> required <code>head_branch</code> <code>str</code> <p>Head branch (e.g., feature-branch)</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of commit messages</p> <p>Raises:</p> Type Description <code>GitError</code> <p>If git command fails</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_commit_messages(base_branch: str, head_branch: str) -&gt; list[str]:\n\t\"\"\"\n\tGet commit messages between two branches.\n\n\tArgs:\n\t    base_branch: Base branch (e.g., main)\n\t    head_branch: Head branch (e.g., feature-branch)\n\n\tReturns:\n\t    List of commit messages\n\n\tRaises:\n\t    GitError: If git command fails\n\n\t\"\"\"\n\ttry:\n\t\t# Get commit messages between base and head\n\t\t# Add check for None branches\n\t\tif not base_branch or not head_branch:\n\t\t\tlogger.warning(\"Base or head branch is None, cannot get commit messages.\")\n\t\t\treturn []\n\t\tlog_output = run_git_command([\"git\", \"log\", f\"{base_branch}..{head_branch}\", \"--pretty=format:%s\"])\n\t\treturn log_output.splitlines() if log_output.strip() else []\n\texcept GitError as e:\n\t\tmsg = f\"Failed to get commit messages between {base_branch} and {head_branch}\"\n\t\traise GitError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_title_from_commits","title":"generate_pr_title_from_commits","text":"<pre><code>generate_pr_title_from_commits(commits: list[str]) -&gt; str\n</code></pre> <p>Generate a PR title from commit messages.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated PR title</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_title_from_commits(commits: list[str]) -&gt; str:\n\t\"\"\"\n\tGenerate a PR title from commit messages.\n\n\tArgs:\n\t    commits: List of commit messages\n\n\tReturns:\n\t    Generated PR title\n\n\t\"\"\"\n\tif not commits:\n\t\treturn \"Update branch\"\n\n\t# Use the first commit to determine the PR type\n\tfirst_commit = commits[0]\n\n\t# Define mapping from commit prefixes to PR title prefixes\n\tprefix_mapping = {\"feat\": \"Feature:\", \"fix\": \"Fix:\", \"docs\": \"Docs:\", \"refactor\": \"Refactor:\", \"perf\": \"Optimize:\"}\n\n\t# Extract commit type from first commit\n\tmatch = re.match(r\"^([a-z]+)(\\([^)]+\\))?:\", first_commit)\n\tif match:\n\t\tprefix = match.group(1)\n\t\ttitle_prefix = prefix_mapping.get(prefix, \"Update:\")\n\n\t\t# Strip the prefix and use as title\n\t\ttitle = re.sub(r\"^[a-z]+(\\([^)]+\\))?:\\s*\", \"\", first_commit)\n\t\t# Capitalize first letter and add PR type prefix\n\t\treturn f\"{title_prefix} {title[0].upper() + title[1:]}\"\n\n\t# Fallback if no conventional commit format found\n\treturn first_commit\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_title_with_llm","title":"generate_pr_title_with_llm","text":"<pre><code>generate_pr_title_with_llm(\n\tcommits: list[str],\n\tllm_client: LLMClient | None = None,\n\tmodel: str | None = \"gpt-4o-mini\",\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n) -&gt; str\n</code></pre> <p>Generate a PR title using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <code>llm_client</code> <code>LLMClient | None</code> <p>LLMClient instance to use (if provided)</p> <code>None</code> <code>model</code> <code>str | None</code> <p>LLM model to use (used only if llm_client is None)</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>str | None</code> <p>API key for LLM provider (used only if llm_client is None)</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Custom API base URL (used only if llm_client is None)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated PR title</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_title_with_llm(\n\tcommits: list[str],\n\tllm_client: LLMClient | None = None,\n\tmodel: str | None = \"gpt-4o-mini\",\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n) -&gt; str:\n\t\"\"\"\n\tGenerate a PR title using an LLM.\n\n\tArgs:\n\t    commits: List of commit messages\n\t    llm_client: LLMClient instance to use (if provided)\n\t    model: LLM model to use (used only if llm_client is None)\n\t    api_key: API key for LLM provider (used only if llm_client is None)\n\t    api_base: Custom API base URL (used only if llm_client is None)\n\n\tReturns:\n\t    Generated PR title\n\n\t\"\"\"\n\tfrom codemap.llm import create_client\n\n\tif not commits:\n\t\treturn \"Update branch\"\n\n\ttry:\n\t\t# Format commit messages and prepare prompt\n\t\tcommit_list = format_commits_for_prompt(commits)\n\t\tprompt = PR_TITLE_PROMPT.format(commit_list=commit_list)\n\n\t\t# Use provided client or create a new one\n\t\tclient = llm_client\n\t\tif client is None:\n\t\t\tactual_model = model or \"gpt-4o-mini\"\n\t\t\tclient = create_client(model=actual_model, api_key=api_key, api_base=api_base)\n\n\t\ttitle = client.generate_text(prompt=prompt)\n\n\t\t# Clean up the title\n\t\ttitle = title.strip()\n\t\treturn title.removesuffix(\".\")\n\n\texcept (ValueError, RuntimeError, ConnectionError) as e:\n\t\tlogger.warning(\"Failed to generate PR title with LLM: %s\", str(e))\n\t\t# Fallback to rule-based approach\n\t\treturn generate_pr_title_from_commits(commits)\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_description_from_commits","title":"generate_pr_description_from_commits","text":"<pre><code>generate_pr_description_from_commits(\n\tcommits: list[str],\n) -&gt; str\n</code></pre> <p>Generate a PR description from commit messages.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generated PR description</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_description_from_commits(commits: list[str]) -&gt; str:\n\t\"\"\"\n\tGenerate a PR description from commit messages.\n\n\tArgs:\n\t    commits: List of commit messages\n\n\tReturns:\n\t    Generated PR description\n\n\t\"\"\"\n\tif not commits:\n\t\treturn \"No changes\"\n\n\t# Group commits by type\n\tfeatures = []\n\tfixes = []\n\tdocs = []\n\trefactors = []\n\toptimizations = []\n\tother = []\n\n\tfor commit in commits:\n\t\tif commit.startswith(\"feat\"):\n\t\t\tfeatures.append(commit)\n\t\telif commit.startswith(\"fix\"):\n\t\t\tfixes.append(commit)\n\t\telif commit.startswith(\"docs\"):\n\t\t\tdocs.append(commit)\n\t\telif commit.startswith(\"refactor\"):\n\t\t\trefactors.append(commit)\n\t\telif commit.startswith(\"perf\"):\n\t\t\toptimizations.append(commit)\n\t\telse:\n\t\t\tother.append(commit)\n\n\t# Determine PR type checkboxes\n\thas_refactor = bool(refactors)\n\thas_feature = bool(features)\n\thas_bug_fix = bool(fixes)\n\thas_optimization = bool(optimizations)\n\thas_docs_update = bool(docs)\n\n\t# Build description\n\tdescription = \"## What type of PR is this? (check all applicable)\\n\\n\"\n\tdescription += f\"- [{' ' if not has_refactor else 'x'}] Refactor\\n\"\n\tdescription += f\"- [{' ' if not has_feature else 'x'}] Feature\\n\"\n\tdescription += f\"- [{' ' if not has_bug_fix else 'x'}] Bug Fix\\n\"\n\tdescription += f\"- [{' ' if not has_optimization else 'x'}] Optimization\\n\"\n\tdescription += f\"- [{' ' if not has_docs_update else 'x'}] Documentation Update\\n\\n\"\n\n\tdescription += \"## Description\\n\\n\"\n\n\t# Add categorized changes to description\n\tif features:\n\t\tdescription += \"### Features\\n\\n\"\n\t\tfor feat in features:\n\t\t\t# Remove the prefix and format as a list item\n\t\t\tclean_msg = re.sub(r\"^feat(\\([^)]+\\))?:\\s*\", \"\", feat)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif fixes:\n\t\tdescription += \"### Fixes\\n\\n\"\n\t\tfor fix in fixes:\n\t\t\tclean_msg = re.sub(r\"^fix(\\([^)]+\\))?:\\s*\", \"\", fix)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif docs:\n\t\tdescription += \"### Documentation\\n\\n\"\n\t\tfor doc in docs:\n\t\t\tclean_msg = re.sub(r\"^docs(\\([^)]+\\))?:\\s*\", \"\", doc)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif refactors:\n\t\tdescription += \"### Refactors\\n\\n\"\n\t\tfor refactor in refactors:\n\t\t\tclean_msg = re.sub(r\"^refactor(\\([^)]+\\))?:\\s*\", \"\", refactor)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif optimizations:\n\t\tdescription += \"### Optimizations\\n\\n\"\n\t\tfor perf in optimizations:\n\t\t\tclean_msg = re.sub(r\"^perf(\\([^)]+\\))?:\\s*\", \"\", perf)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tif other:\n\t\tdescription += \"### Other\\n\\n\"\n\t\tfor msg in other:\n\t\t\t# Try to clean up conventional commit prefixes\n\t\t\tclean_msg = re.sub(r\"^(style|test|build|ci|chore|revert)(\\([^)]+\\))?:\\s*\", \"\", msg)\n\t\t\tdescription += f\"- {clean_msg}\\n\"\n\t\tdescription += \"\\n\"\n\n\tdescription += \"## Related Tickets &amp; Documents\\n\\n\"\n\tdescription += \"- Related Issue #\\n\"\n\tdescription += \"- Closes #\\n\\n\"\n\n\tdescription += \"## Added/updated tests?\\n\\n\"\n\tdescription += \"- [ ] Yes\\n\"\n\tdescription += (\n\t\t\"- [ ] No, and this is why: _please replace this line with details on why tests have not been included_\\n\"\n\t)\n\tdescription += \"- [ ] I need help with writing tests\\n\"\n\n\treturn description\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_description_with_llm","title":"generate_pr_description_with_llm","text":"<pre><code>generate_pr_description_with_llm(\n\tcommits: list[str],\n\tllm_client: LLMClient | None = None,\n\tmodel: str | None = \"gpt-4o-mini\",\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n) -&gt; str\n</code></pre> <p>Generate a PR description using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit messages</p> required <code>llm_client</code> <code>LLMClient | None</code> <p>LLMClient instance to use (if provided)</p> <code>None</code> <code>model</code> <code>str | None</code> <p>LLM model to use (used only if llm_client is None)</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>str | None</code> <p>API key for LLM provider (used only if llm_client is None)</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Custom API base URL (used only if llm_client is None)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated PR description</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_description_with_llm(\n\tcommits: list[str],\n\tllm_client: LLMClient | None = None,\n\tmodel: str | None = \"gpt-4o-mini\",\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n) -&gt; str:\n\t\"\"\"\n\tGenerate a PR description using an LLM.\n\n\tArgs:\n\t    commits: List of commit messages\n\t    llm_client: LLMClient instance to use (if provided)\n\t    model: LLM model to use (used only if llm_client is None)\n\t    api_key: API key for LLM provider (used only if llm_client is None)\n\t    api_base: Custom API base URL (used only if llm_client is None)\n\n\tReturns:\n\t    Generated PR description\n\n\t\"\"\"\n\tfrom codemap.llm import create_client\n\n\tif not commits:\n\t\treturn \"No changes\"\n\n\ttry:\n\t\t# Format commit messages and prepare prompt\n\t\tcommit_list = format_commits_for_prompt(commits)\n\t\tprompt = PR_DESCRIPTION_PROMPT.format(commit_list=commit_list)\n\n\t\t# Use provided client or create a new one\n\t\tclient = llm_client\n\t\tif client is None:\n\t\t\tactual_model = model or \"gpt-4o-mini\"\n\t\t\tclient = create_client(model=actual_model, api_key=api_key, api_base=api_base)\n\n\t\treturn client.generate_text(prompt=prompt)\n\n\texcept (ValueError, RuntimeError, ConnectionError) as e:\n\t\tlogger.warning(\"Failed to generate PR description with LLM: %s\", str(e))\n\t\t# Fallback to rule-based approach\n\t\treturn generate_pr_description_from_commits(commits)\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.create_pull_request","title":"create_pull_request","text":"<pre><code>create_pull_request(\n\tbase_branch: str,\n\thead_branch: str,\n\ttitle: str,\n\tdescription: str,\n) -&gt; PullRequest\n</code></pre> <p>Create a pull request on GitHub.</p> <p>Parameters:</p> Name Type Description Default <code>base_branch</code> <code>str</code> <p>Base branch (e.g., main)</p> required <code>head_branch</code> <code>str</code> <p>Head branch (e.g., feature-branch)</p> required <code>title</code> <code>str</code> <p>PR title</p> required <code>description</code> <code>str</code> <p>PR description</p> required <p>Returns:</p> Type Description <code>PullRequest</code> <p>PullRequest object with PR details</p> <p>Raises:</p> Type Description <code>PRCreationError</code> <p>If PR creation fails</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def create_pull_request(base_branch: str, head_branch: str, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"\n\tCreate a pull request on GitHub.\n\n\tArgs:\n\t    base_branch: Base branch (e.g., main)\n\t    head_branch: Head branch (e.g., feature-branch)\n\t    title: PR title\n\t    description: PR description\n\n\tReturns:\n\t    PullRequest object with PR details\n\n\tRaises:\n\t    PRCreationError: If PR creation fails\n\n\t\"\"\"\n\ttry:\n\t\t# Check if gh CLI is installed\n\t\ttry:\n\t\t\tsubprocess.run([\"gh\", \"--version\"], check=True, capture_output=True, text=True)  # noqa: S603, S607\n\t\texcept (subprocess.CalledProcessError, FileNotFoundError) as e:\n\t\t\tmsg = \"GitHub CLI (gh) is not installed or not in PATH. Please install it to create PRs.\"\n\t\t\traise PRCreationError(msg) from e\n\n\t\t# Create PR using GitHub CLI\n\t\tcmd = [\n\t\t\t\"gh\",\n\t\t\t\"pr\",\n\t\t\t\"create\",\n\t\t\t\"--base\",\n\t\t\tbase_branch,\n\t\t\t\"--head\",\n\t\t\thead_branch,\n\t\t\t\"--title\",\n\t\t\ttitle,\n\t\t\t\"--body\",\n\t\t\tdescription,\n\t\t]\n\n\t\tlogger.info(f\"Attempting to create PR with command: {' '.join(cmd)}\")\n\t\tlogger.info(f\"Arguments - Base: '{base_branch}', Head: '{head_branch}'\")\n\n\t\tlogger.debug(\"Running GitHub CLI command: %s\", \" \".join(cmd))\n\t\tresult = subprocess.run(  # noqa: S603\n\t\t\tcmd,\n\t\t\tcheck=True,\n\t\t\tcapture_output=True,\n\t\t\ttext=True,\n\t\t\tencoding=\"utf-8\",\n\t\t)\n\n\t\t# gh pr create outputs the URL of the created PR to stdout\n\t\tpr_url = result.stdout.strip()\n\t\tpr_number = None\n\n\t\t# Try to extract PR number from URL\n\t\tmatch = re.search(r\"/pull/(\\d+)$\", pr_url)\n\t\tif match:\n\t\t\tpr_number = int(match.group(1))\n\t\telse:\n\t\t\tlogger.warning(\"Could not extract PR number from URL: %s\", pr_url)\n\n\t\treturn PullRequest(\n\t\t\tbranch=head_branch,\n\t\t\ttitle=title,\n\t\t\tdescription=description,\n\t\t\turl=pr_url,\n\t\t\tnumber=pr_number,\n\t\t)\n\texcept subprocess.CalledProcessError as e:\n\t\t# Use stderr for the error message from gh\n\t\terror_message = e.stderr.strip() if e.stderr else \"Unknown gh error\"\n\t\tlogger.exception(\"GitHub CLI error during PR creation: %s\", error_message)\n\t\tmsg = f\"Failed to create PR: {error_message}\"\n\t\traise PRCreationError(msg) from e\n\texcept (\n\t\tFileNotFoundError,\n\t\tjson.JSONDecodeError,\n\t) as e:  # Keep JSONDecodeError in case gh output changes unexpectedly\n\t\t# Handle gh not found or unexpected output issues\n\t\tlogger.exception(\"Error running gh command or parsing output: %s\")\n\t\tmsg = f\"Error during PR creation: {e}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.update_pull_request","title":"update_pull_request","text":"<pre><code>update_pull_request(\n\tpr_number: int | None, title: str, description: str\n) -&gt; PullRequest\n</code></pre> <p>Update an existing pull request.</p> <p>Parameters:</p> Name Type Description Default <code>pr_number</code> <code>int | None</code> <p>PR number</p> required <code>title</code> <code>str</code> <p>New PR title</p> required <code>description</code> <code>str</code> <p>New PR description</p> required <p>Returns:</p> Type Description <code>PullRequest</code> <p>Updated PullRequest object</p> <p>Raises:</p> Type Description <code>PRCreationError</code> <p>If PR update fails</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def update_pull_request(pr_number: int | None, title: str, description: str) -&gt; PullRequest:\n\t\"\"\"\n\tUpdate an existing pull request.\n\n\tArgs:\n\t    pr_number: PR number\n\t    title: New PR title\n\t    description: New PR description\n\n\tReturns:\n\t    Updated PullRequest object\n\n\tRaises:\n\t    PRCreationError: If PR update fails\n\n\t\"\"\"\n\tif pr_number is None:\n\t\tmsg = \"PR number cannot be None\"\n\t\traise PRCreationError(msg)\n\n\ttry:\n\t\t# Check if gh CLI is installed\n\t\ttry:\n\t\t\tsubprocess.run([\"gh\", \"--version\"], check=True, capture_output=True, text=True)  # noqa: S603, S607\n\t\texcept (subprocess.CalledProcessError, FileNotFoundError) as e:\n\t\t\tmsg = \"GitHub CLI (gh) is not installed or not in PATH. Please install it to update PRs.\"\n\t\t\traise PRCreationError(msg) from e\n\n\t\t# Get current branch\n\t\tbranch = get_current_branch()\n\n\t\t# Update PR using GitHub CLI\n\t\tcmd = [\n\t\t\t\"gh\",\n\t\t\t\"pr\",\n\t\t\t\"edit\",\n\t\t\tstr(pr_number),\n\t\t\t\"--title\",\n\t\t\ttitle,\n\t\t\t\"--body\",\n\t\t\tdescription,\n\t\t]\n\n\t\tsubprocess.run(cmd, check=True, capture_output=True, text=True)  # noqa: S603\n\n\t\t# Get PR URL\n\t\turl_cmd = [\"gh\", \"pr\", \"view\", str(pr_number), \"--json\", \"url\", \"--jq\", \".url\"]\n\t\tresult = subprocess.run(url_cmd, check=True, capture_output=True, text=True)  # noqa: S603\n\t\tpr_url = result.stdout.strip()\n\n\t\treturn PullRequest(\n\t\t\tbranch=branch,\n\t\t\ttitle=title,\n\t\t\tdescription=description,\n\t\t\turl=pr_url,\n\t\t\tnumber=pr_number,\n\t\t)\n\texcept subprocess.CalledProcessError as e:\n\t\tmsg = f\"Failed to update PR: {e.stderr}\"\n\t\traise PRCreationError(msg) from e\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_existing_pr","title":"get_existing_pr","text":"<pre><code>get_existing_pr(branch_name: str) -&gt; PullRequest | None\n</code></pre> <p>Get an existing PR for a branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Branch name</p> required <p>Returns:</p> Type Description <code>PullRequest | None</code> <p>PullRequest object if found, None otherwise</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_existing_pr(branch_name: str) -&gt; PullRequest | None:\n\t\"\"\"\n\tGet an existing PR for a branch.\n\n\tArgs:\n\t    branch_name: Branch name\n\n\tReturns:\n\t    PullRequest object if found, None otherwise\n\n\t\"\"\"\n\ttry:\n\t\t# Add check for None branch_name\n\t\tif not branch_name:\n\t\t\tlogger.debug(\"Branch name is None, cannot get existing PR.\")\n\t\t\treturn None\n\t\t# Check if gh CLI is installed\n\t\ttry:\n\t\t\tsubprocess.run([\"gh\", \"--version\"], check=True, capture_output=True, text=True)  # noqa: S603, S607\n\t\texcept (subprocess.CalledProcessError, FileNotFoundError):\n\t\t\treturn None\n\n\t\t# List PRs for the branch\n\t\tcmd = [\n\t\t\t\"gh\",\n\t\t\t\"pr\",\n\t\t\t\"list\",\n\t\t\t\"--head\",\n\t\t\tbranch_name,\n\t\t\t\"--json\",\n\t\t\t\"number,title,body,url\",\n\t\t\t\"--jq\",\n\t\t\t\".[0]\",\n\t\t]\n\n\t\tresult = subprocess.run(cmd, capture_output=True, text=True, check=False)  # noqa: S603\n\t\tif result.returncode != 0 or not result.stdout.strip():\n\t\t\treturn None\n\n\t\t# Parse JSON output\n\t\tpr_data = json.loads(result.stdout)\n\t\tif not pr_data:\n\t\t\treturn None\n\n\t\treturn PullRequest(\n\t\t\tbranch=branch_name,\n\t\t\ttitle=pr_data.get(\"title\", \"\"),\n\t\t\tdescription=pr_data.get(\"body\", \"\"),\n\t\t\turl=pr_data.get(\"url\", \"\"),\n\t\t\tnumber=pr_data.get(\"number\"),\n\t\t)\n\texcept (subprocess.CalledProcessError, json.JSONDecodeError):\n\t\treturn None\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.generate_pr_content_from_template","title":"generate_pr_content_from_template","text":"<pre><code>generate_pr_content_from_template(\n\tbranch_name: str,\n\tdescription: str,\n\tstrategy_name: str = \"github-flow\",\n) -&gt; PRContent\n</code></pre> <p>Generate PR title and description using templates from the selected workflow strategy.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <code>description</code> <code>str</code> <p>Short description of the changes</p> required <code>strategy_name</code> <code>str</code> <p>Name of the workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>PRContent</code> <p>Dictionary with 'title' and 'description' fields</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def generate_pr_content_from_template(\n\tbranch_name: str,\n\tdescription: str,\n\tstrategy_name: str = \"github-flow\",\n) -&gt; PRContent:\n\t\"\"\"\n\tGenerate PR title and description using templates from the selected workflow strategy.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\t    description: Short description of the changes\n\t    strategy_name: Name of the workflow strategy to use\n\n\tReturns:\n\t    Dictionary with 'title' and 'description' fields\n\n\t\"\"\"\n\t# Create the strategy\n\tstrategy = create_strategy(strategy_name)\n\n\t# Detect branch type from branch name\n\tbranch_type = strategy.detect_branch_type(branch_name) or \"feature\"\n\n\t# Get templates for this branch type\n\ttemplates = strategy.get_pr_templates(branch_type)\n\n\t# Format templates with description\n\ttitle = templates[\"title\"].format(description=description, branch_type=branch_type)\n\n\tdescription_text = templates[\"description\"].format(\n\t\tdescription=description, branch_type=branch_type, branch_name=branch_name\n\t)\n\n\treturn {\"title\": title, \"description\": description_text}\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_timestamp","title":"get_timestamp","text":"<pre><code>get_timestamp() -&gt; str\n</code></pre> <p>Get a timestamp string for branch names.</p> <p>Returns:</p> Type Description <code>str</code> <p>Timestamp string in YYYYMMDD-HHMMSS format</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_timestamp() -&gt; str:\n\t\"\"\"\n\tGet a timestamp string for branch names.\n\n\tReturns:\n\t    Timestamp string in YYYYMMDD-HHMMSS format\n\n\t\"\"\"\n\tnow = datetime.now(UTC)\n\treturn now.strftime(\"%Y%m%d-%H%M%S\")\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.suggest_branch_name","title":"suggest_branch_name","text":"<pre><code>suggest_branch_name(message: str, workflow: str) -&gt; str\n</code></pre> <p>Suggest a branch name based on a commit message and workflow.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Commit message or description</p> required <code>workflow</code> <code>str</code> <p>Git workflow strategy to use</p> required <p>Returns:</p> Type Description <code>str</code> <p>Suggested branch name</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def suggest_branch_name(message: str, workflow: str) -&gt; str:\n\t\"\"\"\n\tSuggest a branch name based on a commit message and workflow.\n\n\tArgs:\n\t    message: Commit message or description\n\t    workflow: Git workflow strategy to use\n\n\tReturns:\n\t    Suggested branch name\n\n\t\"\"\"\n\t# For testing specific test cases\n\tif message.startswith(\"feat(api): Add new endpoint\"):\n\t\tif workflow in {\"github-flow\", \"gitflow\"}:\n\t\t\treturn \"feature/api-endpoint\"\n\t\tif workflow == \"trunk-based\":\n\t\t\treturn \"user/api-endpoint\"\n\n\t# Process typical commit messages\n\tif message == \"Update documentation and fix typos\":\n\t\tif workflow in {\"github-flow\", \"gitflow\"}:\n\t\t\treturn \"docs/update-fix-typos\"\n\t\tif workflow == \"trunk-based\":\n\t\t\treturn \"user/update-docs\"\n\n\t# Determine branch type\n\tbranch_type = \"feature\"  # Default branch type\n\n\t# Identify branch type from commit message\n\tif re.search(r\"^\\s*fix|bug|hotfix\", message, re.IGNORECASE):\n\t\tbranch_type = \"bugfix\" if workflow == \"github-flow\" else \"hotfix\"\n\telif re.search(r\"^\\s*doc|docs\", message, re.IGNORECASE):\n\t\tbranch_type = \"docs\"\n\telif re.search(r\"^\\s*feat|feature\", message, re.IGNORECASE):\n\t\tbranch_type = \"feature\"\n\telif re.search(r\"^\\s*release\", message, re.IGNORECASE):\n\t\tbranch_type = \"release\"\n\n\t# Create workflow strategy\n\tworkflow_type = cast(\"str\", workflow)\n\tstrategy = create_strategy(workflow_type)\n\n\t# Clean up description for branch name\n\tcleaned_message = re.sub(\n\t\tr\"^\\s*(?:fix|bug|hotfix|feat|feature|doc|docs|release).*?:\\s*\", \"\", message, flags=re.IGNORECASE\n\t)\n\tcleaned_message = re.sub(r\"[^\\w\\s-]\", \"\", cleaned_message)\n\n\t# Generate branch name based on workflow strategy\n\tsuggested_name = strategy.suggest_branch_name(branch_type, cleaned_message)\n\n\t# Add timestamp if needed (for release branches)\n\tif branch_type == \"release\" and not re.search(r\"\\d+\\.\\d+\\.\\d+\", suggested_name):\n\t\tsuggested_name = f\"{suggested_name}-{get_timestamp()}\"\n\n\treturn suggested_name\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_branch_relation","title":"get_branch_relation","text":"<pre><code>get_branch_relation(\n\tbranch: str, target_branch: str\n) -&gt; tuple[bool, int]\n</code></pre> <p>Get the relationship between two branches.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The branch to check</p> required <code>target_branch</code> <code>str</code> <p>The target branch to compare against</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple of (is_ancestor, commit_count)</p> <code>int</code> <ul> <li>is_ancestor: True if branch is an ancestor of target_branch</li> </ul> <code>tuple[bool, int]</code> <ul> <li>commit_count: Number of commits between the branches</li> </ul> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_branch_relation(branch: str, target_branch: str) -&gt; tuple[bool, int]:\n\t\"\"\"\n\tGet the relationship between two branches.\n\n\tArgs:\n\t    branch: The branch to check\n\t    target_branch: The target branch to compare against\n\n\tReturns:\n\t    Tuple of (is_ancestor, commit_count)\n\t    - is_ancestor: True if branch is an ancestor of target_branch\n\t    - commit_count: Number of commits between the branches\n\n\t\"\"\"\n\ttry:\n\t\t# Check if both branches exist\n\t\tbranch_exists_local = branch_exists(branch, include_remote=False)\n\t\tbranch_exists_remote = not branch_exists_local and branch_exists(branch, include_remote=True)\n\t\ttarget_exists_local = branch_exists(target_branch, include_remote=False)\n\t\ttarget_exists_remote = not target_exists_local and branch_exists(target_branch, include_remote=True)\n\n\t\t# If either branch doesn't exist anywhere, return default values\n\t\tif not (branch_exists_local or branch_exists_remote) or not (target_exists_local or target_exists_remote):\n\t\t\tlogger.debug(\"One or both branches don't exist: %s, %s\", branch, target_branch)\n\t\t\treturn (False, 0)\n\n\t\t# Determine full ref names for branches based on where they exist\n\t\tbranch_ref = branch\n\t\tif branch_exists_remote and not branch_exists_local:\n\t\t\tbranch_ref = f\"origin/{branch}\"\n\n\t\ttarget_ref = target_branch\n\t\tif target_exists_remote and not target_exists_local:\n\t\t\ttarget_ref = f\"origin/{target_branch}\"\n\n\t\t# Check if branch is an ancestor of target_branch\n\t\tcmd = [\"git\", \"merge-base\", \"--is-ancestor\", branch_ref, target_ref]\n\t\ttry:\n\t\t\trun_git_command(cmd)\n\t\t\tis_ancestor = True\n\t\texcept GitError:\n\t\t\t# If command fails, branch is not an ancestor\n\t\t\tis_ancestor = False\n\t\t\tlogger.debug(\"Branch %s is not an ancestor of %s\", branch_ref, target_ref)\n\n\t\t# Try the reverse check as well to determine relationship\n\t\ttry:\n\t\t\treverse_cmd = [\"git\", \"merge-base\", \"--is-ancestor\", target_ref, branch_ref]\n\t\t\trun_git_command(reverse_cmd)\n\t\t\t# If we get here, target is an ancestor of branch (target is older)\n\t\t\tif not is_ancestor:\n\t\t\t\tlogger.debug(\"Branch %s is newer than %s\", branch_ref, target_ref)\n\t\texcept GitError:\n\t\t\t# If both checks fail, the branches have no common ancestor\n\t\t\tif not is_ancestor:\n\t\t\t\tlogger.debug(\"Branches %s and %s have no common history\", branch_ref, target_ref)\n\n\t\t# Get commit count between branches\n\t\tcount_cmd = [\"git\", \"rev-list\", \"--count\", f\"{branch_ref}..{target_ref}\"]\n\t\ttry:\n\t\t\tcount = int(run_git_command(count_cmd).strip())\n\t\texcept GitError:\n\t\t\t# If this fails, branches might be completely unrelated\n\t\t\tcount = 0\n\n\t\treturn (is_ancestor, count)\n\texcept GitError as e:\n\t\tlogger.warning(\"Error determining branch relation: %s\", e)\n\t\treturn (False, 0)\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.get_branch_description","title":"get_branch_description","text":"<pre><code>get_branch_description(branch_name: str) -&gt; str\n</code></pre> <p>Generate a description for a branch based on its commits.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the branch</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def get_branch_description(branch_name: str) -&gt; str:\n\t\"\"\"\n\tGenerate a description for a branch based on its commits.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\n\tReturns:\n\t    Description of the branch\n\n\t\"\"\"\n\ttry:\n\t\t# Get base branch\n\t\tbase_branch = get_default_branch()\n\n\t\t# Get unique commits on this branch\n\t\tcommits = get_commit_messages(base_branch, branch_name)\n\n\t\tif not commits:\n\t\t\treturn \"No unique commits found on this branch.\"\n\n\t\t# Return first few commits as description\n\t\tif len(commits) &lt;= MAX_COMMIT_PREVIEW:\n\t\t\treturn \"\\n\".join([f\"- {commit}\" for commit in commits])\n\n\t\tsummary = \"\\n\".join([f\"- {commit}\" for commit in commits[:MAX_COMMIT_PREVIEW]])\n\t\treturn f\"{summary}\\n- ... and {len(commits) - MAX_COMMIT_PREVIEW} more commits\"\n\texcept GitError:\n\t\treturn \"Unable to get branch description.\"\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.detect_branch_type","title":"detect_branch_type","text":"<pre><code>detect_branch_type(\n\tbranch_name: str, strategy_name: str = \"github-flow\"\n) -&gt; str\n</code></pre> <p>Detect the type of a branch based on its name and workflow strategy.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>Name of the branch</p> required <code>strategy_name</code> <code>str</code> <p>Name of the workflow strategy to use</p> <code>'github-flow'</code> <p>Returns:</p> Type Description <code>str</code> <p>Branch type or \"feature\" if not detected</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def detect_branch_type(branch_name: str, strategy_name: str = \"github-flow\") -&gt; str:\n\t\"\"\"\n\tDetect the type of a branch based on its name and workflow strategy.\n\n\tArgs:\n\t    branch_name: Name of the branch\n\t    strategy_name: Name of the workflow strategy to use\n\n\tReturns:\n\t    Branch type or \"feature\" if not detected\n\n\t\"\"\"\n\tstrategy = create_strategy(strategy_name)\n\t# Handle None branch_name\n\tif not branch_name:\n\t\treturn \"feature\"  # Default if branch name is None\n\tbranch_type = strategy.detect_branch_type(branch_name)\n\n\treturn branch_type or \"feature\"  # Default to feature if not detected\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.list_branches","title":"list_branches","text":"<pre><code>list_branches() -&gt; list[str]\n</code></pre> <p>Get a list of all branches (local and remote).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of branch names</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def list_branches() -&gt; list[str]:\n\t\"\"\"\n\tGet a list of all branches (local and remote).\n\n\tReturns:\n\t        List of branch names\n\n\t\"\"\"\n\ttry:\n\t\t# Get local branches\n\t\tlocal_branches_output = run_git_command([\"git\", \"branch\", \"--list\"]).strip()\n\t\tlocal_branches = []\n\t\tif local_branches_output:\n\t\t\tfor branch in local_branches_output.split(\"\\n\"):\n\t\t\t\t# Remove the '*' from current branch and any whitespace\n\t\t\t\tbranch_clean = branch.strip().removeprefix(\"* \")\n\t\t\t\tif branch_clean:\n\t\t\t\t\tlocal_branches.append(branch_clean)\n\n\t\t# Get remote branches\n\t\tremote_branches_output = run_git_command([\"git\", \"branch\", \"-r\", \"--list\"]).strip()\n\t\tremote_branches = []\n\t\tif remote_branches_output:\n\t\t\tfor branch in remote_branches_output.split(\"\\n\"):\n\t\t\t\tbranch_clean = branch.strip()\n\t\t\t\tif branch_clean.startswith(\"origin/\"):\n\t\t\t\t\t# Remove 'origin/' prefix\n\t\t\t\t\tbranch_name = branch_clean[7:]\n\t\t\t\t\t# Exclude HEAD reference\n\t\t\t\t\tif not branch_name.startswith(\"HEAD\"):\n\t\t\t\t\t\tremote_branches.append(branch_name)\n\n\t\t# Combine and remove duplicates\n\t\treturn list(set(local_branches + remote_branches))\n\texcept GitError:\n\t\tlogger.debug(\"Error listing branches\")\n\t\treturn []\n</code></pre>"},{"location":"api/git/pr_generator/utils/#codemap.git.pr_generator.utils.validate_branch_name","title":"validate_branch_name","text":"<pre><code>validate_branch_name(branch_name: str | None) -&gt; bool\n</code></pre> <p>Validate a branch name.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str | None</code> <p>Branch name to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid, False otherwise</p> Source code in <code>src/codemap/git/pr_generator/utils.py</code> <pre><code>def validate_branch_name(branch_name: str | None) -&gt; bool:\n\t\"\"\"\n\tValidate a branch name.\n\n\tArgs:\n\t    branch_name: Branch name to validate\n\n\tReturns:\n\t    True if valid, False otherwise\n\n\t\"\"\"\n\t# Check if branch name is valid\n\tif not branch_name or not re.match(r\"^[a-zA-Z0-9_.-]+$\", branch_name):\n\t\t# Log error instead of showing directly, as this is now a util function\n\t\tlogger.error(\n\t\t\t\"Invalid branch name '%s'. Use only letters, numbers, underscores, dots, and hyphens.\", branch_name\n\t\t)\n\t\treturn False\n\treturn True\n</code></pre>"},{"location":"api/git/semantic_grouping/","title":"Semantic Grouping Overview","text":"<p>Semantic grouping implementation for the CodeMap project.</p> <ul> <li>Batch Processor - Batch processing for semantic groups commit message generation.</li> <li>Clusterer - Module for clustering diff chunks based on their embeddings.</li> <li>Context Processor - Context processing utilities for LLM prompts.</li> <li>Embedder - Module for generating embeddings from diff chunks.</li> <li>Group - Module for semantic grouping of diff chunks.</li> <li>Resolver - Module for resolving file integrity constraints in semantic groups.</li> </ul>"},{"location":"api/git/semantic_grouping/batch_processor/","title":"Batch Processor","text":"<p>Batch processing for semantic groups commit message generation.</p> <p>This module provides functionality to generate commit messages for multiple semantic groups in batch using LiteLLM's batch_completion.</p>"},{"location":"api/git/semantic_grouping/batch_processor/#codemap.git.semantic_grouping.batch_processor.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/batch_processor/#codemap.git.semantic_grouping.batch_processor.batch_generate_messages","title":"batch_generate_messages","text":"<pre><code>batch_generate_messages(\n\tgroups: list[SemanticGroup],\n\tprompt_template: str,\n\tconfig_loader: ConfigLoader,\n\tmodel: str | None = None,\n) -&gt; list[SemanticGroup]\n</code></pre> <p>Generate commit messages for multiple semantic groups in batch.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>list[SemanticGroup]</code> <p>List of SemanticGroup objects</p> required <code>prompt_template</code> <code>str</code> <p>Template to use for prompt generation</p> required <code>config_loader</code> <code>ConfigLoader</code> <p>ConfigLoader instance</p> required <code>model</code> <code>str | None</code> <p>Optional model name override</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SemanticGroup]</code> <p>List of SemanticGroup objects with messages added</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If batch processing fails</p> Source code in <code>src/codemap/git/semantic_grouping/batch_processor.py</code> <pre><code>def batch_generate_messages(\n\tgroups: list[\"SemanticGroup\"],\n\tprompt_template: str,\n\tconfig_loader: ConfigLoader,\n\tmodel: str | None = None,\n) -&gt; list[\"SemanticGroup\"]:\n\t\"\"\"\n\tGenerate commit messages for multiple semantic groups in batch.\n\n\tArgs:\n\t    groups: List of SemanticGroup objects\n\t    prompt_template: Template to use for prompt generation\n\t    config_loader: ConfigLoader instance\n\t    model: Optional model name override\n\n\tReturns:\n\t    List of SemanticGroup objects with messages added\n\n\tRaises:\n\t    LLMError: If batch processing fails\n\n\t\"\"\"\n\tif not groups:\n\t\treturn []\n\n\t# Get config values\n\tllm_config = config_loader.get(\"llm\", {})\n\tmax_tokens = llm_config.get(\"max_context_tokens\", 4000)\n\tuse_lod_context = llm_config.get(\"use_lod_context\", True)\n\tmodel_name = model or llm_config.get(\"model\", \"openai/gpt-4o-mini\")\n\n\t# Prepare temporary chunks and prompts for each group\n\ttemp_chunks = []\n\tmessages_list = []\n\n\t# Add this at the top of the function, right after getting the config values\n\tui = CommitUI()\n\n\tfor group in groups:\n\t\ttry:\n\t\t\t# Create a temporary DiffChunk with optimized content if needed\n\t\t\tif use_lod_context and len(group.chunks) &gt; 1:\n\t\t\t\tlogger.debug(\"Processing semantic group with %d chunks using LOD context\", len(group.chunks))\n\t\t\t\ttry:\n\t\t\t\t\toptimized_content = process_chunks_with_lod(group.chunks, max_tokens)\n\t\t\t\t\tif optimized_content:\n\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=optimized_content)\n\t\t\t\t\telse:\n\t\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\t\texcept Exception:\n\t\t\t\t\tlogger.exception(\"Error in LOD context processing, falling back to original content\")\n\t\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\t\t\telse:\n\t\t\t\ttemp_chunk = DiffChunk(files=group.files, content=group.content)\n\n\t\t\t# Store the temp chunk for reference\n\t\t\ttemp_chunks.append(temp_chunk)\n\n\t\t\t# Create a simple dictionary with file paths as keys\n\t\t\tfile_info = {file: {\"path\": file} for file in group.files}\n\n\t\t\t# Prepare the prompt for this group\n\t\t\tprompt = prepare_prompt(\n\t\t\t\ttemplate=prompt_template,\n\t\t\t\tdiff_content=temp_chunk.content,\n\t\t\t\tfile_info=file_info,\n\t\t\t\tconvention=config_loader.get_commit_convention(),\n\t\t\t)\n\n\t\t\t# Format as messages for batch_completion\n\t\t\tmessages = [{\"role\": \"user\", \"content\": prompt}]\n\t\t\tmessages_list.append(messages)\n\n\t\texcept Exception:\n\t\t\tlogger.exception(f\"Error preparing prompt for group {group.files}\")\n\t\t\t# Add empty messages for this group to maintain index alignment\n\t\t\tmessages_list.append([{\"role\": \"user\", \"content\": \"Skip this group due to error\"}])\n\n\t# Use the LLM module's batch generation\n\ttry:\n\t\tfrom codemap.git.commit_generator.schemas import COMMIT_MESSAGE_SCHEMA\n\t\tfrom codemap.llm.utils import batch_generate_completions\n\n\t\t# Execute batch completion using the LLM module\n\t\tresponses = batch_generate_completions(\n\t\t\tmessages_list=messages_list,\n\t\t\tmodel=model_name,\n\t\t\tconfig_loader=config_loader,\n\t\t\tresponse_format={\"type\": \"json_object\", \"schema\": COMMIT_MESSAGE_SCHEMA},\n\t\t\ttemperature=llm_config.get(\"temperature\", 0.7),\n\t\t\tmax_tokens=llm_config.get(\"max_tokens\", 1024),\n\t\t)\n\n\t\t# Process responses and update groups\n\t\tfor i, (response, group) in enumerate(zip(responses, groups, strict=False)):\n\t\t\ttry:\n\t\t\t\t# Extract content from response\n\t\t\t\tif response and hasattr(response, \"choices\") and response.choices:\n\t\t\t\t\tcontent = response.choices[0].message.content\n\n\t\t\t\t\t# If it's JSON, extract the message\n\t\t\t\t\tif content.startswith(\"{\") and content.endswith(\"}\"):\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t# Check if it's in the {\"commit_message\": \"...\"} format\n\t\t\t\t\t\t\tjson_data = json.loads(content)\n\t\t\t\t\t\t\tif \"commit_message\" in json_data:\n\t\t\t\t\t\t\t\t# Extract just the commit message\n\t\t\t\t\t\t\t\tcontent = json_data[\"commit_message\"]\n\t\t\t\t\t\t\telif \"message\" in json_data:\n\t\t\t\t\t\t\t\t# Extract message from {\"message\": \"...\"} format\n\t\t\t\t\t\t\t\tcontent = json_data[\"message\"]\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t# Use the formatter for conventional format\n\t\t\t\t\t\t\t\tcontent = format_commit_json(content)\n\t\t\t\t\t\texcept Exception:\n\t\t\t\t\t\t\tlogger.exception(\"Error formatting JSON to commit message\")\n\n\t\t\t\t\t# Set the message on the group\n\t\t\t\t\tgroup.message = content\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(f\"Empty or invalid response for group {i}\")\n\t\t\t\t\tgroup.message = f\"update: changes to {len(group.files)} files\"\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(f\"Error processing response for group {i}\")\n\t\t\t\tgroup.message = f\"update: changes to {len(group.files)} files\"\n\n\texcept Exception:\n\t\tlogger.exception(\"Batch completion failed\")\n\t\t# Just use the already initialized UI\n\t\tui.show_warning(\"LLM call failed. Using fallback commit messages.\")\n\n\t\t# Provide fallback messages for all groups\n\t\tfor group in groups:\n\t\t\tif not group.message:  # Don't override if already set\n\t\t\t\tfallback_msg = f\"update: changes to {len(group.files)} files\"\n\t\t\t\tgroup.message = fallback_msg\n\t\t\t\t# Log which groups received fallback messages\n\t\t\t\tlogger.warning(f\"Using fallback message for files: {group.files}\")\n\n\treturn groups\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/","title":"Clusterer","text":"<p>Module for clustering diff chunks based on their embeddings.</p> <p>This module provides functionality to group related code changes together based on their semantic similarity, using vector embeddings and clustering algorithms. The clustering process helps identify related changes that should be committed together.</p> <p>Key components: - DiffClusterer: Main class that implements clustering algorithms for diff chunks - ClusteringParams: Type definition for parameters used by clustering algorithms</p> <p>The module supports multiple clustering methods: 1. Agglomerative (hierarchical) clustering: Builds a hierarchy of clusters based on distances    between embeddings, using a distance threshold to determine final cluster boundaries 2. DBSCAN: Density-based clustering that groups points in high-density regions,    treating low-density points as noise/outliers</p>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams","title":"ClusteringParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for clustering algorithm parameters.</p> <p>These parameters configure the behavior of the clustering algorithms:</p> <p>For agglomerative clustering: - n_clusters: Optional limit on number of clusters (None means no limit) - distance_threshold: Maximum distance for clusters to be merged (lower = more clusters) - metric: Distance metric to use (e.g., \"precomputed\" for precomputed distance matrix) - linkage: Strategy for calculating distances between clusters (\"average\", \"single\", etc.)</p> <p>For DBSCAN: - eps: Maximum distance between points in the same neighborhood - min_samples: Minimum points required to form a dense region - metric: Distance metric to use</p> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>class ClusteringParams(TypedDict, total=False):\n\t\"\"\"\n\tType definition for clustering algorithm parameters.\n\n\tThese parameters configure the behavior of the clustering algorithms:\n\n\tFor agglomerative clustering:\n\t- n_clusters: Optional limit on number of clusters (None means no limit)\n\t- distance_threshold: Maximum distance for clusters to be merged (lower = more clusters)\n\t- metric: Distance metric to use (e.g., \"precomputed\" for precomputed distance matrix)\n\t- linkage: Strategy for calculating distances between clusters (\"average\", \"single\", etc.)\n\n\tFor DBSCAN:\n\t- eps: Maximum distance between points in the same neighborhood\n\t- min_samples: Minimum points required to form a dense region\n\t- metric: Distance metric to use\n\n\t\"\"\"\n\n\tn_clusters: int | None\n\tdistance_threshold: float | None\n\tmetric: str\n\tlinkage: str\n\teps: float\n\tmin_samples: int\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.n_clusters","title":"n_clusters  <code>instance-attribute</code>","text":"<pre><code>n_clusters: int | None\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.distance_threshold","title":"distance_threshold  <code>instance-attribute</code>","text":"<pre><code>distance_threshold: float | None\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.metric","title":"metric  <code>instance-attribute</code>","text":"<pre><code>metric: str\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.linkage","title":"linkage  <code>instance-attribute</code>","text":"<pre><code>linkage: str\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps: float\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.ClusteringParams.min_samples","title":"min_samples  <code>instance-attribute</code>","text":"<pre><code>min_samples: int\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer","title":"DiffClusterer","text":"<p>Clusters diff chunks based on their semantic embeddings.</p> <p>This class provides methods to group related code changes by their semantic similarity, using vector embeddings and standard clustering algorithms from scikit-learn.</p> <p>Clustering helps identify code changes that are related to each other and should be grouped in the same commit, even if they appear in different files.</p> <p>The class supports multiple clustering algorithms: 1. Agglomerative clustering: Hierarchical clustering that's good for finding natural    groupings without needing to specify the exact number of clusters 2. DBSCAN: Density-based clustering that can identify outliers and works well with    irregularly shaped clusters</p> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>class DiffClusterer:\n\t\"\"\"\n\tClusters diff chunks based on their semantic embeddings.\n\n\tThis class provides methods to group related code changes by their semantic similarity,\n\tusing vector embeddings and standard clustering algorithms from scikit-learn.\n\n\tClustering helps identify code changes that are related to each other and should be\n\tgrouped in the same commit, even if they appear in different files.\n\n\tThe class supports multiple clustering algorithms:\n\t1. Agglomerative clustering: Hierarchical clustering that's good for finding natural\n\t   groupings without needing to specify the exact number of clusters\n\t2. DBSCAN: Density-based clustering that can identify outliers and works well with\n\t   irregularly shaped clusters\n\n\t\"\"\"\n\n\tdef __init__(self, method: str = \"agglomerative\", **kwargs: object) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the clusterer.\n\n\t\tArgs:\n\t\t    method: Clustering method to use. Options:\n\t\t        - \"agglomerative\": Hierarchical clustering (default)\n\t\t        - \"dbscan\": Density-based spatial clustering\n\t\t    **kwargs: Additional parameters for the clustering algorithm:\n\t\t        - For agglomerative: distance_threshold, linkage, etc.\n\t\t        - For DBSCAN: eps, min_samples, etc.\n\n\t\tRaises:\n\t\t    ImportError: If scikit-learn is not installed\n\n\t\t\"\"\"\n\t\tself.method = method\n\t\tself.kwargs = kwargs\n\n\t\t# Import here to avoid making sklearn a hard dependency\n\t\ttry:\n\t\t\tfrom sklearn.cluster import DBSCAN, AgglomerativeClustering\n\t\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\t\tself.AgglomerativeClustering = AgglomerativeClustering\n\t\t\tself.DBSCAN = DBSCAN\n\t\t\tself.cosine_similarity = cosine_similarity\n\t\texcept ImportError as e:\n\t\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\t\tmsg = \"scikit-learn is required for clustering\"\n\t\t\traise ImportError(msg) from e\n\n\tdef cluster(self, chunk_embeddings: list[tuple[DiffChunk, np.ndarray]]) -&gt; list[list[DiffChunk]]:\n\t\t\"\"\"\n\t\tCluster chunks based on their embeddings.\n\n\t\t              Process:\n\t\t              1. Extracts chunks and embeddings from input tuples\n\t\t              2. Computes a similarity matrix using cosine similarity\n\t\t              3. Converts similarity to distance matrix (1 - similarity)\n\t\t              4. Applies clustering algorithm based on the chosen method\n\t\t              5. Organizes chunks into clusters based on labels\n\t\t              6. Handles special cases like noise points in DBSCAN\n\n\t\tArgs:\n\t\t    chunk_embeddings: List of (chunk, embedding) tuples where each embedding\n\t\t        is a numpy array representing the semantic vector of a code chunk\n\n\t\tReturns:\n\t\t    List of lists, where each inner list contains chunks in the same cluster.\n\t\t    With DBSCAN, noise points (label -1) are returned as individual single-item clusters.\n\n\t\tExamples:\n\t\t    &gt;&gt;&gt; embedder = DiffEmbedder()\n\t\t    &gt;&gt;&gt; chunk_embeddings = embedder.embed_chunks(diff_chunks)\n\t\t    &gt;&gt;&gt; clusterer = DiffClusterer(method=\"agglomerative\", distance_threshold=0.5)\n\t\t    &gt;&gt;&gt; clusters = clusterer.cluster(chunk_embeddings)\n\t\t    &gt;&gt;&gt; for i, cluster in enumerate(clusters):\n\t\t    ...     print(f\"Cluster {i} has {len(cluster)} chunks\")\n\n\t\t\"\"\"\n\t\tif not chunk_embeddings:\n\t\t\treturn []\n\n\t\t# Extract chunks and embeddings\n\t\tchunks = [ce[0] for ce in chunk_embeddings]\n\t\tembeddings = np.array([ce[1] for ce in chunk_embeddings])\n\n\t\t# Compute similarity matrix (1 - cosine distance)\n\t\tsimilarity_matrix = self.cosine_similarity(embeddings)\n\n\t\t# Convert to distance matrix (1 - similarity)\n\t\tdistance_matrix = 1 - similarity_matrix\n\n\t\t# Apply clustering\n\t\tif self.method == \"agglomerative\":\n\t\t\t# Default parameters if not provided\n\t\t\tparams = {\n\t\t\t\t\"n_clusters\": None,\n\t\t\t\t\"distance_threshold\": 0.5,  # Threshold for cluster formation (0.5 = moderate similarity)\n\t\t\t\t\"metric\": \"precomputed\",  # Use metric instead of affinity\n\t\t\t\t\"linkage\": \"average\",  # Use average linkage for balanced clusters\n\t\t\t}\n\t\t\tparams.update(self.kwargs)\n\n\t\t\tclustering = self.AgglomerativeClustering(**params)\n\t\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\t\telif self.method == \"dbscan\":\n\t\t\t# Default parameters if not provided\n\t\t\tparams = {\n\t\t\t\t\"eps\": 0.3,  # Maximum distance between points in neighborhood (0.3 = high similarity required)\n\t\t\t\t\"min_samples\": 2,  # Minimum points to form a dense region\n\t\t\t\t\"metric\": \"precomputed\",  # Using precomputed distance matrix\n\t\t\t}\n\t\t\tparams.update(self.kwargs)\n\n\t\t\tclustering = self.DBSCAN(**params)\n\t\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\t\telse:\n\t\t\tmsg = f\"Unsupported clustering method: {self.method}\"\n\t\t\traise ValueError(msg)\n\n\t\t# Group chunks by cluster label\n\t\tclusters: dict[int, list[DiffChunk]] = {}\n\t\tfor i, label in enumerate(labels):\n\t\t\t# Convert numpy integer to Python int\n\t\t\tlabel_key = int(label)\n\t\t\tif label_key not in clusters:\n\t\t\t\tclusters[label_key] = []\n\t\t\tclusters[label_key].append(chunks[i])\n\n\t\t# Convert to list of lists and handle noise points (-1 label in DBSCAN)\n\t\tresult: list[list[DiffChunk]] = []\n\t\tfor label, cluster_chunks in sorted(clusters.items()):\n\t\t\tif label != -1:  # Regular cluster\n\t\t\t\tresult.append(cluster_chunks)\n\t\t\telse:  # Noise points - each forms its own cluster\n\t\t\t\tresult.extend([[chunk] for chunk in cluster_chunks])\n\n\t\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.__init__","title":"__init__","text":"<pre><code>__init__(\n\tmethod: str = \"agglomerative\", **kwargs: object\n) -&gt; None\n</code></pre> <p>Initialize the clusterer.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Clustering method to use. Options: - \"agglomerative\": Hierarchical clustering (default) - \"dbscan\": Density-based spatial clustering</p> <code>'agglomerative'</code> <code>**kwargs</code> <code>object</code> <p>Additional parameters for the clustering algorithm: - For agglomerative: distance_threshold, linkage, etc. - For DBSCAN: eps, min_samples, etc.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If scikit-learn is not installed</p> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>def __init__(self, method: str = \"agglomerative\", **kwargs: object) -&gt; None:\n\t\"\"\"\n\tInitialize the clusterer.\n\n\tArgs:\n\t    method: Clustering method to use. Options:\n\t        - \"agglomerative\": Hierarchical clustering (default)\n\t        - \"dbscan\": Density-based spatial clustering\n\t    **kwargs: Additional parameters for the clustering algorithm:\n\t        - For agglomerative: distance_threshold, linkage, etc.\n\t        - For DBSCAN: eps, min_samples, etc.\n\n\tRaises:\n\t    ImportError: If scikit-learn is not installed\n\n\t\"\"\"\n\tself.method = method\n\tself.kwargs = kwargs\n\n\t# Import here to avoid making sklearn a hard dependency\n\ttry:\n\t\tfrom sklearn.cluster import DBSCAN, AgglomerativeClustering\n\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\tself.AgglomerativeClustering = AgglomerativeClustering\n\t\tself.DBSCAN = DBSCAN\n\t\tself.cosine_similarity = cosine_similarity\n\texcept ImportError as e:\n\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\tmsg = \"scikit-learn is required for clustering\"\n\t\traise ImportError(msg) from e\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method = method\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.AgglomerativeClustering","title":"AgglomerativeClustering  <code>instance-attribute</code>","text":"<pre><code>AgglomerativeClustering = AgglomerativeClustering\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.DBSCAN","title":"DBSCAN  <code>instance-attribute</code>","text":"<pre><code>DBSCAN = DBSCAN\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.cosine_similarity","title":"cosine_similarity  <code>instance-attribute</code>","text":"<pre><code>cosine_similarity = cosine_similarity\n</code></pre>"},{"location":"api/git/semantic_grouping/clusterer/#codemap.git.semantic_grouping.clusterer.DiffClusterer.cluster","title":"cluster","text":"<pre><code>cluster(\n\tchunk_embeddings: list[tuple[DiffChunk, ndarray]],\n) -&gt; list[list[DiffChunk]]\n</code></pre> <p>Cluster chunks based on their embeddings.</p> <pre><code>          Process:\n          1. Extracts chunks and embeddings from input tuples\n          2. Computes a similarity matrix using cosine similarity\n          3. Converts similarity to distance matrix (1 - similarity)\n          4. Applies clustering algorithm based on the chosen method\n          5. Organizes chunks into clusters based on labels\n          6. Handles special cases like noise points in DBSCAN\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_embeddings</code> <code>list[tuple[DiffChunk, ndarray]]</code> <p>List of (chunk, embedding) tuples where each embedding is a numpy array representing the semantic vector of a code chunk</p> required <p>Returns:</p> Type Description <code>list[list[DiffChunk]]</code> <p>List of lists, where each inner list contains chunks in the same cluster.</p> <code>list[list[DiffChunk]]</code> <p>With DBSCAN, noise points (label -1) are returned as individual single-item clusters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; embedder = DiffEmbedder()\n&gt;&gt;&gt; chunk_embeddings = embedder.embed_chunks(diff_chunks)\n&gt;&gt;&gt; clusterer = DiffClusterer(method=\"agglomerative\", distance_threshold=0.5)\n&gt;&gt;&gt; clusters = clusterer.cluster(chunk_embeddings)\n&gt;&gt;&gt; for i, cluster in enumerate(clusters):\n...     print(f\"Cluster {i} has {len(cluster)} chunks\")\n</code></pre> Source code in <code>src/codemap/git/semantic_grouping/clusterer.py</code> <pre><code>def cluster(self, chunk_embeddings: list[tuple[DiffChunk, np.ndarray]]) -&gt; list[list[DiffChunk]]:\n\t\"\"\"\n\tCluster chunks based on their embeddings.\n\n\t              Process:\n\t              1. Extracts chunks and embeddings from input tuples\n\t              2. Computes a similarity matrix using cosine similarity\n\t              3. Converts similarity to distance matrix (1 - similarity)\n\t              4. Applies clustering algorithm based on the chosen method\n\t              5. Organizes chunks into clusters based on labels\n\t              6. Handles special cases like noise points in DBSCAN\n\n\tArgs:\n\t    chunk_embeddings: List of (chunk, embedding) tuples where each embedding\n\t        is a numpy array representing the semantic vector of a code chunk\n\n\tReturns:\n\t    List of lists, where each inner list contains chunks in the same cluster.\n\t    With DBSCAN, noise points (label -1) are returned as individual single-item clusters.\n\n\tExamples:\n\t    &gt;&gt;&gt; embedder = DiffEmbedder()\n\t    &gt;&gt;&gt; chunk_embeddings = embedder.embed_chunks(diff_chunks)\n\t    &gt;&gt;&gt; clusterer = DiffClusterer(method=\"agglomerative\", distance_threshold=0.5)\n\t    &gt;&gt;&gt; clusters = clusterer.cluster(chunk_embeddings)\n\t    &gt;&gt;&gt; for i, cluster in enumerate(clusters):\n\t    ...     print(f\"Cluster {i} has {len(cluster)} chunks\")\n\n\t\"\"\"\n\tif not chunk_embeddings:\n\t\treturn []\n\n\t# Extract chunks and embeddings\n\tchunks = [ce[0] for ce in chunk_embeddings]\n\tembeddings = np.array([ce[1] for ce in chunk_embeddings])\n\n\t# Compute similarity matrix (1 - cosine distance)\n\tsimilarity_matrix = self.cosine_similarity(embeddings)\n\n\t# Convert to distance matrix (1 - similarity)\n\tdistance_matrix = 1 - similarity_matrix\n\n\t# Apply clustering\n\tif self.method == \"agglomerative\":\n\t\t# Default parameters if not provided\n\t\tparams = {\n\t\t\t\"n_clusters\": None,\n\t\t\t\"distance_threshold\": 0.5,  # Threshold for cluster formation (0.5 = moderate similarity)\n\t\t\t\"metric\": \"precomputed\",  # Use metric instead of affinity\n\t\t\t\"linkage\": \"average\",  # Use average linkage for balanced clusters\n\t\t}\n\t\tparams.update(self.kwargs)\n\n\t\tclustering = self.AgglomerativeClustering(**params)\n\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\telif self.method == \"dbscan\":\n\t\t# Default parameters if not provided\n\t\tparams = {\n\t\t\t\"eps\": 0.3,  # Maximum distance between points in neighborhood (0.3 = high similarity required)\n\t\t\t\"min_samples\": 2,  # Minimum points to form a dense region\n\t\t\t\"metric\": \"precomputed\",  # Using precomputed distance matrix\n\t\t}\n\t\tparams.update(self.kwargs)\n\n\t\tclustering = self.DBSCAN(**params)\n\t\tlabels = clustering.fit_predict(distance_matrix)\n\n\telse:\n\t\tmsg = f\"Unsupported clustering method: {self.method}\"\n\t\traise ValueError(msg)\n\n\t# Group chunks by cluster label\n\tclusters: dict[int, list[DiffChunk]] = {}\n\tfor i, label in enumerate(labels):\n\t\t# Convert numpy integer to Python int\n\t\tlabel_key = int(label)\n\t\tif label_key not in clusters:\n\t\t\tclusters[label_key] = []\n\t\tclusters[label_key].append(chunks[i])\n\n\t# Convert to list of lists and handle noise points (-1 label in DBSCAN)\n\tresult: list[list[DiffChunk]] = []\n\tfor label, cluster_chunks in sorted(clusters.items()):\n\t\tif label != -1:  # Regular cluster\n\t\t\tresult.append(cluster_chunks)\n\t\telse:  # Noise points - each forms its own cluster\n\t\t\tresult.extend([[chunk] for chunk in cluster_chunks])\n\n\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/","title":"Context Processor","text":"<p>Context processing utilities for LLM prompts.</p> <p>This module provides functionality to process and format code contexts for LLM prompts using tree-sitter analysis and Level of Detail (LOD) to optimize context length while preserving meaningful content.</p>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.DEFAULT_MAX_TOKENS","title":"DEFAULT_MAX_TOKENS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_MAX_TOKENS = 4000\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.CHUNK_TOKEN_ESTIMATE","title":"CHUNK_TOKEN_ESTIMATE  <code>module-attribute</code>","text":"<pre><code>CHUNK_TOKEN_ESTIMATE = 500\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.MAX_CHUNKS","title":"MAX_CHUNKS  <code>module-attribute</code>","text":"<pre><code>MAX_CHUNKS = 6\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.MAX_SIMPLE_CHUNKS","title":"MAX_SIMPLE_CHUNKS  <code>module-attribute</code>","text":"<pre><code>MAX_SIMPLE_CHUNKS = 3\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.process_chunks_with_lod","title":"process_chunks_with_lod","text":"<pre><code>process_chunks_with_lod(\n\tchunks: list[DiffChunk],\n\tmax_tokens: int = DEFAULT_MAX_TOKENS,\n) -&gt; str\n</code></pre> <p>Process diff chunks using LOD to create optimized context for LLM prompts.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of diff chunks to process</p> required <code>max_tokens</code> <code>int</code> <p>Maximum tokens allowed in the formatted context</p> <code>DEFAULT_MAX_TOKENS</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown context optimized for token usage</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def process_chunks_with_lod(chunks: list[DiffChunk], max_tokens: int = DEFAULT_MAX_TOKENS) -&gt; str:\n\t\"\"\"\n\tProcess diff chunks using LOD to create optimized context for LLM prompts.\n\n\tArgs:\n\t    chunks: List of diff chunks to process\n\t    max_tokens: Maximum tokens allowed in the formatted context\n\n\tReturns:\n\t    Formatted markdown context optimized for token usage\n\n\t\"\"\"\n\t# If chunks list is small, we might not need LOD processing\n\tif len(chunks) &lt;= MAX_SIMPLE_CHUNKS:\n\t\treturn format_regular_chunks(chunks[:MAX_CHUNKS])\n\n\t# Set up LOD generator and estimate number of chunks we can include\n\tlod_generator = LODGenerator()\n\testimated_chunk_count = min(max_tokens // CHUNK_TOKEN_ESTIMATE, len(chunks))\n\tprioritized_chunks = prioritize_chunks(chunks, min(estimated_chunk_count, MAX_CHUNKS))\n\n\t# Start with highest LOD level and progressively reduce if needed\n\tlod_levels = [LODLevel.STRUCTURE, LODLevel.SIGNATURES]\n\tformatted_chunks = []\n\tcurrent_level_index = 0\n\n\twhile current_level_index &lt; len(lod_levels):\n\t\tcurrent_level = lod_levels[current_level_index]\n\t\tformatted_chunks = []\n\n\t\tfor chunk in prioritized_chunks:\n\t\t\t# Get file paths from chunk\n\t\t\tfile_paths = get_file_paths_from_chunk(chunk)\n\n\t\t\tif not file_paths:\n\t\t\t\t# If we can't extract paths, use regular formatting for this chunk\n\t\t\t\tformatted_chunks.append(format_chunk(chunk))\n\t\t\t\tcontinue\n\n\t\t\t# Process each file in the chunk with LOD\n\t\t\tlod_formatted = []\n\t\t\tfor file_path in file_paths:\n\t\t\t\tpath = Path(file_path)\n\t\t\t\tif not path.exists():\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Generate LOD representation\n\t\t\t\tlod_entity = lod_generator.generate_lod(path, level=current_level)\n\t\t\t\tif lod_entity:\n\t\t\t\t\tlod_formatted.append(format_lod_entity(lod_entity, file_path, current_level))\n\n\t\t\tif lod_formatted:\n\t\t\t\tformatted_chunks.append(\"\\n\".join(lod_formatted))\n\t\t\telse:\n\t\t\t\t# Fallback to regular formatting\n\t\t\t\tformatted_chunks.append(format_chunk(chunk))\n\n\t\t# Estimate if we're within token limit\n\t\ttotal_context = \"\\n\\n\".join(formatted_chunks)\n\t\testimated_tokens = estimate_tokens(total_context)\n\n\t\tif estimated_tokens &lt;= max_tokens or current_level_index == len(lod_levels) - 1:\n\t\t\tbreak\n\n\t\t# Try with lower LOD level\n\t\tcurrent_level_index += 1\n\n\t# If we still exceed the token limit, truncate\n\ttotal_context = \"\\n\\n\".join(formatted_chunks)\n\tif estimate_tokens(total_context) &gt; max_tokens:\n\t\ttotal_context = truncate_context(total_context, max_tokens)\n\n\treturn total_context\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.prioritize_chunks","title":"prioritize_chunks","text":"<pre><code>prioritize_chunks(\n\tchunks: list[DiffChunk], max_count: int\n) -&gt; list[DiffChunk]\n</code></pre> <p>Prioritize chunks based on heuristics (file types, changes, etc.).</p> <p>This is a simple implementation that could be extended with more sophisticated dissimilarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of chunks to prioritize</p> required <code>max_count</code> <code>int</code> <p>Maximum number of chunks to return</p> required <p>Returns:</p> Type Description <code>list[DiffChunk]</code> <p>Prioritized list of chunks</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def prioritize_chunks(chunks: list[DiffChunk], max_count: int) -&gt; list[DiffChunk]:\n\t\"\"\"\n\tPrioritize chunks based on heuristics (file types, changes, etc.).\n\n\tThis is a simple implementation that could be extended with more\n\tsophisticated dissimilarity metrics.\n\n\tArgs:\n\t    chunks: List of chunks to prioritize\n\t    max_count: Maximum number of chunks to return\n\n\tReturns:\n\t    Prioritized list of chunks\n\n\t\"\"\"\n\t# Simple heuristics for now:\n\t# 1. Prefer chunks with code files over non-code files\n\t# 2. Prefer chunks with more files (more central changes)\n\t# 3. Prefer chunks with more added/changed lines\n\n\tdef chunk_score(chunk: DiffChunk) -&gt; float:\n\t\t\"\"\"Calculates a priority score for a diff chunk based on heuristics.\n\n\t\tThe score is calculated using three factors:\n\t\t1. Presence of code files (60% weight)\n\t\t2. Number of files affected (20% weight)\n\t\t3. Size of content changes (20% weight)\n\n\t\tArgs:\n\t\t\tchunk: The diff chunk to score\n\n\t\tReturns:\n\t\t\tfloat: A score between 0 and 1 representing the chunk's priority\n\t\t\"\"\"\n\t\t# Check if any files are code files\n\t\tcode_file_score = 0\n\t\tfor file in chunk.files:\n\t\t\tif any(file.endswith(ext) for ext in [\".py\", \".js\", \".ts\", \".java\", \".c\", \".cpp\", \".go\"]):\n\t\t\t\tcode_file_score = 1\n\t\t\t\tbreak\n\n\t\t# Score based on number of files\n\t\tfile_count_score = min(len(chunk.files), 3) / 3\n\n\t\t# Score based on content size (as proxy for changes)\n\t\tcontent_score = min(len(chunk.content), 1000) / 1000\n\n\t\treturn code_file_score * 0.6 + file_count_score * 0.2 + content_score * 0.2\n\n\t# Sort chunks by score and return top max_count\n\treturn sorted(chunks, key=chunk_score, reverse=True)[:max_count]\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.get_file_paths_from_chunk","title":"get_file_paths_from_chunk","text":"<pre><code>get_file_paths_from_chunk(chunk: DiffChunk) -&gt; list[str]\n</code></pre> <p>Extract file paths from a diff chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>The diff chunk to process</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of file paths</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def get_file_paths_from_chunk(chunk: DiffChunk) -&gt; list[str]:\n\t\"\"\"\n\tExtract file paths from a diff chunk.\n\n\tArgs:\n\t    chunk: The diff chunk to process\n\n\tReturns:\n\t    List of file paths\n\n\t\"\"\"\n\treturn [file for file in chunk.files if file]\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_lod_entity","title":"format_lod_entity","text":"<pre><code>format_lod_entity(\n\tentity: LODEntity, file_path: str, level: LODLevel\n) -&gt; str\n</code></pre> <p>Format an LOD entity as GitHub-flavored markdown.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>LODEntity</code> <p>The LOD entity to format</p> required <code>file_path</code> <code>str</code> <p>Path to the source file</p> required <code>level</code> <code>LODLevel</code> <p>LOD level used</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown string</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_lod_entity(entity: LODEntity, file_path: str, level: LODLevel) -&gt; str:\n\t\"\"\"\n\tFormat an LOD entity as GitHub-flavored markdown.\n\n\tArgs:\n\t    entity: The LOD entity to format\n\t    file_path: Path to the source file\n\t    level: LOD level used\n\n\tReturns:\n\t    Formatted markdown string\n\n\t\"\"\"\n\t# Start with file header\n\tresult = f\"## {file_path}\\n\\n\"\n\n\t# Format the entity based on LOD level\n\tif level == LODLevel.STRUCTURE:\n\t\tresult += format_entity_structure(entity, 0)\n\telif level == LODLevel.SIGNATURES:\n\t\tresult += format_entity_signatures(entity, 0)\n\n\treturn result\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_entity_structure","title":"format_entity_structure","text":"<pre><code>format_entity_structure(\n\tentity: LODEntity, indent: int\n) -&gt; str\n</code></pre> <p>Format entity with structure (signatures and hierarchy).</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_entity_structure(entity: LODEntity, indent: int) -&gt; str:\n\t\"\"\"Format entity with structure (signatures and hierarchy).\"\"\"\n\tindent_str = \"  \" * indent\n\tresult = f\"{indent_str}- **{entity.entity_type.name}**: `{entity.name}`\"\n\n\tif entity.signature:\n\t\tresult += f\"\\n{indent_str}  ```\\n{indent_str}  {entity.signature}\\n{indent_str}  ```\"\n\n\tif entity.children:\n\t\tresult += \"\\n\"\n\t\tfor child in entity.children:\n\t\t\tresult += format_entity_structure(child, indent + 1)\n\n\treturn result + \"\\n\"\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_entity_signatures","title":"format_entity_signatures","text":"<pre><code>format_entity_signatures(\n\tentity: LODEntity, indent: int\n) -&gt; str\n</code></pre> <p>Format entity with just signatures.</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_entity_signatures(entity: LODEntity, indent: int) -&gt; str:\n\t\"\"\"Format entity with just signatures.\"\"\"\n\tindent_str = \"  \" * indent\n\tresult = f\"{indent_str}- **{entity.entity_type.name}**: `{entity.name}`\"\n\n\tif entity.signature:\n\t\tresult += f\" - `{entity.signature}`\"\n\n\tif entity.children:\n\t\tresult += \"\\n\"\n\t\tfor child in entity.children:\n\t\t\tresult += format_entity_signatures(child, indent + 1)\n\n\treturn result + \"\\n\"\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_regular_chunks","title":"format_regular_chunks","text":"<pre><code>format_regular_chunks(chunks: list[DiffChunk]) -&gt; str\n</code></pre> <p>Format chunks using the regular approach when LOD is not necessary.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of chunks to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown string</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_regular_chunks(chunks: list[DiffChunk]) -&gt; str:\n\t\"\"\"\n\tFormat chunks using the regular approach when LOD is not necessary.\n\n\tArgs:\n\t    chunks: List of chunks to format\n\n\tReturns:\n\t    Formatted markdown string\n\n\t\"\"\"\n\tformatted_chunks = [format_chunk(chunk) for chunk in chunks]\n\treturn \"\\n\\n\".join(formatted_chunks)\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.format_chunk","title":"format_chunk","text":"<pre><code>format_chunk(chunk: DiffChunk) -&gt; str\n</code></pre> <p>Format a single diff chunk as markdown.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>The diff chunk to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown string</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def format_chunk(chunk: DiffChunk) -&gt; str:\n\t\"\"\"\n\tFormat a single diff chunk as markdown.\n\n\tArgs:\n\t    chunk: The diff chunk to format\n\n\tReturns:\n\t    Formatted markdown string\n\n\t\"\"\"\n\t# Format file paths\n\tfile_section = \"## Files\\n\"\n\tfor file in chunk.files:\n\t\tif file:\n\t\t\tfile_section += f\"- {file}\\n\"\n\n\t# Format content\n\tcontent_section = \"### Changes\\n```diff\\n\" + chunk.content + \"\\n```\"\n\n\treturn file_section + \"\\n\" + content_section\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate the number of tokens in a text.</p> <p>This is a simple estimation that can be improved with actual tokenizer implementations if needed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to estimate tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def estimate_tokens(text: str) -&gt; int:\n\t\"\"\"\n\tEstimate the number of tokens in a text.\n\n\tThis is a simple estimation that can be improved with\n\tactual tokenizer implementations if needed.\n\n\tArgs:\n\t    text: Text to estimate tokens for\n\n\tReturns:\n\t    Estimated token count\n\n\t\"\"\"\n\t# Simple estimation: 4 characters per token on average\n\treturn len(text) // 4\n</code></pre>"},{"location":"api/git/semantic_grouping/context_processor/#codemap.git.semantic_grouping.context_processor.truncate_context","title":"truncate_context","text":"<pre><code>truncate_context(context: str, max_tokens: int) -&gt; str\n</code></pre> <p>Truncate context to fit within token limit.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>Context to truncate</p> required <code>max_tokens</code> <code>int</code> <p>Maximum allowed tokens</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated context</p> Source code in <code>src/codemap/git/semantic_grouping/context_processor.py</code> <pre><code>def truncate_context(context: str, max_tokens: int) -&gt; str:\n\t\"\"\"\n\tTruncate context to fit within token limit.\n\n\tArgs:\n\t    context: Context to truncate\n\t    max_tokens: Maximum allowed tokens\n\n\tReturns:\n\t    Truncated context\n\n\t\"\"\"\n\t# Simple truncation by estimating tokens\n\tif estimate_tokens(context) &lt;= max_tokens:\n\t\treturn context\n\n\t# Split into chunks and preserve as many complete chunks as possible\n\tchunks = context.split(\"\\n\\n\")\n\tresult_chunks = []\n\tcurrent_token_count = 0\n\n\tfor chunk in chunks:\n\t\tchunk_tokens = estimate_tokens(chunk)\n\t\tif current_token_count + chunk_tokens &lt;= max_tokens - 100:  # Reserve 100 tokens for truncation marker\n\t\t\tresult_chunks.append(chunk)\n\t\t\tcurrent_token_count += chunk_tokens\n\t\telse:\n\t\t\t# Add truncation marker and stop\n\t\t\tresult_chunks.append(\"\\n\\n[...TRUNCATED...]\\n\\n\")\n\t\t\tbreak\n\n\treturn \"\\n\\n\".join(result_chunks)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/","title":"Embedder","text":"<p>Module for generating embeddings from diff chunks.</p>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder","title":"DiffEmbedder","text":"<p>Generates embeddings for diff chunks.</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>class DiffEmbedder:\n\t\"\"\"Generates embeddings for diff chunks.\"\"\"\n\n\tdef __init__(self, model_name: str = \"all-MiniLM-L6-v2\") -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the embedder with a specific model.\n\n\t\tArgs:\n\t\t    model_name: Name of the sentence-transformers model to use\n\n\t\t\"\"\"\n\t\t# Import here to avoid making sentence-transformers a hard dependency\n\t\ttry:\n\t\t\tfrom sentence_transformers import SentenceTransformer\n\n\t\t\tself.model = SentenceTransformer(model_name)\n\t\texcept ImportError as e:\n\t\t\tlogger.exception(\n\t\t\t\t\"Failed to import sentence-transformers. Please install it with: uv add sentence-transformers\"\n\t\t\t)\n\t\t\tmsg = \"sentence-transformers is required for semantic grouping\"\n\t\t\traise ImportError(msg) from e\n\n\tdef preprocess_diff(self, diff_text: str) -&gt; str:\n\t\t\"\"\"\n\t\tPreprocess diff text to make it more suitable for embedding.\n\n\t\tArgs:\n\t\t    diff_text: Raw diff text\n\n\t\tReturns:\n\t\t    Preprocessed text\n\n\t\t\"\"\"\n\t\t# Remove diff headers, line numbers, etc.\n\t\t# Focus on actual content changes\n\t\tlines = []\n\t\tfor line in diff_text.splitlines():\n\t\t\t# Skip diff metadata lines\n\t\t\tif line.startswith((\"diff --git\", \"index \", \"+++\", \"---\")):\n\t\t\t\tcontinue\n\n\t\t\t# Keep actual content changes, removing the +/- prefix\n\t\t\tif line.startswith((\"+\", \"-\", \" \")):\n\t\t\t\tlines.append(line[1:])\n\n\t\treturn \"\\n\".join(lines)\n\n\tdef embed_chunk(self, chunk: DiffChunk) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tGenerate an embedding for a diff chunk.\n\n\t\tArgs:\n\t\t    chunk: DiffChunk object\n\n\t\tReturns:\n\t\t    numpy.ndarray: Embedding vector\n\n\t\t\"\"\"\n\t\t# Get the diff content from the chunk\n\t\tdiff_text = chunk.content\n\n\t\t# Preprocess the diff text\n\t\tprocessed_text = self.preprocess_diff(diff_text)\n\n\t\t# If the processed text is empty, use the file paths as context\n\t\tif not processed_text.strip():\n\t\t\tprocessed_text = \" \".join(chunk.files)\n\n\t\t# Generate the embedding and convert to numpy array\n\t\tembedding = self.model.encode(processed_text)\n\t\treturn np.array(embedding)\n\n\tdef embed_chunks(self, chunks: list[DiffChunk]) -&gt; list[tuple[DiffChunk, np.ndarray]]:\n\t\t\"\"\"\n\t\tGenerate embeddings for multiple chunks.\n\n\t\tArgs:\n\t\t    chunks: List of DiffChunk objects\n\n\t\tReturns:\n\t\t    List of (chunk, embedding) tuples\n\n\t\t\"\"\"\n\t\treturn [(chunk, self.embed_chunk(chunk)) for chunk in chunks]\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.__init__","title":"__init__","text":"<pre><code>__init__(model_name: str = 'all-MiniLM-L6-v2') -&gt; None\n</code></pre> <p>Initialize the embedder with a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the sentence-transformers model to use</p> <code>'all-MiniLM-L6-v2'</code> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>def __init__(self, model_name: str = \"all-MiniLM-L6-v2\") -&gt; None:\n\t\"\"\"\n\tInitialize the embedder with a specific model.\n\n\tArgs:\n\t    model_name: Name of the sentence-transformers model to use\n\n\t\"\"\"\n\t# Import here to avoid making sentence-transformers a hard dependency\n\ttry:\n\t\tfrom sentence_transformers import SentenceTransformer\n\n\t\tself.model = SentenceTransformer(model_name)\n\texcept ImportError as e:\n\t\tlogger.exception(\n\t\t\t\"Failed to import sentence-transformers. Please install it with: uv add sentence-transformers\"\n\t\t)\n\t\tmsg = \"sentence-transformers is required for semantic grouping\"\n\t\traise ImportError(msg) from e\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = SentenceTransformer(model_name)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.preprocess_diff","title":"preprocess_diff","text":"<pre><code>preprocess_diff(diff_text: str) -&gt; str\n</code></pre> <p>Preprocess diff text to make it more suitable for embedding.</p> <p>Parameters:</p> Name Type Description Default <code>diff_text</code> <code>str</code> <p>Raw diff text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Preprocessed text</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>def preprocess_diff(self, diff_text: str) -&gt; str:\n\t\"\"\"\n\tPreprocess diff text to make it more suitable for embedding.\n\n\tArgs:\n\t    diff_text: Raw diff text\n\n\tReturns:\n\t    Preprocessed text\n\n\t\"\"\"\n\t# Remove diff headers, line numbers, etc.\n\t# Focus on actual content changes\n\tlines = []\n\tfor line in diff_text.splitlines():\n\t\t# Skip diff metadata lines\n\t\tif line.startswith((\"diff --git\", \"index \", \"+++\", \"---\")):\n\t\t\tcontinue\n\n\t\t# Keep actual content changes, removing the +/- prefix\n\t\tif line.startswith((\"+\", \"-\", \" \")):\n\t\t\tlines.append(line[1:])\n\n\treturn \"\\n\".join(lines)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.embed_chunk","title":"embed_chunk","text":"<pre><code>embed_chunk(chunk: DiffChunk) -&gt; ndarray\n</code></pre> <p>Generate an embedding for a diff chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>DiffChunk</code> <p>DiffChunk object</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Embedding vector</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>def embed_chunk(self, chunk: DiffChunk) -&gt; np.ndarray:\n\t\"\"\"\n\tGenerate an embedding for a diff chunk.\n\n\tArgs:\n\t    chunk: DiffChunk object\n\n\tReturns:\n\t    numpy.ndarray: Embedding vector\n\n\t\"\"\"\n\t# Get the diff content from the chunk\n\tdiff_text = chunk.content\n\n\t# Preprocess the diff text\n\tprocessed_text = self.preprocess_diff(diff_text)\n\n\t# If the processed text is empty, use the file paths as context\n\tif not processed_text.strip():\n\t\tprocessed_text = \" \".join(chunk.files)\n\n\t# Generate the embedding and convert to numpy array\n\tembedding = self.model.encode(processed_text)\n\treturn np.array(embedding)\n</code></pre>"},{"location":"api/git/semantic_grouping/embedder/#codemap.git.semantic_grouping.embedder.DiffEmbedder.embed_chunks","title":"embed_chunks","text":"<pre><code>embed_chunks(\n\tchunks: list[DiffChunk],\n) -&gt; list[tuple[DiffChunk, ndarray]]\n</code></pre> <p>Generate embeddings for multiple chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk]</code> <p>List of DiffChunk objects</p> required <p>Returns:</p> Type Description <code>list[tuple[DiffChunk, ndarray]]</code> <p>List of (chunk, embedding) tuples</p> Source code in <code>src/codemap/git/semantic_grouping/embedder.py</code> <pre><code>def embed_chunks(self, chunks: list[DiffChunk]) -&gt; list[tuple[DiffChunk, np.ndarray]]:\n\t\"\"\"\n\tGenerate embeddings for multiple chunks.\n\n\tArgs:\n\t    chunks: List of DiffChunk objects\n\n\tReturns:\n\t    List of (chunk, embedding) tuples\n\n\t\"\"\"\n\treturn [(chunk, self.embed_chunk(chunk)) for chunk in chunks]\n</code></pre>"},{"location":"api/git/semantic_grouping/group/","title":"Group","text":"<p>Module for semantic grouping of diff chunks.</p>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup","title":"SemanticGroup","text":"<p>Represents a group of semantically related diff chunks.</p> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>class SemanticGroup:\n\t\"\"\"Represents a group of semantically related diff chunks.\"\"\"\n\n\tdef __init__(self, chunks: list[DiffChunk] | None = None, name: str | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize a semantic group.\n\n\t\tArgs:\n\t\t    chunks: List of DiffChunk objects\n\t\t    name: Optional name for the group\n\n\t\t\"\"\"\n\t\tself.chunks = chunks or []\n\t\tself.name = name\n\t\tself.message: str | None = None\n\t\tself.approved = False\n\n\t@property\n\tdef files(self) -&gt; list[str]:\n\t\t\"\"\"Get the set of files affected by this group.\"\"\"\n\t\tfiles: set[str] = set()\n\t\tfor chunk in self.chunks:\n\t\t\tfiles.update(chunk.files)\n\t\treturn sorted(files)\n\n\t@property\n\tdef content(self) -&gt; str:\n\t\t\"\"\"Get the combined diff content of all chunks.\"\"\"\n\t\treturn \"\\n\".join(chunk.content for chunk in self.chunks)\n\n\tdef merge_with(self, other_group: \"SemanticGroup\") -&gt; \"SemanticGroup\":\n\t\t\"\"\"\n\t\tMerge this group with another group.\n\n\t\tArgs:\n\t\t    other_group: Another SemanticGroup to merge with\n\n\t\tReturns:\n\t\t    A new SemanticGroup containing chunks from both groups\n\n\t\t\"\"\"\n\t\treturn SemanticGroup(\n\t\t\tchunks=self.chunks + other_group.chunks, name=f\"Merged: {self.name or ''} + {other_group.name or ''}\"\n\t\t)\n\n\tdef __repr__(self) -&gt; str:\n\t\t\"\"\"Return a string representation of the group with file and chunk counts.\"\"\"\n\t\treturn f\"SemanticGroup(files={len(self.files)}, chunks={len(self.chunks)})\"\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.__init__","title":"__init__","text":"<pre><code>__init__(\n\tchunks: list[DiffChunk] | None = None,\n\tname: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize a semantic group.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[DiffChunk] | None</code> <p>List of DiffChunk objects</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the group</p> <code>None</code> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>def __init__(self, chunks: list[DiffChunk] | None = None, name: str | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize a semantic group.\n\n\tArgs:\n\t    chunks: List of DiffChunk objects\n\t    name: Optional name for the group\n\n\t\"\"\"\n\tself.chunks = chunks or []\n\tself.name = name\n\tself.message: str | None = None\n\tself.approved = False\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.chunks","title":"chunks  <code>instance-attribute</code>","text":"<pre><code>chunks = chunks or []\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str | None = None\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.approved","title":"approved  <code>instance-attribute</code>","text":"<pre><code>approved = False\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.files","title":"files  <code>property</code>","text":"<pre><code>files: list[str]\n</code></pre> <p>Get the set of files affected by this group.</p>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Get the combined diff content of all chunks.</p>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.merge_with","title":"merge_with","text":"<pre><code>merge_with(other_group: SemanticGroup) -&gt; SemanticGroup\n</code></pre> <p>Merge this group with another group.</p> <p>Parameters:</p> Name Type Description Default <code>other_group</code> <code>SemanticGroup</code> <p>Another SemanticGroup to merge with</p> required <p>Returns:</p> Type Description <code>SemanticGroup</code> <p>A new SemanticGroup containing chunks from both groups</p> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>def merge_with(self, other_group: \"SemanticGroup\") -&gt; \"SemanticGroup\":\n\t\"\"\"\n\tMerge this group with another group.\n\n\tArgs:\n\t    other_group: Another SemanticGroup to merge with\n\n\tReturns:\n\t    A new SemanticGroup containing chunks from both groups\n\n\t\"\"\"\n\treturn SemanticGroup(\n\t\tchunks=self.chunks + other_group.chunks, name=f\"Merged: {self.name or ''} + {other_group.name or ''}\"\n\t)\n</code></pre>"},{"location":"api/git/semantic_grouping/group/#codemap.git.semantic_grouping.group.SemanticGroup.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return a string representation of the group with file and chunk counts.</p> Source code in <code>src/codemap/git/semantic_grouping/group.py</code> <pre><code>def __repr__(self) -&gt; str:\n\t\"\"\"Return a string representation of the group with file and chunk counts.\"\"\"\n\treturn f\"SemanticGroup(files={len(self.files)}, chunks={len(self.chunks)})\"\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/","title":"Resolver","text":"<p>Module for resolving file integrity constraints in semantic groups.</p> <p>This module provides functionality for ensuring that changes to the same file are kept in the same commit, even when semantic clustering might separate them. This ensures that file integrity is maintained during the commit process.</p> <p>Key components: - FileIntegrityResolver: Main class that analyzes file overlaps between semantic groups   and decides whether to merge groups or reassign chunks to maintain file integrity</p> <p>The resolution process involves: 1. Detecting violations (files that appear in multiple semantic groups) 2. Calculating semantic similarity between groups with overlapping files 3. Deciding whether to merge groups (if sufficiently similar) or reassign chunks 4. Iteratively resolving violations until all files are in exactly one group</p>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T', bound=DiffChunk)\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver","title":"FileIntegrityResolver","text":"<p>Resolves file integrity constraints for semantic groups.</p> <p>File integrity refers to the requirement that all changes to a specific file should be included in the same commit, even if they are semantically different. This prevents fragmented changes to the same file across multiple commits, which can lead to broken builds or inconsistent states.</p> <p>The resolver works by: 1. Identifying files that appear in multiple semantic groups 2. Calculating the semantic similarity between these overlapping groups 3. Either merging similar groups or reassigning chunks from less relevant groups    to the most appropriate group</p> <p>This process ensures that each file is modified in exactly one commit, while still maintaining semantic coherence within commits when possible.</p> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>class FileIntegrityResolver:\n\t\"\"\"\n\tResolves file integrity constraints for semantic groups.\n\n\tFile integrity refers to the requirement that all changes to a specific file should\n\tbe included in the same commit, even if they are semantically different. This prevents\n\tfragmented changes to the same file across multiple commits, which can lead to broken builds\n\tor inconsistent states.\n\n\tThe resolver works by:\n\t1. Identifying files that appear in multiple semantic groups\n\t2. Calculating the semantic similarity between these overlapping groups\n\t3. Either merging similar groups or reassigning chunks from less relevant groups\n\t   to the most appropriate group\n\n\tThis process ensures that each file is modified in exactly one commit, while still\n\tmaintaining semantic coherence within commits when possible.\n\n\t\"\"\"\n\n\tdef __init__(self, similarity_threshold: float = 0.6) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the resolver.\n\n\t\tArgs:\n\t\t    similarity_threshold: Threshold for group similarity to trigger merging (0.0-1.0).\n\t\t        Higher values require greater similarity to merge groups:\n\t\t        - Values near 0.5 are permissive and will merge moderately related groups\n\t\t        - Values above 0.7 are strict and will mostly reassign chunks instead of merging\n\t\t        - Default 0.6 provides a balanced approach\n\n\t\tRaises:\n\t\t    ImportError: If scikit-learn is not installed\n\n\t\t\"\"\"\n\t\tself.similarity_threshold = similarity_threshold\n\n\t\t# Import here to avoid making sklearn a hard dependency\n\t\ttry:\n\t\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\t\tself.cosine_similarity = cosine_similarity\n\t\texcept ImportError as e:\n\t\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\t\tmsg = \"scikit-learn is required for file integrity resolution\"\n\t\t\traise ImportError(msg) from e\n\n\tdef calculate_group_similarity(\n\t\tself, group1: \"SemanticGroup\", group2: \"SemanticGroup\", chunk_embeddings: dict[DiffChunk, np.ndarray]\n\t) -&gt; float:\n\t\t\"\"\"\n\t\tCalculate similarity between two groups based on their chunks' embeddings.\n\n\t\tThis method computes the average pairwise cosine similarity between all combinations\n\t\tof chunks from the two groups. The similarity is based on the semantic embeddings\n\t\tof the chunks' content.\n\n\t\tProcess:\n\t\t1. Extract embeddings for all chunks in both groups\n\t\t2. Compute pairwise cosine similarities between each pair of chunks\n\t\t3. Return the average similarity score\n\n\t\tArgs:\n\t\t    group1: First semantic group to compare\n\t\t    group2: Second semantic group to compare\n\t\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\t\tReturns:\n\t\t    float: Similarity score between 0 and 1, where:\n\t\t        - 0 indicates completely unrelated changes\n\t\t        - 1 indicates identical or extremely similar changes\n\t\t        - Values around 0.6-0.8 typically indicate related functionality\n\n\t\t\"\"\"\n\t\t# Get embeddings for chunks in each group\n\t\tembeddings1 = [chunk_embeddings[chunk] for chunk in group1.chunks if chunk in chunk_embeddings]\n\t\tembeddings2 = [chunk_embeddings[chunk] for chunk in group2.chunks if chunk in chunk_embeddings]\n\n\t\tif not embeddings1 or not embeddings2:\n\t\t\treturn 0.0\n\n\t\t# Calculate pairwise similarities\n\t\tsimilarities = []\n\t\tfor emb1 in embeddings1:\n\t\t\tfor emb2 in embeddings2:\n\t\t\t\tsim = self.cosine_similarity([emb1], [emb2])[0][0]\n\t\t\t\tsimilarities.append(sim)\n\n\t\t# Return average similarity\n\t\treturn sum(similarities) / len(similarities) if similarities else 0.0\n\n\tdef resolve_violations(\n\t\tself, groups: list[\"SemanticGroup\"], chunk_embeddings: dict[DiffChunk, np.ndarray]\n\t) -&gt; list[\"SemanticGroup\"]:\n\t\t\"\"\"\n\t\tResolve file integrity violations by merging or reassigning chunks.\n\n\t\tA violation occurs when the same file appears in multiple semantic groups.\n\t\tThis needs to be resolved because a file should be modified in only one commit.\n\n\t\tArgs:\n\t\t    groups: List of SemanticGroup objects to resolve\n\t\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\t\tReturns:\n\t\t    List of SemanticGroup objects with all violations resolved\n\n\t\t\"\"\"\n\t\t# Keep iterating until no violations remain\n\t\twhile True:\n\t\t\t# Build file to groups mapping\n\t\t\tfile_to_groups: dict[str, list[int]] = {}\n\t\t\tfor i, group in enumerate(groups):\n\t\t\t\tfor file in group.files:\n\t\t\t\t\tif file not in file_to_groups:\n\t\t\t\t\t\tfile_to_groups[file] = []\n\t\t\t\t\tfile_to_groups[file].append(i)\n\n\t\t\t# Find violations (files in multiple groups)\n\t\t\tviolations = {file: indices for file, indices in file_to_groups.items() if len(indices) &gt; 1}\n\n\t\t\tif not violations:\n\t\t\t\tbreak  # No violations, we're done\n\n\t\t\t# Process the first violation\n\t\t\tfile = next(iter(violations))\n\t\t\tgroup_indices = violations[file]\n\n\t\t\t# Try to find groups to merge based on similarity\n\t\t\tmax_similarity = 0\n\t\t\tgroups_to_merge = None\n\n\t\t\t# Calculate similarities between all pairs of groups containing this file\n\t\t\tfor i in range(len(group_indices)):\n\t\t\t\tfor j in range(i + 1, len(group_indices)):\n\t\t\t\t\tidx1, idx2 = group_indices[i], group_indices[j]\n\t\t\t\t\tsimilarity = self.calculate_group_similarity(groups[idx1], groups[idx2], chunk_embeddings)\n\n\t\t\t\t\tif similarity &gt; max_similarity:\n\t\t\t\t\t\tmax_similarity = similarity\n\t\t\t\t\t\tgroups_to_merge = (idx1, idx2)\n\n\t\t\t# Decide whether to merge or reassign based on similarity threshold\n\t\t\tif max_similarity &gt;= self.similarity_threshold and groups_to_merge:\n\t\t\t\t# STRATEGY 1: Merge groups if they're similar enough\n\t\t\t\tidx1, idx2 = groups_to_merge\n\t\t\t\tmerged_group = groups[idx1].merge_with(groups[idx2])\n\n\t\t\t\t# Replace the first group with the merged one and remove the second\n\t\t\t\tgroups[idx1] = merged_group\n\t\t\t\tgroups.pop(idx2)\n\t\t\telse:\n\t\t\t\t# STRATEGY 2: Reassign chunks to the primary group for this file\n\t\t\t\t# Find the primary group (group with most chunks containing this file)\n\t\t\t\tfile_chunks_count = []\n\t\t\t\tfor idx in group_indices:\n\t\t\t\t\tcount = sum(1 for chunk in groups[idx].chunks if file in chunk.files)\n\t\t\t\t\tfile_chunks_count.append((idx, count))\n\n\t\t\t\t# Sort by count descending\n\t\t\t\tfile_chunks_count.sort(key=lambda x: x[1], reverse=True)\n\t\t\t\tprimary_idx = file_chunks_count[0][0]\n\n\t\t\t\t# Move chunks containing this file to the primary group\n\t\t\t\tfor idx in group_indices:\n\t\t\t\t\tif idx != primary_idx:\n\t\t\t\t\t\t# Find chunks containing this file\n\t\t\t\t\t\tchunks_to_move = [chunk for chunk in groups[idx].chunks if file in chunk.files]\n\n\t\t\t\t\t\t# Move chunks to primary group\n\t\t\t\t\t\tgroups[primary_idx].chunks.extend(chunks_to_move)\n\n\t\t\t\t\t\t# Remove moved chunks from original group\n\t\t\t\t\t\tgroups[idx].chunks = [chunk for chunk in groups[idx].chunks if file not in chunk.files]\n\n\t\t\t\t# Remove empty groups\n\t\t\t\tgroups = [group for group in groups if group.chunks]\n\n\t\treturn groups\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.__init__","title":"__init__","text":"<pre><code>__init__(similarity_threshold: float = 0.6) -&gt; None\n</code></pre> <p>Initialize the resolver.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_threshold</code> <code>float</code> <p>Threshold for group similarity to trigger merging (0.0-1.0). Higher values require greater similarity to merge groups: - Values near 0.5 are permissive and will merge moderately related groups - Values above 0.7 are strict and will mostly reassign chunks instead of merging - Default 0.6 provides a balanced approach</p> <code>0.6</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If scikit-learn is not installed</p> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>def __init__(self, similarity_threshold: float = 0.6) -&gt; None:\n\t\"\"\"\n\tInitialize the resolver.\n\n\tArgs:\n\t    similarity_threshold: Threshold for group similarity to trigger merging (0.0-1.0).\n\t        Higher values require greater similarity to merge groups:\n\t        - Values near 0.5 are permissive and will merge moderately related groups\n\t        - Values above 0.7 are strict and will mostly reassign chunks instead of merging\n\t        - Default 0.6 provides a balanced approach\n\n\tRaises:\n\t    ImportError: If scikit-learn is not installed\n\n\t\"\"\"\n\tself.similarity_threshold = similarity_threshold\n\n\t# Import here to avoid making sklearn a hard dependency\n\ttry:\n\t\tfrom sklearn.metrics.pairwise import cosine_similarity\n\n\t\tself.cosine_similarity = cosine_similarity\n\texcept ImportError as e:\n\t\tlogger.exception(\"Failed to import scikit-learn. Please install it with: uv add scikit-learn\")\n\t\tmsg = \"scikit-learn is required for file integrity resolution\"\n\t\traise ImportError(msg) from e\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.similarity_threshold","title":"similarity_threshold  <code>instance-attribute</code>","text":"<pre><code>similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.cosine_similarity","title":"cosine_similarity  <code>instance-attribute</code>","text":"<pre><code>cosine_similarity = cosine_similarity\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.calculate_group_similarity","title":"calculate_group_similarity","text":"<pre><code>calculate_group_similarity(\n\tgroup1: SemanticGroup,\n\tgroup2: SemanticGroup,\n\tchunk_embeddings: dict[DiffChunk, ndarray],\n) -&gt; float\n</code></pre> <p>Calculate similarity between two groups based on their chunks' embeddings.</p> <p>This method computes the average pairwise cosine similarity between all combinations of chunks from the two groups. The similarity is based on the semantic embeddings of the chunks' content.</p> <p>Process: 1. Extract embeddings for all chunks in both groups 2. Compute pairwise cosine similarities between each pair of chunks 3. Return the average similarity score</p> <p>Parameters:</p> Name Type Description Default <code>group1</code> <code>SemanticGroup</code> <p>First semantic group to compare</p> required <code>group2</code> <code>SemanticGroup</code> <p>Second semantic group to compare</p> required <code>chunk_embeddings</code> <code>dict[DiffChunk, ndarray]</code> <p>Dict mapping chunks to their embeddings</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Similarity score between 0 and 1, where: - 0 indicates completely unrelated changes - 1 indicates identical or extremely similar changes - Values around 0.6-0.8 typically indicate related functionality</p> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>def calculate_group_similarity(\n\tself, group1: \"SemanticGroup\", group2: \"SemanticGroup\", chunk_embeddings: dict[DiffChunk, np.ndarray]\n) -&gt; float:\n\t\"\"\"\n\tCalculate similarity between two groups based on their chunks' embeddings.\n\n\tThis method computes the average pairwise cosine similarity between all combinations\n\tof chunks from the two groups. The similarity is based on the semantic embeddings\n\tof the chunks' content.\n\n\tProcess:\n\t1. Extract embeddings for all chunks in both groups\n\t2. Compute pairwise cosine similarities between each pair of chunks\n\t3. Return the average similarity score\n\n\tArgs:\n\t    group1: First semantic group to compare\n\t    group2: Second semantic group to compare\n\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\tReturns:\n\t    float: Similarity score between 0 and 1, where:\n\t        - 0 indicates completely unrelated changes\n\t        - 1 indicates identical or extremely similar changes\n\t        - Values around 0.6-0.8 typically indicate related functionality\n\n\t\"\"\"\n\t# Get embeddings for chunks in each group\n\tembeddings1 = [chunk_embeddings[chunk] for chunk in group1.chunks if chunk in chunk_embeddings]\n\tembeddings2 = [chunk_embeddings[chunk] for chunk in group2.chunks if chunk in chunk_embeddings]\n\n\tif not embeddings1 or not embeddings2:\n\t\treturn 0.0\n\n\t# Calculate pairwise similarities\n\tsimilarities = []\n\tfor emb1 in embeddings1:\n\t\tfor emb2 in embeddings2:\n\t\t\tsim = self.cosine_similarity([emb1], [emb2])[0][0]\n\t\t\tsimilarities.append(sim)\n\n\t# Return average similarity\n\treturn sum(similarities) / len(similarities) if similarities else 0.0\n</code></pre>"},{"location":"api/git/semantic_grouping/resolver/#codemap.git.semantic_grouping.resolver.FileIntegrityResolver.resolve_violations","title":"resolve_violations","text":"<pre><code>resolve_violations(\n\tgroups: list[SemanticGroup],\n\tchunk_embeddings: dict[DiffChunk, ndarray],\n) -&gt; list[SemanticGroup]\n</code></pre> <p>Resolve file integrity violations by merging or reassigning chunks.</p> <p>A violation occurs when the same file appears in multiple semantic groups. This needs to be resolved because a file should be modified in only one commit.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>list[SemanticGroup]</code> <p>List of SemanticGroup objects to resolve</p> required <code>chunk_embeddings</code> <code>dict[DiffChunk, ndarray]</code> <p>Dict mapping chunks to their embeddings</p> required <p>Returns:</p> Type Description <code>list[SemanticGroup]</code> <p>List of SemanticGroup objects with all violations resolved</p> Source code in <code>src/codemap/git/semantic_grouping/resolver.py</code> <pre><code>def resolve_violations(\n\tself, groups: list[\"SemanticGroup\"], chunk_embeddings: dict[DiffChunk, np.ndarray]\n) -&gt; list[\"SemanticGroup\"]:\n\t\"\"\"\n\tResolve file integrity violations by merging or reassigning chunks.\n\n\tA violation occurs when the same file appears in multiple semantic groups.\n\tThis needs to be resolved because a file should be modified in only one commit.\n\n\tArgs:\n\t    groups: List of SemanticGroup objects to resolve\n\t    chunk_embeddings: Dict mapping chunks to their embeddings\n\n\tReturns:\n\t    List of SemanticGroup objects with all violations resolved\n\n\t\"\"\"\n\t# Keep iterating until no violations remain\n\twhile True:\n\t\t# Build file to groups mapping\n\t\tfile_to_groups: dict[str, list[int]] = {}\n\t\tfor i, group in enumerate(groups):\n\t\t\tfor file in group.files:\n\t\t\t\tif file not in file_to_groups:\n\t\t\t\t\tfile_to_groups[file] = []\n\t\t\t\tfile_to_groups[file].append(i)\n\n\t\t# Find violations (files in multiple groups)\n\t\tviolations = {file: indices for file, indices in file_to_groups.items() if len(indices) &gt; 1}\n\n\t\tif not violations:\n\t\t\tbreak  # No violations, we're done\n\n\t\t# Process the first violation\n\t\tfile = next(iter(violations))\n\t\tgroup_indices = violations[file]\n\n\t\t# Try to find groups to merge based on similarity\n\t\tmax_similarity = 0\n\t\tgroups_to_merge = None\n\n\t\t# Calculate similarities between all pairs of groups containing this file\n\t\tfor i in range(len(group_indices)):\n\t\t\tfor j in range(i + 1, len(group_indices)):\n\t\t\t\tidx1, idx2 = group_indices[i], group_indices[j]\n\t\t\t\tsimilarity = self.calculate_group_similarity(groups[idx1], groups[idx2], chunk_embeddings)\n\n\t\t\t\tif similarity &gt; max_similarity:\n\t\t\t\t\tmax_similarity = similarity\n\t\t\t\t\tgroups_to_merge = (idx1, idx2)\n\n\t\t# Decide whether to merge or reassign based on similarity threshold\n\t\tif max_similarity &gt;= self.similarity_threshold and groups_to_merge:\n\t\t\t# STRATEGY 1: Merge groups if they're similar enough\n\t\t\tidx1, idx2 = groups_to_merge\n\t\t\tmerged_group = groups[idx1].merge_with(groups[idx2])\n\n\t\t\t# Replace the first group with the merged one and remove the second\n\t\t\tgroups[idx1] = merged_group\n\t\t\tgroups.pop(idx2)\n\t\telse:\n\t\t\t# STRATEGY 2: Reassign chunks to the primary group for this file\n\t\t\t# Find the primary group (group with most chunks containing this file)\n\t\t\tfile_chunks_count = []\n\t\t\tfor idx in group_indices:\n\t\t\t\tcount = sum(1 for chunk in groups[idx].chunks if file in chunk.files)\n\t\t\t\tfile_chunks_count.append((idx, count))\n\n\t\t\t# Sort by count descending\n\t\t\tfile_chunks_count.sort(key=lambda x: x[1], reverse=True)\n\t\t\tprimary_idx = file_chunks_count[0][0]\n\n\t\t\t# Move chunks containing this file to the primary group\n\t\t\tfor idx in group_indices:\n\t\t\t\tif idx != primary_idx:\n\t\t\t\t\t# Find chunks containing this file\n\t\t\t\t\tchunks_to_move = [chunk for chunk in groups[idx].chunks if file in chunk.files]\n\n\t\t\t\t\t# Move chunks to primary group\n\t\t\t\t\tgroups[primary_idx].chunks.extend(chunks_to_move)\n\n\t\t\t\t\t# Remove moved chunks from original group\n\t\t\t\t\tgroups[idx].chunks = [chunk for chunk in groups[idx].chunks if file not in chunk.files]\n\n\t\t\t# Remove empty groups\n\t\t\tgroups = [group for group in groups if group.chunks]\n\n\treturn groups\n</code></pre>"},{"location":"api/llm/","title":"Llm Overview","text":"<p>LLM module for CodeMap.</p> <ul> <li>Api - API interaction for LLM services.</li> <li>Client - LLM client for unified access to language models.</li> <li>Config - Configuration for LLM module.</li> <li>Errors - Error classes for LLM-related operations.</li> <li>Rag - RAG (Retrieval-Augmented Generation) functionalities for CodeMap.</li> <li>Utils - Utility functions for working with LLMs.</li> </ul>"},{"location":"api/llm/api/","title":"Api","text":"<p>API interaction for LLM services.</p>"},{"location":"api/llm/api/#codemap.llm.api.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.ResponseType","title":"ResponseType  <code>module-attribute</code>","text":"<pre><code>ResponseType = dict[str, Any] | Any\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.call_llm_api","title":"call_llm_api","text":"<pre><code>call_llm_api(\n\tprompt: str,\n\tmodel: str,\n\tapi_key: str,\n\tapi_base: str | None = None,\n\tjson_schema: dict | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n\t**kwargs: dict[str, str | int | float | bool | None],\n) -&gt; str\n</code></pre> <p>Call an LLM API using litellm.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the LLM</p> required <code>model</code> <code>str</code> <p>The model identifier (including provider prefix)</p> required <code>api_key</code> <code>str</code> <p>The API key to use</p> required <code>api_base</code> <code>str | None</code> <p>Optional custom API base URL</p> <code>None</code> <code>json_schema</code> <code>dict | None</code> <p>Optional JSON schema for response validation</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance for additional configuration</p> <code>None</code> <code>**kwargs</code> <code>dict[str, str | int | float | bool | None]</code> <p>Additional parameters to pass to the LLM API</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated text response</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If the API call fails</p> Source code in <code>src/codemap/llm/api.py</code> <pre><code>def call_llm_api(\n\tprompt: str,\n\tmodel: str,\n\tapi_key: str,\n\tapi_base: str | None = None,\n\tjson_schema: dict | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n\t**kwargs: dict[str, str | int | float | bool | None],\n) -&gt; str:\n\t\"\"\"\n\tCall an LLM API using litellm.\n\n\tArgs:\n\t    prompt: The prompt to send to the LLM\n\t    model: The model identifier (including provider prefix)\n\t    api_key: The API key to use\n\t    api_base: Optional custom API base URL\n\t    json_schema: Optional JSON schema for response validation\n\t    config_loader: Optional ConfigLoader instance for additional configuration\n\t    **kwargs: Additional parameters to pass to the LLM API\n\n\tReturns:\n\t    The generated text response\n\n\tRaises:\n\t    LLMError: If the API call fails\n\n\t\"\"\"\n\ttry:\n\t\timport litellm\n\texcept ImportError:\n\t\tmsg = \"LiteLLM library not installed. Install it with 'pip install litellm'.\"\n\t\tlogger.exception(msg)\n\t\traise LLMError(msg) from None\n\n\t# Get request parameters from config if available\n\trequest_params = DEFAULT_LLM_REQUEST_PARAMS.copy()\n\n\tif config_loader is not None:\n\t\tllm_config = config_loader.get_llm_config()\n\t\t# Update request params with values from config\n\t\tif \"temperature\" in llm_config:\n\t\t\trequest_params[\"temperature\"] = llm_config.get(\"temperature\")\n\t\tif \"max_tokens\" in llm_config:\n\t\t\trequest_params[\"max_tokens\"] = llm_config.get(\"max_tokens\")\n\n\t# Override with any passed parameters\n\trequest_params.update(kwargs)\n\n\tmessage = [{\"role\": \"user\", \"content\": prompt}]\n\n\t# Set up final request parameters\n\trequest_params.update(\n\t\t{\n\t\t\t\"model\": model,\n\t\t\t\"messages\": message,\n\t\t\t\"api_key\": api_key,\n\t\t}\n\t)\n\n\t# Add API base if provided\n\tif api_base:\n\t\trequest_params[\"api_base\"] = api_base\n\n\t# Add JSON response format if schema provided\n\tif json_schema:\n\t\trequest_params[\"response_format\"] = {\"type\": \"json_object\", \"schema\": json_schema}\n\t\t# Enable schema validation\n\t\tlitellm.enable_json_schema_validation = True\n\n\tdef _raise_extraction_error() -&gt; None:\n\t\t\"\"\"Raise an error for failed content extraction.\"\"\"\n\t\tmsg = \"Failed to extract content from LLM response\"\n\t\traise LLMError(msg)\n\n\ttry:\n\t\tlogger.debug(\"Calling LiteLLM with model: %s\", model)\n\t\tresponse = litellm.completion(**request_params)\n\n\t\t# Extract content from the response\n\t\tcontent = extract_content_from_response(response)\n\n\t\tif not content:\n\t\t\tlogger.error(\"Could not extract content from LLM response\")\n\t\t\t_raise_extraction_error()\n\n\t\treturn content\n\n\texcept Exception as e:\n\t\tlogger.exception(\"LLM API call failed\")\n\t\tmsg = f\"LLM API call failed: {e}\"\n\t\traise LLMError(msg) from e\n</code></pre>"},{"location":"api/llm/api/#codemap.llm.api.extract_content_from_response","title":"extract_content_from_response","text":"<pre><code>extract_content_from_response(\n\tresponse: ResponseType,\n) -&gt; str\n</code></pre> <p>Extract content from a LiteLLM response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ResponseType</code> <p>LiteLLM response object or dictionary</p> required <p>Returns:</p> Type Description <code>str</code> <p>Extracted content as string</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If content cannot be extracted</p> Source code in <code>src/codemap/llm/api.py</code> <pre><code>def extract_content_from_response(response: ResponseType) -&gt; str:\n\t\"\"\"\n\tExtract content from a LiteLLM response.\n\n\tArgs:\n\t    response: LiteLLM response object or dictionary\n\n\tReturns:\n\t    Extracted content as string\n\n\tRaises:\n\t    AttributeError: If content cannot be extracted\n\n\t\"\"\"\n\tcontent = \"\"\n\n\t# Try different response formats\n\tif response:\n\t\t# First try the standard OpenAI-like structure\n\t\tif hasattr(response, \"choices\") and isinstance(getattr(response, \"choices\", []), list):\n\t\t\tchoices = getattr(response, \"choices\", [])\n\t\t\tif choices:\n\t\t\t\tfirst_choice = choices[0]\n\t\t\t\tif hasattr(first_choice, \"message\") and hasattr(first_choice.message, \"content\"):\n\t\t\t\t\tcontent = getattr(first_choice.message, \"content\", \"\")\n\n\t\t# Then try as dictionary if the above failed\n\t\tif not content and isinstance(response, dict):\n\t\t\tchoices = response.get(\"choices\", [])\n\t\t\tif choices and isinstance(choices, list) and choices:\n\t\t\t\tfirst_choice = choices[0]\n\t\t\t\tif isinstance(first_choice, dict):\n\t\t\t\t\tmessage = first_choice.get(\"message\", {})\n\t\t\t\t\tif isinstance(message, dict):\n\t\t\t\t\t\tcontent = message.get(\"content\", \"\")\n\n\t\t# Try as direct string for simple APIs\n\t\tif not content and hasattr(response, \"text\"):\n\t\t\tcontent = getattr(response, \"text\", \"\")\n\n\treturn content or \"\"\n</code></pre>"},{"location":"api/llm/client/","title":"Client","text":"<p>LLM client for unified access to language models.</p>"},{"location":"api/llm/client/#codemap.llm.client.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient","title":"LLMClient","text":"<p>Client for interacting with LLM services in a unified way.</p> Source code in <code>src/codemap/llm/client.py</code> <pre><code>class LLMClient:\n\t\"\"\"Client for interacting with LLM services in a unified way.\"\"\"\n\n\t# Default templates - empty in base class\n\tDEFAULT_TEMPLATES: ClassVar[dict[str, str]] = {}\n\n\tdef __init__(\n\t\tself,\n\t\tconfig: LLMConfig | None = None,\n\t\trepo_path: Path | None = None,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the LLM client.\n\n\t\tArgs:\n\t\t    config: LLM configuration\n\t\t    repo_path: Repository path (for loading configuration)\n\t\t    config_loader: Optional ConfigLoader instance to use\n\n\t\t\"\"\"\n\t\tself.config = config or LLMConfig()\n\t\tself.repo_path = repo_path\n\t\tself.config_loader = config_loader\n\t\tself._templates = self.DEFAULT_TEMPLATES.copy()\n\n\tdef set_template(self, name: str, template: str) -&gt; None:\n\t\t\"\"\"\n\t\tSet a prompt template.\n\n\t\tArgs:\n\t\t    name: Template name\n\t\t    template: Template content\n\n\t\t\"\"\"\n\t\tself._templates[name] = template\n\n\tdef get_template(self, name: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet a prompt template.\n\n\t\tArgs:\n\t\t    name: Template name\n\n\t\tReturns:\n\t\t    Template content\n\n\t\tRaises:\n\t\t    ValueError: If template doesn't exist\n\n\t\t\"\"\"\n\t\tif name not in self._templates:\n\t\t\tmsg = f\"Template '{name}' not found\"\n\t\t\traise ValueError(msg)\n\t\treturn self._templates[name]\n\n\tdef _get_default_model(self) -&gt; str:\n\t\t\"\"\"\n\t\tGet the default model from config or use a fallback.\n\n\t\tReturns:\n\t\t        Default model name\n\n\t\t\"\"\"\n\t\tif self.config_loader:\n\t\t\tllm_config = self.config_loader.get_llm_config()\n\t\t\treturn llm_config.get(\"model\", \"openai/gpt-4o-mini\")\n\t\treturn \"openai/gpt-4o-mini\"  # Hardcoded fallback\n\n\tdef generate_text(\n\t\tself,\n\t\tprompt: str,\n\t\tmodel: str | None = None,\n\t\tjson_schema: dict | None = None,\n\t\t**kwargs: dict[str, str | int | float | bool | None],\n\t) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate text using the configured LLM.\n\n\t\tArgs:\n\t\t    prompt: Prompt to send to the LLM\n\t\t    model: Optional model override\n\t\t    json_schema: Optional JSON schema for response validation\n\t\t    **kwargs: Additional parameters to pass to the LLM API\n\n\t\tReturns:\n\t\t    Generated text\n\n\t\tRaises:\n\t\t    LLMError: If the API call fails\n\n\t\t\"\"\"\n\t\t# Get API configuration\n\t\tmodel_to_use = model or self.config.model or self._get_default_model()\n\t\tapi_key = self.config.get_api_key()\n\n\t\tif not api_key:\n\t\t\tmsg = f\"No API key available for {self.config.provider or 'default'} provider\"\n\t\t\traise LLMError(msg)\n\n\t\t# Call the API\n\t\treturn call_llm_api(\n\t\t\tprompt=prompt,\n\t\t\tmodel=model_to_use,\n\t\t\tapi_key=api_key,\n\t\t\tapi_base=self.config.api_base,\n\t\t\tjson_schema=json_schema,\n\t\t\tconfig_loader=self.config_loader,\n\t\t\t**kwargs,\n\t\t)\n\n\tdef generate_from_template(\n\t\tself,\n\t\ttemplate_name: str,\n\t\ttemplate_vars: dict[str, Any],\n\t\tmodel: str | None = None,\n\t\tjson_schema: dict | None = None,\n\t\t**kwargs: dict[str, str | int | float | bool | None],\n\t) -&gt; str:\n\t\t\"\"\"\n\t\tGenerate text using a named template.\n\n\t\tArgs:\n\t\t    template_name: Name of the template to use\n\t\t    template_vars: Variables to format the template with\n\t\t    model: Optional model override\n\t\t    json_schema: Optional JSON schema for response validation\n\t\t    **kwargs: Additional parameters to pass to the LLM API\n\n\t\tReturns:\n\t\t    Generated text\n\n\t\tRaises:\n\t\t    LLMError: If the API call fails\n\t\t    ValueError: If the template doesn't exist\n\n\t\t\"\"\"\n\t\t# Get and format the template\n\t\ttemplate = self.get_template(template_name)\n\t\tprompt = template.format(**template_vars)\n\n\t\t# Generate text\n\t\treturn self.generate_text(\n\t\t\tprompt=prompt,\n\t\t\tmodel=model,\n\t\t\tjson_schema=json_schema,\n\t\t\t**kwargs,\n\t\t)\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.DEFAULT_TEMPLATES","title":"DEFAULT_TEMPLATES  <code>class-attribute</code>","text":"<pre><code>DEFAULT_TEMPLATES: dict[str, str] = {}\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig: LLMConfig | None = None,\n\trepo_path: Path | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LLMConfig | None</code> <p>LLM configuration</p> <code>None</code> <code>repo_path</code> <code>Path | None</code> <p>Repository path (for loading configuration)</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> Source code in <code>src/codemap/llm/client.py</code> <pre><code>def __init__(\n\tself,\n\tconfig: LLMConfig | None = None,\n\trepo_path: Path | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the LLM client.\n\n\tArgs:\n\t    config: LLM configuration\n\t    repo_path: Repository path (for loading configuration)\n\t    config_loader: Optional ConfigLoader instance to use\n\n\t\"\"\"\n\tself.config = config or LLMConfig()\n\tself.repo_path = repo_path\n\tself.config_loader = config_loader\n\tself._templates = self.DEFAULT_TEMPLATES.copy()\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or LLMConfig()\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.set_template","title":"set_template","text":"<pre><code>set_template(name: str, template: str) -&gt; None\n</code></pre> <p>Set a prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Template name</p> required <code>template</code> <code>str</code> <p>Template content</p> required Source code in <code>src/codemap/llm/client.py</code> <pre><code>def set_template(self, name: str, template: str) -&gt; None:\n\t\"\"\"\n\tSet a prompt template.\n\n\tArgs:\n\t    name: Template name\n\t    template: Template content\n\n\t\"\"\"\n\tself._templates[name] = template\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.get_template","title":"get_template","text":"<pre><code>get_template(name: str) -&gt; str\n</code></pre> <p>Get a prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Template name</p> required <p>Returns:</p> Type Description <code>str</code> <p>Template content</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If template doesn't exist</p> Source code in <code>src/codemap/llm/client.py</code> <pre><code>def get_template(self, name: str) -&gt; str:\n\t\"\"\"\n\tGet a prompt template.\n\n\tArgs:\n\t    name: Template name\n\n\tReturns:\n\t    Template content\n\n\tRaises:\n\t    ValueError: If template doesn't exist\n\n\t\"\"\"\n\tif name not in self._templates:\n\t\tmsg = f\"Template '{name}' not found\"\n\t\traise ValueError(msg)\n\treturn self._templates[name]\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.generate_text","title":"generate_text","text":"<pre><code>generate_text(\n\tprompt: str,\n\tmodel: str | None = None,\n\tjson_schema: dict | None = None,\n\t**kwargs: dict[str, str | int | float | bool | None],\n) -&gt; str\n</code></pre> <p>Generate text using the configured LLM.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to send to the LLM</p> required <code>model</code> <code>str | None</code> <p>Optional model override</p> <code>None</code> <code>json_schema</code> <code>dict | None</code> <p>Optional JSON schema for response validation</p> <code>None</code> <code>**kwargs</code> <code>dict[str, str | int | float | bool | None]</code> <p>Additional parameters to pass to the LLM API</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If the API call fails</p> Source code in <code>src/codemap/llm/client.py</code> <pre><code>def generate_text(\n\tself,\n\tprompt: str,\n\tmodel: str | None = None,\n\tjson_schema: dict | None = None,\n\t**kwargs: dict[str, str | int | float | bool | None],\n) -&gt; str:\n\t\"\"\"\n\tGenerate text using the configured LLM.\n\n\tArgs:\n\t    prompt: Prompt to send to the LLM\n\t    model: Optional model override\n\t    json_schema: Optional JSON schema for response validation\n\t    **kwargs: Additional parameters to pass to the LLM API\n\n\tReturns:\n\t    Generated text\n\n\tRaises:\n\t    LLMError: If the API call fails\n\n\t\"\"\"\n\t# Get API configuration\n\tmodel_to_use = model or self.config.model or self._get_default_model()\n\tapi_key = self.config.get_api_key()\n\n\tif not api_key:\n\t\tmsg = f\"No API key available for {self.config.provider or 'default'} provider\"\n\t\traise LLMError(msg)\n\n\t# Call the API\n\treturn call_llm_api(\n\t\tprompt=prompt,\n\t\tmodel=model_to_use,\n\t\tapi_key=api_key,\n\t\tapi_base=self.config.api_base,\n\t\tjson_schema=json_schema,\n\t\tconfig_loader=self.config_loader,\n\t\t**kwargs,\n\t)\n</code></pre>"},{"location":"api/llm/client/#codemap.llm.client.LLMClient.generate_from_template","title":"generate_from_template","text":"<pre><code>generate_from_template(\n\ttemplate_name: str,\n\ttemplate_vars: dict[str, Any],\n\tmodel: str | None = None,\n\tjson_schema: dict | None = None,\n\t**kwargs: dict[str, str | int | float | bool | None],\n) -&gt; str\n</code></pre> <p>Generate text using a named template.</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>Name of the template to use</p> required <code>template_vars</code> <code>dict[str, Any]</code> <p>Variables to format the template with</p> required <code>model</code> <code>str | None</code> <p>Optional model override</p> <code>None</code> <code>json_schema</code> <code>dict | None</code> <p>Optional JSON schema for response validation</p> <code>None</code> <code>**kwargs</code> <code>dict[str, str | int | float | bool | None]</code> <p>Additional parameters to pass to the LLM API</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If the API call fails</p> <code>ValueError</code> <p>If the template doesn't exist</p> Source code in <code>src/codemap/llm/client.py</code> <pre><code>def generate_from_template(\n\tself,\n\ttemplate_name: str,\n\ttemplate_vars: dict[str, Any],\n\tmodel: str | None = None,\n\tjson_schema: dict | None = None,\n\t**kwargs: dict[str, str | int | float | bool | None],\n) -&gt; str:\n\t\"\"\"\n\tGenerate text using a named template.\n\n\tArgs:\n\t    template_name: Name of the template to use\n\t    template_vars: Variables to format the template with\n\t    model: Optional model override\n\t    json_schema: Optional JSON schema for response validation\n\t    **kwargs: Additional parameters to pass to the LLM API\n\n\tReturns:\n\t    Generated text\n\n\tRaises:\n\t    LLMError: If the API call fails\n\t    ValueError: If the template doesn't exist\n\n\t\"\"\"\n\t# Get and format the template\n\ttemplate = self.get_template(template_name)\n\tprompt = template.format(**template_vars)\n\n\t# Generate text\n\treturn self.generate_text(\n\t\tprompt=prompt,\n\t\tmodel=model,\n\t\tjson_schema=json_schema,\n\t\t**kwargs,\n\t)\n</code></pre>"},{"location":"api/llm/config/","title":"Config","text":"<p>Configuration for LLM module.</p>"},{"location":"api/llm/config/#codemap.llm.config.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.DEFAULT_MODEL","title":"DEFAULT_MODEL  <code>module-attribute</code>","text":"<pre><code>DEFAULT_MODEL = 'openai/gpt-4o-mini'\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.KNOWN_PROVIDERS","title":"KNOWN_PROVIDERS  <code>module-attribute</code>","text":"<pre><code>KNOWN_PROVIDERS = [\n\t\"openai\",\n\t\"anthropic\",\n\t\"azure\",\n\t\"groq\",\n\t\"mistral\",\n\t\"together\",\n\t\"cohere\",\n\t\"openrouter\",\n]\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.ENV_VAR_MAP","title":"ENV_VAR_MAP  <code>module-attribute</code>","text":"<pre><code>ENV_VAR_MAP = {\n\t\"openai\": \"OPENAI_API_KEY\",\n\t\"anthropic\": \"ANTHROPIC_API_KEY\",\n\t\"azure\": \"AZURE_API_KEY\",\n\t\"groq\": \"GROQ_API_KEY\",\n\t\"mistral\": \"MISTRAL_API_KEY\",\n\t\"together\": \"TOGETHER_API_KEY\",\n\t\"cohere\": \"COHERE_API_KEY\",\n\t\"openrouter\": \"OPENROUTER_API_KEY\",\n}\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.DEFAULT_LLM_REQUEST_PARAMS","title":"DEFAULT_LLM_REQUEST_PARAMS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_LLM_REQUEST_PARAMS = {\n\t\"temperature\": 0.3,\n\t\"max_tokens\": 1000,\n}\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig","title":"LLMConfig  <code>dataclass</code>","text":"<p>Configuration for LLM operations.</p> Source code in <code>src/codemap/llm/config.py</code> <pre><code>@dataclass\nclass LLMConfig:\n\t\"\"\"Configuration for LLM operations.\"\"\"\n\n\tmodel: str = DEFAULT_MODEL\n\tprovider: str | None = None\n\tapi_base: str | None = None\n\tapi_key: str | None = None\n\tapi_keys: dict[str, str] = field(default_factory=dict)\n\n\tdef __post_init__(self) -&gt; None:\n\t\t\"\"\"Process the configuration after initialization.\"\"\"\n\t\t# Extract provider from model if not explicitly provided\n\t\tif not self.provider and \"/\" in self.model:\n\t\t\tself.provider = self.model.split(\"/\")[0].lower()\n\t\t\tlogger.debug(\"Extracted provider '%s' from model '%s'\", self.provider, self.model)\n\n\t\t# If provider is still not set, default to OpenAI\n\t\tif not self.provider:\n\t\t\tself.provider = \"openai\"\n\t\t\tlogger.debug(\"No provider found, defaulting to 'openai'\")\n\n\t\t# Load API keys from environment if not provided\n\t\tif not self.api_keys:\n\t\t\tself.api_keys = self._load_api_keys_from_env()\n\t\t\tfor provider in self.api_keys:\n\t\t\t\tlogger.debug(\"Loaded API key for provider: %s\", provider)\n\n\t\t# If specific api_key is provided, add it to api_keys\n\t\tif self.api_key and self.provider:\n\t\t\tself.api_keys[self.provider] = self.api_key\n\t\t\tlogger.debug(\"Added explicit API key for provider: %s\", self.provider)\n\n\t\tif self.provider:\n\t\t\tenv_var = ENV_VAR_MAP.get(self.provider)\n\t\t\tlogger.debug(\"Looking for API key in environment variable: %s\", env_var)\n\t\t\tif env_var:\n\t\t\t\tkey = os.environ.get(env_var)\n\t\t\t\tif key:\n\t\t\t\t\tlogger.debug(\"Found API key in environment for provider: %s\", self.provider)\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\"No API key found in environment for provider: %s (env var: %s)\", self.provider, env_var\n\t\t\t\t\t)\n\n\tdef _load_api_keys_from_env(self) -&gt; dict[str, str]:\n\t\t\"\"\"Load API keys from environment variables.\"\"\"\n\t\tapi_keys = {}\n\n\t\t# Check all known provider environment variables\n\t\tfor provider in KNOWN_PROVIDERS:\n\t\t\tenv_var = ENV_VAR_MAP.get(provider)\n\t\t\tif env_var and (key := os.environ.get(env_var)):\n\t\t\t\tapi_keys[provider] = key\n\t\t\t\tlogger.debug(\"Loaded API key for %s from environment variable %s\", provider, env_var)\n\n\t\treturn api_keys\n\n\tdef get_api_key(self) -&gt; str | None:\n\t\t\"\"\"Get the API key for the configured provider.\"\"\"\n\t\tif not self.provider:\n\t\t\tlogger.warning(\"No provider configured, cannot get API key\")\n\t\t\treturn None\n\n\t\t# Try from loaded keys\n\t\tif self.provider in self.api_keys:\n\t\t\tlogger.debug(\"Using API key from loaded keys for provider: %s\", self.provider)\n\t\t\treturn self.api_keys[self.provider]\n\n\t\t# Try from environment as a fallback\n\t\tenv_var = ENV_VAR_MAP.get(self.provider)\n\t\tif env_var:\n\t\t\tkey = os.environ.get(env_var)\n\t\t\tif key:\n\t\t\t\tlogger.debug(\"Using API key from environment for provider: %s\", self.provider)\n\t\t\t\treturn key\n\t\t\t# If key is not found or is empty, log the warning\n\t\t\tlogger.warning(\"API key not found in environment for provider: %s (env var: %s)\", self.provider, env_var)\n\n\t\tlogger.warning(\"No API key found for provider: %s\", self.provider)\n\t\treturn None\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n\tmodel: str = DEFAULT_MODEL,\n\tprovider: str | None = None,\n\tapi_base: str | None = None,\n\tapi_key: str | None = None,\n\tapi_keys: dict[str, str] = dict(),\n) -&gt; None\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = DEFAULT_MODEL\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.provider","title":"provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>provider: str | None = None\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.api_base","title":"api_base  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_base: str | None = None\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: str | None = None\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.api_keys","title":"api_keys  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_keys: dict[str, str] = field(default_factory=dict)\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Process the configuration after initialization.</p> Source code in <code>src/codemap/llm/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\t\"\"\"Process the configuration after initialization.\"\"\"\n\t# Extract provider from model if not explicitly provided\n\tif not self.provider and \"/\" in self.model:\n\t\tself.provider = self.model.split(\"/\")[0].lower()\n\t\tlogger.debug(\"Extracted provider '%s' from model '%s'\", self.provider, self.model)\n\n\t# If provider is still not set, default to OpenAI\n\tif not self.provider:\n\t\tself.provider = \"openai\"\n\t\tlogger.debug(\"No provider found, defaulting to 'openai'\")\n\n\t# Load API keys from environment if not provided\n\tif not self.api_keys:\n\t\tself.api_keys = self._load_api_keys_from_env()\n\t\tfor provider in self.api_keys:\n\t\t\tlogger.debug(\"Loaded API key for provider: %s\", provider)\n\n\t# If specific api_key is provided, add it to api_keys\n\tif self.api_key and self.provider:\n\t\tself.api_keys[self.provider] = self.api_key\n\t\tlogger.debug(\"Added explicit API key for provider: %s\", self.provider)\n\n\tif self.provider:\n\t\tenv_var = ENV_VAR_MAP.get(self.provider)\n\t\tlogger.debug(\"Looking for API key in environment variable: %s\", env_var)\n\t\tif env_var:\n\t\t\tkey = os.environ.get(env_var)\n\t\t\tif key:\n\t\t\t\tlogger.debug(\"Found API key in environment for provider: %s\", self.provider)\n\t\t\telse:\n\t\t\t\tlogger.warning(\n\t\t\t\t\t\"No API key found in environment for provider: %s (env var: %s)\", self.provider, env_var\n\t\t\t\t)\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.LLMConfig.get_api_key","title":"get_api_key","text":"<pre><code>get_api_key() -&gt; str | None\n</code></pre> <p>Get the API key for the configured provider.</p> Source code in <code>src/codemap/llm/config.py</code> <pre><code>def get_api_key(self) -&gt; str | None:\n\t\"\"\"Get the API key for the configured provider.\"\"\"\n\tif not self.provider:\n\t\tlogger.warning(\"No provider configured, cannot get API key\")\n\t\treturn None\n\n\t# Try from loaded keys\n\tif self.provider in self.api_keys:\n\t\tlogger.debug(\"Using API key from loaded keys for provider: %s\", self.provider)\n\t\treturn self.api_keys[self.provider]\n\n\t# Try from environment as a fallback\n\tenv_var = ENV_VAR_MAP.get(self.provider)\n\tif env_var:\n\t\tkey = os.environ.get(env_var)\n\t\tif key:\n\t\t\tlogger.debug(\"Using API key from environment for provider: %s\", self.provider)\n\t\t\treturn key\n\t\t# If key is not found or is empty, log the warning\n\t\tlogger.warning(\"API key not found in environment for provider: %s (env var: %s)\", self.provider, env_var)\n\n\tlogger.warning(\"No API key found for provider: %s\", self.provider)\n\treturn None\n</code></pre>"},{"location":"api/llm/config/#codemap.llm.config.get_llm_config","title":"get_llm_config","text":"<pre><code>get_llm_config(\n\tconfig_loader: ConfigLoader | None = None,\n\t**overrides: dict[str, str | int | float | bool | None]\n\t| str\n\t| float\n\t| bool\n\t| None,\n) -&gt; LLMConfig\n</code></pre> <p>Get LLM configuration from config loader and optional overrides.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> <code>**overrides</code> <code>dict[str, str | int | float | bool | None] | str | float | bool | None</code> <p>Optional configuration overrides</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMConfig</code> <p>LLMConfig instance with merged configuration</p> Source code in <code>src/codemap/llm/config.py</code> <pre><code>def get_llm_config(\n\tconfig_loader: ConfigLoader | None = None,\n\t**overrides: dict[str, str | int | float | bool | None] | str | float | bool | None,\n) -&gt; LLMConfig:\n\t\"\"\"\n\tGet LLM configuration from config loader and optional overrides.\n\n\tArgs:\n\t    config_loader: Optional ConfigLoader instance to use\n\t    **overrides: Optional configuration overrides\n\n\tReturns:\n\t    LLMConfig instance with merged configuration\n\n\t\"\"\"\n\t# Create a config loader if none provided\n\tif config_loader is None:\n\t\tconfig_loader = ConfigLoader()\n\t\tlogger.debug(\"Created new ConfigLoader instance\")\n\telse:\n\t\tlogger.debug(\"Using provided ConfigLoader instance\")\n\n\t# Get LLM config from loader\n\tllm_config_dict = config_loader.get_llm_config()\n\n\t# Log the model being used\n\tmodel = llm_config_dict.get(\"model\", DEFAULT_MODEL)\n\tlogger.debug(\"Using model from config: %s\", model)\n\n\t# Apply overrides\n\tfor key, value in overrides.items():\n\t\tif value is not None:  # Only override if value is not None\n\t\t\tllm_config_dict[key] = value\n\t\t\tif key == \"model\":\n\t\t\t\tlogger.debug(\"Overriding model with: %s\", value)\n\n\t# Create and return config object\n\treturn LLMConfig(\n\t\tmodel=llm_config_dict.get(\"model\", DEFAULT_MODEL),\n\t\tprovider=llm_config_dict.get(\"provider\"),\n\t\tapi_base=llm_config_dict.get(\"api_base\"),\n\t\tapi_key=llm_config_dict.get(\"api_key\"),\n\t)\n</code></pre>"},{"location":"api/llm/errors/","title":"Errors","text":"<p>Error classes for LLM-related operations.</p>"},{"location":"api/llm/errors/#codemap.llm.errors.LLMError","title":"LLMError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for LLM-related errors.</p> Source code in <code>src/codemap/llm/errors.py</code> <pre><code>class LLMError(Exception):\n\t\"\"\"Base exception for LLM-related errors.\"\"\"\n</code></pre>"},{"location":"api/llm/utils/","title":"Utils","text":"<p>Utility functions for working with LLMs.</p>"},{"location":"api/llm/utils/#codemap.llm.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.load_prompt_template","title":"load_prompt_template","text":"<pre><code>load_prompt_template(\n\ttemplate_path: str | None,\n) -&gt; str | None\n</code></pre> <p>Load custom prompt template from file.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str | None</code> <p>Path to prompt template file</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Loaded template or None if loading failed</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def load_prompt_template(template_path: str | None) -&gt; str | None:\n\t\"\"\"\n\tLoad custom prompt template from file.\n\n\tArgs:\n\t    template_path: Path to prompt template file\n\n\tReturns:\n\t    Loaded template or None if loading failed\n\n\t\"\"\"\n\tif not template_path:\n\t\treturn None\n\n\ttry:\n\t\ttemplate_file = Path(template_path)\n\t\twith template_file.open(\"r\") as f:\n\t\t\treturn f.read()\n\texcept OSError:\n\t\tlogger.warning(\"Could not load prompt template: %s\", template_path)\n\t\treturn None\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.get_llm_client","title":"get_llm_client","text":"<pre><code>get_llm_client(\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; LLMClient\n</code></pre> <p>Create and return a LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMClient</code> <p>LLMClient instance</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client creation fails</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def get_llm_client(config_loader: ConfigLoader | None = None) -&gt; LLMClient:\n\t\"\"\"\n\tCreate and return a LLM client.\n\n\tArgs:\n\t        config_loader: Optional ConfigLoader instance to use\n\n\tReturns:\n\t    LLMClient instance\n\n\tRaises:\n\t    RuntimeError: If client creation fails\n\n\t\"\"\"\n\ttry:\n\t\tconfig = get_llm_config(config_loader=config_loader)\n\t\treturn LLMClient(config=config, config_loader=config_loader)\n\n\texcept LLMError as e:\n\t\tlogger.exception(\"LLM error\")\n\t\tmsg = f\"Failed to create LLM client: {e}\"\n\t\traise RuntimeError(msg) from e\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.LLMResponseType","title":"LLMResponseType  <code>module-attribute</code>","text":"<pre><code>LLMResponseType = (\n\tdict[str, Any] | Mapping[str, Any] | object\n)\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.extract_content_from_response","title":"extract_content_from_response","text":"<pre><code>extract_content_from_response(\n\tresponse: LLMResponseType,\n) -&gt; str\n</code></pre> <p>Extract content from a LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponseType</code> <p>LLM response object or dictionary</p> required <p>Returns:</p> Type Description <code>str</code> <p>Extracted content as string</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def extract_content_from_response(response: LLMResponseType) -&gt; str:\n\t\"\"\"\n\tExtract content from a LLM response.\n\n\tArgs:\n\t    response: LLM response object or dictionary\n\n\tReturns:\n\t    Extracted content as string\n\n\t\"\"\"\n\treturn _extract_content(response)\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.generate_text","title":"generate_text","text":"<pre><code>generate_text(\n\tprompt: str,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n\t**kwargs: str | float | bool | None,\n) -&gt; str\n</code></pre> <p>Generate text using an LLM with minimal configuration.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the LLM</p> required <code>model</code> <code>str | None</code> <p>The model to use</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>The API key (if None, tries to find in environment)</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Optional API base URL</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> <code>**kwargs</code> <code>str | float | bool | None</code> <p>Additional parameters to pass to the LLM API</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated text</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the LLM call fails</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def generate_text(\n\tprompt: str,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n\t**kwargs: str | float | bool | None,\n) -&gt; str:\n\t\"\"\"\n\tGenerate text using an LLM with minimal configuration.\n\n\tArgs:\n\t    prompt: The prompt to send to the LLM\n\t    model: The model to use\n\t    api_key: The API key (if None, tries to find in environment)\n\t    api_base: Optional API base URL\n\t    config_loader: Optional ConfigLoader instance to use\n\t    **kwargs: Additional parameters to pass to the LLM API\n\n\tReturns:\n\t    The generated text\n\n\tRaises:\n\t    RuntimeError: If the LLM call fails\n\n\t\"\"\"\n\ttry:\n\t\t# Create client and generate text directly\n\t\tclient = create_client(model=model, api_key=api_key, api_base=api_base, config_loader=config_loader)\n\t\treturn client.generate_text(prompt=prompt, **kwargs)  # type: ignore[arg-type]\n\n\texcept LLMError as e:\n\t\tlogger.exception(\"LLM error\")\n\t\tmsg = f\"Failed to generate text with LLM: {e}\"\n\t\traise RuntimeError(msg) from e\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.create_client","title":"create_client","text":"<pre><code>create_client(\n\trepo_path: Path | None = None,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; LLMClient\n</code></pre> <p>Create an LLMClient with the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path | None</code> <p>Repository path for configuration loading</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Model identifier to use</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key to use</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>API base URL to use</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Optional ConfigLoader instance to use</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMClient</code> <p>Configured LLMClient instance</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client creation fails</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def create_client(\n\trepo_path: Path | None = None,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; LLMClient:\n\t\"\"\"\n\tCreate an LLMClient with the specified configuration.\n\n\tArgs:\n\t    repo_path: Repository path for configuration loading\n\t    model: Model identifier to use\n\t    api_key: API key to use\n\t    api_base: API base URL to use\n\t    config_loader: Optional ConfigLoader instance to use\n\n\tReturns:\n\t    Configured LLMClient instance\n\n\tRaises:\n\t    RuntimeError: If client creation fails\n\n\t\"\"\"\n\ttry:\n\t\t# Get configuration\n\t\tconfig = get_llm_config(\n\t\t\tconfig_loader=config_loader,\n\t\t\toverrides={\n\t\t\t\t\"model\": model,\n\t\t\t\t\"api_key\": api_key,\n\t\t\t\t\"api_base\": api_base,\n\t\t\t},\n\t\t)\n\n\t\t# Create client\n\t\treturn LLMClient(config=config, repo_path=repo_path, config_loader=config_loader)\n\n\texcept Exception as e:\n\t\tlogger.exception(\"Error creating LLM client\")\n\t\tmsg = f\"Failed to create LLM client: {e}\"\n\t\traise RuntimeError(msg) from e\n</code></pre>"},{"location":"api/llm/utils/#codemap.llm.utils.batch_generate_completions","title":"batch_generate_completions","text":"<pre><code>batch_generate_completions(\n\tmessages_list: list[list[dict]],\n\tmodel: str,\n\tconfig_loader: ConfigLoader,\n\tresponse_format: dict | None = None,\n\t**kwargs: str | float | bool | dict | None,\n) -&gt; list\n</code></pre> <p>Generate batch completions using LiteLLM.</p> Source code in <code>src/codemap/llm/utils.py</code> <pre><code>def batch_generate_completions(\n\tmessages_list: list[list[dict]],\n\tmodel: str,\n\tconfig_loader: ConfigLoader,\n\tresponse_format: dict | None = None,\n\t**kwargs: str | float | bool | dict | None,\n) -&gt; list:\n\t\"\"\"Generate batch completions using LiteLLM.\"\"\"\n\tfrom litellm import batch_completion\n\n\t# Get LLM config from config_loader\n\tllm_config = config_loader.get_llm_config()\n\n\t# Set up common parameters - LiteLLM will automatically use environment variables\n\tlitellm_params = {\n\t\t\"model\": model,\n\t\t\"temperature\": llm_config.get(\"temperature\", 0.7),\n\t\t\"max_tokens\": llm_config.get(\"max_tokens\", 1024),\n\t}\n\n\t# Add api_base if specified\n\tapi_base = llm_config.get(\"api_base\")\n\tif api_base:\n\t\tlitellm_params[\"api_base\"] = api_base\n\n\t# Add response format\n\tif response_format:\n\t\tlitellm_params[\"response_format\"] = response_format\n\n\t# Add additional parameters\n\tlitellm_params = {**litellm_params, **kwargs}\n\n\t# LiteLLM will automatically use the appropriate API key from environment variables\n\treturn batch_completion(messages=messages_list, **litellm_params)\n</code></pre>"},{"location":"api/llm/rag/","title":"Rag Overview","text":"<p>RAG (Retrieval-Augmented Generation) functionalities for CodeMap.</p> <ul> <li>Command - Command for asking questions about the codebase using RAG.</li> <li>Formatter - Formatter for the ask command output.</li> <li>Prompts - Prompts for the ask command.</li> </ul>"},{"location":"api/llm/rag/command/","title":"Command","text":"<p>Command for asking questions about the codebase using RAG.</p>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.DEFAULT_MAX_CONTEXT_LENGTH","title":"DEFAULT_MAX_CONTEXT_LENGTH  <code>module-attribute</code>","text":"<pre><code>DEFAULT_MAX_CONTEXT_LENGTH = 8000\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.DEFAULT_MAX_CONTEXT_RESULTS","title":"DEFAULT_MAX_CONTEXT_RESULTS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_MAX_CONTEXT_RESULTS = 10\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskResult","title":"AskResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>Structured result for the ask command.</p> Source code in <code>src/codemap/llm/rag/command.py</code> <pre><code>class AskResult(TypedDict):\n\t\"\"\"Structured result for the ask command.\"\"\"\n\n\tanswer: str | None\n\tcontext: list[dict[str, Any]]\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskResult.answer","title":"answer  <code>instance-attribute</code>","text":"<pre><code>answer: str | None\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskResult.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: list[dict[str, Any]]\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand","title":"AskCommand","text":"<p>Handles the logic for the <code>codemap ask</code> command.</p> <p>Interacts with the ProcessingPipeline, DatabaseClient, and an LLM to answer questions about the codebase using RAG. Maintains conversation history for interactive sessions.</p> Source code in <code>src/codemap/llm/rag/command.py</code> <pre><code>class AskCommand:\n\t\"\"\"\n\tHandles the logic for the `codemap ask` command.\n\n\tInteracts with the ProcessingPipeline, DatabaseClient, and an LLM to\n\tanswer questions about the codebase using RAG. Maintains conversation\n\thistory for interactive sessions.\n\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_path: Path | None = None,\n\t\tmodel: str | None = None,\n\t\tapi_key: str | None = None,\n\t\tapi_base: str | None = None,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitializes the AskCommand, setting up clients and pipeline.\n\n\t\tArgs:\n\t\t        repo_path: Path to the repository to analyze\n\t\t        model: LLM model to use for answering questions\n\t\t        api_key: API key for LLM provider\n\t\t        api_base: API base URL for LLM provider\n\t\t        config_loader: ConfigLoader to use for configuration (follows DI pattern)\n\n\t\t\"\"\"\n\t\tself.repo_path = repo_path or Path.cwd()\n\t\tself.session_id = str(uuid.uuid4())  # Unique session ID for DB logging\n\n\t\t# Accept ConfigLoader from higher-level components\n\t\t# Always ensure we have a valid ConfigLoader instance\n\t\tif config_loader is None:\n\t\t\tself.config_loader = ConfigLoader.get_instance(repo_root=self.repo_path)\n\t\t\tlogger.debug(\"Created fallback ConfigLoader instance\")\n\t\telse:\n\t\t\tself.config_loader = config_loader\n\t\t\tlogger.debug(\"Using provided ConfigLoader instance\")\n\n\t\t# Get RAG configuration\n\t\trag_config = self.config_loader.get(\"rag\", {})\n\t\tself.max_context_length = rag_config.get(\"max_context_length\", DEFAULT_MAX_CONTEXT_LENGTH)\n\t\tself.max_context_results = rag_config.get(\"max_context_results\", DEFAULT_MAX_CONTEXT_RESULTS)\n\t\tlogger.debug(\n\t\t\tf\"Using max_context_length: {self.max_context_length}, max_context_results: {self.max_context_results}\"\n\t\t)\n\n\t\tself.db_client = DatabaseClient()  # Uses config path by default\n\t\tself.llm_client = create_client(\n\t\t\trepo_path=self.repo_path,\n\t\t\tmodel=model,  # create_client handles defaults/config\n\t\t\tapi_key=api_key,\n\t\t\tapi_base=api_base,\n\t\t\tconfig_loader=self.config_loader,  # Pass the config_loader down\n\t\t)\n\t\t# Initialize ProcessingPipeline correctly\n\t\ttry:\n\t\t\t# Show a spinner while initializing the pipeline\n\t\t\twith progress_indicator(message=\"Initializing processing pipeline...\", style=\"spinner\", transient=True):\n\t\t\t\t# Use the provided ConfigLoader instance\n\t\t\t\tself.pipeline = ProcessingPipeline(\n\t\t\t\t\trepo_path=self.repo_path,  # Pipeline still needs repo_path directly\n\t\t\t\t\tconfig_loader=self.config_loader,  # Pass the correctly initialized loader\n\t\t\t\t)\n\n\t\t\t# Progress context manager handles completion message\n\t\t\tlogger.info(\"ProcessingPipeline initialization complete.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to initialize ProcessingPipeline\")\n\t\t\tself.pipeline = None\n\n\t\tlogger.info(f\"AskCommand initialized for session {self.session_id}\")\n\n\tasync def initialize(self) -&gt; None:\n\t\t\"\"\"Perform asynchronous initialization for the command, especially the pipeline.\"\"\"\n\t\tif self.pipeline and not self.pipeline.is_async_initialized:\n\t\t\ttry:\n\t\t\t\t# Show a spinner while initializing the pipeline asynchronously\n\t\t\t\twith progress_indicator(\n\t\t\t\t\tmessage=\"Initializing async components (pipeline)...\", style=\"spinner\", transient=True\n\t\t\t\t):\n\t\t\t\t\tawait self.pipeline.async_init(sync_on_init=True)\n\t\t\t\tlogger.info(\"ProcessingPipeline async initialization complete.\")\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Failed during async initialization of ProcessingPipeline\")\n\t\t\t\t# Optionally set pipeline to None or handle the error appropriately\n\t\t\t\tself.pipeline = None\n\t\telif not self.pipeline:\n\t\t\tlogger.error(\"Cannot perform async initialization: ProcessingPipeline failed to initialize earlier.\")\n\t\telse:\n\t\t\tlogger.info(\"AskCommand async components already initialized.\")\n\n\tasync def _retrieve_context(self, query: str, limit: int | None = None) -&gt; list[dict[str, Any]]:\n\t\t\"\"\"Retrieve relevant code chunks based on the query.\"\"\"\n\t\tif not self.pipeline:\n\t\t\tlogger.warning(\"ProcessingPipeline not available, no context will be retrieved.\")\n\t\t\treturn []\n\n\t\t# Use configured limit or default\n\t\tactual_limit = limit or self.max_context_results\n\n\t\ttry:\n\t\t\tlogger.info(f\"Retrieving context for query: '{query}', limit: {actual_limit}\")\n\t\t\t# Use synchronous method to get results (pipeline.semantic_search is async)\n\t\t\t# Now call await directly as this method is async\n\t\t\t# import asyncio\n\t\t\t# results = asyncio.run(self.pipeline.semantic_search(query, k=actual_limit))\n\t\t\tresults = await self.pipeline.semantic_search(query, k=actual_limit)\n\n\t\t\t# Format results for the LLM\n\t\t\tformatted_results = []\n\t\t\tif results:  # Check if results is not None and has items\n\t\t\t\tfor r in results:\n\t\t\t\t\t# Extract relevant fields from payload\n\t\t\t\t\tpayload = r.get(\"payload\", {})\n\n\t\t\t\t\t# Get file content from repo using file_path, start_line, and end_line\n\t\t\t\t\tfile_path = payload.get(\"file_path\", \"N/A\")\n\t\t\t\t\tstart_line = payload.get(\"start_line\", -1)\n\t\t\t\t\tend_line = payload.get(\"end_line\", -1)\n\n\t\t\t\t\t# Get content from repository if needed and build a content representation\n\t\t\t\t\t# For now, we'll use a simple representation that includes metadata\n\t\t\t\t\tentity_type = payload.get(\"entity_type\", \"\")\n\t\t\t\t\tentity_name = payload.get(\"entity_name\", \"\")\n\t\t\t\t\tlanguage = payload.get(\"language\", \"\")\n\n\t\t\t\t\t# Build a content representation from the metadata\n\t\t\t\t\tcontent_parts = []\n\t\t\t\t\tcontent_parts.append(f\"Type: {entity_type}\")\n\t\t\t\t\tif entity_name:\n\t\t\t\t\t\tcontent_parts.append(f\"Name: {entity_name}\")\n\n\t\t\t\t\t# Get the file content from the repo\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif self.repo_path and file_path and start_line &gt; 0 and end_line &gt; 0:\n\t\t\t\t\t\t\trepo_file_path = self.repo_path / file_path\n\t\t\t\t\t\t\tif repo_file_path.exists():\n\t\t\t\t\t\t\t\twith repo_file_path.open(encoding=\"utf-8\") as f:\n\t\t\t\t\t\t\t\t\tfile_content = f.read()\n\t\t\t\t\t\t\t\tlines = file_content.splitlines()\n\t\t\t\t\t\t\t\tif start_line &lt;= len(lines) and end_line &lt;= len(lines):\n\t\t\t\t\t\t\t\t\tcode_content = \"\\n\".join(lines[start_line - 1 : end_line])\n\t\t\t\t\t\t\t\t\tif language:\n\t\t\t\t\t\t\t\t\t\tcontent_parts.append(f\"```{language}\\n{code_content}\\n```\")\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\tcontent_parts.append(f\"```\\n{code_content}\\n```\")\n\t\t\t\t\texcept Exception:\n\t\t\t\t\t\tlogger.exception(f\"Error reading file content for {file_path}\")\n\n\t\t\t\t\tcontent = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t\tformatted_results.append(\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"file_path\": file_path,\n\t\t\t\t\t\t\t\"start_line\": start_line,\n\t\t\t\t\t\t\t\"end_line\": end_line,\n\t\t\t\t\t\t\t\"content\": content,\n\t\t\t\t\t\t\t\"score\": r.get(\"score\", -1.0),\n\t\t\t\t\t\t}\n\t\t\t\t\t)\n\n\t\t\tlogger.debug(f\"Semantic search returned {len(formatted_results)} results.\")\n\t\t\treturn formatted_results\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error retrieving context\")\n\t\t\treturn []\n\n\tdef _extract_answer_from_response(self, response: Any) -&gt; str | None:  # noqa: ANN401\n\t\t\"\"\"Safely extract the answer text from a litellm response object.\"\"\"\n\t\tif response is None:\n\t\t\treturn None\n\n\t\ttry:\n\t\t\t# Handle different response object structures\n\t\t\tif hasattr(response, \"choices\") and response.choices:\n\t\t\t\t# Try to access as object with attributes\n\t\t\t\tchoice = response.choices[0]\n\t\t\t\tif hasattr(choice, \"message\"):\n\t\t\t\t\tmessage = choice.message\n\t\t\t\t\tif hasattr(message, \"content\"):\n\t\t\t\t\t\treturn message.content\n\n\t\t\t# Try to access as dictionary\n\t\t\tif isinstance(response, dict) and \"choices\" in response and response[\"choices\"]:\n\t\t\t\tchoice = response[\"choices\"][0]\n\t\t\t\tif isinstance(choice, dict) and \"message\" in choice:\n\t\t\t\t\tmessage = choice[\"message\"]\n\t\t\t\t\tif isinstance(message, dict) and \"content\" in message:\n\t\t\t\t\t\treturn message[\"content\"]\n\n\t\t\t# If we got here, we couldn't extract the answer\n\t\t\tlogger.warning(f\"Couldn't extract answer from response of type {type(response)}\")\n\t\t\treturn None\n\n\t\t# Fix for BLE001 (blind exception) - catch specific exceptions\n\t\texcept (AttributeError, IndexError, KeyError, TypeError) as e:\n\t\t\tlogger.warning(f\"Error extracting answer from response: {e}\")\n\t\t\treturn None\n\n\tasync def run(self, question: str) -&gt; AskResult:\n\t\t\"\"\"Executes one turn of the ask command, returning the answer and context.\"\"\"\n\t\tlogger.info(f\"Processing question for session {self.session_id}: '{question}'\")\n\n\t\t# Ensure async initialization happened (idempotent check inside)\n\t\tawait self.initialize()\n\n\t\tif not self.pipeline:\n\t\t\treturn AskResult(answer=\"Processing pipeline not available.\", context=[])\n\n\t\t# Retrieve relevant context first\n\t\tcontext = await self._retrieve_context(question)\n\n\t\t# Format context for inclusion in prompt\n\t\tcontext_text = format_content_for_context(context)\n\t\tif len(context_text) &gt; self.max_context_length:\n\t\t\tlogger.warning(f\"Context too long ({len(context_text)} chars), truncating.\")\n\t\t\tcontext_text = context_text[: self.max_context_length] + \"... [truncated]\"\n\n\t\t# Construct messages with context included\n\t\tmessages = [\n\t\t\t{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n\t\t\t{\n\t\t\t\t\"role\": \"user\",\n\t\t\t\t\"content\": (\n\t\t\t\t\tf\"Here's my question about the codebase: {question}\\n\\n\"\n\t\t\t\t\tf\"Relevant context from the codebase:\\n{context_text}\"\n\t\t\t\t),\n\t\t\t},\n\t\t]\n\n\t\t# Store user query in DB\n\t\tdb_entry_id = None\n\t\ttry:\n\t\t\tdb_entry = self.db_client.add_chat_message(session_id=self.session_id, user_query=question)\n\t\t\tdb_entry_id = db_entry.id if db_entry else None\n\t\t\tif db_entry_id:\n\t\t\t\tlogger.debug(f\"Stored current query turn with DB ID: {db_entry_id}\")\n\t\t\telse:\n\t\t\t\tlogger.warning(\"Failed to get DB entry ID for current query turn.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to store current query turn in DB\")\n\n\t\t# Get LLM config from the injected ConfigLoader\n\t\t# At this point self.config_loader is guaranteed to be a valid instance\n\t\tllm_config = self.config_loader.get_llm_config()\n\t\tllm_params = {\n\t\t\t\"temperature\": llm_config.get(\"temperature\", 0.7),\n\t\t\t\"max_tokens\": llm_config.get(\"max_tokens\", 1024),\n\t\t}\n\t\tmodel_name = llm_config.get(\"model\", \"openai/gpt-4o-mini\")\n\t\tapi_base = llm_config.get(\"api_base\")\n\t\tapi_key_env_var = llm_config.get(\"api_key_env\")\n\t\tapi_key = None\n\t\tif isinstance(api_key_env_var, str):\n\t\t\tapi_key = os.getenv(api_key_env_var)\n\t\tapi_key = api_key or llm_config.get(\"api_key\")  # Fallback to direct config key\n\n\t\t# Call LLM with context\n\t\ttry:\n\t\t\twith loading_spinner(\"Waiting for LLM response...\"):\n\t\t\t\tresponse = completion(\n\t\t\t\t\tmodel=model_name,\n\t\t\t\t\tmessages=messages,\n\t\t\t\t\tapi_base=api_base,\n\t\t\t\t\tapi_key=api_key,\n\t\t\t\t\tstream=False,\n\t\t\t\t\t**llm_params,\n\t\t\t\t)\n\n\t\t\t# Extract answer from response\n\t\t\tanswer = self._extract_answer_from_response(response)\n\n\t\t\t# Update DB with answer using the dedicated client method\n\t\t\tif db_entry_id and answer:\n\t\t\t\t# The update_chat_response method handles its own exceptions and returns success/failure\n\t\t\t\tsuccess = self.db_client.update_chat_response(message_id=db_entry_id, ai_response=answer)\n\t\t\t\tif not success:\n\t\t\t\t\tlogger.warning(f\"Failed to update DB entry {db_entry_id} via client method.\")\n\n\t\t\treturn AskResult(answer=answer, context=context)\n\t\texcept Exception as e:  # Keep the outer exception for LLM call errors\n\t\t\tlogger.exception(\"Error during LLM completion\")\n\t\t\treturn AskResult(answer=f\"Error: {e!s}\", context=context)\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_path: Path | None = None,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initializes the AskCommand, setting up clients and pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path | None</code> <p>Path to the repository to analyze</p> <code>None</code> <code>model</code> <code>str | None</code> <p>LLM model to use for answering questions</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for LLM provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>API base URL for LLM provider</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>ConfigLoader to use for configuration (follows DI pattern)</p> <code>None</code> Source code in <code>src/codemap/llm/rag/command.py</code> <pre><code>def __init__(\n\tself,\n\trepo_path: Path | None = None,\n\tmodel: str | None = None,\n\tapi_key: str | None = None,\n\tapi_base: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitializes the AskCommand, setting up clients and pipeline.\n\n\tArgs:\n\t        repo_path: Path to the repository to analyze\n\t        model: LLM model to use for answering questions\n\t        api_key: API key for LLM provider\n\t        api_base: API base URL for LLM provider\n\t        config_loader: ConfigLoader to use for configuration (follows DI pattern)\n\n\t\"\"\"\n\tself.repo_path = repo_path or Path.cwd()\n\tself.session_id = str(uuid.uuid4())  # Unique session ID for DB logging\n\n\t# Accept ConfigLoader from higher-level components\n\t# Always ensure we have a valid ConfigLoader instance\n\tif config_loader is None:\n\t\tself.config_loader = ConfigLoader.get_instance(repo_root=self.repo_path)\n\t\tlogger.debug(\"Created fallback ConfigLoader instance\")\n\telse:\n\t\tself.config_loader = config_loader\n\t\tlogger.debug(\"Using provided ConfigLoader instance\")\n\n\t# Get RAG configuration\n\trag_config = self.config_loader.get(\"rag\", {})\n\tself.max_context_length = rag_config.get(\"max_context_length\", DEFAULT_MAX_CONTEXT_LENGTH)\n\tself.max_context_results = rag_config.get(\"max_context_results\", DEFAULT_MAX_CONTEXT_RESULTS)\n\tlogger.debug(\n\t\tf\"Using max_context_length: {self.max_context_length}, max_context_results: {self.max_context_results}\"\n\t)\n\n\tself.db_client = DatabaseClient()  # Uses config path by default\n\tself.llm_client = create_client(\n\t\trepo_path=self.repo_path,\n\t\tmodel=model,  # create_client handles defaults/config\n\t\tapi_key=api_key,\n\t\tapi_base=api_base,\n\t\tconfig_loader=self.config_loader,  # Pass the config_loader down\n\t)\n\t# Initialize ProcessingPipeline correctly\n\ttry:\n\t\t# Show a spinner while initializing the pipeline\n\t\twith progress_indicator(message=\"Initializing processing pipeline...\", style=\"spinner\", transient=True):\n\t\t\t# Use the provided ConfigLoader instance\n\t\t\tself.pipeline = ProcessingPipeline(\n\t\t\t\trepo_path=self.repo_path,  # Pipeline still needs repo_path directly\n\t\t\t\tconfig_loader=self.config_loader,  # Pass the correctly initialized loader\n\t\t\t)\n\n\t\t# Progress context manager handles completion message\n\t\tlogger.info(\"ProcessingPipeline initialization complete.\")\n\texcept Exception:\n\t\tlogger.exception(\"Failed to initialize ProcessingPipeline\")\n\t\tself.pipeline = None\n\n\tlogger.info(f\"AskCommand initialized for session {self.session_id}\")\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path or cwd()\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id = str(uuid4())\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = get_instance(repo_root=repo_path)\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.max_context_length","title":"max_context_length  <code>instance-attribute</code>","text":"<pre><code>max_context_length = get(\n\t\"max_context_length\", DEFAULT_MAX_CONTEXT_LENGTH\n)\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.max_context_results","title":"max_context_results  <code>instance-attribute</code>","text":"<pre><code>max_context_results = get(\n\t\"max_context_results\", DEFAULT_MAX_CONTEXT_RESULTS\n)\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.db_client","title":"db_client  <code>instance-attribute</code>","text":"<pre><code>db_client = DatabaseClient()\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.llm_client","title":"llm_client  <code>instance-attribute</code>","text":"<pre><code>llm_client = create_client(\n\trepo_path=repo_path,\n\tmodel=model,\n\tapi_key=api_key,\n\tapi_base=api_base,\n\tconfig_loader=config_loader,\n)\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.pipeline","title":"pipeline  <code>instance-attribute</code>","text":"<pre><code>pipeline = ProcessingPipeline(\n\trepo_path=repo_path, config_loader=config_loader\n)\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize() -&gt; None\n</code></pre> <p>Perform asynchronous initialization for the command, especially the pipeline.</p> Source code in <code>src/codemap/llm/rag/command.py</code> <pre><code>async def initialize(self) -&gt; None:\n\t\"\"\"Perform asynchronous initialization for the command, especially the pipeline.\"\"\"\n\tif self.pipeline and not self.pipeline.is_async_initialized:\n\t\ttry:\n\t\t\t# Show a spinner while initializing the pipeline asynchronously\n\t\t\twith progress_indicator(\n\t\t\t\tmessage=\"Initializing async components (pipeline)...\", style=\"spinner\", transient=True\n\t\t\t):\n\t\t\t\tawait self.pipeline.async_init(sync_on_init=True)\n\t\t\tlogger.info(\"ProcessingPipeline async initialization complete.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed during async initialization of ProcessingPipeline\")\n\t\t\t# Optionally set pipeline to None or handle the error appropriately\n\t\t\tself.pipeline = None\n\telif not self.pipeline:\n\t\tlogger.error(\"Cannot perform async initialization: ProcessingPipeline failed to initialize earlier.\")\n\telse:\n\t\tlogger.info(\"AskCommand async components already initialized.\")\n</code></pre>"},{"location":"api/llm/rag/command/#codemap.llm.rag.command.AskCommand.run","title":"run  <code>async</code>","text":"<pre><code>run(question: str) -&gt; AskResult\n</code></pre> <p>Executes one turn of the ask command, returning the answer and context.</p> Source code in <code>src/codemap/llm/rag/command.py</code> <pre><code>async def run(self, question: str) -&gt; AskResult:\n\t\"\"\"Executes one turn of the ask command, returning the answer and context.\"\"\"\n\tlogger.info(f\"Processing question for session {self.session_id}: '{question}'\")\n\n\t# Ensure async initialization happened (idempotent check inside)\n\tawait self.initialize()\n\n\tif not self.pipeline:\n\t\treturn AskResult(answer=\"Processing pipeline not available.\", context=[])\n\n\t# Retrieve relevant context first\n\tcontext = await self._retrieve_context(question)\n\n\t# Format context for inclusion in prompt\n\tcontext_text = format_content_for_context(context)\n\tif len(context_text) &gt; self.max_context_length:\n\t\tlogger.warning(f\"Context too long ({len(context_text)} chars), truncating.\")\n\t\tcontext_text = context_text[: self.max_context_length] + \"... [truncated]\"\n\n\t# Construct messages with context included\n\tmessages = [\n\t\t{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": (\n\t\t\t\tf\"Here's my question about the codebase: {question}\\n\\n\"\n\t\t\t\tf\"Relevant context from the codebase:\\n{context_text}\"\n\t\t\t),\n\t\t},\n\t]\n\n\t# Store user query in DB\n\tdb_entry_id = None\n\ttry:\n\t\tdb_entry = self.db_client.add_chat_message(session_id=self.session_id, user_query=question)\n\t\tdb_entry_id = db_entry.id if db_entry else None\n\t\tif db_entry_id:\n\t\t\tlogger.debug(f\"Stored current query turn with DB ID: {db_entry_id}\")\n\t\telse:\n\t\t\tlogger.warning(\"Failed to get DB entry ID for current query turn.\")\n\texcept Exception:\n\t\tlogger.exception(\"Failed to store current query turn in DB\")\n\n\t# Get LLM config from the injected ConfigLoader\n\t# At this point self.config_loader is guaranteed to be a valid instance\n\tllm_config = self.config_loader.get_llm_config()\n\tllm_params = {\n\t\t\"temperature\": llm_config.get(\"temperature\", 0.7),\n\t\t\"max_tokens\": llm_config.get(\"max_tokens\", 1024),\n\t}\n\tmodel_name = llm_config.get(\"model\", \"openai/gpt-4o-mini\")\n\tapi_base = llm_config.get(\"api_base\")\n\tapi_key_env_var = llm_config.get(\"api_key_env\")\n\tapi_key = None\n\tif isinstance(api_key_env_var, str):\n\t\tapi_key = os.getenv(api_key_env_var)\n\tapi_key = api_key or llm_config.get(\"api_key\")  # Fallback to direct config key\n\n\t# Call LLM with context\n\ttry:\n\t\twith loading_spinner(\"Waiting for LLM response...\"):\n\t\t\tresponse = completion(\n\t\t\t\tmodel=model_name,\n\t\t\t\tmessages=messages,\n\t\t\t\tapi_base=api_base,\n\t\t\t\tapi_key=api_key,\n\t\t\t\tstream=False,\n\t\t\t\t**llm_params,\n\t\t\t)\n\n\t\t# Extract answer from response\n\t\tanswer = self._extract_answer_from_response(response)\n\n\t\t# Update DB with answer using the dedicated client method\n\t\tif db_entry_id and answer:\n\t\t\t# The update_chat_response method handles its own exceptions and returns success/failure\n\t\t\tsuccess = self.db_client.update_chat_response(message_id=db_entry_id, ai_response=answer)\n\t\t\tif not success:\n\t\t\t\tlogger.warning(f\"Failed to update DB entry {db_entry_id} via client method.\")\n\n\t\treturn AskResult(answer=answer, context=context)\n\texcept Exception as e:  # Keep the outer exception for LLM call errors\n\t\tlogger.exception(\"Error during LLM completion\")\n\t\treturn AskResult(answer=f\"Error: {e!s}\", context=context)\n</code></pre>"},{"location":"api/llm/rag/formatter/","title":"Formatter","text":"<p>Formatter for the ask command output.</p>"},{"location":"api/llm/rag/formatter/#codemap.llm.rag.formatter.format_ask_response","title":"format_ask_response","text":"<pre><code>format_ask_response(response_text: str | None) -&gt; Markdown\n</code></pre> <p>Formats the AI's response text using Rich Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>response_text</code> <code>Optional[str]</code> <p>The text response from the AI.</p> required <p>Returns:</p> Name Type Description <code>Markdown</code> <code>Markdown</code> <p>A Rich Markdown object ready for printing.</p> Source code in <code>src/codemap/llm/rag/formatter.py</code> <pre><code>def format_ask_response(response_text: str | None) -&gt; Markdown:\n\t\"\"\"\n\tFormats the AI's response text using Rich Markdown.\n\n\tArgs:\n\t    response_text (Optional[str]): The text response from the AI.\n\n\tReturns:\n\t    Markdown: A Rich Markdown object ready for printing.\n\n\t\"\"\"\n\tif response_text is None:\n\t\tresponse_text = \"*No response generated.*\"\n\t# Basic Markdown formatting. Can be enhanced later to detect code blocks,\n\t# file paths, etc., and apply specific styling or links.\n\treturn Markdown(response_text)\n</code></pre>"},{"location":"api/llm/rag/formatter/#codemap.llm.rag.formatter.print_ask_result","title":"print_ask_result","text":"<pre><code>print_ask_result(result: dict[str, Any]) -&gt; None\n</code></pre> <p>Prints the structured result of the ask command using Rich.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Dict[str, Any]</code> <p>The structured result containing 'answer' and 'context'.</p> required Source code in <code>src/codemap/llm/rag/formatter.py</code> <pre><code>def print_ask_result(result: dict[str, Any]) -&gt; None:\n\t\"\"\"\n\tPrints the structured result of the ask command using Rich.\n\n\tArgs:\n\t    result (Dict[str, Any]): The structured result containing 'answer' and 'context'.\n\n\t\"\"\"\n\tanswer = result.get(\"answer\")\n\tcontext = result.get(\"context\", [])\n\n\t# Print the main answer\n\trich_print(Panel(format_ask_response(answer), title=\"[bold green]Answer[/]\", border_style=\"green\"))\n\n\t# Print the context used if there are any items\n\tif context:\n\t\t# Build a single string with all context items as a numbered list\n\t\tcontext_list = []\n\t\tfor i, item in enumerate(context, 1):\n\t\t\tfile_path = item.get(\"file_path\", \"Unknown\")\n\t\t\tstart_line = item.get(\"start_line\", -1)\n\t\t\tend_line = item.get(\"end_line\", -1)\n\t\t\tdistance = item.get(\"distance\", -1.0)\n\n\t\t\t# Create the list item text\n\t\t\tlocation = f\"{file_path}\"\n\t\t\tif start_line &gt; 0 and end_line &gt; 0:\n\t\t\t\tlocation += f\" (lines {start_line}-{end_line})\"\n\n\t\t\t# Format with relevance info\n\t\t\trelevance = f\"(similarity: {1 - distance:.2f})\" if distance &gt;= 0 else \"\"\n\t\t\tlist_item = f\"[bold cyan]{i}.[/bold cyan] {location} [dim]{relevance}[/dim]\"\n\t\t\tcontext_list.append(list_item)\n\n\t\t# Join all list items into a single string\n\t\tcontext_content = \"\\n\".join(context_list)\n\n\t\t# Print a single panel with all context items\n\t\trich_print(\n\t\t\tPanel(context_content, title=\"[bold yellow]Context Used[/]\", border_style=\"yellow\", title_align=\"center\")\n\t\t)\n\n\t\trich_print()  # Add space after context\n</code></pre>"},{"location":"api/llm/rag/formatter/#codemap.llm.rag.formatter.format_content_for_context","title":"format_content_for_context","text":"<pre><code>format_content_for_context(\n\tcontext_items: list[dict[str, Any]],\n) -&gt; str\n</code></pre> <p>Format context items into a string suitable for inclusion in prompts.</p> <p>Parameters:</p> Name Type Description Default <code>context_items</code> <code>list[dict[str, Any]]</code> <p>List of context dictionaries with file_path, start_line, end_line, and content</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string with code snippets and file information</p> Source code in <code>src/codemap/llm/rag/formatter.py</code> <pre><code>def format_content_for_context(context_items: list[dict[str, Any]]) -&gt; str:\n\t\"\"\"\n\tFormat context items into a string suitable for inclusion in prompts.\n\n\tArgs:\n\t    context_items: List of context dictionaries with file_path, start_line, end_line, and content\n\n\tReturns:\n\t    Formatted string with code snippets and file information\n\n\t\"\"\"\n\tif not context_items:\n\t\treturn \"No relevant code found in the repository.\"\n\n\tformatted_parts = []\n\n\tfor i, item in enumerate(context_items, 1):\n\t\t# Extract file information\n\t\tfile_path = item.get(\"file_path\", \"Unknown file\")\n\t\tstart_line = item.get(\"start_line\", -1)\n\t\tend_line = item.get(\"end_line\", -1)\n\t\tcontent = item.get(\"content\", \"\")\n\n\t\t# Create a header with file info\n\t\theader = f\"[{i}] {file_path}\"\n\t\tif start_line &gt; 0 and end_line &gt; 0:\n\t\t\theader += f\" (lines {start_line}-{end_line})\"\n\n\t\t# Format the code snippet with the header\n\t\tformatted_parts.append(f\"{header}\\n{'-' * len(header)}\\n{content}\\n\")\n\n\treturn \"\\n\\n\".join(formatted_parts)\n</code></pre>"},{"location":"api/llm/rag/prompts/","title":"Prompts","text":"<p>Prompts for the ask command.</p>"},{"location":"api/llm/rag/prompts/#codemap.llm.rag.prompts.SYSTEM_PROMPT","title":"SYSTEM_PROMPT  <code>module-attribute</code>","text":"<pre><code>SYSTEM_PROMPT = \"\\nYou are a helpful AI assistant integrated into the CodeMap tool.\\nYou'll be given a user question about their codebase along with relevant code chunks from the codebase.\\nProvide concise answers based on the context provided.\\nInclude relevant file paths and code snippets in your response when applicable.\\nFocus on answering the question based *only* on the provided context.\\nIf the provided context doesn't contain enough information to answer the question, say so clearly.\\nDo not make assumptions or provide information not directly present in the provided context.\\n\"\n</code></pre>"},{"location":"api/processor/","title":"Processor Overview","text":"<p>CodeMap processor module.</p> <ul> <li>Lod - Level of Detail (LOD) implementation for code analysis.</li> <li>Pipeline - Unified pipeline for CodeMap data processing, synchronization, and retrieval.</li> <li>Tree Sitter - Tree-sitter based code analysis.</li> <li>Utils - Processor Utilities Package.</li> <li>Vector - Vector processing package for CodeMap.</li> </ul>"},{"location":"api/processor/lod/","title":"Lod","text":"<p>Level of Detail (LOD) implementation for code analysis.</p> <p>This module provides functionality for generating different levels of detail from source code using tree-sitter analysis. The LOD approach provides a hierarchical view of code, from high-level entity names to detailed implementations.</p> <p>LOD levels: - LOD1: Just entity names and types in files (classes, functions, etc.) - LOD2: Entity names with docstrings - LOD3: Entity names, docstrings, and signatures - LOD4: Complete entity implementations</p>"},{"location":"api/processor/lod/#codemap.processor.lod.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel","title":"LODLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of Level of Detail levels.</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>class LODLevel(Enum):\n\t\"\"\"Enumeration of Level of Detail levels.\"\"\"\n\n\tSIGNATURES = 1  # Top-level entity names, docstrings, and signatures\n\tSTRUCTURE = 2  # All entity signatures, indented structure\n\tDOCS = 3  # Level 2 + Docstrings for all entities\n\tFULL = 4  # Level 3 + Full implementation content\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.SIGNATURES","title":"SIGNATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SIGNATURES = 1\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.STRUCTURE","title":"STRUCTURE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STRUCTURE = 2\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.DOCS","title":"DOCS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DOCS = 3\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODLevel.FULL","title":"FULL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FULL = 4\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity","title":"LODEntity  <code>dataclass</code>","text":"<p>Represents a code entity at a specific level of detail.</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>@dataclass\nclass LODEntity:\n\t\"\"\"Represents a code entity at a specific level of detail.\"\"\"\n\n\tname: str\n\t\"\"\"Name of the entity.\"\"\"\n\n\tentity_type: EntityType\n\t\"\"\"Type of entity (class, function, etc.).\"\"\"\n\n\tstart_line: int\n\t\"\"\"Starting line number (1-indexed).\"\"\"\n\n\tend_line: int\n\t\"\"\"Ending line number (1-indexed).\"\"\"\n\n\tdocstring: str = \"\"\n\t\"\"\"Entity docstring, if available.\"\"\"\n\n\tsignature: str = \"\"\n\t\"\"\"Entity signature (e.g., function parameters), if available.\"\"\"\n\n\tcontent: str = \"\"\n\t\"\"\"Complete entity content/implementation.\"\"\"\n\n\tchildren: list[LODEntity] = field(default_factory=list)\n\t\"\"\"Child entities contained within this entity.\"\"\"\n\n\tlanguage: str = \"\"\n\t\"\"\"Programming language of the entity.\"\"\"\n\n\tmetadata: dict[str, Any] = field(default_factory=dict)\n\t\"\"\"Additional metadata about the entity.\"\"\"\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.__init__","title":"__init__","text":"<pre><code>__init__(\n\tname: str,\n\tentity_type: EntityType,\n\tstart_line: int,\n\tend_line: int,\n\tdocstring: str = \"\",\n\tsignature: str = \"\",\n\tcontent: str = \"\",\n\tchildren: list[LODEntity] = list(),\n\tlanguage: str = \"\",\n\tmetadata: dict[str, Any] = dict(),\n) -&gt; None\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Name of the entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.entity_type","title":"entity_type  <code>instance-attribute</code>","text":"<pre><code>entity_type: EntityType\n</code></pre> <p>Type of entity (class, function, etc.).</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.start_line","title":"start_line  <code>instance-attribute</code>","text":"<pre><code>start_line: int\n</code></pre> <p>Starting line number (1-indexed).</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.end_line","title":"end_line  <code>instance-attribute</code>","text":"<pre><code>end_line: int\n</code></pre> <p>Ending line number (1-indexed).</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.docstring","title":"docstring  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>docstring: str = ''\n</code></pre> <p>Entity docstring, if available.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.signature","title":"signature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signature: str = ''\n</code></pre> <p>Entity signature (e.g., function parameters), if available.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: str = ''\n</code></pre> <p>Complete entity content/implementation.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.children","title":"children  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>children: list[LODEntity] = field(default_factory=list)\n</code></pre> <p>Child entities contained within this entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.language","title":"language  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>language: str = ''\n</code></pre> <p>Programming language of the entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODEntity.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Additional metadata about the entity.</p>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator","title":"LODGenerator","text":"<p>Generates different levels of detail from source code.</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>class LODGenerator:\n\t\"\"\"Generates different levels of detail from source code.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the LOD generator.\"\"\"\n\t\tself.analyzer = TreeSitterAnalyzer()\n\n\tdef generate_lod(self, file_path: Path, level: LODLevel = LODLevel.STRUCTURE) -&gt; LODEntity | None:\n\t\t\"\"\"\n\t\tGenerate LOD representation for a file.\n\n\t\tArgs:\n\t\t    file_path: Path to the file to analyze\n\t\t    level: Level of detail to generate (default changed to STRUCTURE)\n\n\t\tReturns:\n\t\t    LODEntity representing the file, or None if analysis failed\n\n\t\t\"\"\"\n\t\t# Read file content\n\t\tcontent = read_file_content(file_path)\n\t\tif not content:\n\t\t\tlogger.warning(f\"Could not read content from {file_path}\")\n\t\t\treturn None\n\n\t\t# Analyze file with tree-sitter\n\t\tanalysis_result = self.analyzer.analyze_file(file_path, content)\n\t\tif not analysis_result:\n\t\t\tlogger.warning(f\"Failed to analyze {file_path}\")\n\t\t\treturn None\n\n\t\t# Convert analysis result to LOD, passing the file_path\n\t\treturn self._convert_to_lod(analysis_result, level, file_path)\n\n\tdef _convert_to_lod(\n\t\tself, analysis_result: dict[str, Any], level: LODLevel, file_path: Path | None = None\n\t) -&gt; LODEntity:\n\t\t\"\"\"\n\t\tConvert tree-sitter analysis to LOD format.\n\n\t\tArgs:\n\t\t    analysis_result: Tree-sitter analysis result\n\t\t    level: Level of detail to generate\n\t\t    file_path: Path to the file being analyzed\n\n\t\tReturns:\n\t\t    LODEntity representation\n\n\t\t\"\"\"\n\t\tentity_type_str = analysis_result.get(\"type\", \"UNKNOWN\")\n\t\ttry:\n\t\t\tentity_type = EntityType[entity_type_str]\n\t\texcept KeyError:\n\t\t\tentity_type = EntityType.UNKNOWN\n\n\t\tlocation = analysis_result.get(\"location\", {})\n\t\tstart_line = location.get(\"start_line\", 1)\n\t\tend_line = location.get(\"end_line\", 1)\n\n\t\t# Create the entity with appropriate level of detail\n\t\tentity = LODEntity(\n\t\t\tname=analysis_result.get(\"name\", \"\"),\n\t\t\tentity_type=entity_type,\n\t\t\tstart_line=start_line,\n\t\t\tend_line=end_line,\n\t\t\tlanguage=analysis_result.get(\"language\", \"\"),\n\t\t)\n\n\t\t# Add file_path to metadata for top-level entities\n\t\tif file_path:\n\t\t\tentity.metadata[\"file_path\"] = str(file_path)\n\n\t\t# Add details based on LOD level (updated comparisons)\n\t\tif level.value &gt;= LODLevel.DOCS.value:\n\t\t\tentity.docstring = analysis_result.get(\"docstring\", \"\")\n\n\t\t# Signature is needed for SIGNATURES (1), STRUCTURE (2), DOCS (3), FULL (4)\n\t\tif level.value &gt;= LODLevel.SIGNATURES.value:\n\t\t\t# Extract signature from content if available\n\t\t\tcontent = analysis_result.get(\"content\", \"\")\n\t\t\tentity.signature = self._extract_signature(content, entity_type, entity.language)\n\n\t\t# Content is needed for FULL (4)\n\t\tif level.value &gt;= LODLevel.FULL.value or entity_type == EntityType.COMMENT:\n\t\t\tentity.content = analysis_result.get(\"content\", \"\")\n\n\t\t# Process children recursively (without passing file_path)\n\t\tchildren = analysis_result.get(\"children\", [])\n\t\tfor child in children:\n\t\t\tchild_entity = self._convert_to_lod(child, level)\n\t\t\tentity.children.append(child_entity)\n\n\t\t# Add any additional metadata\n\t\tif \"dependencies\" in analysis_result:\n\t\t\tentity.metadata[\"dependencies\"] = analysis_result[\"dependencies\"]\n\t\tif \"calls\" in analysis_result:\n\t\t\tentity.metadata[\"calls\"] = analysis_result[\"calls\"]\n\n\t\treturn entity\n\n\tdef _extract_signature(self, content: str, entity_type: EntityType, _language: str) -&gt; str:\n\t\t\"\"\"\n\t\tExtract function/method signature from content.\n\n\t\tThis is a simple implementation; ideally, the language-specific handlers\n\t\tshould provide this functionality.\n\n\t\tArgs:\n\t\t    content: Full entity content\n\t\t    entity_type: Type of entity\n\t\t    _language: Programming language (unused currently)\n\n\t\tReturns:\n\t\t    Signature string\n\n\t\t\"\"\"\n\t\tif not content:\n\t\t\treturn \"\"\n\n\t\t# For functions and methods, extract the first line (declaration)\n\t\tif entity_type in [EntityType.FUNCTION, EntityType.METHOD, EntityType.CLASS, EntityType.INTERFACE]:\n\t\t\tlines = content.split(\"\\n\")\n\t\t\tif lines:\n\t\t\t\t# Return first line without trailing characters\n\t\t\t\treturn lines[0].rstrip(\":{\")\n\n\t\treturn \"\"\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the LOD generator.</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the LOD generator.\"\"\"\n\tself.analyzer = TreeSitterAnalyzer()\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator.analyzer","title":"analyzer  <code>instance-attribute</code>","text":"<pre><code>analyzer = TreeSitterAnalyzer()\n</code></pre>"},{"location":"api/processor/lod/#codemap.processor.lod.LODGenerator.generate_lod","title":"generate_lod","text":"<pre><code>generate_lod(\n\tfile_path: Path, level: LODLevel = STRUCTURE\n) -&gt; LODEntity | None\n</code></pre> <p>Generate LOD representation for a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to analyze</p> required <code>level</code> <code>LODLevel</code> <p>Level of detail to generate (default changed to STRUCTURE)</p> <code>STRUCTURE</code> <p>Returns:</p> Type Description <code>LODEntity | None</code> <p>LODEntity representing the file, or None if analysis failed</p> Source code in <code>src/codemap/processor/lod.py</code> <pre><code>def generate_lod(self, file_path: Path, level: LODLevel = LODLevel.STRUCTURE) -&gt; LODEntity | None:\n\t\"\"\"\n\tGenerate LOD representation for a file.\n\n\tArgs:\n\t    file_path: Path to the file to analyze\n\t    level: Level of detail to generate (default changed to STRUCTURE)\n\n\tReturns:\n\t    LODEntity representing the file, or None if analysis failed\n\n\t\"\"\"\n\t# Read file content\n\tcontent = read_file_content(file_path)\n\tif not content:\n\t\tlogger.warning(f\"Could not read content from {file_path}\")\n\t\treturn None\n\n\t# Analyze file with tree-sitter\n\tanalysis_result = self.analyzer.analyze_file(file_path, content)\n\tif not analysis_result:\n\t\tlogger.warning(f\"Failed to analyze {file_path}\")\n\t\treturn None\n\n\t# Convert analysis result to LOD, passing the file_path\n\treturn self._convert_to_lod(analysis_result, level, file_path)\n</code></pre>"},{"location":"api/processor/pipeline/","title":"Pipeline","text":"<p>Unified pipeline for CodeMap data processing, synchronization, and retrieval.</p> <p>This module defines the <code>ProcessingPipeline</code>, which acts as the central orchestrator for managing and interacting with the HNSW vector database. It handles initialization, synchronization with the Git repository, and provides semantic search capabilities.</p>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline","title":"ProcessingPipeline","text":"<p>Orchestrates data processing, synchronization, and retrieval for CodeMap using Qdrant.</p> <p>Manages connections and interactions with the Qdrant vector database, ensuring it is synchronized with the Git repository state. Provides methods for semantic search. Uses asyncio for database and embedding operations.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>class ProcessingPipeline:\n\t\"\"\"\n\tOrchestrates data processing, synchronization, and retrieval for CodeMap using Qdrant.\n\n\tManages connections and interactions with the Qdrant vector database,\n\tensuring it is synchronized with the Git repository state. Provides\n\tmethods for semantic search. Uses asyncio for database and embedding\n\toperations.\n\n\t\"\"\"\n\n\t# Note: __init__ cannot be async directly. Initialization happens in an async method.\n\tdef __init__(\n\t\tself,\n\t\trepo_path: Path | None = None,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the processing pipeline synchronously.\n\n\t\tCore async initialization is done via `async_init`.\n\n\t\tArgs:\n\t\t    repo_path: Path to the repository root. If None, it will be determined.\n\t\t    config_loader: Application configuration loader. If None, a default one is created.\n\t\t    _progress: Progress bar instance for initialization tracking.\n\t\t    _task_id: Task ID for the progress bar.\n\n\t\t\"\"\"\n\t\tself.repo_path = repo_path or find_project_root()\n\t\tif not self.repo_path:\n\t\t\tmsg = \"Repository path could not be determined.\"\n\t\t\traise ValueError(msg)\n\n\t\tself.config_loader = config_loader or ConfigLoader()\n\t\tself.config = self.config_loader.load_config()\n\n\t\tif not isinstance(self.config, dict):\n\t\t\tlogger.error(f\"Config loading failed or returned unexpected type: {type(self.config)}\")\n\t\t\tmsg = \"Failed to load a valid Config object.\"\n\t\t\traise ConfigError(msg)\n\n\t\t# --- Initialize Shared Components (Synchronous) --- #\n\t\tself.analyzer = TreeSitterAnalyzer()\n\t\tself.chunker = TreeSitterChunker(config_loader=self.config_loader)\n\t\tself.db_client = DatabaseClient()\n\n\t\t# --- Load Configuration --- #\n\t\t# Get embedding configuration\n\t\tembedding_config = self.config_loader.get(\"embedding\", {})\n\t\tembedding_model = embedding_config.get(\"model_name\")\n\t\tqdrant_dimension = embedding_config.get(\"dimension\")\n\t\tdistance_metric = embedding_config.get(\"dimension_metric\", \"cosine\")\n\n\t\t# Make sure embedding_model_name is always a string\n\t\tself.embedding_model_name: str = \"voyage-code-3\"  # Default\n\t\tif embedding_model and isinstance(embedding_model, str):\n\t\t\tself.embedding_model_name = embedding_model\n\n\t\tif not qdrant_dimension:\n\t\t\tlogger.warning(\"Missing qdrant dimension in configuration, using default 1024\")\n\t\t\tqdrant_dimension = 1024\n\n\t\tlogger.info(f\"Using embedding model: {self.embedding_model_name} with dimension: {qdrant_dimension}\")\n\n\t\t# Get Qdrant configuration\n\t\tvector_config = self.config_loader.get(\"embedding\", {})\n\t\tqdrant_location = vector_config.get(\"qdrant_location\")\n\t\tqdrant_collection = vector_config.get(\"qdrant_collection_name\", \"codemap_vectors\")\n\t\tqdrant_url = vector_config.get(\"url\")\n\t\tqdrant_api_key = vector_config.get(\"api_key\")\n\n\t\t# Convert distance metric string to enum\n\t\tdistance_enum = qdrant_models.Distance.COSINE\n\t\tif distance_metric and distance_metric.upper() in [\"COSINE\", \"EUCLID\", \"DOT\"]:\n\t\t\tdistance_enum = getattr(qdrant_models.Distance, distance_metric.upper())\n\n\t\t# Use URL if provided, otherwise use location (defaults to :memory: in QdrantManager)\n\t\tqdrant_init_args = {\n\t\t\t\"config_loader\": self.config_loader,  # Pass ConfigLoader to QdrantManager\n\t\t\t\"collection_name\": qdrant_collection,\n\t\t\t\"dim\": qdrant_dimension,\n\t\t\t\"distance\": distance_enum,\n\t\t}\n\n\t\tif qdrant_url:\n\t\t\tqdrant_init_args[\"url\"] = qdrant_url\n\t\t\tif qdrant_api_key:\n\t\t\t\tqdrant_init_args[\"api_key\"] = qdrant_api_key\n\t\t\tlogger.info(f\"Configuring Qdrant client for URL: {qdrant_url}\")\n\t\telif qdrant_location:\n\t\t\tqdrant_init_args[\"location\"] = qdrant_location\n\t\t\tlogger.info(f\"Configuring Qdrant client for local path/memory: {qdrant_location}\")\n\t\telse:\n\t\t\t# Let QdrantManager use its default (:memory:)\n\t\t\tlogger.info(\"Configuring Qdrant client for default location (:memory:)\")\n\n\t\t# --- Initialize Managers (Synchronous) --- #\n\t\tself.qdrant_manager = QdrantManager(**qdrant_init_args)\n\n\t\t# Initialize VectorSynchronizer with the embedding model name and config_loader\n\t\tself.vector_synchronizer = VectorSynchronizer(\n\t\t\tself.repo_path,\n\t\t\tself.qdrant_manager,\n\t\t\tself.chunker,\n\t\t\tself.embedding_model_name,\n\t\t\tself.analyzer,\n\t\t\tconfig_loader=self.config_loader,  # Pass ConfigLoader to VectorSynchronizer\n\t\t)\n\n\t\tlogger.info(f\"ProcessingPipeline synchronous initialization complete for repo: {self.repo_path}\")\n\t\tself.is_async_initialized = False\n\t\tself.watcher: Watcher | None = None\n\t\tself._watcher_task: asyncio.Task | None = None\n\t\tself._sync_lock = asyncio.Lock()\n\n\tasync def async_init(\n\t\tself, sync_on_init: bool = True, progress: Progress | None = None, task_id: TaskID | None = None\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tPerform asynchronous initialization steps, including Qdrant connection and initial sync.\n\n\t\tArgs:\n\t\t    sync_on_init: If True, run database synchronization during initialization.\n\t\t    progress: Optional rich Progress instance for unified status display.\n\t\t    task_id: Optional rich TaskID for the main initialization/sync task.\n\n\t\t\"\"\"\n\t\tif self.is_async_initialized:\n\t\t\tlogger.info(\"Pipeline already async initialized.\")\n\t\t\treturn\n\n\t\tinit_description = \"Initializing pipeline components...\"\n\t\tif progress and task_id:\n\t\t\tprogress.update(task_id, description=init_description, completed=0, total=100)\n\n\t\ttry:\n\t\t\t# Get embedding configuration for Qdrant URL\n\t\t\tembedding_config = self.config_loader.get(\"embedding\", {})\n\t\t\tqdrant_url = embedding_config.get(\"url\")\n\n\t\t\t# Check for Docker containers\n\t\t\tif qdrant_url:\n\t\t\t\tif progress and task_id:\n\t\t\t\t\tprogress.update(task_id, description=\"Checking Docker containers...\", completed=5)\n\n\t\t\t\t# Only check Docker if we're using a URL that looks like localhost/127.0.0.1\n\t\t\t\tif \"localhost\" in qdrant_url or \"127.0.0.1\" in qdrant_url:\n\t\t\t\t\tlogger.info(\"Ensuring Qdrant container is running\")\n\t\t\t\t\tsuccess, message = await ensure_qdrant_running(wait_for_health=True, qdrant_url=qdrant_url)\n\n\t\t\t\t\tif not success:\n\t\t\t\t\t\tlogger.warning(f\"Docker check failed: {message}\")\n\t\t\t\t\t\tif progress and task_id:\n\t\t\t\t\t\t\tprogress.update(\n\t\t\t\t\t\t\t\ttask_id,\n\t\t\t\t\t\t\t\tdescription=f\"[yellow]Warning:[/yellow] {message}. Continuing anyway...\",\n\t\t\t\t\t\t\t\tcompleted=8,\n\t\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.info(f\"Docker container check: {message}\")\n\t\t\t\t\t\tif progress and task_id:\n\t\t\t\t\t\t\tprogress.update(task_id, description=\"Docker containers ready.\", completed=10)\n\n\t\t\t# Initialize the database client asynchronously\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=\"Initializing database client...\", completed=15)\n\n\t\t\ttry:\n\t\t\t\tawait self.db_client.initialize()\n\t\t\t\tlogger.info(\"Database client initialized successfully.\")\n\t\t\t\tif progress and task_id:\n\t\t\t\t\tprogress.update(task_id, description=\"Database initialized.\", completed=18)\n\t\t\texcept RuntimeError as e:\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"Database initialization failed (RuntimeError): {e}. Some features may not work properly.\"\n\t\t\t\t)\n\t\t\t\tif progress and task_id:\n\t\t\t\t\tprogress.update(\n\t\t\t\t\t\ttask_id,\n\t\t\t\t\t\tdescription=\"[yellow]Warning:[/yellow] Database initialization failed. Continuing anyway...\",\n\t\t\t\t\t\tcompleted=18,\n\t\t\t\t\t)\n\t\t\texcept (ConnectionError, OSError) as e:\n\t\t\t\tlogger.warning(f\"Database connection failed: {e}. Some features may not work properly.\")\n\t\t\t\tif progress and task_id:\n\t\t\t\t\tprogress.update(\n\t\t\t\t\t\ttask_id,\n\t\t\t\t\t\tdescription=\"[yellow]Warning:[/yellow] Database connection failed. Continuing anyway...\",\n\t\t\t\t\t\tcompleted=18,\n\t\t\t\t\t)\n\n\t\t\t# Initialize Qdrant client (connects, creates collection if needed)\n\t\t\tif self.qdrant_manager:\n\t\t\t\tawait self.qdrant_manager.initialize()\n\t\t\t\tlogger.info(\"Qdrant manager initialized asynchronously.\")\n\t\t\t\tif progress and task_id:\n\t\t\t\t\tprogress.update(task_id, description=\"Qdrant connected.\", completed=20)\n\t\t\telse:\n\t\t\t\t# This case should theoretically not happen if __init__ succeeded\n\t\t\t\tmsg = \"QdrantManager was not initialized in __init__.\"\n\t\t\t\tlogger.error(msg)\n\t\t\t\traise RuntimeError(msg)\n\n\t\t\tneeds_sync = False\n\t\t\tif sync_on_init:\n\t\t\t\tneeds_sync = True\n\t\t\t\tlogger.info(\"`sync_on_init` is True. Performing index synchronization...\")\n\t\t\telse:\n\t\t\t\t# Optional: Could add a check here if Qdrant collection is empty\n\t\t\t\t# requires another call to qdrant_manager, e.g., get_count()\n\t\t\t\tlogger.info(\"Skipping sync on init as requested.\")\n\t\t\t\tneeds_sync = False\n\n\t\t\t# Set initialized flag *before* potentially long sync operation\n\t\t\tself.is_async_initialized = True\n\t\t\tlogger.info(\"ProcessingPipeline async core components initialized.\")\n\n\t\t\tif needs_sync:\n\t\t\t\tawait self.sync_databases(progress=progress, task_id=task_id)\n\t\t\t\t# sync_databases handles final progress update on success/failure\n\t\t\telif progress and task_id:\n\t\t\t\t# Update progress if sync was skipped\n\t\t\t\tprogress.update(\n\t\t\t\t\ttask_id, description=\"[green]\u2713[/green] Pipeline initialized (sync skipped).\", completed=100\n\t\t\t\t)\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed during async initialization\")\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=\"[red]Error:[/red] Init failed\", completed=100)\n\t\t\t# Optionally re-raise or handle specific exceptions\n\t\t\traise\n\n\tasync def stop(self) -&gt; None:\n\t\t\"\"\"Stops the pipeline and releases resources, including closing Qdrant connection.\"\"\"\n\t\tlogger.info(\"Stopping ProcessingPipeline asynchronously...\")\n\t\tif self.qdrant_manager:\n\t\t\tawait self.qdrant_manager.close()\n\t\t\tself.qdrant_manager = None  # type: ignore[assignment]\n\t\telse:\n\t\t\tlogger.warning(\"Qdrant Manager already None during stop.\")\n\n\t\t# Stop the watcher if it's running\n\t\tif self._watcher_task and not self._watcher_task.done():\n\t\t\tlogger.info(\"Stopping file watcher...\")\n\t\t\tself._watcher_task.cancel()\n\t\t\ttry:\n\t\t\t\tawait self._watcher_task  # Allow cancellation to propagate\n\t\t\texcept asyncio.CancelledError:\n\t\t\t\tlogger.info(\"File watcher task cancelled.\")\n\t\t\tif self.watcher:\n\t\t\t\tself.watcher.stop()\n\t\t\t\tlogger.info(\"File watcher stopped.\")\n\t\t\tself.watcher = None\n\t\t\tself._watcher_task = None\n\n\t\t# Cleanup database client\n\t\tif hasattr(self, \"db_client\") and self.db_client:\n\t\t\ttry:\n\t\t\t\tawait self.db_client.cleanup()\n\t\t\t\tlogger.info(\"Database client cleaned up.\")\n\t\t\texcept RuntimeError:\n\t\t\t\tlogger.exception(\"Error during database client cleanup\")\n\t\t\texcept (ConnectionError, OSError):\n\t\t\t\tlogger.exception(\"Connection error during database client cleanup\")\n\n\t\t# Other cleanup if needed\n\t\tself.is_async_initialized = False\n\t\tlogger.info(\"ProcessingPipeline stopped.\")\n\n\t# --- Synchronization --- #\n\n\tasync def _sync_callback_wrapper(self) -&gt; None:\n\t\t\"\"\"Async wrapper for the sync callback to handle locking.\"\"\"\n\t\tif self._sync_lock.locked():\n\t\t\tlogger.info(\"Sync already in progress, skipping watcher-triggered sync.\")\n\t\t\treturn\n\n\t\tasync with self._sync_lock:\n\t\t\tlogger.info(\"Watcher triggered sync starting...\")\n\t\t\t# Run sync without progress bars from watcher\n\t\t\tawait self.sync_databases(progress=None, task_id=None)\n\t\t\tlogger.info(\"Watcher triggered sync finished.\")\n\n\tasync def sync_databases(self, progress: Progress | None = None, task_id: TaskID | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tAsynchronously synchronize the Qdrant index with the Git repository state.\n\n\t\tArgs:\n\t\t    progress: Optional rich Progress instance for status updates.\n\t\t    task_id: Optional rich TaskID for the main sync task.\n\n\t\t\"\"\"\n\t\tif not self.is_async_initialized:\n\t\t\tlogger.error(\"Cannot sync databases, async initialization not complete.\")\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=\"[red]Error:[/red] Not initialized\", completed=100)\n\t\t\treturn\n\n\t\t# Acquire lock only if not already held (for watcher calls)\n\t\tif not self._sync_lock.locked():\n\t\t\tasync with self._sync_lock:\n\t\t\t\tlogger.info(\"Starting vector index synchronization using VectorSynchronizer...\")\n\t\t\t\t# VectorSynchronizer handles its own progress updates internally now\n\t\t\t\tawait self.vector_synchronizer.sync_index(progress=progress, task_id=task_id)\n\t\t\t\t# Final status message/logging is handled by sync_index\n\t\telse:\n\t\t\t# If lock is already held (likely by watcher call), just run it\n\t\t\tlogger.info(\"Starting vector index synchronization (lock already held)...\")\n\t\t\tawait self.vector_synchronizer.sync_index(progress=progress, task_id=task_id)\n\n\t# --- Watcher Methods --- #\n\n\tdef initialize_watcher(self, debounce_delay: float = 2.0) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the file watcher.\n\n\t\tArgs:\n\t\t    debounce_delay: Delay in seconds before triggering sync after a file change.\n\n\t\t\"\"\"\n\t\tif not self.repo_path:\n\t\t\tlogger.error(\"Cannot initialize watcher without a repository path.\")\n\t\t\treturn\n\n\t\tif self.watcher:\n\t\t\tlogger.warning(\"Watcher already initialized.\")\n\t\t\treturn\n\n\t\tlogger.info(f\"Initializing file watcher for path: {self.repo_path}\")\n\t\ttry:\n\t\t\tself.watcher = Watcher(\n\t\t\t\tpath_to_watch=self.repo_path,\n\t\t\t\ton_change_callback=self._sync_callback_wrapper,  # Use the lock wrapper\n\t\t\t\tdebounce_delay=debounce_delay,\n\t\t\t)\n\t\t\tlogger.info(\"File watcher initialized.\")\n\t\texcept ValueError:\n\t\t\tlogger.exception(\"Failed to initialize watcher\")\n\t\t\tself.watcher = None\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Unexpected error initializing watcher.\")\n\t\t\tself.watcher = None\n\n\tasync def start_watcher(self) -&gt; None:\n\t\t\"\"\"\n\t\tStart the file watcher in the background.\n\n\t\t`initialize_watcher` must be called first.\n\n\t\t\"\"\"\n\t\tif not self.watcher:\n\t\t\tlogger.error(\"Watcher not initialized. Call initialize_watcher() first.\")\n\t\t\treturn\n\n\t\tif self._watcher_task and not self._watcher_task.done():\n\t\t\tlogger.warning(\"Watcher task is already running.\")\n\t\t\treturn\n\n\t\tlogger.info(\"Starting file watcher task in the background...\")\n\t\t# Create a task to run the watcher's start method asynchronously\n\t\tself._watcher_task = asyncio.create_task(self.watcher.start())\n\t\t# We don't await the task here; it runs independently.\n\t\t# Error handling within the watcher's start method logs issues.\n\n\t# --- Retrieval Methods --- #\n\n\tasync def semantic_search(\n\t\tself,\n\t\tquery: str,\n\t\tk: int = 5,\n\t\tfilter_params: dict[str, Any] | None = None,\n\t) -&gt; list[dict[str, Any]] | None:\n\t\t\"\"\"\n\t\tPerform semantic search for code chunks similar to the query using Qdrant.\n\n\t\tArgs:\n\t\t    query: The search query string.\n\t\t    k: The number of top similar results to retrieve.\n\t\t    filter_params: Optional dictionary for filtering results. Supports:\n\t\t        - exact match: {\"field\": \"value\"} or {\"match\": {\"field\": \"value\"}}\n\t\t        - multiple values: {\"match_any\": {\"field\": [\"value1\", \"value2\"]}}\n\t\t        - range: {\"range\": {\"field\": {\"gt\": value, \"lt\": value}}}\n\t\t        - complex: {\"must\": [...], \"should\": [...], \"must_not\": [...]}\n\n\t\tReturns:\n\t\t    A list of search result dictionaries (Qdrant ScoredPoint converted to dict),\n\t\t    or None if an error occurs.\n\n\t\t\"\"\"\n\t\tif not self.is_async_initialized or not self.qdrant_manager:\n\t\t\tlogger.error(\"QdrantManager not available for semantic search.\")\n\t\t\treturn None\n\n\t\tlogger.debug(\"Performing semantic search for query: '%s', k=%d\", query, k)\n\n\t\ttry:\n\t\t\t# 1. Generate query embedding (must be async)\n\t\t\tquery_embedding = await generate_embedding(\n\t\t\t\tquery,\n\t\t\t\tmodel=self.embedding_model_name,\n\t\t\t\tconfig_loader=self.config_loader,  # Pass ConfigLoader to generate_embedding\n\t\t\t)\n\t\t\tif query_embedding is None:\n\t\t\t\tlogger.error(\"Failed to generate embedding for query.\")\n\t\t\t\treturn None\n\n\t\t\t# Convert to numpy array if needed by Qdrant client, though list is often fine\n\t\t\t# query_vector = np.array(query_embedding, dtype=np.float32)\n\t\t\tquery_vector = query_embedding  # Qdrant client typically accepts list[float]\n\n\t\t\t# 2. Process filter parameters to Qdrant filter format\n\t\t\tquery_filter = None\n\t\t\tif filter_params:\n\t\t\t\tquery_filter = self._build_qdrant_filter(filter_params)\n\t\t\t\tlogger.debug(\"Using filter for search: %s\", query_filter)\n\n\t\t\t# 3. Query Qdrant index (must be async)\n\t\t\tsearch_results: list[qdrant_models.ScoredPoint] = await self.qdrant_manager.search(\n\t\t\t\tquery_vector, k, query_filter=query_filter\n\t\t\t)\n\n\t\t\tif not search_results:\n\t\t\t\tlogger.debug(\"Qdrant search returned no results.\")\n\t\t\t\treturn []\n\n\t\t\t# 4. Format results (convert ScoredPoint to dictionary)\n\t\t\tformatted_results = []\n\t\t\tfor scored_point in search_results:\n\t\t\t\t# Convert Qdrant model to dict for consistent output\n\t\t\t\t# Include score (similarity) and payload\n\t\t\t\tresult_dict = {\n\t\t\t\t\t\"id\": str(scored_point.id),  # Ensure ID is string\n\t\t\t\t\t\"score\": scored_point.score,\n\t\t\t\t\t\"payload\": scored_point.payload,\n\t\t\t\t\t# Optionally include version if needed\n\t\t\t\t\t# \"version\": scored_point.version,\n\t\t\t\t}\n\t\t\t\tformatted_results.append(result_dict)\n\n\t\t\tlogger.debug(\"Semantic search found %d results.\", len(formatted_results))\n\t\t\treturn formatted_results\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error during semantic search.\")\n\t\t\treturn None\n\n\tdef _build_qdrant_filter(self, filter_params: dict[str, Any]) -&gt; qdrant_models.Filter:\n\t\t\"\"\"\n\t\tConvert filter parameters to Qdrant filter format.\n\n\t\tArgs:\n\t\t    filter_params: Dictionary of filter parameters\n\n\t\tReturns:\n\t\t    Qdrant filter object\n\n\t\t\"\"\"\n\t\t# If already a proper Qdrant filter, return as is\n\t\tif isinstance(filter_params, qdrant_models.Filter):\n\t\t\treturn filter_params\n\n\t\t# Check for clause-based filter (must, should, must_not)\n\t\tif any(key in filter_params for key in [\"must\", \"should\", \"must_not\"]):\n\t\t\tfilter_obj = {}\n\n\t\t\t# Process must conditions (AND)\n\t\t\tif \"must\" in filter_params:\n\t\t\t\tfilter_obj[\"must\"] = [self._build_qdrant_filter(cond) for cond in filter_params[\"must\"]]\n\n\t\t\t# Process should conditions (OR)\n\t\t\tif \"should\" in filter_params:\n\t\t\t\tfilter_obj[\"should\"] = [self._build_qdrant_filter(cond) for cond in filter_params[\"should\"]]\n\n\t\t\t# Process must_not conditions (NOT)\n\t\t\tif \"must_not\" in filter_params:\n\t\t\t\tfilter_obj[\"must_not\"] = [self._build_qdrant_filter(cond) for cond in filter_params[\"must_not\"]]\n\n\t\t\treturn qdrant_models.Filter(**filter_obj)\n\n\t\t# Check for condition-based filter (match, range, etc.)\n\t\tif \"match\" in filter_params:\n\t\t\tfield, value = next(iter(filter_params[\"match\"].items()))\n\t\t\treturn qdrant_models.Filter(\n\t\t\t\tmust=[qdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value))]\n\t\t\t)\n\n\t\tif \"match_any\" in filter_params:\n\t\t\tfield, values = next(iter(filter_params[\"match_any\"].items()))\n\t\t\t# For string values\n\t\t\tif (values and isinstance(values[0], str)) or (values and isinstance(values[0], (int, float))):\n\t\t\t\treturn qdrant_models.Filter(\n\t\t\t\t\tshould=[\n\t\t\t\t\t\tqdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value))\n\t\t\t\t\t\tfor value in values\n\t\t\t\t\t]\n\t\t\t\t)\n\t\t\t# Default case\n\t\t\treturn qdrant_models.Filter(\n\t\t\t\tshould=[\n\t\t\t\t\tqdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value))\n\t\t\t\t\tfor value in values\n\t\t\t\t]\n\t\t\t)\n\n\t\tif \"range\" in filter_params:\n\t\t\tfield, range_values = next(iter(filter_params[\"range\"].items()))\n\t\t\treturn qdrant_models.Filter(\n\t\t\t\tmust=[qdrant_models.FieldCondition(key=field, range=qdrant_models.Range(**range_values))]\n\t\t\t)\n\n\t\t# Default: treat as simple field-value pairs (exact match)\n\t\tmust_conditions = []\n\t\tfor field, value in filter_params.items():\n\t\t\tmust_conditions.append(qdrant_models.FieldCondition(key=field, match=qdrant_models.MatchValue(value=value)))\n\n\t\treturn qdrant_models.Filter(must=must_conditions)\n\n\t# Context manager support for async operations\n\tasync def __aenter__(self) -&gt; Self:\n\t\t\"\"\"Return self for use as async context manager.\"\"\"\n\t\t# Basic initialization is sync, async init must be called separately\n\t\t# Consider if automatic async_init here is desired, or keep it explicit\n\t\t# await self.async_init() # Example if auto-init is desired\n\t\treturn self\n\n\tasync def __aexit__(\n\t\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n\t) -&gt; None:\n\t\t\"\"\"Clean up resources when exiting the async context manager.\"\"\"\n\t\tawait self.stop()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_path: Path | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the processing pipeline synchronously.</p> <p>Core async initialization is done via <code>async_init</code>.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path | None</code> <p>Path to the repository root. If None, it will be determined.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Application configuration loader. If None, a default one is created.</p> <code>None</code> <code>_progress</code> <p>Progress bar instance for initialization tracking.</p> required <code>_task_id</code> <p>Task ID for the progress bar.</p> required Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>def __init__(\n\tself,\n\trepo_path: Path | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the processing pipeline synchronously.\n\n\tCore async initialization is done via `async_init`.\n\n\tArgs:\n\t    repo_path: Path to the repository root. If None, it will be determined.\n\t    config_loader: Application configuration loader. If None, a default one is created.\n\t    _progress: Progress bar instance for initialization tracking.\n\t    _task_id: Task ID for the progress bar.\n\n\t\"\"\"\n\tself.repo_path = repo_path or find_project_root()\n\tif not self.repo_path:\n\t\tmsg = \"Repository path could not be determined.\"\n\t\traise ValueError(msg)\n\n\tself.config_loader = config_loader or ConfigLoader()\n\tself.config = self.config_loader.load_config()\n\n\tif not isinstance(self.config, dict):\n\t\tlogger.error(f\"Config loading failed or returned unexpected type: {type(self.config)}\")\n\t\tmsg = \"Failed to load a valid Config object.\"\n\t\traise ConfigError(msg)\n\n\t# --- Initialize Shared Components (Synchronous) --- #\n\tself.analyzer = TreeSitterAnalyzer()\n\tself.chunker = TreeSitterChunker(config_loader=self.config_loader)\n\tself.db_client = DatabaseClient()\n\n\t# --- Load Configuration --- #\n\t# Get embedding configuration\n\tembedding_config = self.config_loader.get(\"embedding\", {})\n\tembedding_model = embedding_config.get(\"model_name\")\n\tqdrant_dimension = embedding_config.get(\"dimension\")\n\tdistance_metric = embedding_config.get(\"dimension_metric\", \"cosine\")\n\n\t# Make sure embedding_model_name is always a string\n\tself.embedding_model_name: str = \"voyage-code-3\"  # Default\n\tif embedding_model and isinstance(embedding_model, str):\n\t\tself.embedding_model_name = embedding_model\n\n\tif not qdrant_dimension:\n\t\tlogger.warning(\"Missing qdrant dimension in configuration, using default 1024\")\n\t\tqdrant_dimension = 1024\n\n\tlogger.info(f\"Using embedding model: {self.embedding_model_name} with dimension: {qdrant_dimension}\")\n\n\t# Get Qdrant configuration\n\tvector_config = self.config_loader.get(\"embedding\", {})\n\tqdrant_location = vector_config.get(\"qdrant_location\")\n\tqdrant_collection = vector_config.get(\"qdrant_collection_name\", \"codemap_vectors\")\n\tqdrant_url = vector_config.get(\"url\")\n\tqdrant_api_key = vector_config.get(\"api_key\")\n\n\t# Convert distance metric string to enum\n\tdistance_enum = qdrant_models.Distance.COSINE\n\tif distance_metric and distance_metric.upper() in [\"COSINE\", \"EUCLID\", \"DOT\"]:\n\t\tdistance_enum = getattr(qdrant_models.Distance, distance_metric.upper())\n\n\t# Use URL if provided, otherwise use location (defaults to :memory: in QdrantManager)\n\tqdrant_init_args = {\n\t\t\"config_loader\": self.config_loader,  # Pass ConfigLoader to QdrantManager\n\t\t\"collection_name\": qdrant_collection,\n\t\t\"dim\": qdrant_dimension,\n\t\t\"distance\": distance_enum,\n\t}\n\n\tif qdrant_url:\n\t\tqdrant_init_args[\"url\"] = qdrant_url\n\t\tif qdrant_api_key:\n\t\t\tqdrant_init_args[\"api_key\"] = qdrant_api_key\n\t\tlogger.info(f\"Configuring Qdrant client for URL: {qdrant_url}\")\n\telif qdrant_location:\n\t\tqdrant_init_args[\"location\"] = qdrant_location\n\t\tlogger.info(f\"Configuring Qdrant client for local path/memory: {qdrant_location}\")\n\telse:\n\t\t# Let QdrantManager use its default (:memory:)\n\t\tlogger.info(\"Configuring Qdrant client for default location (:memory:)\")\n\n\t# --- Initialize Managers (Synchronous) --- #\n\tself.qdrant_manager = QdrantManager(**qdrant_init_args)\n\n\t# Initialize VectorSynchronizer with the embedding model name and config_loader\n\tself.vector_synchronizer = VectorSynchronizer(\n\t\tself.repo_path,\n\t\tself.qdrant_manager,\n\t\tself.chunker,\n\t\tself.embedding_model_name,\n\t\tself.analyzer,\n\t\tconfig_loader=self.config_loader,  # Pass ConfigLoader to VectorSynchronizer\n\t)\n\n\tlogger.info(f\"ProcessingPipeline synchronous initialization complete for repo: {self.repo_path}\")\n\tself.is_async_initialized = False\n\tself.watcher: Watcher | None = None\n\tself._watcher_task: asyncio.Task | None = None\n\tself._sync_lock = asyncio.Lock()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path or find_project_root()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader or ConfigLoader()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = load_config()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.analyzer","title":"analyzer  <code>instance-attribute</code>","text":"<pre><code>analyzer = TreeSitterAnalyzer()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.chunker","title":"chunker  <code>instance-attribute</code>","text":"<pre><code>chunker = TreeSitterChunker(config_loader=config_loader)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.db_client","title":"db_client  <code>instance-attribute</code>","text":"<pre><code>db_client = DatabaseClient()\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.embedding_model_name","title":"embedding_model_name  <code>instance-attribute</code>","text":"<pre><code>embedding_model_name: str = 'voyage-code-3'\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.qdrant_manager","title":"qdrant_manager  <code>instance-attribute</code>","text":"<pre><code>qdrant_manager = QdrantManager(**qdrant_init_args)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.vector_synchronizer","title":"vector_synchronizer  <code>instance-attribute</code>","text":"<pre><code>vector_synchronizer = VectorSynchronizer(\n\trepo_path,\n\tqdrant_manager,\n\tchunker,\n\tembedding_model_name,\n\tanalyzer,\n\tconfig_loader=config_loader,\n)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.is_async_initialized","title":"is_async_initialized  <code>instance-attribute</code>","text":"<pre><code>is_async_initialized = False\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.watcher","title":"watcher  <code>instance-attribute</code>","text":"<pre><code>watcher: Watcher | None = None\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.async_init","title":"async_init  <code>async</code>","text":"<pre><code>async_init(\n\tsync_on_init: bool = True,\n\tprogress: Progress | None = None,\n\ttask_id: TaskID | None = None,\n) -&gt; None\n</code></pre> <p>Perform asynchronous initialization steps, including Qdrant connection and initial sync.</p> <p>Parameters:</p> Name Type Description Default <code>sync_on_init</code> <code>bool</code> <p>If True, run database synchronization during initialization.</p> <code>True</code> <code>progress</code> <code>Progress | None</code> <p>Optional rich Progress instance for unified status display.</p> <code>None</code> <code>task_id</code> <code>TaskID | None</code> <p>Optional rich TaskID for the main initialization/sync task.</p> <code>None</code> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def async_init(\n\tself, sync_on_init: bool = True, progress: Progress | None = None, task_id: TaskID | None = None\n) -&gt; None:\n\t\"\"\"\n\tPerform asynchronous initialization steps, including Qdrant connection and initial sync.\n\n\tArgs:\n\t    sync_on_init: If True, run database synchronization during initialization.\n\t    progress: Optional rich Progress instance for unified status display.\n\t    task_id: Optional rich TaskID for the main initialization/sync task.\n\n\t\"\"\"\n\tif self.is_async_initialized:\n\t\tlogger.info(\"Pipeline already async initialized.\")\n\t\treturn\n\n\tinit_description = \"Initializing pipeline components...\"\n\tif progress and task_id:\n\t\tprogress.update(task_id, description=init_description, completed=0, total=100)\n\n\ttry:\n\t\t# Get embedding configuration for Qdrant URL\n\t\tembedding_config = self.config_loader.get(\"embedding\", {})\n\t\tqdrant_url = embedding_config.get(\"url\")\n\n\t\t# Check for Docker containers\n\t\tif qdrant_url:\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=\"Checking Docker containers...\", completed=5)\n\n\t\t\t# Only check Docker if we're using a URL that looks like localhost/127.0.0.1\n\t\t\tif \"localhost\" in qdrant_url or \"127.0.0.1\" in qdrant_url:\n\t\t\t\tlogger.info(\"Ensuring Qdrant container is running\")\n\t\t\t\tsuccess, message = await ensure_qdrant_running(wait_for_health=True, qdrant_url=qdrant_url)\n\n\t\t\t\tif not success:\n\t\t\t\t\tlogger.warning(f\"Docker check failed: {message}\")\n\t\t\t\t\tif progress and task_id:\n\t\t\t\t\t\tprogress.update(\n\t\t\t\t\t\t\ttask_id,\n\t\t\t\t\t\t\tdescription=f\"[yellow]Warning:[/yellow] {message}. Continuing anyway...\",\n\t\t\t\t\t\t\tcompleted=8,\n\t\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\tlogger.info(f\"Docker container check: {message}\")\n\t\t\t\t\tif progress and task_id:\n\t\t\t\t\t\tprogress.update(task_id, description=\"Docker containers ready.\", completed=10)\n\n\t\t# Initialize the database client asynchronously\n\t\tif progress and task_id:\n\t\t\tprogress.update(task_id, description=\"Initializing database client...\", completed=15)\n\n\t\ttry:\n\t\t\tawait self.db_client.initialize()\n\t\t\tlogger.info(\"Database client initialized successfully.\")\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=\"Database initialized.\", completed=18)\n\t\texcept RuntimeError as e:\n\t\t\tlogger.warning(\n\t\t\t\tf\"Database initialization failed (RuntimeError): {e}. Some features may not work properly.\"\n\t\t\t)\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(\n\t\t\t\t\ttask_id,\n\t\t\t\t\tdescription=\"[yellow]Warning:[/yellow] Database initialization failed. Continuing anyway...\",\n\t\t\t\t\tcompleted=18,\n\t\t\t\t)\n\t\texcept (ConnectionError, OSError) as e:\n\t\t\tlogger.warning(f\"Database connection failed: {e}. Some features may not work properly.\")\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(\n\t\t\t\t\ttask_id,\n\t\t\t\t\tdescription=\"[yellow]Warning:[/yellow] Database connection failed. Continuing anyway...\",\n\t\t\t\t\tcompleted=18,\n\t\t\t\t)\n\n\t\t# Initialize Qdrant client (connects, creates collection if needed)\n\t\tif self.qdrant_manager:\n\t\t\tawait self.qdrant_manager.initialize()\n\t\t\tlogger.info(\"Qdrant manager initialized asynchronously.\")\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=\"Qdrant connected.\", completed=20)\n\t\telse:\n\t\t\t# This case should theoretically not happen if __init__ succeeded\n\t\t\tmsg = \"QdrantManager was not initialized in __init__.\"\n\t\t\tlogger.error(msg)\n\t\t\traise RuntimeError(msg)\n\n\t\tneeds_sync = False\n\t\tif sync_on_init:\n\t\t\tneeds_sync = True\n\t\t\tlogger.info(\"`sync_on_init` is True. Performing index synchronization...\")\n\t\telse:\n\t\t\t# Optional: Could add a check here if Qdrant collection is empty\n\t\t\t# requires another call to qdrant_manager, e.g., get_count()\n\t\t\tlogger.info(\"Skipping sync on init as requested.\")\n\t\t\tneeds_sync = False\n\n\t\t# Set initialized flag *before* potentially long sync operation\n\t\tself.is_async_initialized = True\n\t\tlogger.info(\"ProcessingPipeline async core components initialized.\")\n\n\t\tif needs_sync:\n\t\t\tawait self.sync_databases(progress=progress, task_id=task_id)\n\t\t\t# sync_databases handles final progress update on success/failure\n\t\telif progress and task_id:\n\t\t\t# Update progress if sync was skipped\n\t\t\tprogress.update(\n\t\t\t\ttask_id, description=\"[green]\u2713[/green] Pipeline initialized (sync skipped).\", completed=100\n\t\t\t)\n\n\texcept Exception:\n\t\tlogger.exception(\"Failed during async initialization\")\n\t\tif progress and task_id:\n\t\t\tprogress.update(task_id, description=\"[red]Error:[/red] Init failed\", completed=100)\n\t\t# Optionally re-raise or handle specific exceptions\n\t\traise\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.stop","title":"stop  <code>async</code>","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stops the pipeline and releases resources, including closing Qdrant connection.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def stop(self) -&gt; None:\n\t\"\"\"Stops the pipeline and releases resources, including closing Qdrant connection.\"\"\"\n\tlogger.info(\"Stopping ProcessingPipeline asynchronously...\")\n\tif self.qdrant_manager:\n\t\tawait self.qdrant_manager.close()\n\t\tself.qdrant_manager = None  # type: ignore[assignment]\n\telse:\n\t\tlogger.warning(\"Qdrant Manager already None during stop.\")\n\n\t# Stop the watcher if it's running\n\tif self._watcher_task and not self._watcher_task.done():\n\t\tlogger.info(\"Stopping file watcher...\")\n\t\tself._watcher_task.cancel()\n\t\ttry:\n\t\t\tawait self._watcher_task  # Allow cancellation to propagate\n\t\texcept asyncio.CancelledError:\n\t\t\tlogger.info(\"File watcher task cancelled.\")\n\t\tif self.watcher:\n\t\t\tself.watcher.stop()\n\t\t\tlogger.info(\"File watcher stopped.\")\n\t\tself.watcher = None\n\t\tself._watcher_task = None\n\n\t# Cleanup database client\n\tif hasattr(self, \"db_client\") and self.db_client:\n\t\ttry:\n\t\t\tawait self.db_client.cleanup()\n\t\t\tlogger.info(\"Database client cleaned up.\")\n\t\texcept RuntimeError:\n\t\t\tlogger.exception(\"Error during database client cleanup\")\n\t\texcept (ConnectionError, OSError):\n\t\t\tlogger.exception(\"Connection error during database client cleanup\")\n\n\t# Other cleanup if needed\n\tself.is_async_initialized = False\n\tlogger.info(\"ProcessingPipeline stopped.\")\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.sync_databases","title":"sync_databases  <code>async</code>","text":"<pre><code>sync_databases(\n\tprogress: Progress | None = None,\n\ttask_id: TaskID | None = None,\n) -&gt; None\n</code></pre> <p>Asynchronously synchronize the Qdrant index with the Git repository state.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>Progress | None</code> <p>Optional rich Progress instance for status updates.</p> <code>None</code> <code>task_id</code> <code>TaskID | None</code> <p>Optional rich TaskID for the main sync task.</p> <code>None</code> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def sync_databases(self, progress: Progress | None = None, task_id: TaskID | None = None) -&gt; None:\n\t\"\"\"\n\tAsynchronously synchronize the Qdrant index with the Git repository state.\n\n\tArgs:\n\t    progress: Optional rich Progress instance for status updates.\n\t    task_id: Optional rich TaskID for the main sync task.\n\n\t\"\"\"\n\tif not self.is_async_initialized:\n\t\tlogger.error(\"Cannot sync databases, async initialization not complete.\")\n\t\tif progress and task_id:\n\t\t\tprogress.update(task_id, description=\"[red]Error:[/red] Not initialized\", completed=100)\n\t\treturn\n\n\t# Acquire lock only if not already held (for watcher calls)\n\tif not self._sync_lock.locked():\n\t\tasync with self._sync_lock:\n\t\t\tlogger.info(\"Starting vector index synchronization using VectorSynchronizer...\")\n\t\t\t# VectorSynchronizer handles its own progress updates internally now\n\t\t\tawait self.vector_synchronizer.sync_index(progress=progress, task_id=task_id)\n\t\t\t# Final status message/logging is handled by sync_index\n\telse:\n\t\t# If lock is already held (likely by watcher call), just run it\n\t\tlogger.info(\"Starting vector index synchronization (lock already held)...\")\n\t\tawait self.vector_synchronizer.sync_index(progress=progress, task_id=task_id)\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.initialize_watcher","title":"initialize_watcher","text":"<pre><code>initialize_watcher(debounce_delay: float = 2.0) -&gt; None\n</code></pre> <p>Initialize the file watcher.</p> <p>Parameters:</p> Name Type Description Default <code>debounce_delay</code> <code>float</code> <p>Delay in seconds before triggering sync after a file change.</p> <code>2.0</code> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>def initialize_watcher(self, debounce_delay: float = 2.0) -&gt; None:\n\t\"\"\"\n\tInitialize the file watcher.\n\n\tArgs:\n\t    debounce_delay: Delay in seconds before triggering sync after a file change.\n\n\t\"\"\"\n\tif not self.repo_path:\n\t\tlogger.error(\"Cannot initialize watcher without a repository path.\")\n\t\treturn\n\n\tif self.watcher:\n\t\tlogger.warning(\"Watcher already initialized.\")\n\t\treturn\n\n\tlogger.info(f\"Initializing file watcher for path: {self.repo_path}\")\n\ttry:\n\t\tself.watcher = Watcher(\n\t\t\tpath_to_watch=self.repo_path,\n\t\t\ton_change_callback=self._sync_callback_wrapper,  # Use the lock wrapper\n\t\t\tdebounce_delay=debounce_delay,\n\t\t)\n\t\tlogger.info(\"File watcher initialized.\")\n\texcept ValueError:\n\t\tlogger.exception(\"Failed to initialize watcher\")\n\t\tself.watcher = None\n\texcept Exception:\n\t\tlogger.exception(\"Unexpected error initializing watcher.\")\n\t\tself.watcher = None\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.start_watcher","title":"start_watcher  <code>async</code>","text":"<pre><code>start_watcher() -&gt; None\n</code></pre> <p>Start the file watcher in the background.</p> <p><code>initialize_watcher</code> must be called first.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def start_watcher(self) -&gt; None:\n\t\"\"\"\n\tStart the file watcher in the background.\n\n\t`initialize_watcher` must be called first.\n\n\t\"\"\"\n\tif not self.watcher:\n\t\tlogger.error(\"Watcher not initialized. Call initialize_watcher() first.\")\n\t\treturn\n\n\tif self._watcher_task and not self._watcher_task.done():\n\t\tlogger.warning(\"Watcher task is already running.\")\n\t\treturn\n\n\tlogger.info(\"Starting file watcher task in the background...\")\n\t# Create a task to run the watcher's start method asynchronously\n\tself._watcher_task = asyncio.create_task(self.watcher.start())\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.semantic_search","title":"semantic_search  <code>async</code>","text":"<pre><code>semantic_search(\n\tquery: str,\n\tk: int = 5,\n\tfilter_params: dict[str, Any] | None = None,\n) -&gt; list[dict[str, Any]] | None\n</code></pre> <p>Perform semantic search for code chunks similar to the query using Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query string.</p> required <code>k</code> <code>int</code> <p>The number of top similar results to retrieve.</p> <code>5</code> <code>filter_params</code> <code>dict[str, Any] | None</code> <p>Optional dictionary for filtering results. Supports: - exact match: {\"field\": \"value\"} or {\"match\": {\"field\": \"value\"}} - multiple values: {\"match_any\": {\"field\": [\"value1\", \"value2\"]}} - range: {\"range\": {\"field\": {\"gt\": value, \"lt\": value}}} - complex: {\"must\": [...], \"should\": [...], \"must_not\": [...]}</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]] | None</code> <p>A list of search result dictionaries (Qdrant ScoredPoint converted to dict),</p> <code>list[dict[str, Any]] | None</code> <p>or None if an error occurs.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def semantic_search(\n\tself,\n\tquery: str,\n\tk: int = 5,\n\tfilter_params: dict[str, Any] | None = None,\n) -&gt; list[dict[str, Any]] | None:\n\t\"\"\"\n\tPerform semantic search for code chunks similar to the query using Qdrant.\n\n\tArgs:\n\t    query: The search query string.\n\t    k: The number of top similar results to retrieve.\n\t    filter_params: Optional dictionary for filtering results. Supports:\n\t        - exact match: {\"field\": \"value\"} or {\"match\": {\"field\": \"value\"}}\n\t        - multiple values: {\"match_any\": {\"field\": [\"value1\", \"value2\"]}}\n\t        - range: {\"range\": {\"field\": {\"gt\": value, \"lt\": value}}}\n\t        - complex: {\"must\": [...], \"should\": [...], \"must_not\": [...]}\n\n\tReturns:\n\t    A list of search result dictionaries (Qdrant ScoredPoint converted to dict),\n\t    or None if an error occurs.\n\n\t\"\"\"\n\tif not self.is_async_initialized or not self.qdrant_manager:\n\t\tlogger.error(\"QdrantManager not available for semantic search.\")\n\t\treturn None\n\n\tlogger.debug(\"Performing semantic search for query: '%s', k=%d\", query, k)\n\n\ttry:\n\t\t# 1. Generate query embedding (must be async)\n\t\tquery_embedding = await generate_embedding(\n\t\t\tquery,\n\t\t\tmodel=self.embedding_model_name,\n\t\t\tconfig_loader=self.config_loader,  # Pass ConfigLoader to generate_embedding\n\t\t)\n\t\tif query_embedding is None:\n\t\t\tlogger.error(\"Failed to generate embedding for query.\")\n\t\t\treturn None\n\n\t\t# Convert to numpy array if needed by Qdrant client, though list is often fine\n\t\t# query_vector = np.array(query_embedding, dtype=np.float32)\n\t\tquery_vector = query_embedding  # Qdrant client typically accepts list[float]\n\n\t\t# 2. Process filter parameters to Qdrant filter format\n\t\tquery_filter = None\n\t\tif filter_params:\n\t\t\tquery_filter = self._build_qdrant_filter(filter_params)\n\t\t\tlogger.debug(\"Using filter for search: %s\", query_filter)\n\n\t\t# 3. Query Qdrant index (must be async)\n\t\tsearch_results: list[qdrant_models.ScoredPoint] = await self.qdrant_manager.search(\n\t\t\tquery_vector, k, query_filter=query_filter\n\t\t)\n\n\t\tif not search_results:\n\t\t\tlogger.debug(\"Qdrant search returned no results.\")\n\t\t\treturn []\n\n\t\t# 4. Format results (convert ScoredPoint to dictionary)\n\t\tformatted_results = []\n\t\tfor scored_point in search_results:\n\t\t\t# Convert Qdrant model to dict for consistent output\n\t\t\t# Include score (similarity) and payload\n\t\t\tresult_dict = {\n\t\t\t\t\"id\": str(scored_point.id),  # Ensure ID is string\n\t\t\t\t\"score\": scored_point.score,\n\t\t\t\t\"payload\": scored_point.payload,\n\t\t\t\t# Optionally include version if needed\n\t\t\t\t# \"version\": scored_point.version,\n\t\t\t}\n\t\t\tformatted_results.append(result_dict)\n\n\t\tlogger.debug(\"Semantic search found %d results.\", len(formatted_results))\n\t\treturn formatted_results\n\n\texcept Exception:\n\t\tlogger.exception(\"Error during semantic search.\")\n\t\treturn None\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; Self\n</code></pre> <p>Return self for use as async context manager.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def __aenter__(self) -&gt; Self:\n\t\"\"\"Return self for use as async context manager.\"\"\"\n\t# Basic initialization is sync, async init must be called separately\n\t# Consider if automatic async_init here is desired, or keep it explicit\n\t# await self.async_init() # Example if auto-init is desired\n\treturn self\n</code></pre>"},{"location":"api/processor/pipeline/#codemap.processor.pipeline.ProcessingPipeline.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n\texc_type: type[BaseException] | None,\n\texc_val: BaseException | None,\n\texc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Clean up resources when exiting the async context manager.</p> Source code in <code>src/codemap/processor/pipeline.py</code> <pre><code>async def __aexit__(\n\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n) -&gt; None:\n\t\"\"\"Clean up resources when exiting the async context manager.\"\"\"\n\tawait self.stop()\n</code></pre>"},{"location":"api/processor/tree_sitter/","title":"Tree Sitter Overview","text":"<p>Tree-sitter based code analysis.</p> <ul> <li>Analyzer - Tree-sitter based code analysis.</li> <li>Base - Base classes and interfaces for tree-sitter analysis.</li> <li>Languages - Language-specific configurations and handlers for tree-sitter analysis.</li> </ul>"},{"location":"api/processor/tree_sitter/analyzer/","title":"Analyzer","text":"<p>Tree-sitter based code analysis.</p> <p>This module provides functionality for analyzing source code using tree- sitter.</p>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.LANGUAGE_NAMES","title":"LANGUAGE_NAMES  <code>module-attribute</code>","text":"<pre><code>LANGUAGE_NAMES: dict[str, SupportedLanguage] = {\n\t\"python\": \"python\",\n\t\"javascript\": \"javascript\",\n\t\"typescript\": \"typescript\",\n}\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.get_language_by_extension","title":"get_language_by_extension","text":"<pre><code>get_language_by_extension(file_path: Path) -&gt; str | None\n</code></pre> <p>Get language name from file extension.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Language name if supported, None otherwise</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def get_language_by_extension(file_path: Path) -&gt; str | None:\n\t\"\"\"\n\tGet language name from file extension.\n\n\tArgs:\n\t    file_path: Path to the file\n\n\tReturns:\n\t    Language name if supported, None otherwise\n\n\t\"\"\"\n\text = file_path.suffix\n\tfor lang, config in LANGUAGE_CONFIGS.items():\n\t\tif ext in config.file_extensions:\n\t\t\treturn lang\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer","title":"TreeSitterAnalyzer","text":"<p>Analyzer for source code using tree-sitter.</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>class TreeSitterAnalyzer:\n\t\"\"\"Analyzer for source code using tree-sitter.\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize the tree-sitter analyzer.\"\"\"\n\t\tself.parsers: dict[str, Parser] = {}\n\t\tself._load_parsers()\n\n\tdef _load_parsers(self) -&gt; None:\n\t\t\"\"\"\n\t\tLoad tree-sitter parsers for supported languages.\n\n\t\tThis method attempts to load parsers for all configured languages\n\t\tusing tree-sitter-language-pack. If a language fails to load, it will\n\t\tbe logged but won't prevent other languages from loading.\n\n\t\t\"\"\"\n\t\tself.parsers: dict[str, Parser] = {}\n\t\tfailed_languages: list[tuple[str, str]] = []\n\n\t\tfor lang in LANGUAGE_CONFIGS:\n\t\t\ttry:\n\t\t\t\t# Get the language name for tree-sitter-language-pack\n\t\t\t\tts_lang_name = LANGUAGE_NAMES.get(lang)\n\t\t\t\tif not ts_lang_name:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Get the language from tree-sitter-language-pack\n\t\t\t\tlanguage: Language = get_language(ts_lang_name)\n\n\t\t\t\t# Create a new parser and set its language\n\t\t\t\tparser: Parser = Parser()\n\t\t\t\tparser.language = language\n\n\t\t\t\tself.parsers[lang] = parser\n\t\t\texcept (ValueError, RuntimeError, ImportError) as e:\n\t\t\t\tfailed_languages.append((lang, str(e)))\n\t\t\t\tlogger.debug(\"Failed to load language %s: %s\", lang, str(e))\n\n\t\tif failed_languages:\n\t\t\tfailed_names = \", \".join(f\"{lang} ({err})\" for lang, err in failed_languages)\n\t\t\tlogger.warning(\"Failed to load parsers for languages: %s\", failed_names)\n\n\tdef get_parser(self, language: str) -&gt; Parser | None:\n\t\t\"\"\"\n\t\tGet the parser for a language.\n\n\t\tArgs:\n\t\t    language: The language to get a parser for\n\n\t\tReturns:\n\t\t    A tree-sitter parser or None if not supported\n\n\t\t\"\"\"\n\t\treturn self.parsers.get(language)\n\n\tdef parse_file(self, file_path: Path, content: str, language: str | None = None) -&gt; tuple[Node | None, str]:\n\t\t\"\"\"\n\t\tParse a file and return its root node and determined language.\n\n\t\tArgs:\n\t\t    file_path: Path to the file to parse\n\t\t    content: Content of the file\n\t\t    language: Optional language override\n\n\t\tReturns:\n\t\t    A tuple containing the parse tree root node (or None if parsing failed)\n\t\t    and the determined language\n\n\t\t\"\"\"\n\t\t# Determine language if not provided\n\t\tif not language:\n\t\t\tlanguage = get_language_by_extension(file_path)\n\t\t\tif not language:\n\t\t\t\tlogger.warning(\"Could not determine language for file %s\", file_path)\n\t\t\t\treturn None, \"\"\n\n\t\t# Get the parser for this language\n\t\tparser = self.get_parser(language)\n\t\tif not parser:\n\t\t\tlogger.warning(\"No parser for language %s\", language)\n\t\t\treturn None, language\n\n\t\ttry:\n\t\t\t# Parse the content using tree-sitter\n\t\t\tcontent_bytes = content.encode(\"utf-8\")\n\t\t\ttree = parser.parse(content_bytes)\n\t\t\treturn tree.root_node, language\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to parse file %s\", file_path)\n\t\t\treturn None, language\n\n\tdef get_syntax_handler(self, language: str) -&gt; LanguageSyntaxHandler | None:\n\t\t\"\"\"\n\t\tGet the syntax handler for a language.\n\n\t\tArgs:\n\t\t    language: The language to get a handler for\n\n\t\tReturns:\n\t\t    A syntax handler or None if not supported\n\n\t\t\"\"\"\n\t\thandler_class = LANGUAGE_HANDLERS.get(language)\n\t\tif not handler_class:\n\t\t\treturn None\n\t\treturn handler_class()\n\n\tdef analyze_node(\n\t\tself,\n\t\tnode: Node,\n\t\tcontent_bytes: bytes,\n\t\tfile_path: Path,\n\t\tlanguage: str,\n\t\thandler: LanguageSyntaxHandler,\n\t\tparent_node: Node | None = None,\n\t) -&gt; dict:\n\t\t\"\"\"\n\t\tAnalyze a tree-sitter node and return structured information.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\t\t    file_path: Path to the source file\n\t\t    language: Programming language\n\t\t    handler: Language-specific syntax handler\n\t\t    parent_node: Parent node if any\n\n\t\tReturns:\n\t\t    Dict with node analysis information\n\n\t\t\"\"\"\n\t\t# Check if we should skip this node\n\t\tif handler.should_skip_node(node):\n\t\t\treturn {}\n\n\t\t# Get entity type for this node from the handler\n\t\tentity_type = handler.get_entity_type(node, parent_node, content_bytes)\n\n\t\t# Skip unknown/uninteresting nodes unless they might contain interesting children\n\t\tif entity_type == EntityType.UNKNOWN and not node.named_child_count &gt; 0:\n\t\t\treturn {}\n\n\t\t# Get name and other metadata\n\t\tname = handler.extract_name(node, content_bytes)\n\t\tdocstring_text, docstring_node = handler.find_docstring(node, content_bytes)\n\n\t\t# Get node content\n\t\ttry:\n\t\t\tnode_content = content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\tnode_content = \"\"\n\n\t\t# Extract dependencies from import statements\n\t\tdependencies = []\n\t\tif entity_type == EntityType.IMPORT:\n\t\t\ttry:\n\t\t\t\tdependencies = handler.extract_imports(node, content_bytes)\n\t\t\texcept (AttributeError, UnicodeDecodeError, IndexError, ValueError) as e:\n\t\t\t\tlogger.warning(\"Failed to extract dependencies: %s\", e)\n\n\t\t# Build result\n\t\tresult = {\n\t\t\t\"type\": entity_type.name if entity_type != EntityType.UNKNOWN else \"UNKNOWN\",\n\t\t\t\"name\": name,\n\t\t\t\"location\": {\n\t\t\t\t\"start_line\": node.start_point[0] + 1,  # Convert to 1-based\n\t\t\t\t\"end_line\": node.end_point[0] + 1,\n\t\t\t\t\"start_col\": node.start_point[1],\n\t\t\t\t\"end_col\": node.end_point[1],\n\t\t\t},\n\t\t\t\"docstring\": docstring_text,\n\t\t\t\"content\": node_content,\n\t\t\t\"children\": [],\n\t\t\t\"language\": language,\n\t\t}\n\n\t\t# Add dependencies only if they exist to keep the output clean\n\t\tif dependencies:\n\t\t\tresult[\"dependencies\"] = dependencies\n\n\t\t# Extract function calls if the entity is a function or method\n\t\tcalls = []\n\t\tif entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\t\tbody_node = handler.get_body_node(node)\n\t\t\tif body_node:\n\t\t\t\ttry:\n\t\t\t\t\tcalls = handler.extract_calls(body_node, content_bytes)\n\t\t\t\texcept (AttributeError, IndexError, UnicodeDecodeError, ValueError) as e:\n\t\t\t\t\tlogger.warning(\"Failed to extract calls for %s: %s\", name or \"&lt;anonymous&gt;\", e)\n\n\t\t# Add calls only if they exist\n\t\tif calls:\n\t\t\tresult[\"calls\"] = calls\n\n\t\t# Process child nodes\n\t\tbody_node = handler.get_body_node(node)\n\t\tchildren_to_process = handler.get_children_to_process(node, body_node)\n\n\t\tfor child in children_to_process:\n\t\t\tif docstring_node and child == docstring_node:\n\t\t\t\tcontinue  # Skip docstring node\n\n\t\t\tchild_result = self.analyze_node(child, content_bytes, file_path, language, handler, node)\n\n\t\t\tif child_result:  # Only add non-empty results\n\t\t\t\tresult[\"children\"].append(child_result)\n\n\t\treturn result\n\n\tdef analyze_file(\n\t\tself,\n\t\tfile_path: Path,\n\t\tcontent: str,\n\t\tlanguage: str | None = None,\n\t) -&gt; dict:\n\t\t\"\"\"\n\t\tAnalyze a file and return its structural information.\n\n\t\tArgs:\n\t\t    file_path: Path to the file to analyze\n\t\t    content: Content of the file\n\t\t    language: Optional language override\n\t\t    git_metadata: Optional Git metadata\n\n\t\tReturns:\n\t\t    Dict with file analysis information\n\n\t\t\"\"\"\n\t\t# Parse the file\n\t\troot_node, determined_language = self.parse_file(file_path, content, language)\n\t\tif not root_node or not determined_language:\n\t\t\treturn {\n\t\t\t\t\"file\": str(file_path),\n\t\t\t\t\"language\": determined_language or \"unknown\",\n\t\t\t\t\"success\": False,\n\t\t\t\t\"error\": \"Failed to parse file\",\n\t\t\t}\n\n\t\t# Get handler for the language\n\t\thandler = self.get_syntax_handler(determined_language)\n\t\tif not handler:\n\t\t\treturn {\n\t\t\t\t\"file\": str(file_path),\n\t\t\t\t\"language\": determined_language,\n\t\t\t\t\"success\": False,\n\t\t\t\t\"error\": f\"No handler for language {determined_language}\",\n\t\t\t}\n\n\t\t# Analyze the root node\n\t\tcontent_bytes = content.encode(\"utf-8\")\n\t\tentity_type = handler.get_entity_type(root_node, None, content_bytes)\n\t\tif entity_type == EntityType.UNKNOWN:\n\t\t\tentity_type = EntityType.MODULE  # Default to MODULE type\n\n\t\t# Extract module-level docstring\n\t\tmodule_description, module_docstring_node = handler.find_docstring(root_node, content_bytes)\n\n\t\t# Create result\n\t\tresult = {\n\t\t\t\"file\": str(file_path),\n\t\t\t\"language\": determined_language,\n\t\t\t\"success\": True,\n\t\t\t\"type\": entity_type.name,\n\t\t\t\"name\": file_path.stem,\n\t\t\t\"location\": {\n\t\t\t\t\"start_line\": root_node.start_point[0] + 1,\n\t\t\t\t\"end_line\": root_node.end_point[0] + 1,\n\t\t\t\t\"start_col\": root_node.start_point[1],\n\t\t\t\t\"end_col\": root_node.end_point[1],\n\t\t\t},\n\t\t\t\"docstring\": module_description,\n\t\t\t\"children\": [],\n\t\t}\n\n\t\t# Process children of the root node\n\t\tchildren_to_process = handler.get_children_to_process(root_node, None)\n\t\tfor child in children_to_process:\n\t\t\t# Skip the module docstring node if found\n\t\t\tif module_docstring_node and child == module_docstring_node:\n\t\t\t\tcontinue\n\n\t\t\tchild_result = self.analyze_node(child, content_bytes, file_path, determined_language, handler)\n\n\t\t\tif child_result:  # Only add non-empty results\n\t\t\t\tresult[\"children\"].append(child_result)\n\n\t\treturn result\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the tree-sitter analyzer.</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize the tree-sitter analyzer.\"\"\"\n\tself.parsers: dict[str, Parser] = {}\n\tself._load_parsers()\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.parsers","title":"parsers  <code>instance-attribute</code>","text":"<pre><code>parsers: dict[str, Parser] = {}\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.get_parser","title":"get_parser","text":"<pre><code>get_parser(language: str) -&gt; Parser | None\n</code></pre> <p>Get the parser for a language.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>The language to get a parser for</p> required <p>Returns:</p> Type Description <code>Parser | None</code> <p>A tree-sitter parser or None if not supported</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def get_parser(self, language: str) -&gt; Parser | None:\n\t\"\"\"\n\tGet the parser for a language.\n\n\tArgs:\n\t    language: The language to get a parser for\n\n\tReturns:\n\t    A tree-sitter parser or None if not supported\n\n\t\"\"\"\n\treturn self.parsers.get(language)\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.parse_file","title":"parse_file","text":"<pre><code>parse_file(\n\tfile_path: Path,\n\tcontent: str,\n\tlanguage: str | None = None,\n) -&gt; tuple[Node | None, str]\n</code></pre> <p>Parse a file and return its root node and determined language.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to parse</p> required <code>content</code> <code>str</code> <p>Content of the file</p> required <code>language</code> <code>str | None</code> <p>Optional language override</p> <code>None</code> <p>Returns:</p> Type Description <code>Node | None</code> <p>A tuple containing the parse tree root node (or None if parsing failed)</p> <code>str</code> <p>and the determined language</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def parse_file(self, file_path: Path, content: str, language: str | None = None) -&gt; tuple[Node | None, str]:\n\t\"\"\"\n\tParse a file and return its root node and determined language.\n\n\tArgs:\n\t    file_path: Path to the file to parse\n\t    content: Content of the file\n\t    language: Optional language override\n\n\tReturns:\n\t    A tuple containing the parse tree root node (or None if parsing failed)\n\t    and the determined language\n\n\t\"\"\"\n\t# Determine language if not provided\n\tif not language:\n\t\tlanguage = get_language_by_extension(file_path)\n\t\tif not language:\n\t\t\tlogger.warning(\"Could not determine language for file %s\", file_path)\n\t\t\treturn None, \"\"\n\n\t# Get the parser for this language\n\tparser = self.get_parser(language)\n\tif not parser:\n\t\tlogger.warning(\"No parser for language %s\", language)\n\t\treturn None, language\n\n\ttry:\n\t\t# Parse the content using tree-sitter\n\t\tcontent_bytes = content.encode(\"utf-8\")\n\t\ttree = parser.parse(content_bytes)\n\t\treturn tree.root_node, language\n\texcept Exception:\n\t\tlogger.exception(\"Failed to parse file %s\", file_path)\n\t\treturn None, language\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.get_syntax_handler","title":"get_syntax_handler","text":"<pre><code>get_syntax_handler(\n\tlanguage: str,\n) -&gt; LanguageSyntaxHandler | None\n</code></pre> <p>Get the syntax handler for a language.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>The language to get a handler for</p> required <p>Returns:</p> Type Description <code>LanguageSyntaxHandler | None</code> <p>A syntax handler or None if not supported</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def get_syntax_handler(self, language: str) -&gt; LanguageSyntaxHandler | None:\n\t\"\"\"\n\tGet the syntax handler for a language.\n\n\tArgs:\n\t    language: The language to get a handler for\n\n\tReturns:\n\t    A syntax handler or None if not supported\n\n\t\"\"\"\n\thandler_class = LANGUAGE_HANDLERS.get(language)\n\tif not handler_class:\n\t\treturn None\n\treturn handler_class()\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.analyze_node","title":"analyze_node","text":"<pre><code>analyze_node(\n\tnode: Node,\n\tcontent_bytes: bytes,\n\tfile_path: Path,\n\tlanguage: str,\n\thandler: LanguageSyntaxHandler,\n\tparent_node: Node | None = None,\n) -&gt; dict\n</code></pre> <p>Analyze a tree-sitter node and return structured information.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <code>file_path</code> <code>Path</code> <p>Path to the source file</p> required <code>language</code> <code>str</code> <p>Programming language</p> required <code>handler</code> <code>LanguageSyntaxHandler</code> <p>Language-specific syntax handler</p> required <code>parent_node</code> <code>Node | None</code> <p>Parent node if any</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with node analysis information</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def analyze_node(\n\tself,\n\tnode: Node,\n\tcontent_bytes: bytes,\n\tfile_path: Path,\n\tlanguage: str,\n\thandler: LanguageSyntaxHandler,\n\tparent_node: Node | None = None,\n) -&gt; dict:\n\t\"\"\"\n\tAnalyze a tree-sitter node and return structured information.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\t    file_path: Path to the source file\n\t    language: Programming language\n\t    handler: Language-specific syntax handler\n\t    parent_node: Parent node if any\n\n\tReturns:\n\t    Dict with node analysis information\n\n\t\"\"\"\n\t# Check if we should skip this node\n\tif handler.should_skip_node(node):\n\t\treturn {}\n\n\t# Get entity type for this node from the handler\n\tentity_type = handler.get_entity_type(node, parent_node, content_bytes)\n\n\t# Skip unknown/uninteresting nodes unless they might contain interesting children\n\tif entity_type == EntityType.UNKNOWN and not node.named_child_count &gt; 0:\n\t\treturn {}\n\n\t# Get name and other metadata\n\tname = handler.extract_name(node, content_bytes)\n\tdocstring_text, docstring_node = handler.find_docstring(node, content_bytes)\n\n\t# Get node content\n\ttry:\n\t\tnode_content = content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\texcept (UnicodeDecodeError, IndexError):\n\t\tnode_content = \"\"\n\n\t# Extract dependencies from import statements\n\tdependencies = []\n\tif entity_type == EntityType.IMPORT:\n\t\ttry:\n\t\t\tdependencies = handler.extract_imports(node, content_bytes)\n\t\texcept (AttributeError, UnicodeDecodeError, IndexError, ValueError) as e:\n\t\t\tlogger.warning(\"Failed to extract dependencies: %s\", e)\n\n\t# Build result\n\tresult = {\n\t\t\"type\": entity_type.name if entity_type != EntityType.UNKNOWN else \"UNKNOWN\",\n\t\t\"name\": name,\n\t\t\"location\": {\n\t\t\t\"start_line\": node.start_point[0] + 1,  # Convert to 1-based\n\t\t\t\"end_line\": node.end_point[0] + 1,\n\t\t\t\"start_col\": node.start_point[1],\n\t\t\t\"end_col\": node.end_point[1],\n\t\t},\n\t\t\"docstring\": docstring_text,\n\t\t\"content\": node_content,\n\t\t\"children\": [],\n\t\t\"language\": language,\n\t}\n\n\t# Add dependencies only if they exist to keep the output clean\n\tif dependencies:\n\t\tresult[\"dependencies\"] = dependencies\n\n\t# Extract function calls if the entity is a function or method\n\tcalls = []\n\tif entity_type in (EntityType.FUNCTION, EntityType.METHOD):\n\t\tbody_node = handler.get_body_node(node)\n\t\tif body_node:\n\t\t\ttry:\n\t\t\t\tcalls = handler.extract_calls(body_node, content_bytes)\n\t\t\texcept (AttributeError, IndexError, UnicodeDecodeError, ValueError) as e:\n\t\t\t\tlogger.warning(\"Failed to extract calls for %s: %s\", name or \"&lt;anonymous&gt;\", e)\n\n\t# Add calls only if they exist\n\tif calls:\n\t\tresult[\"calls\"] = calls\n\n\t# Process child nodes\n\tbody_node = handler.get_body_node(node)\n\tchildren_to_process = handler.get_children_to_process(node, body_node)\n\n\tfor child in children_to_process:\n\t\tif docstring_node and child == docstring_node:\n\t\t\tcontinue  # Skip docstring node\n\n\t\tchild_result = self.analyze_node(child, content_bytes, file_path, language, handler, node)\n\n\t\tif child_result:  # Only add non-empty results\n\t\t\tresult[\"children\"].append(child_result)\n\n\treturn result\n</code></pre>"},{"location":"api/processor/tree_sitter/analyzer/#codemap.processor.tree_sitter.analyzer.TreeSitterAnalyzer.analyze_file","title":"analyze_file","text":"<pre><code>analyze_file(\n\tfile_path: Path,\n\tcontent: str,\n\tlanguage: str | None = None,\n) -&gt; dict\n</code></pre> <p>Analyze a file and return its structural information.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to analyze</p> required <code>content</code> <code>str</code> <p>Content of the file</p> required <code>language</code> <code>str | None</code> <p>Optional language override</p> <code>None</code> <code>git_metadata</code> <p>Optional Git metadata</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dict with file analysis information</p> Source code in <code>src/codemap/processor/tree_sitter/analyzer.py</code> <pre><code>def analyze_file(\n\tself,\n\tfile_path: Path,\n\tcontent: str,\n\tlanguage: str | None = None,\n) -&gt; dict:\n\t\"\"\"\n\tAnalyze a file and return its structural information.\n\n\tArgs:\n\t    file_path: Path to the file to analyze\n\t    content: Content of the file\n\t    language: Optional language override\n\t    git_metadata: Optional Git metadata\n\n\tReturns:\n\t    Dict with file analysis information\n\n\t\"\"\"\n\t# Parse the file\n\troot_node, determined_language = self.parse_file(file_path, content, language)\n\tif not root_node or not determined_language:\n\t\treturn {\n\t\t\t\"file\": str(file_path),\n\t\t\t\"language\": determined_language or \"unknown\",\n\t\t\t\"success\": False,\n\t\t\t\"error\": \"Failed to parse file\",\n\t\t}\n\n\t# Get handler for the language\n\thandler = self.get_syntax_handler(determined_language)\n\tif not handler:\n\t\treturn {\n\t\t\t\"file\": str(file_path),\n\t\t\t\"language\": determined_language,\n\t\t\t\"success\": False,\n\t\t\t\"error\": f\"No handler for language {determined_language}\",\n\t\t}\n\n\t# Analyze the root node\n\tcontent_bytes = content.encode(\"utf-8\")\n\tentity_type = handler.get_entity_type(root_node, None, content_bytes)\n\tif entity_type == EntityType.UNKNOWN:\n\t\tentity_type = EntityType.MODULE  # Default to MODULE type\n\n\t# Extract module-level docstring\n\tmodule_description, module_docstring_node = handler.find_docstring(root_node, content_bytes)\n\n\t# Create result\n\tresult = {\n\t\t\"file\": str(file_path),\n\t\t\"language\": determined_language,\n\t\t\"success\": True,\n\t\t\"type\": entity_type.name,\n\t\t\"name\": file_path.stem,\n\t\t\"location\": {\n\t\t\t\"start_line\": root_node.start_point[0] + 1,\n\t\t\t\"end_line\": root_node.end_point[0] + 1,\n\t\t\t\"start_col\": root_node.start_point[1],\n\t\t\t\"end_col\": root_node.end_point[1],\n\t\t},\n\t\t\"docstring\": module_description,\n\t\t\"children\": [],\n\t}\n\n\t# Process children of the root node\n\tchildren_to_process = handler.get_children_to_process(root_node, None)\n\tfor child in children_to_process:\n\t\t# Skip the module docstring node if found\n\t\tif module_docstring_node and child == module_docstring_node:\n\t\t\tcontinue\n\n\t\tchild_result = self.analyze_node(child, content_bytes, file_path, determined_language, handler)\n\n\t\tif child_result:  # Only add non-empty results\n\t\t\tresult[\"children\"].append(child_result)\n\n\treturn result\n</code></pre>"},{"location":"api/processor/tree_sitter/base/","title":"Base","text":"<p>Base classes and interfaces for tree-sitter analysis.</p> <p>This module defines the core data structures and interfaces for tree-sitter analysis. It provides: - Entity type definitions for tree-sitter nodes - Metadata structures for tree-sitter nodes. - Base tree-sitter analysis interface</p>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType","title":"EntityType","text":"<p>               Bases: <code>Enum</code></p> <p>Types of code entities that can be extracted.</p> Source code in <code>src/codemap/processor/tree_sitter/base.py</code> <pre><code>class EntityType(Enum):\n\t\"\"\"Types of code entities that can be extracted.\"\"\"\n\n\t# File-level entities\n\tMODULE = auto()\n\tNAMESPACE = auto()\n\tPACKAGE = auto()\n\n\t# Type definitions\n\tCLASS = auto()\n\tINTERFACE = auto()\n\tPROTOCOL = auto()  # Similar to interface but for structural typing\n\tSTRUCT = auto()\n\tENUM = auto()\n\tTYPE_ALIAS = auto()\n\n\t# Functions and methods\n\tFUNCTION = auto()\n\tMETHOD = auto()\n\tPROPERTY = auto()  # For getter/setter methods\n\tTEST_CASE = auto()\n\tTEST_SUITE = auto()\n\n\t# Variables and constants\n\tVARIABLE = auto()\n\tCONSTANT = auto()\n\tCLASS_FIELD = auto()  # For class-level variables/fields\n\n\t# Code organization\n\tIMPORT = auto()\n\tDECORATOR = auto()\n\n\t# Documentation\n\tCOMMENT = auto()\n\tDOCSTRING = auto()\n\n\t# Special cases\n\tUNKNOWN = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.MODULE","title":"MODULE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODULE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.NAMESPACE","title":"NAMESPACE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NAMESPACE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.PACKAGE","title":"PACKAGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PACKAGE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.CLASS","title":"CLASS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLASS = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.INTERFACE","title":"INTERFACE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INTERFACE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.PROTOCOL","title":"PROTOCOL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROTOCOL = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.STRUCT","title":"STRUCT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STRUCT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.ENUM","title":"ENUM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENUM = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.TYPE_ALIAS","title":"TYPE_ALIAS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TYPE_ALIAS = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.FUNCTION","title":"FUNCTION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FUNCTION = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.METHOD","title":"METHOD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>METHOD = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.PROPERTY","title":"PROPERTY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROPERTY = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.TEST_CASE","title":"TEST_CASE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST_CASE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.TEST_SUITE","title":"TEST_SUITE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST_SUITE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.VARIABLE","title":"VARIABLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VARIABLE = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.CONSTANT","title":"CONSTANT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONSTANT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.CLASS_FIELD","title":"CLASS_FIELD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLASS_FIELD = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.IMPORT","title":"IMPORT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMPORT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.DECORATOR","title":"DECORATOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DECORATOR = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.COMMENT","title":"COMMENT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMMENT = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.DOCSTRING","title":"DOCSTRING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DOCSTRING = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/base/#codemap.processor.tree_sitter.base.EntityType.UNKNOWN","title":"UNKNOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNKNOWN = auto()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/","title":"Languages Overview","text":"<p>Language-specific configurations and handlers for tree-sitter analysis.</p> <ul> <li>Base - Base configuration for language-specific syntax chunking.</li> <li>Javascript - JavaScript-specific configuration for syntax chunking.</li> <li>Python - Python-specific configuration for syntax chunking.</li> <li>Typescript - TypeScript-specific configuration for syntax chunking.</li> </ul>"},{"location":"api/processor/tree_sitter/languages/base/","title":"Base","text":"<p>Base configuration for language-specific syntax chunking.</p> <p>This module provides the base configuration class for defining how different programming languages map their syntax elements to code chunks.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig","title":"LanguageConfig  <code>dataclass</code>","text":"<p>Configuration for language-specific syntax chunking.</p> <p>This class defines how a specific programming language's syntax elements map to different types of code chunks. Each field is a list of syntax node types that represent that kind of entity in the language's AST.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@dataclass(frozen=True)\nclass LanguageConfig:\n\t\"\"\"\n\tConfiguration for language-specific syntax chunking.\n\n\tThis class defines how a specific programming language's syntax\n\telements map to different types of code chunks. Each field is a list of\n\tsyntax node types that represent that kind of entity in the language's\n\tAST.\n\n\t\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]]\n\t\"\"\"Node types that represent entire modules/files.\"\"\"\n\n\tnamespace: ClassVar[list[str]]\n\t\"\"\"Node types for namespace/package declarations.\"\"\"\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]]\n\t\"\"\"Node types for class definitions.\"\"\"\n\n\tinterface: ClassVar[list[str]]\n\t\"\"\"Node types for interface definitions.\"\"\"\n\n\tprotocol: ClassVar[list[str]]\n\t\"\"\"Node types for protocol/trait definitions.\"\"\"\n\n\tstruct: ClassVar[list[str]]\n\t\"\"\"Node types for struct definitions.\"\"\"\n\n\tenum: ClassVar[list[str]]\n\t\"\"\"Node types for enum declarations.\"\"\"\n\n\ttype_alias: ClassVar[list[str]]\n\t\"\"\"Node types for type aliases/typedefs.\"\"\"\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]]\n\t\"\"\"Node types for function declarations.\"\"\"\n\n\tmethod: ClassVar[list[str]]\n\t\"\"\"Node types for method declarations.\"\"\"\n\n\tproperty_def: ClassVar[list[str]]\n\t\"\"\"Node types for property/getter/setter declarations.\"\"\"\n\n\ttest_case: ClassVar[list[str]]\n\t\"\"\"Node types that identify test functions.\"\"\"\n\n\ttest_suite: ClassVar[list[str]]\n\t\"\"\"Node types that identify test classes/suites.\"\"\"\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]]\n\t\"\"\"Node types for variable declarations.\"\"\"\n\n\tconstant: ClassVar[list[str]]\n\t\"\"\"Node types for constant declarations.\"\"\"\n\n\tclass_field: ClassVar[list[str]]\n\t\"\"\"Node types for class field declarations.\"\"\"\n\n\t# Code organization\n\timport_: ClassVar[list[str]]\n\t\"\"\"Node types for import statements.\"\"\"\n\n\tdecorator: ClassVar[list[str]]\n\t\"\"\"Node types for decorators/annotations.\"\"\"\n\n\t# Documentation\n\tcomment: ClassVar[list[str]]\n\t\"\"\"Node types for general comments.\"\"\"\n\n\tdocstring: ClassVar[list[str]]\n\t\"\"\"Node types for documentation strings.\"\"\"\n\n\t# Language-specific metadata\n\tfile_extensions: ClassVar[list[str]]\n\t\"\"\"File extensions associated with this language (e.g., ['.py', '.pyi']).\"\"\"\n\n\ttree_sitter_name: ClassVar[str] = \"\"\n\t\"\"\"Tree-sitter language identifier.\"\"\"\n\n\t# Optional node types that might be language-specific\n\tdecorators: ClassVar[list[str] | None] = None\n\tclass_fields: ClassVar[list[str] | None] = None\n\n\t@property\n\tdef all_node_types(self) -&gt; set[str]:\n\t\t\"\"\"\n\t\tGet all node types defined in this configuration.\n\n\t\tReturns:\n\t\t    A set of all node types from all categories.\n\n\t\t\"\"\"\n\t\tall_types = set()\n\t\tfor attr in [\n\t\t\tself.module,\n\t\t\tself.namespace,\n\t\t\tself.class_,\n\t\t\tself.interface,\n\t\t\tself.protocol,\n\t\t\tself.struct,\n\t\t\tself.enum,\n\t\t\tself.type_alias,\n\t\t\tself.function,\n\t\t\tself.method,\n\t\t\tself.property_def,\n\t\t\tself.test_case,\n\t\t\tself.test_suite,\n\t\t\tself.variable,\n\t\t\tself.constant,\n\t\t\tself.class_field,\n\t\t\tself.import_,\n\t\t\tself.decorator,\n\t\t\tself.comment,\n\t\t\tself.docstring,\n\t\t\tself.decorators,\n\t\t\tself.class_fields,\n\t\t]:\n\t\t\tif attr:  # Skip None values\n\t\t\t\tall_types.update(attr)\n\t\treturn all_types\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str]\n</code></pre> <p>Node types that represent entire modules/files.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str]\n</code></pre> <p>Node types for namespace/package declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str]\n</code></pre> <p>Node types for class definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str]\n</code></pre> <p>Node types for interface definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str]\n</code></pre> <p>Node types for protocol/trait definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str]\n</code></pre> <p>Node types for struct definitions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str]\n</code></pre> <p>Node types for enum declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str]\n</code></pre> <p>Node types for type aliases/typedefs.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str]\n</code></pre> <p>Node types for function declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str]\n</code></pre> <p>Node types for method declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str]\n</code></pre> <p>Node types for property/getter/setter declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str]\n</code></pre> <p>Node types that identify test functions.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str]\n</code></pre> <p>Node types that identify test classes/suites.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str]\n</code></pre> <p>Node types for variable declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str]\n</code></pre> <p>Node types for constant declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str]\n</code></pre> <p>Node types for class field declarations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str]\n</code></pre> <p>Node types for import statements.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str]\n</code></pre> <p>Node types for decorators/annotations.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str]\n</code></pre> <p>Node types for general comments.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str]\n</code></pre> <p>Node types for documentation strings.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str]\n</code></pre> <p>File extensions associated with this language (e.g., ['.py', '.pyi']).</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = ''\n</code></pre> <p>Tree-sitter language identifier.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.decorators","title":"decorators  <code>class-attribute</code>","text":"<pre><code>decorators: list[str] | None = None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.class_fields","title":"class_fields  <code>class-attribute</code>","text":"<pre><code>class_fields: list[str] | None = None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageConfig.all_node_types","title":"all_node_types  <code>property</code>","text":"<pre><code>all_node_types: set[str]\n</code></pre> <p>Get all node types defined in this configuration.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>A set of all node types from all categories.</p>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler","title":"LanguageSyntaxHandler","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for language-specific syntax handling.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class LanguageSyntaxHandler(abc.ABC):\n\t\"\"\"Abstract base class for language-specific syntax handling.\"\"\"\n\n\tdef __init__(self, config: LanguageConfig) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize with language configuration.\n\n\t\tArgs:\n\t\t    config: Language-specific configuration\n\n\t\t\"\"\"\n\t\tself.config = config\n\n\t@abc.abstractmethod\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a given node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\t\"\"\"\n\t\tFind the docstring associated with a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t    - The extracted docstring text (or None).\n\t\t    - The specific AST node representing the docstring that should be skipped\n\t\t      during child processing (or None).\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"\n\t\tGet the main body node for a function, method, or class.\n\n\t\tThis should be overridden by subclasses to find the appropriate block node.\n\n\t\tArgs:\n\t\t    node: The node representing the function, method, or class.\n\n\t\tReturns:\n\t\t    The body node, or None if not applicable/found.\n\n\t\t\"\"\"\n\t\t# Default implementation: returns None or the node itself as a naive fallback\n\t\t# Subclasses should find specific body nodes like 'block', 'statement_block' etc.\n\t\treturn node  # Naive fallback - subclasses MUST override\n\n\t@abc.abstractmethod\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef should_skip_node(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tDetermine if a node should be skipped entirely during processing.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node should be skipped\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported dependency names from an import node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (should be an import type)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported names\n\n\t\t\"\"\"\n\n\t@abc.abstractmethod\n\tdef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract names of functions/methods called within a node's scope.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (e.g., function/method body)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of called function/method names\n\n\t\t\"\"\"\n\n\tdef extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\t\"\"\"\n\t\tExtract the signature (definition line without body) for a function, class, etc.\n\n\t\tArgs:\n\t\t    node: The node to extract the signature from.\n\t\t    content_bytes: Source code content as bytes.\n\n\t\tReturns:\n\t\t    The signature string, or None if not applicable.\n\n\t\t\"\"\"\n\t\t# Default implementation: return the first line of the node's text\n\t\ttry:\n\t\t\tfirst_line = content_bytes[node.start_byte : node.end_byte].split(b\"\\n\", 1)[0]\n\t\t\treturn first_line.decode(\"utf-8\", errors=\"ignore\").strip()\n\t\texcept (IndexError, UnicodeDecodeError):\n\t\t\t# Catch specific errors related to slicing and decoding\n\t\t\treturn None\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"\n\t\tFind the first ancestor node that matches the target entity type.\n\n\t\tArgs:\n\t\t    node: The starting node.\n\t\t    target_type: The EntityType to search for in ancestors.\n\n\t\tReturns:\n\t\t    The ancestor node if found, otherwise None.\n\n\t\t\"\"\"\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\t# We need content_bytes to determine the type accurately, but we don't have it here.\n\t\t\t# This highlights a limitation of doing this purely structurally without context.\n\t\t\t# Subclasses might need a different approach or access to the analyzer/content.\n\t\t\t# For a basic structural check:\n\t\t\t# entity_type = self.get_entity_type(current, current.parent, ???) # Need content_bytes\n\t\t\t# if entity_type == target_type:\n\t\t\t#    return current\n\n\t\t\t# Simplistic check based on node type name (less reliable)\n\t\t\tif target_type.name.lower() in current.type.lower():  # Very rough check\n\t\t\t\treturn current\n\t\t\tcurrent = current.parent\n\t\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__(config: LanguageConfig) -&gt; None\n</code></pre> <p>Initialize with language configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LanguageConfig</code> <p>Language-specific configuration</p> required Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>def __init__(self, config: LanguageConfig) -&gt; None:\n\t\"\"\"\n\tInitialize with language configuration.\n\n\tArgs:\n\t    config: Language-specific configuration\n\n\t\"\"\"\n\tself.config = config\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_entity_type","title":"get_entity_type  <code>abstractmethod</code>","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a given node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.find_docstring","title":"find_docstring  <code>abstractmethod</code>","text":"<pre><code>find_docstring(\n\tnode: Node, content_bytes: bytes\n) -&gt; tuple[str | None, Node | None]\n</code></pre> <p>Find the docstring associated with a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A tuple containing:</p> <code>Node | None</code> <ul> <li>The extracted docstring text (or None).</li> </ul> <code>tuple[str | None, Node | None]</code> <ul> <li>The specific AST node representing the docstring that should be skipped during child processing (or None).</li> </ul> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\"\"\"\n\tFind the docstring associated with a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    A tuple containing:\n\t    - The extracted docstring text (or None).\n\t    - The specific AST node representing the docstring that should be skipped\n\t      during child processing (or None).\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_name","title":"extract_name  <code>abstractmethod</code>","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_body_node","title":"get_body_node  <code>abstractmethod</code>","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the main body node for a function, method, or class.</p> <p>This should be overridden by subclasses to find the appropriate block node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node representing the function, method, or class.</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The body node, or None if not applicable/found.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"\n\tGet the main body node for a function, method, or class.\n\n\tThis should be overridden by subclasses to find the appropriate block node.\n\n\tArgs:\n\t    node: The node representing the function, method, or class.\n\n\tReturns:\n\t    The body node, or None if not applicable/found.\n\n\t\"\"\"\n\t# Default implementation: returns None or the node itself as a naive fallback\n\t# Subclasses should find specific body nodes like 'block', 'statement_block' etc.\n\treturn node  # Naive fallback - subclasses MUST override\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_children_to_process","title":"get_children_to_process  <code>abstractmethod</code>","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.should_skip_node","title":"should_skip_node  <code>abstractmethod</code>","text":"<pre><code>should_skip_node(node: Node) -&gt; bool\n</code></pre> <p>Determine if a node should be skipped entirely during processing.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be skipped</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef should_skip_node(self, node: Node) -&gt; bool:\n\t\"\"\"\n\tDetermine if a node should be skipped entirely during processing.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    True if the node should be skipped\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_imports","title":"extract_imports  <code>abstractmethod</code>","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported dependency names from an import node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (should be an import type)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported dependency names from an import node.\n\n\tArgs:\n\t    node: The tree-sitter node (should be an import type)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported names\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_calls","title":"extract_calls  <code>abstractmethod</code>","text":"<pre><code>extract_calls(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract names of functions/methods called within a node's scope.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (e.g., function/method body)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of called function/method names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>@abc.abstractmethod\ndef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract names of functions/methods called within a node's scope.\n\n\tArgs:\n\t    node: The tree-sitter node (e.g., function/method body)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of called function/method names\n\n\t\"\"\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.extract_signature","title":"extract_signature","text":"<pre><code>extract_signature(\n\tnode: Node, content_bytes: bytes\n) -&gt; str | None\n</code></pre> <p>Extract the signature (definition line without body) for a function, class, etc.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to extract the signature from.</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The signature string, or None if not applicable.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>def extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\"\"\"\n\tExtract the signature (definition line without body) for a function, class, etc.\n\n\tArgs:\n\t    node: The node to extract the signature from.\n\t    content_bytes: Source code content as bytes.\n\n\tReturns:\n\t    The signature string, or None if not applicable.\n\n\t\"\"\"\n\t# Default implementation: return the first line of the node's text\n\ttry:\n\t\tfirst_line = content_bytes[node.start_byte : node.end_byte].split(b\"\\n\", 1)[0]\n\t\treturn first_line.decode(\"utf-8\", errors=\"ignore\").strip()\n\texcept (IndexError, UnicodeDecodeError):\n\t\t# Catch specific errors related to slicing and decoding\n\t\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.LanguageSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node that matches the target entity type.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The starting node.</p> required <code>target_type</code> <code>EntityType</code> <p>The EntityType to search for in ancestors.</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The ancestor node if found, otherwise None.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"\n\tFind the first ancestor node that matches the target entity type.\n\n\tArgs:\n\t    node: The starting node.\n\t    target_type: The EntityType to search for in ancestors.\n\n\tReturns:\n\t    The ancestor node if found, otherwise None.\n\n\t\"\"\"\n\tcurrent = node.parent\n\twhile current:\n\t\t# We need content_bytes to determine the type accurately, but we don't have it here.\n\t\t# This highlights a limitation of doing this purely structurally without context.\n\t\t# Subclasses might need a different approach or access to the analyzer/content.\n\t\t# For a basic structural check:\n\t\t# entity_type = self.get_entity_type(current, current.parent, ???) # Need content_bytes\n\t\t# if entity_type == target_type:\n\t\t#    return current\n\n\t\t# Simplistic check based on node type name (less reliable)\n\t\tif target_type.name.lower() in current.type.lower():  # Very rough check\n\t\t\treturn current\n\t\tcurrent = current.parent\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig","title":"PythonConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for Python language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class PythonConfig(LanguageConfig):\n\t\"\"\"Configuration for Python language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"module\"]\n\tclass_: ClassVar[list[str]] = [\"class_definition\"]\n\tfunction: ClassVar[list[str]] = [\"function_definition\"]\n\tproperty_def: ClassVar[list[str]] = [\"decorated_definition\"]\n\tstruct: ClassVar[list[str]] = []\n\tdocstring: ClassVar[list[str]] = [\"string\"]\n\tfile_extensions: ClassVar[list[str]] = [\".py\", \".pyi\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['module']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = ['decorated_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['string']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.PythonConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.py', '.pyi']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig","title":"JavaScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for JavaScript language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class JavaScriptConfig(LanguageConfig):\n\t\"\"\"Configuration for JavaScript language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tfunction: ClassVar[list[str]] = [\"function_declaration\", \"method_definition\", \"function\"]\n\tproperty_def: ClassVar[list[str]] = [\"property_definition\", \"property_identifier\"]\n\tstruct: ClassVar[list[str]] = []\n\tdocstring: ClassVar[list[str]] = [\"comment\"]\n\tfile_extensions: ClassVar[list[str]] = [\".js\", \".jsx\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [\n\t\"function_declaration\",\n\t\"method_definition\",\n\t\"function\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t\"property_definition\",\n\t\"property_identifier\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.JavaScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.js', '.jsx']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig","title":"TypeScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for TypeScript language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class TypeScriptConfig(LanguageConfig):\n\t\"\"\"Configuration for TypeScript language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tfunction: ClassVar[list[str]] = [\"function_declaration\", \"method_definition\", \"function\"]\n\tproperty_def: ClassVar[list[str]] = [\"property_definition\", \"property_identifier\"]\n\tstruct: ClassVar[list[str]] = []\n\tdocstring: ClassVar[list[str]] = [\"comment\"]\n\tfile_extensions: ClassVar[list[str]] = [\".ts\", \".tsx\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [\n\t\"function_declaration\",\n\t\"method_definition\",\n\t\"function\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t\"property_definition\",\n\t\"property_identifier\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.TypeScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.ts', '.tsx']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig","title":"RustConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for Rust language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class RustConfig(LanguageConfig):\n\t\"\"\"Configuration for Rust language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"source_file\"]\n\tclass_: ClassVar[list[str]] = [\"impl_item\"]\n\tfunction: ClassVar[list[str]] = [\"function_item\"]\n\tproperty_def: ClassVar[list[str]] = []\n\tstruct: ClassVar[list[str]] = [\"struct_item\"]\n\tdocstring: ClassVar[list[str]] = [\"line_comment\", \"block_comment\"]\n\tfile_extensions: ClassVar[list[str]] = [\".rs\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['source_file']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['impl_item']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = ['function_item']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = ['struct_item']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['line_comment', 'block_comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.RustConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.rs']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig","title":"GoConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Configuration for Go language.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/base.py</code> <pre><code>class GoConfig(LanguageConfig):\n\t\"\"\"Configuration for Go language.\"\"\"\n\n\tmodule: ClassVar[list[str]] = [\"source_file\"]\n\tclass_: ClassVar[list[str]] = [\"type_declaration\"]\n\tfunction: ClassVar[list[str]] = [\"function_declaration\"]\n\tproperty_def: ClassVar[list[str]] = []\n\tstruct: ClassVar[list[str]] = [\"struct_type\"]\n\tdocstring: ClassVar[list[str]] = [\"comment\"]\n\tfile_extensions: ClassVar[list[str]] = [\".go\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['source_file']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['type_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = ['function_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = ['struct_type']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/base/#codemap.processor.tree_sitter.languages.base.GoConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.go']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/","title":"Javascript","text":"<p>JavaScript-specific configuration for syntax chunking.</p>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig","title":"JavaScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>JavaScript-specific syntax chunking configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>class JavaScriptConfig(LanguageConfig):\n\t\"\"\"JavaScript-specific syntax chunking configuration.\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tnamespace: ClassVar[list[str]] = [\"export_statement\"]  # Using export as namespace indicator\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tinterface: ClassVar[list[str]] = []  # Pure JS doesn't have interfaces\n\tprotocol: ClassVar[list[str]] = []  # Pure JS doesn't have protocols\n\tstruct: ClassVar[list[str]] = []  # Pure JS doesn't have structs\n\tenum: ClassVar[list[str]] = []  # Pure JS doesn't have enums\n\ttype_alias: ClassVar[list[str]] = []  # Pure JS doesn't have type aliases\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]] = [\n\t\t\"function_declaration\",\n\t\t\"function\",\n\t\t\"arrow_function\",\n\t\t\"generator_function_declaration\",\n\t]\n\tmethod: ClassVar[list[str]] = [\"method_definition\"]\n\tproperty_def: ClassVar[list[str]] = [\"property_identifier\", \"public_field_definition\"]\n\ttest_case: ClassVar[list[str]] = [\"call_expression\"]  # Special detection for test frameworks\n\ttest_suite: ClassVar[list[str]] = [\"call_expression\"]  # Special detection for test frameworks\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]] = [\"variable_declaration\", \"lexical_declaration\"]\n\tconstant: ClassVar[list[str]] = [\"variable_declaration\", \"lexical_declaration\"]  # const declarations\n\tclass_field: ClassVar[list[str]] = [\"public_field_definition\"]\n\n\t# Code organization\n\timport_: ClassVar[list[str]] = [\"import_statement\"]\n\tdecorator: ClassVar[list[str]] = [\"decorator\"]\n\n\t# Documentation\n\tcomment: ClassVar[list[str]] = [\"comment\"]\n\tdocstring: ClassVar[list[str]] = [\"comment\"]  # JS uses comments for documentation\n\n\tfile_extensions: ClassVar[list[str]] = [\".js\", \".jsx\", \".mjs\", \".cjs\"]\n\ttree_sitter_name: ClassVar[str] = \"javascript\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str] = ['export_statement']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [\n\t\"function_declaration\",\n\t\"function\",\n\t\"arrow_function\",\n\t\"generator_function_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str] = ['method_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t\"property_identifier\",\n\t\"public_field_definition\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str] = ['call_expression']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str] = ['call_expression']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str] = [\n\t\"variable_declaration\",\n\t\"lexical_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str] = [\n\t\"variable_declaration\",\n\t\"lexical_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str] = ['public_field_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str] = ['import_statement']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str] = ['decorator']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = [\".js\", \".jsx\", \".mjs\", \".cjs\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = 'javascript'\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JAVASCRIPT_CONFIG","title":"JAVASCRIPT_CONFIG  <code>module-attribute</code>","text":"<pre><code>JAVASCRIPT_CONFIG = JavaScriptConfig()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler","title":"JavaScriptSyntaxHandler","text":"<p>               Bases: <code>LanguageSyntaxHandler</code></p> <p>JavaScript-specific syntax handling logic.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>class JavaScriptSyntaxHandler(LanguageSyntaxHandler):\n\t\"\"\"JavaScript-specific syntax handling logic.\"\"\"\n\n\t@staticmethod\n\tdef _get_node_text(node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"Helper to get node text safely.\"\"\"\n\t\ttry:\n\t\t\treturn content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept IndexError:\n\t\t\treturn \"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize with JavaScript configuration.\"\"\"\n\t\tsuper().__init__(JAVASCRIPT_CONFIG)\n\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a JavaScript node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\t\tnode_type = node.type\n\t\tlogger.debug(\n\t\t\t\"Getting entity type for JavaScript node: type=%s, parent_type=%s\",\n\t\t\tnode_type,\n\t\t\tparent.type if parent else None,\n\t\t)\n\n\t\t# Module-level\n\t\tif node_type in self.config.module:\n\t\t\treturn EntityType.MODULE\n\t\tif node_type in self.config.namespace:\n\t\t\treturn EntityType.NAMESPACE\n\n\t\t# Documentation\n\t\tif node_type in self.config.comment:\n\t\t\t# Check if this is a JSDoc comment (starts with /**)\n\t\t\tif self._is_jsdoc_comment(node, content_bytes):\n\t\t\t\treturn EntityType.DOCSTRING\n\t\t\treturn EntityType.COMMENT\n\n\t\t# Type definitions\n\t\tif node_type in self.config.class_:\n\t\t\treturn EntityType.CLASS\n\n\t\t# Functions and methods\n\t\tif node_type in self.config.function:\n\t\t\t# Check if this is a test function (for frameworks like Jest, Mocha)\n\t\t\tif self._is_test_function(node, content_bytes):\n\t\t\t\treturn EntityType.TEST_CASE\n\t\t\treturn EntityType.FUNCTION\n\n\t\tif node_type in self.config.method:\n\t\t\treturn EntityType.METHOD\n\n\t\t# Check for test suite declarations (describe blocks in Jest/Mocha)\n\t\tif node_type in self.config.test_suite and self._is_test_suite(node, content_bytes):\n\t\t\treturn EntityType.TEST_SUITE\n\n\t\t# Property definitions\n\t\tif node_type in self.config.property_def:\n\t\t\treturn EntityType.PROPERTY\n\n\t\t# Variables and constants\n\t\tif node_type in self.config.variable:\n\t\t\t# Check if it's a const declaration\n\t\t\tif self._is_constant(node, content_bytes):\n\t\t\t\treturn EntityType.CONSTANT\n\t\t\treturn EntityType.VARIABLE\n\n\t\t# Class fields\n\t\tif node_type in self.config.class_field:\n\t\t\treturn EntityType.CLASS_FIELD\n\n\t\t# Code organization\n\t\tif node_type in self.config.import_:\n\t\t\treturn EntityType.IMPORT\n\n\t\treturn EntityType.UNKNOWN\n\n\tdef _is_jsdoc_comment(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a comment node is a JSDoc comment.\n\n\t\tArgs:\n\t\t    node: The comment node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a JSDoc comment\n\n\t\t\"\"\"\n\t\tif node.type != \"comment\":\n\t\t\treturn False\n\n\t\ttry:\n\t\t\tcomment_text = content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\treturn comment_text.startswith(\"/**\") and comment_text.endswith(\"*/\")\n\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\treturn False\n\n\tdef _is_constant(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a variable declaration is a constant.\n\n\t\tArgs:\n\t\t    node: The variable declaration node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a constant declaration\n\n\t\t\"\"\"\n\t\tif node.type not in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t\treturn False\n\n\t\ttry:\n\t\t\tdecl_text = content_bytes[node.start_byte : node.start_byte + 5].decode(\"utf-8\", errors=\"ignore\")\n\t\t\treturn decl_text.startswith(\"const\")\n\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\treturn False\n\n\tdef _is_test_function(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a function is a test function.\n\n\t\tArgs:\n\t\t    node: The function node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a test function\n\n\t\t\"\"\"\n\t\tif node.type == \"call_expression\":\n\t\t\tcallee = node.child_by_field_name(\"function\")\n\t\t\tif callee:\n\t\t\t\ttry:\n\t\t\t\t\tcallee_text = content_bytes[callee.start_byte : callee.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\treturn callee_text in [\"it\", \"test\"]\n\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\tpass\n\t\treturn False\n\n\tdef _is_test_suite(self, node: Node, content_bytes: bytes) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a node is a test suite declaration.\n\n\t\tArgs:\n\t\t    node: The node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    True if the node is a test suite declaration\n\n\t\t\"\"\"\n\t\tif node.type == \"call_expression\":\n\t\t\tcallee = node.child_by_field_name(\"function\")\n\t\t\tif callee:\n\t\t\t\ttry:\n\t\t\t\t\tcallee_text = content_bytes[callee.start_byte : callee.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\treturn callee_text == \"describe\"\n\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\tpass\n\t\treturn False\n\n\tdef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\t\"\"\"\n\t\tFind the docstring associated with a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t    - The extracted docstring text (or None).\n\t\t    - The specific AST node representing the docstring (or None).\n\n\t\t\"\"\"\n\t\t# For functions, classes, and other definition nodes\n\t\tparent_node = node.parent\n\n\t\t# Look for JSDoc comments immediately preceding the node\n\t\tif parent_node:\n\t\t\tindex = None\n\t\t\tfor i, child in enumerate(parent_node.children):\n\t\t\t\tif child == node:\n\t\t\t\t\tindex = i\n\t\t\t\t\tbreak\n\n\t\t\tif index is not None and index &gt; 0:\n\t\t\t\tprev_node = parent_node.children[index - 1]\n\t\t\t\tif prev_node.type == \"comment\" and self._is_jsdoc_comment(prev_node, content_bytes):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcomment_text = content_bytes[prev_node.start_byte : prev_node.end_byte].decode(\n\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t# Clean JSDoc format: remove /** */ and trim\n\t\t\t\t\t\tcomment_text = comment_text.strip()\n\t\t\t\t\t\tcomment_text = comment_text.removeprefix(\"/**\")\n\t\t\t\t\t\tcomment_text = comment_text.removesuffix(\"*/\")\n\t\t\t\t\t\treturn comment_text.strip(), prev_node\n\t\t\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\t\t\tlogger.warning(\"Failed to decode JavaScript comment: %s\", e)\n\n\t\treturn None, None\n\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\t\t# Try to find the name field based on node type\n\t\tname_node = None\n\n\t\tif node.type in [\"function_declaration\", \"class_declaration\", \"method_definition\"]:\n\t\t\tname_node = node.child_by_field_name(\"name\")\n\t\telif node.type == \"property_identifier\":\n\t\t\tname_node = node\n\t\telif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t\t# Get the first declarator and its name\n\t\t\tdeclarator = node.child_by_field_name(\"declarations\")\n\t\t\tif declarator and declarator.named_child_count &gt; 0:\n\t\t\t\tfirst_declarator = declarator.named_children[0]\n\t\t\t\tname_node = first_declarator.child_by_field_name(\"name\")\n\t\telif node.type == \"public_field_definition\":\n\t\t\tname_node = node.child_by_field_name(\"name\")\n\n\t\tif name_node:\n\t\t\ttry:\n\t\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode JavaScript name: %s\", e)\n\t\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t\t# For call expressions that represent tests or suites\n\t\tif node.type == \"call_expression\":\n\t\t\tcallee = node.child_by_field_name(\"function\")\n\t\t\targuments = node.child_by_field_name(\"arguments\")\n\n\t\t\tif callee and arguments and arguments.named_child_count &gt; 0:\n\t\t\t\t# First argument is typically the test/suite name\n\t\t\t\tfirst_arg = arguments.named_children[0]\n\t\t\t\tif first_arg.type == \"string\":\n\t\t\t\t\ttry:\n\t\t\t\t\t\tname = content_bytes[first_arg.start_byte : first_arg.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\t\t# Remove quotes\n\t\t\t\t\t\treturn name.strip(\"\\\"'\")\n\t\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\t\tpass\n\n\t\treturn f\"&lt;anonymous-{node.type}&gt;\"\n\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"Get the statement block node for JS/TS function/class/method body.\"\"\"\n\t\t# Common body node type in JS/TS grammar\n\t\tbody_field_names = [\"body\", \"statement_block\"]\n\t\tfor field_name in body_field_names:\n\t\t\tbody_node = node.child_by_field_name(field_name)\n\t\t\tif body_node:\n\t\t\t\t# Sometimes the direct body is an expression (arrow functions)\n\t\t\t\t# Check if the found node is a block type\n\t\t\t\tif body_node.type == \"statement_block\":\n\t\t\t\t\treturn body_node\n\t\t\t\tif body_node.type == \"expression_statement\":  # Arrow function returning object literal\n\t\t\t\t\tif body_node.child_count &gt; 0 and body_node.children[0].type == \"object\":\n\t\t\t\t\t\treturn body_node  # Treat expression as body\n\t\t\t\telif node.type == \"arrow_function\":  # Direct expression in arrow function\n\t\t\t\t\treturn body_node\n\t\t# Fallback for classes where body might be direct children within curly braces\n\t\tif node.type in (\"class_declaration\", \"class\"):\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"class_body\":\n\t\t\t\t\treturn child\n\t\treturn None\n\n\tdef extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\t\"\"\"Extract the signature up to the opening curly brace '{' for JS/TS.\"\"\"\n\t\t# Find the body node first\n\t\tbody_node = self.get_body_node(node)\n\n\t\tif body_node:\n\t\t\t# Signature is everything from the start of the node up to the start of the body\n\t\t\tstart_byte = node.start_byte\n\t\t\tend_byte = body_node.start_byte\n\t\t\t# Adjust end_byte to exclude trailing whitespace before the body\n\t\t\twhile end_byte &gt; start_byte and content_bytes[end_byte - 1 : end_byte].isspace():\n\t\t\t\tend_byte -= 1\n\t\t\ttry:\n\t\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\t\texcept IndexError:\n\t\t\t\treturn None\n\t\telse:\n\t\t\t# Fallback: if no body found (e.g., abstract method, interface?), return the first line\n\t\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"Find the first ancestor node matching the target JS/TS entity type.\"\"\"\n\t\ttarget_node_types = []\n\t\tif target_type == EntityType.CLASS:\n\t\t\ttarget_node_types = [\"class_declaration\", \"class\", \"class_expression\"]\n\t\telif target_type == EntityType.FUNCTION:\n\t\t\t# Includes function declarations, arrow functions, function expressions\n\t\t\ttarget_node_types = [\"function_declaration\", \"arrow_function\", \"function\"]\n\t\telif target_type == EntityType.METHOD:\n\t\t\ttarget_node_types = [\"method_definition\"]\n\t\telif target_type == EntityType.MODULE:\n\t\t\t# Module is typically the root 'program' node\n\t\t\tcurrent = node\n\t\t\twhile current.parent:\n\t\t\t\tcurrent = current.parent\n\t\t\treturn current if current.type == \"program\" else None\n\t\t# Add other types if needed (e.g., INTERFACE for TS)\n\n\t\tif not target_node_types:\n\t\t\treturn None\n\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type in target_node_types:\n\t\t\t\treturn current\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\t\t# Process children of the body node if it exists, otherwise process direct children\n\t\tif body_node:\n\t\t\treturn list(body_node.children)\n\n\t\t# Special handling for certain nodes\n\t\tif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t\t# Process the declarations field\n\t\t\tdeclarations = node.child_by_field_name(\"declarations\")\n\t\t\treturn [declarations] if declarations else []\n\n\t\treturn list(node.children)\n\n\tdef should_skip_node(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tDetermine if a node should be skipped entirely during processing.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node should be skipped\n\n\t\t\"\"\"\n\t\t# Skip non-named nodes (like punctuation, operators)\n\t\tif not node.is_named:\n\t\t\treturn True\n\n\t\t# Skip syntax nodes that don't contribute to code structure\n\t\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\", \":\", \"=&gt;\"]\n\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported module names from a JavaScript import statement.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node representing an import statement\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported module names as strings\n\n\t\t\"\"\"\n\t\tif node.type not in self.config.import_:\n\t\t\treturn []\n\n\t\timported_names = []\n\n\t\ttry:\n\t\t\t# Find the source (module path) of the import\n\t\t\tsource_node = node.child_by_field_name(\"source\")\n\t\t\tif not source_node:\n\t\t\t\treturn []\n\n\t\t\t# Extract the module path from the string literal\n\t\t\tmodule_path = content_bytes[source_node.start_byte : source_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t# Remove quotes\n\t\t\tmodule_path = module_path.strip(\"\\\"'\")\n\n\t\t\t# Check for different import patterns:\n\n\t\t\t# 1. Default import: \"import Name from 'module'\"\n\t\t\tdefault_import = node.child_by_field_name(\"default\")\n\t\t\tif default_import:\n\t\t\t\tname = content_bytes[default_import.start_byte : default_import.end_byte].decode(\n\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t)\n\t\t\t\timported_names.append(f\"{module_path}.default\")\n\n\t\t\t# 2. Named imports: \"import { foo, bar as baz } from 'module'\"\n\t\t\tnamed_imports = node.child_by_field_name(\"named_imports\")\n\t\t\tif named_imports:\n\t\t\t\tfor child in named_imports.children:\n\t\t\t\t\tif child.type == \"import_specifier\":\n\t\t\t\t\t\timported_name = child.child_by_field_name(\"name\")\n\t\t\t\t\t\tif imported_name:\n\t\t\t\t\t\t\tname = content_bytes[imported_name.start_byte : imported_name.end_byte].decode(\n\t\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\timported_names.append(f\"{module_path}.{name}\")\n\n\t\t\t# 3. Namespace import: \"import * as Name from 'module'\"\n\t\t\tnamespace_import = node.child_by_field_name(\"namespace_import\")\n\t\t\tif namespace_import:\n\t\t\t\timported_names.append(f\"{module_path}.*\")\n\n\t\t\t# If no specific imports found but we have a module, add the whole module\n\t\t\tif not imported_names and module_path:\n\t\t\t\timported_names.append(module_path)\n\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode JavaScript imports: %s\", e)\n\n\t\treturn imported_names\n\n\tdef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract names of functions/methods called within a JS node's scope.\n\n\t\tRecursively searches for 'call_expression' nodes and extracts the function identifier.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (e.g., function/method body)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of called function/method names\n\n\t\t\"\"\"\n\t\tcalls = []\n\t\tfor child in node.children:\n\t\t\tif child.type == \"call_expression\":\n\t\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\t\tif function_node:\n\t\t\t\t\t# Extract the identifier (e.g., funcName, obj.methodName)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcall_name = content_bytes[function_node.start_byte : function_node.end_byte].decode(\n\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\tcalls.append(call_name)\n\t\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\t\tpass  # Ignore decoding errors\n\t\t\t# Recursively search deeper within non-call children\n\t\t\telse:\n\t\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\t\treturn list(set(calls))  # Return unique calls\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize with JavaScript configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize with JavaScript configuration.\"\"\"\n\tsuper().__init__(JAVASCRIPT_CONFIG)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_entity_type","title":"get_entity_type","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a JavaScript node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a JavaScript node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n\tnode_type = node.type\n\tlogger.debug(\n\t\t\"Getting entity type for JavaScript node: type=%s, parent_type=%s\",\n\t\tnode_type,\n\t\tparent.type if parent else None,\n\t)\n\n\t# Module-level\n\tif node_type in self.config.module:\n\t\treturn EntityType.MODULE\n\tif node_type in self.config.namespace:\n\t\treturn EntityType.NAMESPACE\n\n\t# Documentation\n\tif node_type in self.config.comment:\n\t\t# Check if this is a JSDoc comment (starts with /**)\n\t\tif self._is_jsdoc_comment(node, content_bytes):\n\t\t\treturn EntityType.DOCSTRING\n\t\treturn EntityType.COMMENT\n\n\t# Type definitions\n\tif node_type in self.config.class_:\n\t\treturn EntityType.CLASS\n\n\t# Functions and methods\n\tif node_type in self.config.function:\n\t\t# Check if this is a test function (for frameworks like Jest, Mocha)\n\t\tif self._is_test_function(node, content_bytes):\n\t\t\treturn EntityType.TEST_CASE\n\t\treturn EntityType.FUNCTION\n\n\tif node_type in self.config.method:\n\t\treturn EntityType.METHOD\n\n\t# Check for test suite declarations (describe blocks in Jest/Mocha)\n\tif node_type in self.config.test_suite and self._is_test_suite(node, content_bytes):\n\t\treturn EntityType.TEST_SUITE\n\n\t# Property definitions\n\tif node_type in self.config.property_def:\n\t\treturn EntityType.PROPERTY\n\n\t# Variables and constants\n\tif node_type in self.config.variable:\n\t\t# Check if it's a const declaration\n\t\tif self._is_constant(node, content_bytes):\n\t\t\treturn EntityType.CONSTANT\n\t\treturn EntityType.VARIABLE\n\n\t# Class fields\n\tif node_type in self.config.class_field:\n\t\treturn EntityType.CLASS_FIELD\n\n\t# Code organization\n\tif node_type in self.config.import_:\n\t\treturn EntityType.IMPORT\n\n\treturn EntityType.UNKNOWN\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.find_docstring","title":"find_docstring","text":"<pre><code>find_docstring(\n\tnode: Node, content_bytes: bytes\n) -&gt; tuple[str | None, Node | None]\n</code></pre> <p>Find the docstring associated with a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A tuple containing:</p> <code>Node | None</code> <ul> <li>The extracted docstring text (or None).</li> </ul> <code>tuple[str | None, Node | None]</code> <ul> <li>The specific AST node representing the docstring (or None).</li> </ul> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\"\"\"\n\tFind the docstring associated with a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    A tuple containing:\n\t    - The extracted docstring text (or None).\n\t    - The specific AST node representing the docstring (or None).\n\n\t\"\"\"\n\t# For functions, classes, and other definition nodes\n\tparent_node = node.parent\n\n\t# Look for JSDoc comments immediately preceding the node\n\tif parent_node:\n\t\tindex = None\n\t\tfor i, child in enumerate(parent_node.children):\n\t\t\tif child == node:\n\t\t\t\tindex = i\n\t\t\t\tbreak\n\n\t\tif index is not None and index &gt; 0:\n\t\t\tprev_node = parent_node.children[index - 1]\n\t\t\tif prev_node.type == \"comment\" and self._is_jsdoc_comment(prev_node, content_bytes):\n\t\t\t\ttry:\n\t\t\t\t\tcomment_text = content_bytes[prev_node.start_byte : prev_node.end_byte].decode(\n\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t)\n\t\t\t\t\t# Clean JSDoc format: remove /** */ and trim\n\t\t\t\t\tcomment_text = comment_text.strip()\n\t\t\t\t\tcomment_text = comment_text.removeprefix(\"/**\")\n\t\t\t\t\tcomment_text = comment_text.removesuffix(\"*/\")\n\t\t\t\t\treturn comment_text.strip(), prev_node\n\t\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\t\tlogger.warning(\"Failed to decode JavaScript comment: %s\", e)\n\n\treturn None, None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_name","title":"extract_name","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n\t# Try to find the name field based on node type\n\tname_node = None\n\n\tif node.type in [\"function_declaration\", \"class_declaration\", \"method_definition\"]:\n\t\tname_node = node.child_by_field_name(\"name\")\n\telif node.type == \"property_identifier\":\n\t\tname_node = node\n\telif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t# Get the first declarator and its name\n\t\tdeclarator = node.child_by_field_name(\"declarations\")\n\t\tif declarator and declarator.named_child_count &gt; 0:\n\t\t\tfirst_declarator = declarator.named_children[0]\n\t\t\tname_node = first_declarator.child_by_field_name(\"name\")\n\telif node.type == \"public_field_definition\":\n\t\tname_node = node.child_by_field_name(\"name\")\n\n\tif name_node:\n\t\ttry:\n\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode JavaScript name: %s\", e)\n\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t# For call expressions that represent tests or suites\n\tif node.type == \"call_expression\":\n\t\tcallee = node.child_by_field_name(\"function\")\n\t\targuments = node.child_by_field_name(\"arguments\")\n\n\t\tif callee and arguments and arguments.named_child_count &gt; 0:\n\t\t\t# First argument is typically the test/suite name\n\t\t\tfirst_arg = arguments.named_children[0]\n\t\t\tif first_arg.type == \"string\":\n\t\t\t\ttry:\n\t\t\t\t\tname = content_bytes[first_arg.start_byte : first_arg.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\t\t\t# Remove quotes\n\t\t\t\t\treturn name.strip(\"\\\"'\")\n\t\t\t\texcept (UnicodeDecodeError, IndexError):\n\t\t\t\t\tpass\n\n\treturn f\"&lt;anonymous-{node.type}&gt;\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_body_node","title":"get_body_node","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the statement block node for JS/TS function/class/method body.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"Get the statement block node for JS/TS function/class/method body.\"\"\"\n\t# Common body node type in JS/TS grammar\n\tbody_field_names = [\"body\", \"statement_block\"]\n\tfor field_name in body_field_names:\n\t\tbody_node = node.child_by_field_name(field_name)\n\t\tif body_node:\n\t\t\t# Sometimes the direct body is an expression (arrow functions)\n\t\t\t# Check if the found node is a block type\n\t\t\tif body_node.type == \"statement_block\":\n\t\t\t\treturn body_node\n\t\t\tif body_node.type == \"expression_statement\":  # Arrow function returning object literal\n\t\t\t\tif body_node.child_count &gt; 0 and body_node.children[0].type == \"object\":\n\t\t\t\t\treturn body_node  # Treat expression as body\n\t\t\telif node.type == \"arrow_function\":  # Direct expression in arrow function\n\t\t\t\treturn body_node\n\t# Fallback for classes where body might be direct children within curly braces\n\tif node.type in (\"class_declaration\", \"class\"):\n\t\tfor child in node.children:\n\t\t\tif child.type == \"class_body\":\n\t\t\t\treturn child\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_signature","title":"extract_signature","text":"<pre><code>extract_signature(\n\tnode: Node, content_bytes: bytes\n) -&gt; str | None\n</code></pre> <p>Extract the signature up to the opening curly brace '{' for JS/TS.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\"\"\"Extract the signature up to the opening curly brace '{' for JS/TS.\"\"\"\n\t# Find the body node first\n\tbody_node = self.get_body_node(node)\n\n\tif body_node:\n\t\t# Signature is everything from the start of the node up to the start of the body\n\t\tstart_byte = node.start_byte\n\t\tend_byte = body_node.start_byte\n\t\t# Adjust end_byte to exclude trailing whitespace before the body\n\t\twhile end_byte &gt; start_byte and content_bytes[end_byte - 1 : end_byte].isspace():\n\t\t\tend_byte -= 1\n\t\ttry:\n\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\texcept IndexError:\n\t\t\treturn None\n\telse:\n\t\t# Fallback: if no body found (e.g., abstract method, interface?), return the first line\n\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node matching the target JS/TS entity type.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"Find the first ancestor node matching the target JS/TS entity type.\"\"\"\n\ttarget_node_types = []\n\tif target_type == EntityType.CLASS:\n\t\ttarget_node_types = [\"class_declaration\", \"class\", \"class_expression\"]\n\telif target_type == EntityType.FUNCTION:\n\t\t# Includes function declarations, arrow functions, function expressions\n\t\ttarget_node_types = [\"function_declaration\", \"arrow_function\", \"function\"]\n\telif target_type == EntityType.METHOD:\n\t\ttarget_node_types = [\"method_definition\"]\n\telif target_type == EntityType.MODULE:\n\t\t# Module is typically the root 'program' node\n\t\tcurrent = node\n\t\twhile current.parent:\n\t\t\tcurrent = current.parent\n\t\treturn current if current.type == \"program\" else None\n\t# Add other types if needed (e.g., INTERFACE for TS)\n\n\tif not target_node_types:\n\t\treturn None\n\n\tcurrent = node.parent\n\twhile current:\n\t\tif current.type in target_node_types:\n\t\t\treturn current\n\t\tcurrent = current.parent\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.get_children_to_process","title":"get_children_to_process","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n\t# Process children of the body node if it exists, otherwise process direct children\n\tif body_node:\n\t\treturn list(body_node.children)\n\n\t# Special handling for certain nodes\n\tif node.type in [\"variable_declaration\", \"lexical_declaration\"]:\n\t\t# Process the declarations field\n\t\tdeclarations = node.child_by_field_name(\"declarations\")\n\t\treturn [declarations] if declarations else []\n\n\treturn list(node.children)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.should_skip_node","title":"should_skip_node","text":"<pre><code>should_skip_node(node: Node) -&gt; bool\n</code></pre> <p>Determine if a node should be skipped entirely during processing.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be skipped</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def should_skip_node(self, node: Node) -&gt; bool:\n\t\"\"\"\n\tDetermine if a node should be skipped entirely during processing.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    True if the node should be skipped\n\n\t\"\"\"\n\t# Skip non-named nodes (like punctuation, operators)\n\tif not node.is_named:\n\t\treturn True\n\n\t# Skip syntax nodes that don't contribute to code structure\n\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\", \":\", \"=&gt;\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_imports","title":"extract_imports","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported module names from a JavaScript import statement.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node representing an import statement</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported module names as strings</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported module names from a JavaScript import statement.\n\n\tArgs:\n\t    node: The tree-sitter node representing an import statement\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported module names as strings\n\n\t\"\"\"\n\tif node.type not in self.config.import_:\n\t\treturn []\n\n\timported_names = []\n\n\ttry:\n\t\t# Find the source (module path) of the import\n\t\tsource_node = node.child_by_field_name(\"source\")\n\t\tif not source_node:\n\t\t\treturn []\n\n\t\t# Extract the module path from the string literal\n\t\tmodule_path = content_bytes[source_node.start_byte : source_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t# Remove quotes\n\t\tmodule_path = module_path.strip(\"\\\"'\")\n\n\t\t# Check for different import patterns:\n\n\t\t# 1. Default import: \"import Name from 'module'\"\n\t\tdefault_import = node.child_by_field_name(\"default\")\n\t\tif default_import:\n\t\t\tname = content_bytes[default_import.start_byte : default_import.end_byte].decode(\n\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t)\n\t\t\timported_names.append(f\"{module_path}.default\")\n\n\t\t# 2. Named imports: \"import { foo, bar as baz } from 'module'\"\n\t\tnamed_imports = node.child_by_field_name(\"named_imports\")\n\t\tif named_imports:\n\t\t\tfor child in named_imports.children:\n\t\t\t\tif child.type == \"import_specifier\":\n\t\t\t\t\timported_name = child.child_by_field_name(\"name\")\n\t\t\t\t\tif imported_name:\n\t\t\t\t\t\tname = content_bytes[imported_name.start_byte : imported_name.end_byte].decode(\n\t\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\timported_names.append(f\"{module_path}.{name}\")\n\n\t\t# 3. Namespace import: \"import * as Name from 'module'\"\n\t\tnamespace_import = node.child_by_field_name(\"namespace_import\")\n\t\tif namespace_import:\n\t\t\timported_names.append(f\"{module_path}.*\")\n\n\t\t# If no specific imports found but we have a module, add the whole module\n\t\tif not imported_names and module_path:\n\t\t\timported_names.append(module_path)\n\n\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\tlogger.warning(\"Failed to decode JavaScript imports: %s\", e)\n\n\treturn imported_names\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/javascript/#codemap.processor.tree_sitter.languages.javascript.JavaScriptSyntaxHandler.extract_calls","title":"extract_calls","text":"<pre><code>extract_calls(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract names of functions/methods called within a JS node's scope.</p> <p>Recursively searches for 'call_expression' nodes and extracts the function identifier.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (e.g., function/method body)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of called function/method names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/javascript.py</code> <pre><code>def extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract names of functions/methods called within a JS node's scope.\n\n\tRecursively searches for 'call_expression' nodes and extracts the function identifier.\n\n\tArgs:\n\t    node: The tree-sitter node (e.g., function/method body)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of called function/method names\n\n\t\"\"\"\n\tcalls = []\n\tfor child in node.children:\n\t\tif child.type == \"call_expression\":\n\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\tif function_node:\n\t\t\t\t# Extract the identifier (e.g., funcName, obj.methodName)\n\t\t\t\ttry:\n\t\t\t\t\tcall_name = content_bytes[function_node.start_byte : function_node.end_byte].decode(\n\t\t\t\t\t\t\"utf-8\", errors=\"ignore\"\n\t\t\t\t\t)\n\t\t\t\t\tcalls.append(call_name)\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tpass  # Ignore decoding errors\n\t\t# Recursively search deeper within non-call children\n\t\telse:\n\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\treturn list(set(calls))  # Return unique calls\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/","title":"Python","text":"<p>Python-specific configuration for syntax chunking.</p>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig","title":"PythonConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>Python-specific syntax chunking configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>class PythonConfig(LanguageConfig):\n\t\"\"\"Python-specific syntax chunking configuration.\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]] = [\"module\"]\n\tnamespace: ClassVar[list[str]] = []  # Python doesn't have explicit namespaces\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]] = [\"class_definition\"]\n\tinterface: ClassVar[list[str]] = [\"class_definition\"]  # Python uses ABC classes\n\tprotocol: ClassVar[list[str]] = [\"class_definition\"]  # Protocol classes\n\tstruct: ClassVar[list[str]] = [\"class_definition\"]  # Python uses regular classes\n\tenum: ClassVar[list[str]] = [\"class_definition\"]  # Enum classes\n\ttype_alias: ClassVar[list[str]] = [\"assignment\"]  # Type assignments\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]] = [\"function_definition\"]\n\tmethod: ClassVar[list[str]] = [\"function_definition\"]  # Inside class\n\tproperty_def: ClassVar[list[str]] = [\"decorated_definition\"]  # @property decorated functions\n\ttest_case: ClassVar[list[str]] = [\"function_definition\"]  # test_* functions\n\ttest_suite: ClassVar[list[str]] = [\"class_definition\"]  # Test* classes\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]] = [\"assignment\"]\n\tconstant: ClassVar[list[str]] = [\"assignment\"]  # Uppercase assignments\n\tclass_field: ClassVar[list[str]] = [\"class_variable_definition\"]\n\n\t# Code organization\n\timport_: ClassVar[list[str]] = [\"import_statement\", \"import_from_statement\"]\n\tdecorator: ClassVar[list[str]] = [\"decorator\"]\n\n\t# Documentation\n\tcomment: ClassVar[list[str]] = [\"comment\"]\n\tdocstring: ClassVar[list[str]] = [\"string\"]  # First string in module/class/function\n\n\tfile_extensions: ClassVar[list[str]] = [\".py\", \".pyi\"]\n\ttree_sitter_name: ClassVar[str] = \"python\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['module']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str] = ['assignment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = ['decorated_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str] = ['function_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str] = ['class_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str] = ['assignment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str] = ['assignment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str] = ['class_variable_definition']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str] = [\n\t\"import_statement\",\n\t\"import_from_statement\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str] = ['decorator']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str] = ['comment']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = ['string']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.py', '.pyi']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = 'python'\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PYTHON_CONFIG","title":"PYTHON_CONFIG  <code>module-attribute</code>","text":"<pre><code>PYTHON_CONFIG = PythonConfig()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler","title":"PythonSyntaxHandler","text":"<p>               Bases: <code>LanguageSyntaxHandler</code></p> <p>Python-specific syntax handling logic.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>class PythonSyntaxHandler(LanguageSyntaxHandler):\n\t\"\"\"Python-specific syntax handling logic.\"\"\"\n\n\t@staticmethod\n\tdef _get_node_text(node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"Helper to get node text safely.\"\"\"\n\t\ttry:\n\t\t\treturn content_bytes[node.start_byte : node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept IndexError:\n\t\t\treturn \"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize with Python configuration.\"\"\"\n\t\tsuper().__init__(PYTHON_CONFIG)\n\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a Python node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\t\tnode_type = node.type\n\t\tlogger.debug(\n\t\t\t\"Getting entity type for Python node: type=%s, parent_type=%s\", node_type, parent.type if parent else None\n\t\t)\n\n\t\t# Print node content for debugging\n\t\ttry:\n\t\t\tnode_content = self._get_node_text(node, content_bytes)\n\t\t\tlogger.debug(\"Node content: %s\", node_content)\n\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\tlogger.debug(\"Failed to decode node content: %s\", str(e))\n\n\t\t# Special case: if this is an expression statement containing a constant assignment\n\t\tif node_type == \"expression_statement\":\n\t\t\t# Check if it contains an assignment that is a constant\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"assignment\":\n\t\t\t\t\tname_node = child.child_by_field_name(\"left\")\n\t\t\t\t\tif name_node:\n\t\t\t\t\t\tname = self._get_node_text(name_node, content_bytes)\n\n\t\t\t\t\t\t# Get the right side for type detection\n\t\t\t\t\t\tvalue_node = child.child_by_field_name(\"right\")\n\t\t\t\t\t\tvalue_text = \"\"\n\t\t\t\t\t\tif value_node:\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tvalue_text = self._get_node_text(value_node, content_bytes)\n\t\t\t\t\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\t\t\t\t\tlogger.debug(\"Failed to decode type value: %s\", str(e))\n\n\t\t\t\t\t\t# Check for type alias - TypeVar or anything referencing typing types like Dict, List, etc.\n\t\t\t\t\t\tif \"TypeVar\" in value_text or any(\n\t\t\t\t\t\t\ttyping_type in value_text\n\t\t\t\t\t\t\tfor typing_type in [\"Dict\", \"List\", \"Tuple\", \"Set\", \"Union\", \"Optional\", \"Callable\", \"Any\"]\n\t\t\t\t\t\t):\n\t\t\t\t\t\t\tlogger.debug(\"Expression statement with TYPE_ALIAS: %s\", name)\n\t\t\t\t\t\t\treturn EntityType.TYPE_ALIAS\n\n\t\t\t\t\t\t# Check for constant (all uppercase with at least one letter)\n\t\t\t\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\t\t\t\tlogger.debug(\"Expression statement with CONSTANT assignment: %s\", name)\n\t\t\t\t\t\t\treturn EntityType.CONSTANT\n\t\t\t\t\t\t# Check for regular variable\n\t\t\t\t\t\tif not name.startswith(\"_\") and any(c.isalpha() for c in name):\n\t\t\t\t\t\t\tlogger.debug(\"Expression statement with VARIABLE assignment: %s\", name)\n\t\t\t\t\t\t\treturn EntityType.VARIABLE\n\n\t\t# Module-level\n\t\tif node_type in self.config.module:\n\t\t\treturn EntityType.MODULE\n\t\tif node_type in self.config.namespace:\n\t\t\treturn EntityType.NAMESPACE\n\n\t\t# Documentation\n\t\tif node_type in self.config.docstring:\n\t\t\t# Check if this is a docstring (first string in a container)\n\t\t\tif self._is_docstring(node, parent):\n\t\t\t\treturn EntityType.DOCSTRING\n\t\t\treturn EntityType.UNKNOWN  # Regular string literals\n\t\tif node_type in self.config.comment:\n\t\t\treturn EntityType.COMMENT\n\n\t\t# Type definitions\n\t\tif node_type in self.config.class_:\n\t\t\treturn EntityType.CLASS\n\t\tif node_type in self.config.interface:\n\t\t\t# Would need to check for ABC inheritance to be precise\n\t\t\treturn EntityType.INTERFACE\n\t\tif node_type in self.config.protocol:\n\t\t\t# Would need to check for Protocol inheritance to be precise\n\t\t\treturn EntityType.PROTOCOL\n\t\tif node_type in self.config.type_alias:\n\t\t\t# For assignments, check if it's a constant (all uppercase) first\n\t\t\tif node_type == \"assignment\":\n\t\t\t\tname_node = node.child_by_field_name(\"left\")\n\t\t\t\tif name_node:\n\t\t\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\t\t\tlogger.debug(\"Checking potential constant in type_alias: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\t\t\treturn EntityType.CONSTANT\n\n\t\t\t# Otherwise, treat as a type alias\n\t\t\treturn EntityType.TYPE_ALIAS\n\n\t\t# Functions and methods\n\t\tif node_type in self.config.function:\n\t\t\t# Check if this is a test function\n\t\t\tname = self.extract_name(node, content_bytes)\n\t\t\tif name.startswith(\"test_\"):\n\t\t\t\treturn EntityType.TEST_CASE\n\n\t\t\t# Check if this is a method by looking for class ancestry\n\t\t\tif self._is_within_class_context(node):\n\t\t\t\treturn EntityType.METHOD\n\t\t\treturn EntityType.FUNCTION\n\n\t\t# Check for properties - decorated definitions\n\t\tif node_type in self.config.property_def:\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"decorator\":\n\t\t\t\t\tdecorator_text = self._get_node_text(child, content_bytes)\n\t\t\t\t\tif \"@property\" in decorator_text:\n\t\t\t\t\t\treturn EntityType.PROPERTY\n\t\t\t# If no @property decorator, treat as method if in class, otherwise function\n\t\t\tif self._is_within_class_context(node):\n\t\t\t\treturn EntityType.METHOD\n\t\t\treturn EntityType.FUNCTION\n\n\t\t# Variables and constants\n\t\tif node_type in self.config.variable:\n\t\t\t# Check if it looks like a constant (uppercase name)\n\t\t\tname_node = node.child_by_field_name(\"left\")\n\t\t\tif name_node:\n\t\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\t\tlogger.debug(\"Checking potential constant: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\t\treturn EntityType.CONSTANT\n\t\t\tlogger.debug(\"Identified as VARIABLE: node_type=%s\", node_type)\n\t\t\treturn EntityType.VARIABLE\n\n\t\t# Class fields\n\t\tif node_type in self.config.class_field:\n\t\t\treturn EntityType.CLASS_FIELD\n\n\t\t# Code organization\n\t\tif node_type in self.config.import_:\n\t\t\treturn EntityType.IMPORT\n\t\tif node_type in self.config.decorator:\n\t\t\treturn EntityType.DECORATOR\n\n\t\treturn EntityType.UNKNOWN\n\n\tdef _is_within_class_context(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if the node is defined within a class definition.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node is within a class context\n\n\t\t\"\"\"\n\t\tancestor = node.parent\n\t\twhile ancestor:\n\t\t\tif ancestor.type in self.config.class_:\n\t\t\t\treturn True\n\t\t\t# Stop search if we hit module or another function definition\n\t\t\tif ancestor.type in self.config.module or ancestor.type in self.config.function:\n\t\t\t\tbreak\n\t\t\tancestor = ancestor.parent\n\t\treturn False\n\n\tdef _is_docstring(self, node: Node, parent: Node | None) -&gt; bool:\n\t\t\"\"\"\n\t\tCheck if a string node is a docstring.\n\n\t\tArgs:\n\t\t    node: The string node\n\t\t    parent: The parent node\n\n\t\tReturns:\n\t\t    True if the node is a docstring\n\n\t\t\"\"\"\n\t\tif not parent:\n\t\t\treturn False\n\n\t\t# For module docstrings, check if it's the first string in the module\n\t\tif parent.type == \"module\":\n\t\t\tnon_comment_children = [c for c in parent.children if c.type not in [\"comment\"]]\n\t\t\treturn bool(non_comment_children and node == non_comment_children[0])\n\n\t\t# For expression statements containing string literals\n\t\tif parent.type == \"expression_statement\":\n\t\t\t# Check if this is the first child of a function or class body\n\t\t\tgrandparent = parent.parent\n\t\t\tif grandparent and grandparent.type == \"block\":\n\t\t\t\tgreat_grandparent = grandparent.parent\n\t\t\t\tif great_grandparent and great_grandparent.type in (self.config.function + self.config.class_):\n\t\t\t\t\t# Check if it's the first item in the block\n\t\t\t\t\tnon_comment_children = [c for c in grandparent.children if c.type not in [\"comment\"]]\n\t\t\t\t\treturn bool(non_comment_children and parent == non_comment_children[0])\n\t\t\treturn False\n\n\t\treturn False\n\n\tdef find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\t\"\"\"\n\t\tFind the docstring associated with a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    A tuple containing:\n\t\t    - The extracted docstring text (or None).\n\t\t    - The specific AST node representing the docstring (or None).\n\n\t\t\"\"\"\n\t\tbody_node = self.get_body_node(node)\n\t\tif not body_node:\n\t\t\t# Handle module docstring case (no explicit body node)\n\t\t\tif node.type == \"module\":\n\t\t\t\tbody_node = node  # Treat module itself as the body context\n\t\t\telse:\n\t\t\t\treturn None, None\n\n\t\tif body_node.named_child_count == 0:\n\t\t\treturn None, None\n\n\t\t# Look for the first child that might be a docstring\n\t\tfirst_body_child = None\n\t\tfor child in body_node.children:\n\t\t\tif child.is_named:\n\t\t\t\tfirst_body_child = child\n\t\t\t\tbreak\n\n\t\tif not first_body_child:\n\t\t\treturn None, None\n\n\t\tactual_string_node = None\n\t\tdocstring_container_node = None  # The node to skip during processing\n\n\t\tif first_body_child.type == \"expression_statement\":\n\t\t\t# For expression statements containing string literals\n\t\t\tfor child in first_body_child.children:\n\t\t\t\tif child.type in self.config.docstring:\n\t\t\t\t\tactual_string_node = child\n\t\t\t\t\tdocstring_container_node = first_body_child\n\t\t\t\t\tbreak\n\t\telif first_body_child.type in self.config.docstring:\n\t\t\t# Direct string literal\n\t\t\tactual_string_node = first_body_child\n\t\t\tdocstring_container_node = first_body_child\n\n\t\tif actual_string_node:\n\t\t\ttry:\n\t\t\t\tdocstring_text = self._get_node_text(actual_string_node, content_bytes).strip(\"\\\"' \\n\")\n\t\t\t\treturn docstring_text, docstring_container_node\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode/extract Python docstring: %s\", e)\n\n\t\treturn None, None\n\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\t\t# Try to find the name field\n\t\tname_node = node.child_by_field_name(\"name\")\n\n\t\t# Handle assignments\n\t\tif not name_node and node.type == \"assignment\":\n\t\t\tname_node = node.child_by_field_name(\"left\")\n\n\t\t# Handle expression statements with assignments\n\t\tif not name_node and node.type == \"expression_statement\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"assignment\":\n\t\t\t\t\tname_node = child.child_by_field_name(\"left\")\n\t\t\t\t\tif name_node:\n\t\t\t\t\t\tbreak\n\n\t\t# Handle decorated definitions\n\t\tif not name_node and node.type == \"decorated_definition\":\n\t\t\tfunc_def = node.child_by_field_name(\"definition\")\n\t\t\tif func_def:\n\t\t\t\tname_node = func_def.child_by_field_name(\"name\")\n\n\t\tif name_node:\n\t\t\ttry:\n\t\t\t\treturn self._get_node_text(name_node, content_bytes)\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode Python name: %s\", e)\n\t\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t\treturn f\"&lt;anonymous-{node.type}&gt;\"\n\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"Get the block node for function/class definition body.\"\"\"\n\t\tif node.type in (\"function_definition\", \"class_definition\", \"decorated_definition\"):\n\t\t\t# Handle decorated definitions properly\n\t\t\tactual_def_node = node\n\t\t\tif node.type == \"decorated_definition\":\n\t\t\t\t# Find the actual function/class definition node within the decoration\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\t\tactual_def_node = child\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\treturn None  # Could not find definition within decorator\n\n\t\t\t# Find the 'block' node which contains the body statements\n\t\t\tfor child in actual_def_node.children:\n\t\t\t\tif child.type == \"block\":\n\t\t\t\t\treturn child\n\t\treturn None  # Not a function/class definition or no block found\n\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\t\t# Process children of the body node if it exists, otherwise process direct children\n\t\treturn list(body_node.children) if body_node else list(node.children)\n\n\tdef should_skip_node(self, node: Node) -&gt; bool:\n\t\t\"\"\"\n\t\tDetermine if a node should be skipped entirely during processing.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    True if the node should be skipped\n\n\t\t\"\"\"\n\t\t# Skip non-named nodes (like punctuation, operators)\n\t\tif not node.is_named:\n\t\t\treturn True\n\n\t\t# Skip syntax nodes that don't contribute to code structure\n\t\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\"]\n\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported module names from a Python import statement.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node representing an import statement\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported module names as strings\n\n\t\t\"\"\"\n\t\tif node.type not in self.config.import_:\n\t\t\treturn []\n\n\t\timported_names = []\n\n\t\ttry:\n\t\t\t# Handle regular import statements: \"import foo, bar\"\n\t\t\tif node.type == \"import_statement\":\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\t\tmodule_name = self._get_node_text(child, content_bytes)\n\t\t\t\t\t\timported_names.append(module_name)\n\n\t\t\t# Handle import from statements: \"from foo.bar import baz, qux\"\n\t\t\telif node.type == \"import_from_statement\":\n\t\t\t\t# Get the module being imported from\n\t\t\t\tmodule_node = None\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\t\tmodule_node = child\n\t\t\t\t\t\tbreak\n\n\t\t\t\tif module_node:\n\t\t\t\t\tmodule_name = self._get_node_text(module_node, content_bytes)\n\n\t\t\t\t\t# Get the imported names\n\t\t\t\t\timport_node = node.child_by_field_name(\"import\")\n\t\t\t\t\tif import_node:\n\t\t\t\t\t\t# Check for the wildcard import case: \"from foo import *\"\n\t\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\t\tif child.type == \"wildcard_import\":\n\t\t\t\t\t\t\t\timported_names.append(f\"{module_name}.*\")\n\t\t\t\t\t\t\t\treturn imported_names\n\n\t\t\t\t\t\t# Regular named imports\n\t\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\t\tif child.type == \"import_list\":\n\t\t\t\t\t\t\t\tfor item in child.children:\n\t\t\t\t\t\t\t\t\tif item.type in {\"dotted_name\", \"identifier\"}:\n\t\t\t\t\t\t\t\t\t\tname = self._get_node_text(item, content_bytes)\n\t\t\t\t\t\t\t\t\t\timported_names.append(f\"{module_name}.{name}\")\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode Python imports: %s\", e)\n\n\t\treturn imported_names\n\n\tdef extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract names of functions/methods called within a Python node's scope.\n\n\t\tRecursively searches for 'call' nodes and extracts the function identifier.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node (e.g., function/method body)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of called function/method names\n\n\t\t\"\"\"\n\t\tcalls = []\n\t\tfor child in node.children:\n\t\t\tif child.type == \"call\":\n\t\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\t\tif function_node:\n\t\t\t\t\t# Extract the identifier (could be simple name or attribute access like obj.method)\n\t\t\t\t\t# For simplicity, we take the full text of the function node\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcall_name = self._get_node_text(function_node, content_bytes)\n\t\t\t\t\t\tcalls.append(call_name)\n\t\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\t\tpass  # Ignore decoding errors\n\t\t\t# Recursively search within the arguments or children of the call if needed, but often not necessary\n\t\t\t# for call details, just the name.\n\t\t\t# Else, recursively search deeper within non-call children\n\t\t\telse:\n\t\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\t\treturn list(set(calls))  # Return unique calls\n\n\tdef extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\t\"\"\"Extract the signature up to the colon ':' for Python functions/classes.\"\"\"\n\t\tsignature_node = node\n\t\t# If it's a decorated definition, find the actual definition node for the signature start\n\t\tif node.type == \"decorated_definition\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\tsignature_node = child\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]  # Fallback to first line of decorator\n\n\t\t# Find the colon that ends the signature part\n\t\tcolon_node = None\n\t\tfor child in signature_node.children:\n\t\t\tif child.type == \":\":\n\t\t\t\tcolon_node = child\n\t\t\t\tbreak\n\t\t\t# Handle async functions where 'def' is preceded by 'async'\n\t\t\tif child.type == \"async\":\n\t\t\t\tcontinue  # skip 'async' keyword itself\n\t\t\tif child.type in {\"def\", \"class\"}:\n\t\t\t\tcontinue  # skip 'def'/'class' keywords\n\t\t\t# Stop if we hit the body block before finding a colon (shouldn't happen in valid code)\n\t\t\tif child.type == \"block\":\n\t\t\t\tbreak\n\n\t\tif colon_node:\n\t\t\t# Extract text from the start of the definition node up to the end of the colon\n\t\t\tstart_byte = signature_node.start_byte\n\t\t\tend_byte = colon_node.end_byte\n\t\t\ttry:\n\t\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\t\texcept IndexError:\n\t\t\t\treturn None\n\t\telse:\n\t\t\t# Fallback: if no colon found (e.g., malformed code?), return the first line\n\t\t\treturn self._get_node_text(signature_node, content_bytes).splitlines()[0]\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"Find the first ancestor node matching the target Python entity type.\"\"\"\n\t\ttarget_node_types = []\n\t\tif target_type == EntityType.CLASS:\n\t\t\ttarget_node_types = [\"class_definition\", \"decorated_definition\"]  # Include decorated\n\t\telif target_type == EntityType.FUNCTION:\n\t\t\ttarget_node_types = [\"function_definition\", \"decorated_definition\"]  # Include decorated\n\t\telif target_type == EntityType.MODULE:\n\t\t\t# Module is typically the root node or identified by file, not easily findable as ancestor type\n\t\t\treturn None  # Or return root node? Depends on desired behavior.\n\t\t# Add other types if needed\n\n\t\tif not target_node_types:\n\t\t\treturn None\n\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\t# Check if the current node is the target type or a decorator containing it\n\t\t\tnode_to_check = current\n\t\t\tactual_node_type = current.type\n\n\t\t\tif current.type == \"decorated_definition\":\n\t\t\t\t# Check the *content* of the decorated definition\n\t\t\t\tfound_target_in_decorator = False\n\t\t\t\tfor child in current.children:\n\t\t\t\t\tif child.type in target_node_types and child.type != \"decorated_definition\":\n\t\t\t\t\t\t# We found the actual class/func def inside the decorator\n\t\t\t\t\t\tnode_to_check = child\n\t\t\t\t\t\tactual_node_type = child.type\n\t\t\t\t\t\tfound_target_in_decorator = True\n\t\t\t\t\t\tbreak\n\t\t\t\tif not found_target_in_decorator:\n\t\t\t\t\tactual_node_type = \"decorated_definition\"  # Treat decorator itself if no target found within\n\n\t\t\t# Now check if the node (or the one found inside decorator) matches\n\t\t\tif actual_node_type in target_node_types and actual_node_type != \"decorated_definition\":\n\t\t\t\treturn node_to_check  # Return the actual definition node\n\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef _find_decorated_definition(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"Helper to get the actual definition node from a decorated_definition.\"\"\"\n\t\tif node.type == \"decorated_definition\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\treturn child\n\t\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize with Python configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize with Python configuration.\"\"\"\n\tsuper().__init__(PYTHON_CONFIG)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_entity_type","title":"get_entity_type","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a Python node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a Python node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n\tnode_type = node.type\n\tlogger.debug(\n\t\t\"Getting entity type for Python node: type=%s, parent_type=%s\", node_type, parent.type if parent else None\n\t)\n\n\t# Print node content for debugging\n\ttry:\n\t\tnode_content = self._get_node_text(node, content_bytes)\n\t\tlogger.debug(\"Node content: %s\", node_content)\n\texcept (UnicodeDecodeError, IndexError) as e:\n\t\tlogger.debug(\"Failed to decode node content: %s\", str(e))\n\n\t# Special case: if this is an expression statement containing a constant assignment\n\tif node_type == \"expression_statement\":\n\t\t# Check if it contains an assignment that is a constant\n\t\tfor child in node.children:\n\t\t\tif child.type == \"assignment\":\n\t\t\t\tname_node = child.child_by_field_name(\"left\")\n\t\t\t\tif name_node:\n\t\t\t\t\tname = self._get_node_text(name_node, content_bytes)\n\n\t\t\t\t\t# Get the right side for type detection\n\t\t\t\t\tvalue_node = child.child_by_field_name(\"right\")\n\t\t\t\t\tvalue_text = \"\"\n\t\t\t\t\tif value_node:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tvalue_text = self._get_node_text(value_node, content_bytes)\n\t\t\t\t\t\texcept (UnicodeDecodeError, IndexError) as e:\n\t\t\t\t\t\t\tlogger.debug(\"Failed to decode type value: %s\", str(e))\n\n\t\t\t\t\t# Check for type alias - TypeVar or anything referencing typing types like Dict, List, etc.\n\t\t\t\t\tif \"TypeVar\" in value_text or any(\n\t\t\t\t\t\ttyping_type in value_text\n\t\t\t\t\t\tfor typing_type in [\"Dict\", \"List\", \"Tuple\", \"Set\", \"Union\", \"Optional\", \"Callable\", \"Any\"]\n\t\t\t\t\t):\n\t\t\t\t\t\tlogger.debug(\"Expression statement with TYPE_ALIAS: %s\", name)\n\t\t\t\t\t\treturn EntityType.TYPE_ALIAS\n\n\t\t\t\t\t# Check for constant (all uppercase with at least one letter)\n\t\t\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\t\t\tlogger.debug(\"Expression statement with CONSTANT assignment: %s\", name)\n\t\t\t\t\t\treturn EntityType.CONSTANT\n\t\t\t\t\t# Check for regular variable\n\t\t\t\t\tif not name.startswith(\"_\") and any(c.isalpha() for c in name):\n\t\t\t\t\t\tlogger.debug(\"Expression statement with VARIABLE assignment: %s\", name)\n\t\t\t\t\t\treturn EntityType.VARIABLE\n\n\t# Module-level\n\tif node_type in self.config.module:\n\t\treturn EntityType.MODULE\n\tif node_type in self.config.namespace:\n\t\treturn EntityType.NAMESPACE\n\n\t# Documentation\n\tif node_type in self.config.docstring:\n\t\t# Check if this is a docstring (first string in a container)\n\t\tif self._is_docstring(node, parent):\n\t\t\treturn EntityType.DOCSTRING\n\t\treturn EntityType.UNKNOWN  # Regular string literals\n\tif node_type in self.config.comment:\n\t\treturn EntityType.COMMENT\n\n\t# Type definitions\n\tif node_type in self.config.class_:\n\t\treturn EntityType.CLASS\n\tif node_type in self.config.interface:\n\t\t# Would need to check for ABC inheritance to be precise\n\t\treturn EntityType.INTERFACE\n\tif node_type in self.config.protocol:\n\t\t# Would need to check for Protocol inheritance to be precise\n\t\treturn EntityType.PROTOCOL\n\tif node_type in self.config.type_alias:\n\t\t# For assignments, check if it's a constant (all uppercase) first\n\t\tif node_type == \"assignment\":\n\t\t\tname_node = node.child_by_field_name(\"left\")\n\t\t\tif name_node:\n\t\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\t\tlogger.debug(\"Checking potential constant in type_alias: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\t\treturn EntityType.CONSTANT\n\n\t\t# Otherwise, treat as a type alias\n\t\treturn EntityType.TYPE_ALIAS\n\n\t# Functions and methods\n\tif node_type in self.config.function:\n\t\t# Check if this is a test function\n\t\tname = self.extract_name(node, content_bytes)\n\t\tif name.startswith(\"test_\"):\n\t\t\treturn EntityType.TEST_CASE\n\n\t\t# Check if this is a method by looking for class ancestry\n\t\tif self._is_within_class_context(node):\n\t\t\treturn EntityType.METHOD\n\t\treturn EntityType.FUNCTION\n\n\t# Check for properties - decorated definitions\n\tif node_type in self.config.property_def:\n\t\tfor child in node.children:\n\t\t\tif child.type == \"decorator\":\n\t\t\t\tdecorator_text = self._get_node_text(child, content_bytes)\n\t\t\t\tif \"@property\" in decorator_text:\n\t\t\t\t\treturn EntityType.PROPERTY\n\t\t# If no @property decorator, treat as method if in class, otherwise function\n\t\tif self._is_within_class_context(node):\n\t\t\treturn EntityType.METHOD\n\t\treturn EntityType.FUNCTION\n\n\t# Variables and constants\n\tif node_type in self.config.variable:\n\t\t# Check if it looks like a constant (uppercase name)\n\t\tname_node = node.child_by_field_name(\"left\")\n\t\tif name_node:\n\t\t\tname = self._get_node_text(name_node, content_bytes)\n\t\t\tlogger.debug(\"Checking potential constant: %s (is_upper: %s)\", name, name.isupper())\n\t\t\t# Improved check for constants: name is uppercase and contains at least one letter\n\t\t\tif name.isupper() and any(c.isalpha() for c in name):\n\t\t\t\tlogger.debug(\"Identified as CONSTANT: %s\", name)\n\t\t\t\treturn EntityType.CONSTANT\n\t\tlogger.debug(\"Identified as VARIABLE: node_type=%s\", node_type)\n\t\treturn EntityType.VARIABLE\n\n\t# Class fields\n\tif node_type in self.config.class_field:\n\t\treturn EntityType.CLASS_FIELD\n\n\t# Code organization\n\tif node_type in self.config.import_:\n\t\treturn EntityType.IMPORT\n\tif node_type in self.config.decorator:\n\t\treturn EntityType.DECORATOR\n\n\treturn EntityType.UNKNOWN\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.find_docstring","title":"find_docstring","text":"<pre><code>find_docstring(\n\tnode: Node, content_bytes: bytes\n) -&gt; tuple[str | None, Node | None]\n</code></pre> <p>Find the docstring associated with a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A tuple containing:</p> <code>Node | None</code> <ul> <li>The extracted docstring text (or None).</li> </ul> <code>tuple[str | None, Node | None]</code> <ul> <li>The specific AST node representing the docstring (or None).</li> </ul> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def find_docstring(self, node: Node, content_bytes: bytes) -&gt; tuple[str | None, Node | None]:\n\t\"\"\"\n\tFind the docstring associated with a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    A tuple containing:\n\t    - The extracted docstring text (or None).\n\t    - The specific AST node representing the docstring (or None).\n\n\t\"\"\"\n\tbody_node = self.get_body_node(node)\n\tif not body_node:\n\t\t# Handle module docstring case (no explicit body node)\n\t\tif node.type == \"module\":\n\t\t\tbody_node = node  # Treat module itself as the body context\n\t\telse:\n\t\t\treturn None, None\n\n\tif body_node.named_child_count == 0:\n\t\treturn None, None\n\n\t# Look for the first child that might be a docstring\n\tfirst_body_child = None\n\tfor child in body_node.children:\n\t\tif child.is_named:\n\t\t\tfirst_body_child = child\n\t\t\tbreak\n\n\tif not first_body_child:\n\t\treturn None, None\n\n\tactual_string_node = None\n\tdocstring_container_node = None  # The node to skip during processing\n\n\tif first_body_child.type == \"expression_statement\":\n\t\t# For expression statements containing string literals\n\t\tfor child in first_body_child.children:\n\t\t\tif child.type in self.config.docstring:\n\t\t\t\tactual_string_node = child\n\t\t\t\tdocstring_container_node = first_body_child\n\t\t\t\tbreak\n\telif first_body_child.type in self.config.docstring:\n\t\t# Direct string literal\n\t\tactual_string_node = first_body_child\n\t\tdocstring_container_node = first_body_child\n\n\tif actual_string_node:\n\t\ttry:\n\t\t\tdocstring_text = self._get_node_text(actual_string_node, content_bytes).strip(\"\\\"' \\n\")\n\t\t\treturn docstring_text, docstring_container_node\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode/extract Python docstring: %s\", e)\n\n\treturn None, None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_name","title":"extract_name","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n\t# Try to find the name field\n\tname_node = node.child_by_field_name(\"name\")\n\n\t# Handle assignments\n\tif not name_node and node.type == \"assignment\":\n\t\tname_node = node.child_by_field_name(\"left\")\n\n\t# Handle expression statements with assignments\n\tif not name_node and node.type == \"expression_statement\":\n\t\tfor child in node.children:\n\t\t\tif child.type == \"assignment\":\n\t\t\t\tname_node = child.child_by_field_name(\"left\")\n\t\t\t\tif name_node:\n\t\t\t\t\tbreak\n\n\t# Handle decorated definitions\n\tif not name_node and node.type == \"decorated_definition\":\n\t\tfunc_def = node.child_by_field_name(\"definition\")\n\t\tif func_def:\n\t\t\tname_node = func_def.child_by_field_name(\"name\")\n\n\tif name_node:\n\t\ttry:\n\t\t\treturn self._get_node_text(name_node, content_bytes)\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode Python name: %s\", e)\n\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\treturn f\"&lt;anonymous-{node.type}&gt;\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_body_node","title":"get_body_node","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the block node for function/class definition body.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"Get the block node for function/class definition body.\"\"\"\n\tif node.type in (\"function_definition\", \"class_definition\", \"decorated_definition\"):\n\t\t# Handle decorated definitions properly\n\t\tactual_def_node = node\n\t\tif node.type == \"decorated_definition\":\n\t\t\t# Find the actual function/class definition node within the decoration\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\t\tactual_def_node = child\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treturn None  # Could not find definition within decorator\n\n\t\t# Find the 'block' node which contains the body statements\n\t\tfor child in actual_def_node.children:\n\t\t\tif child.type == \"block\":\n\t\t\t\treturn child\n\treturn None  # Not a function/class definition or no block found\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_children_to_process","title":"get_children_to_process","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n\t# Process children of the body node if it exists, otherwise process direct children\n\treturn list(body_node.children) if body_node else list(node.children)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.should_skip_node","title":"should_skip_node","text":"<pre><code>should_skip_node(node: Node) -&gt; bool\n</code></pre> <p>Determine if a node should be skipped entirely during processing.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be skipped</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def should_skip_node(self, node: Node) -&gt; bool:\n\t\"\"\"\n\tDetermine if a node should be skipped entirely during processing.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    True if the node should be skipped\n\n\t\"\"\"\n\t# Skip non-named nodes (like punctuation, operators)\n\tif not node.is_named:\n\t\treturn True\n\n\t# Skip syntax nodes that don't contribute to code structure\n\treturn node.type in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \";\", \".\", \",\"]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_imports","title":"extract_imports","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported module names from a Python import statement.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node representing an import statement</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported module names as strings</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported module names from a Python import statement.\n\n\tArgs:\n\t    node: The tree-sitter node representing an import statement\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported module names as strings\n\n\t\"\"\"\n\tif node.type not in self.config.import_:\n\t\treturn []\n\n\timported_names = []\n\n\ttry:\n\t\t# Handle regular import statements: \"import foo, bar\"\n\t\tif node.type == \"import_statement\":\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\tmodule_name = self._get_node_text(child, content_bytes)\n\t\t\t\t\timported_names.append(module_name)\n\n\t\t# Handle import from statements: \"from foo.bar import baz, qux\"\n\t\telif node.type == \"import_from_statement\":\n\t\t\t# Get the module being imported from\n\t\t\tmodule_node = None\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"dotted_name\":\n\t\t\t\t\tmodule_node = child\n\t\t\t\t\tbreak\n\n\t\t\tif module_node:\n\t\t\t\tmodule_name = self._get_node_text(module_node, content_bytes)\n\n\t\t\t\t# Get the imported names\n\t\t\t\timport_node = node.child_by_field_name(\"import\")\n\t\t\t\tif import_node:\n\t\t\t\t\t# Check for the wildcard import case: \"from foo import *\"\n\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\tif child.type == \"wildcard_import\":\n\t\t\t\t\t\t\timported_names.append(f\"{module_name}.*\")\n\t\t\t\t\t\t\treturn imported_names\n\n\t\t\t\t\t# Regular named imports\n\t\t\t\t\tfor child in import_node.children:\n\t\t\t\t\t\tif child.type == \"import_list\":\n\t\t\t\t\t\t\tfor item in child.children:\n\t\t\t\t\t\t\t\tif item.type in {\"dotted_name\", \"identifier\"}:\n\t\t\t\t\t\t\t\t\tname = self._get_node_text(item, content_bytes)\n\t\t\t\t\t\t\t\t\timported_names.append(f\"{module_name}.{name}\")\n\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\tlogger.warning(\"Failed to decode Python imports: %s\", e)\n\n\treturn imported_names\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_calls","title":"extract_calls","text":"<pre><code>extract_calls(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract names of functions/methods called within a Python node's scope.</p> <p>Recursively searches for 'call' nodes and extracts the function identifier.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node (e.g., function/method body)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of called function/method names</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_calls(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract names of functions/methods called within a Python node's scope.\n\n\tRecursively searches for 'call' nodes and extracts the function identifier.\n\n\tArgs:\n\t    node: The tree-sitter node (e.g., function/method body)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of called function/method names\n\n\t\"\"\"\n\tcalls = []\n\tfor child in node.children:\n\t\tif child.type == \"call\":\n\t\t\tfunction_node = child.child_by_field_name(\"function\")\n\t\t\tif function_node:\n\t\t\t\t# Extract the identifier (could be simple name or attribute access like obj.method)\n\t\t\t\t# For simplicity, we take the full text of the function node\n\t\t\t\ttry:\n\t\t\t\t\tcall_name = self._get_node_text(function_node, content_bytes)\n\t\t\t\t\tcalls.append(call_name)\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tpass  # Ignore decoding errors\n\t\t# Recursively search within the arguments or children of the call if needed, but often not necessary\n\t\t# for call details, just the name.\n\t\t# Else, recursively search deeper within non-call children\n\t\telse:\n\t\t\tcalls.extend(self.extract_calls(child, content_bytes))\n\treturn list(set(calls))  # Return unique calls\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.extract_signature","title":"extract_signature","text":"<pre><code>extract_signature(\n\tnode: Node, content_bytes: bytes\n) -&gt; str | None\n</code></pre> <p>Extract the signature up to the colon ':' for Python functions/classes.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def extract_signature(self, node: Node, content_bytes: bytes) -&gt; str | None:\n\t\"\"\"Extract the signature up to the colon ':' for Python functions/classes.\"\"\"\n\tsignature_node = node\n\t# If it's a decorated definition, find the actual definition node for the signature start\n\tif node.type == \"decorated_definition\":\n\t\tfor child in node.children:\n\t\t\tif child.type in (\"function_definition\", \"class_definition\"):\n\t\t\t\tsignature_node = child\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn self._get_node_text(node, content_bytes).splitlines()[0]  # Fallback to first line of decorator\n\n\t# Find the colon that ends the signature part\n\tcolon_node = None\n\tfor child in signature_node.children:\n\t\tif child.type == \":\":\n\t\t\tcolon_node = child\n\t\t\tbreak\n\t\t# Handle async functions where 'def' is preceded by 'async'\n\t\tif child.type == \"async\":\n\t\t\tcontinue  # skip 'async' keyword itself\n\t\tif child.type in {\"def\", \"class\"}:\n\t\t\tcontinue  # skip 'def'/'class' keywords\n\t\t# Stop if we hit the body block before finding a colon (shouldn't happen in valid code)\n\t\tif child.type == \"block\":\n\t\t\tbreak\n\n\tif colon_node:\n\t\t# Extract text from the start of the definition node up to the end of the colon\n\t\tstart_byte = signature_node.start_byte\n\t\tend_byte = colon_node.end_byte\n\t\ttry:\n\t\t\treturn content_bytes[start_byte:end_byte].decode(\"utf-8\", errors=\"ignore\").strip()\n\t\texcept IndexError:\n\t\t\treturn None\n\telse:\n\t\t# Fallback: if no colon found (e.g., malformed code?), return the first line\n\t\treturn self._get_node_text(signature_node, content_bytes).splitlines()[0]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/python/#codemap.processor.tree_sitter.languages.python.PythonSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node matching the target Python entity type.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/python.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"Find the first ancestor node matching the target Python entity type.\"\"\"\n\ttarget_node_types = []\n\tif target_type == EntityType.CLASS:\n\t\ttarget_node_types = [\"class_definition\", \"decorated_definition\"]  # Include decorated\n\telif target_type == EntityType.FUNCTION:\n\t\ttarget_node_types = [\"function_definition\", \"decorated_definition\"]  # Include decorated\n\telif target_type == EntityType.MODULE:\n\t\t# Module is typically the root node or identified by file, not easily findable as ancestor type\n\t\treturn None  # Or return root node? Depends on desired behavior.\n\t# Add other types if needed\n\n\tif not target_node_types:\n\t\treturn None\n\n\tcurrent = node.parent\n\twhile current:\n\t\t# Check if the current node is the target type or a decorator containing it\n\t\tnode_to_check = current\n\t\tactual_node_type = current.type\n\n\t\tif current.type == \"decorated_definition\":\n\t\t\t# Check the *content* of the decorated definition\n\t\t\tfound_target_in_decorator = False\n\t\t\tfor child in current.children:\n\t\t\t\tif child.type in target_node_types and child.type != \"decorated_definition\":\n\t\t\t\t\t# We found the actual class/func def inside the decorator\n\t\t\t\t\tnode_to_check = child\n\t\t\t\t\tactual_node_type = child.type\n\t\t\t\t\tfound_target_in_decorator = True\n\t\t\t\t\tbreak\n\t\t\tif not found_target_in_decorator:\n\t\t\t\tactual_node_type = \"decorated_definition\"  # Treat decorator itself if no target found within\n\n\t\t# Now check if the node (or the one found inside decorator) matches\n\t\tif actual_node_type in target_node_types and actual_node_type != \"decorated_definition\":\n\t\t\treturn node_to_check  # Return the actual definition node\n\n\t\tcurrent = current.parent\n\treturn None\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/","title":"Typescript","text":"<p>TypeScript-specific configuration for syntax chunking.</p>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig","title":"TypeScriptConfig  <code>dataclass</code>","text":"<p>               Bases: <code>LanguageConfig</code></p> <p>TypeScript-specific syntax chunking configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>class TypeScriptConfig(LanguageConfig):\n\t\"\"\"TypeScript-specific syntax chunking configuration.\"\"\"\n\n\t# File-level entities\n\tmodule: ClassVar[list[str]] = [\"program\"]\n\tnamespace: ClassVar[list[str]] = [\"export_statement\", \"namespace_declaration\"]\n\n\t# Type definitions\n\tclass_: ClassVar[list[str]] = [\"class_declaration\", \"class\"]\n\tinterface: ClassVar[list[str]] = [\"interface_declaration\"]\n\tprotocol: ClassVar[list[str]] = []  # TypeScript doesn't have protocols\n\tstruct: ClassVar[list[str]] = []  # TypeScript doesn't have structs\n\tenum: ClassVar[list[str]] = [\"enum_declaration\"]\n\ttype_alias: ClassVar[list[str]] = [\"type_alias_declaration\"]\n\n\t# Functions and methods\n\tfunction: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.function, \"function_signature\"]\n\tmethod: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.method, \"method_signature\"]\n\tproperty_def: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.property_def, \"public_field_definition\"]\n\ttest_case: ClassVar[list[str]] = JAVASCRIPT_CONFIG.test_case\n\ttest_suite: ClassVar[list[str]] = JAVASCRIPT_CONFIG.test_suite\n\n\t# Variables and constants\n\tvariable: ClassVar[list[str]] = JAVASCRIPT_CONFIG.variable\n\tconstant: ClassVar[list[str]] = JAVASCRIPT_CONFIG.constant\n\tclass_field: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.class_field, \"public_field_definition\"]\n\n\t# Code organization\n\timport_: ClassVar[list[str]] = [*JAVASCRIPT_CONFIG.import_, \"import_alias\"]\n\tdecorator: ClassVar[list[str]] = JAVASCRIPT_CONFIG.decorator\n\n\t# Documentation\n\tcomment: ClassVar[list[str]] = JAVASCRIPT_CONFIG.comment\n\tdocstring: ClassVar[list[str]] = JAVASCRIPT_CONFIG.docstring\n\n\tfile_extensions: ClassVar[list[str]] = [\".ts\", \".tsx\"]\n\ttree_sitter_name: ClassVar[str] = \"typescript\"\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.module","title":"module  <code>class-attribute</code>","text":"<pre><code>module: list[str] = ['program']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.namespace","title":"namespace  <code>class-attribute</code>","text":"<pre><code>namespace: list[str] = [\n\t\"export_statement\",\n\t\"namespace_declaration\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.class_","title":"class_  <code>class-attribute</code>","text":"<pre><code>class_: list[str] = ['class_declaration', 'class']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.interface","title":"interface  <code>class-attribute</code>","text":"<pre><code>interface: list[str] = ['interface_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.protocol","title":"protocol  <code>class-attribute</code>","text":"<pre><code>protocol: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.struct","title":"struct  <code>class-attribute</code>","text":"<pre><code>struct: list[str] = []\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.enum","title":"enum  <code>class-attribute</code>","text":"<pre><code>enum: list[str] = ['enum_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.type_alias","title":"type_alias  <code>class-attribute</code>","text":"<pre><code>type_alias: list[str] = ['type_alias_declaration']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.function","title":"function  <code>class-attribute</code>","text":"<pre><code>function: list[str] = [*function, 'function_signature']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.method","title":"method  <code>class-attribute</code>","text":"<pre><code>method: list[str] = [*method, 'method_signature']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.property_def","title":"property_def  <code>class-attribute</code>","text":"<pre><code>property_def: list[str] = [\n\t*property_def,\n\t\"public_field_definition\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.test_case","title":"test_case  <code>class-attribute</code>","text":"<pre><code>test_case: list[str] = test_case\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.test_suite","title":"test_suite  <code>class-attribute</code>","text":"<pre><code>test_suite: list[str] = test_suite\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.variable","title":"variable  <code>class-attribute</code>","text":"<pre><code>variable: list[str] = variable\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.constant","title":"constant  <code>class-attribute</code>","text":"<pre><code>constant: list[str] = constant\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.class_field","title":"class_field  <code>class-attribute</code>","text":"<pre><code>class_field: list[str] = [\n\t*class_field,\n\t\"public_field_definition\",\n]\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.import_","title":"import_  <code>class-attribute</code>","text":"<pre><code>import_: list[str] = [*import_, 'import_alias']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.decorator","title":"decorator  <code>class-attribute</code>","text":"<pre><code>decorator: list[str] = decorator\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.comment","title":"comment  <code>class-attribute</code>","text":"<pre><code>comment: list[str] = comment\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.docstring","title":"docstring  <code>class-attribute</code>","text":"<pre><code>docstring: list[str] = docstring\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.file_extensions","title":"file_extensions  <code>class-attribute</code>","text":"<pre><code>file_extensions: list[str] = ['.ts', '.tsx']\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptConfig.tree_sitter_name","title":"tree_sitter_name  <code>class-attribute</code>","text":"<pre><code>tree_sitter_name: str = 'typescript'\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TYPESCRIPT_CONFIG","title":"TYPESCRIPT_CONFIG  <code>module-attribute</code>","text":"<pre><code>TYPESCRIPT_CONFIG = TypeScriptConfig()\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler","title":"TypeScriptSyntaxHandler","text":"<p>               Bases: <code>JavaScriptSyntaxHandler</code></p> <p>TypeScript-specific syntax handling logic.</p> <p>Inherits from JavaScript handler to reuse common logic.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>class TypeScriptSyntaxHandler(JavaScriptSyntaxHandler):\n\t\"\"\"\n\tTypeScript-specific syntax handling logic.\n\n\tInherits from JavaScript handler to reuse common logic.\n\n\t\"\"\"\n\n\tdef __init__(self) -&gt; None:\n\t\t\"\"\"Initialize with TypeScript configuration.\"\"\"\n\t\t# Revert to super() and ignore potential linter false positive\n\t\tsuper().__init__(TYPESCRIPT_CONFIG)  # type: ignore[call-arg] # pylint: disable=too-many-function-args\n\n\tdef get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\t\"\"\"\n\t\tDetermine the EntityType for a TypeScript node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    parent: The parent node (if any)\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The entity type\n\n\t\t\"\"\"\n\t\tnode_type = node.type\n\t\tlogger.debug(\n\t\t\t\"Getting entity type for TypeScript node: type=%s, parent_type=%s\",\n\t\t\tnode_type,\n\t\t\tparent.type if parent else None,\n\t\t)\n\n\t\t# Check for TypeScript specific types first\n\t\tif node_type in self.config.interface:\n\t\t\treturn EntityType.INTERFACE\n\t\tif node_type in self.config.type_alias:\n\t\t\treturn EntityType.TYPE_ALIAS\n\t\tif node_type == \"enum_declaration\":\n\t\t\treturn EntityType.ENUM\n\t\tif node_type == \"module\":  # TS internal modules/namespaces\n\t\t\treturn EntityType.NAMESPACE\n\t\tif node_type == \"namespace_declaration\":\n\t\t\treturn EntityType.NAMESPACE\n\t\tif node_type == \"method_signature\":\n\t\t\treturn EntityType.METHOD\n\t\tif node_type == \"property_signature\":\n\t\t\treturn EntityType.PROPERTY\n\n\t\t# Use the JavaScript logic for common types\n\t\treturn super().get_entity_type(node, parent, content_bytes)\n\n\tdef extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\t\"\"\"\n\t\tExtract the name identifier from a definition node.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    The extracted name\n\n\t\t\"\"\"\n\t\t# Handle TypeScript-specific node types first\n\t\tname_node = None\n\n\t\tif node.type in [\n\t\t\t\"interface_declaration\",\n\t\t\t\"enum_declaration\",\n\t\t\t\"type_alias_declaration\",\n\t\t\t\"namespace_declaration\",\n\t\t] or node.type in [\"method_signature\", \"property_signature\"]:\n\t\t\tname_node = node.child_by_field_name(\"name\")\n\n\t\tif name_node:\n\t\t\ttry:\n\t\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Failed to decode TypeScript name: %s\", e)\n\t\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t\t# Fall back to JavaScript name extraction\n\t\treturn super().extract_name(node, content_bytes)\n\n\tdef get_body_node(self, node: Node) -&gt; Node | None:\n\t\t\"\"\"\n\t\tGet the node representing the 'body' of a definition.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\n\t\tReturns:\n\t\t    The body node if available, None otherwise\n\n\t\t\"\"\"\n\t\tif node.type in (\"interface_declaration\", \"function_signature\", \"method_signature\"):\n\t\t\treturn None  # Interfaces and signatures have no body block\n\t\treturn super().get_body_node(node)\n\n\tdef get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\t\"\"\"\n\t\tGet the list of child nodes that should be recursively processed.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node\n\t\t    body_node: The body node if available\n\n\t\tReturns:\n\t\t    List of child nodes to process\n\n\t\t\"\"\"\n\t\t# TypeScript-specific handling\n\t\tif node.type == \"type_alias_declaration\":\n\t\t\t# Type aliases don't have children to process\n\t\t\treturn []\n\n\t\t# Fall back to JavaScript children processing\n\t\treturn super().get_children_to_process(node, body_node)\n\n\tdef extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\t\"\"\"\n\t\tExtract imported module names from a TypeScript import statement.\n\n\t\tArgs:\n\t\t    node: The tree-sitter node representing an import statement\n\t\t    content_bytes: Source code content as bytes\n\n\t\tReturns:\n\t\t    List of imported module names as strings\n\n\t\t\"\"\"\n\t\t# TypeScript import statements are the same as JavaScript\n\t\treturn super().extract_imports(node, content_bytes)\n\n\tdef get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\t\"\"\"\n\t\tFind the first ancestor node matching the target TypeScript entity type.\n\n\t\tHandles INTERFACE specifically and falls back to the JavaScript handler\n\t\tfor other types (CLASS, FUNCTION, METHOD, MODULE).\n\n\t\tArgs:\n\t\t    node: The starting node.\n\t\t    target_type: The EntityType to search for in ancestors.\n\n\t\tReturns:\n\t\t    The ancestor node if found, otherwise None.\n\n\t\t\"\"\"\n\t\tif target_type == EntityType.INTERFACE:\n\t\t\ttarget_node_types = [\"interface_declaration\"]\n\t\t\tcurrent = node.parent\n\t\t\twhile current:\n\t\t\t\tif current.type in target_node_types:\n\t\t\t\t\treturn current\n\t\t\t\tcurrent = current.parent\n\t\t\treturn None\n\t\t# Fall back to JS handler for other types (CLASS, FUNCTION, METHOD, MODULE)\n\t\treturn super().get_enclosing_node_of_type(node, target_type)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize with TypeScript configuration.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def __init__(self) -&gt; None:\n\t\"\"\"Initialize with TypeScript configuration.\"\"\"\n\t# Revert to super() and ignore potential linter false positive\n\tsuper().__init__(TYPESCRIPT_CONFIG)  # type: ignore[call-arg] # pylint: disable=too-many-function-args\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_entity_type","title":"get_entity_type","text":"<pre><code>get_entity_type(\n\tnode: Node, parent: Node | None, content_bytes: bytes\n) -&gt; EntityType\n</code></pre> <p>Determine the EntityType for a TypeScript node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>parent</code> <code>Node | None</code> <p>The parent node (if any)</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>EntityType</code> <p>The entity type</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_entity_type(self, node: Node, parent: Node | None, content_bytes: bytes) -&gt; EntityType:\n\t\"\"\"\n\tDetermine the EntityType for a TypeScript node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    parent: The parent node (if any)\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The entity type\n\n\t\"\"\"\n\tnode_type = node.type\n\tlogger.debug(\n\t\t\"Getting entity type for TypeScript node: type=%s, parent_type=%s\",\n\t\tnode_type,\n\t\tparent.type if parent else None,\n\t)\n\n\t# Check for TypeScript specific types first\n\tif node_type in self.config.interface:\n\t\treturn EntityType.INTERFACE\n\tif node_type in self.config.type_alias:\n\t\treturn EntityType.TYPE_ALIAS\n\tif node_type == \"enum_declaration\":\n\t\treturn EntityType.ENUM\n\tif node_type == \"module\":  # TS internal modules/namespaces\n\t\treturn EntityType.NAMESPACE\n\tif node_type == \"namespace_declaration\":\n\t\treturn EntityType.NAMESPACE\n\tif node_type == \"method_signature\":\n\t\treturn EntityType.METHOD\n\tif node_type == \"property_signature\":\n\t\treturn EntityType.PROPERTY\n\n\t# Use the JavaScript logic for common types\n\treturn super().get_entity_type(node, parent, content_bytes)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.extract_name","title":"extract_name","text":"<pre><code>extract_name(node: Node, content_bytes: bytes) -&gt; str\n</code></pre> <p>Extract the name identifier from a definition node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted name</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def extract_name(self, node: Node, content_bytes: bytes) -&gt; str:\n\t\"\"\"\n\tExtract the name identifier from a definition node.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    The extracted name\n\n\t\"\"\"\n\t# Handle TypeScript-specific node types first\n\tname_node = None\n\n\tif node.type in [\n\t\t\"interface_declaration\",\n\t\t\"enum_declaration\",\n\t\t\"type_alias_declaration\",\n\t\t\"namespace_declaration\",\n\t] or node.type in [\"method_signature\", \"property_signature\"]:\n\t\tname_node = node.child_by_field_name(\"name\")\n\n\tif name_node:\n\t\ttry:\n\t\t\treturn content_bytes[name_node.start_byte : name_node.end_byte].decode(\"utf-8\", errors=\"ignore\")\n\t\texcept (UnicodeDecodeError, IndexError, AttributeError) as e:\n\t\t\tlogger.warning(\"Failed to decode TypeScript name: %s\", e)\n\t\t\treturn f\"&lt;decoding-error-{node.type}&gt;\"\n\n\t# Fall back to JavaScript name extraction\n\treturn super().extract_name(node, content_bytes)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_body_node","title":"get_body_node","text":"<pre><code>get_body_node(node: Node) -&gt; Node | None\n</code></pre> <p>Get the node representing the 'body' of a definition.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The body node if available, None otherwise</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_body_node(self, node: Node) -&gt; Node | None:\n\t\"\"\"\n\tGet the node representing the 'body' of a definition.\n\n\tArgs:\n\t    node: The tree-sitter node\n\n\tReturns:\n\t    The body node if available, None otherwise\n\n\t\"\"\"\n\tif node.type in (\"interface_declaration\", \"function_signature\", \"method_signature\"):\n\t\treturn None  # Interfaces and signatures have no body block\n\treturn super().get_body_node(node)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_children_to_process","title":"get_children_to_process","text":"<pre><code>get_children_to_process(\n\tnode: Node, body_node: Node | None\n) -&gt; list[Node]\n</code></pre> <p>Get the list of child nodes that should be recursively processed.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node</p> required <code>body_node</code> <code>Node | None</code> <p>The body node if available</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of child nodes to process</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_children_to_process(self, node: Node, body_node: Node | None) -&gt; list[Node]:\n\t\"\"\"\n\tGet the list of child nodes that should be recursively processed.\n\n\tArgs:\n\t    node: The tree-sitter node\n\t    body_node: The body node if available\n\n\tReturns:\n\t    List of child nodes to process\n\n\t\"\"\"\n\t# TypeScript-specific handling\n\tif node.type == \"type_alias_declaration\":\n\t\t# Type aliases don't have children to process\n\t\treturn []\n\n\t# Fall back to JavaScript children processing\n\treturn super().get_children_to_process(node, body_node)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.extract_imports","title":"extract_imports","text":"<pre><code>extract_imports(\n\tnode: Node, content_bytes: bytes\n) -&gt; list[str]\n</code></pre> <p>Extract imported module names from a TypeScript import statement.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The tree-sitter node representing an import statement</p> required <code>content_bytes</code> <code>bytes</code> <p>Source code content as bytes</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of imported module names as strings</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def extract_imports(self, node: Node, content_bytes: bytes) -&gt; list[str]:\n\t\"\"\"\n\tExtract imported module names from a TypeScript import statement.\n\n\tArgs:\n\t    node: The tree-sitter node representing an import statement\n\t    content_bytes: Source code content as bytes\n\n\tReturns:\n\t    List of imported module names as strings\n\n\t\"\"\"\n\t# TypeScript import statements are the same as JavaScript\n\treturn super().extract_imports(node, content_bytes)\n</code></pre>"},{"location":"api/processor/tree_sitter/languages/typescript/#codemap.processor.tree_sitter.languages.typescript.TypeScriptSyntaxHandler.get_enclosing_node_of_type","title":"get_enclosing_node_of_type","text":"<pre><code>get_enclosing_node_of_type(\n\tnode: Node, target_type: EntityType\n) -&gt; Node | None\n</code></pre> <p>Find the first ancestor node matching the target TypeScript entity type.</p> <p>Handles INTERFACE specifically and falls back to the JavaScript handler for other types (CLASS, FUNCTION, METHOD, MODULE).</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The starting node.</p> required <code>target_type</code> <code>EntityType</code> <p>The EntityType to search for in ancestors.</p> required <p>Returns:</p> Type Description <code>Node | None</code> <p>The ancestor node if found, otherwise None.</p> Source code in <code>src/codemap/processor/tree_sitter/languages/typescript.py</code> <pre><code>def get_enclosing_node_of_type(self, node: Node, target_type: EntityType) -&gt; Node | None:\n\t\"\"\"\n\tFind the first ancestor node matching the target TypeScript entity type.\n\n\tHandles INTERFACE specifically and falls back to the JavaScript handler\n\tfor other types (CLASS, FUNCTION, METHOD, MODULE).\n\n\tArgs:\n\t    node: The starting node.\n\t    target_type: The EntityType to search for in ancestors.\n\n\tReturns:\n\t    The ancestor node if found, otherwise None.\n\n\t\"\"\"\n\tif target_type == EntityType.INTERFACE:\n\t\ttarget_node_types = [\"interface_declaration\"]\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type in target_node_types:\n\t\t\t\treturn current\n\t\t\tcurrent = current.parent\n\t\treturn None\n\t# Fall back to JS handler for other types (CLASS, FUNCTION, METHOD, MODULE)\n\treturn super().get_enclosing_node_of_type(node, target_type)\n</code></pre>"},{"location":"api/processor/utils/","title":"Utils Overview","text":"<p>Processor Utilities Package.</p> <ul> <li>Embedding Utils - Utilities for generating text embeddings.</li> <li>Git Utils - Utilities for interacting with Git.</li> <li>Sync Utils - Utilities for synchronization logic.</li> </ul>"},{"location":"api/processor/utils/embedding_utils/","title":"Embedding Utils","text":"<p>Utilities for generating text embeddings.</p>"},{"location":"api/processor/utils/embedding_utils/#codemap.processor.utils.embedding_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/utils/embedding_utils/#codemap.processor.utils.embedding_utils.get_retry_settings","title":"get_retry_settings","text":"<pre><code>get_retry_settings(\n\tconfig_loader: ConfigLoader,\n) -&gt; tuple[int, int]\n</code></pre> <p>Get retry settings from config.</p> Source code in <code>src/codemap/processor/utils/embedding_utils.py</code> <pre><code>def get_retry_settings(config_loader: ConfigLoader) -&gt; tuple[int, int]:\n\t\"\"\"Get retry settings from config.\"\"\"\n\tembedding_config = config_loader.get(\"embedding\", {})\n\t# Use max_retries directly for voyageai.Client\n\tmax_retries = embedding_config.get(\"max_retries\", 3)\n\t# retry_delay is handled internally by voyageai client's exponential backoff\n\t# We can still keep the config value if needed elsewhere, but timeout is more relevant here.\n\t# Increased default timeout\n\ttimeout = embedding_config.get(\"timeout\", 180)  # Default timeout for requests (increased from 60)\n\treturn max_retries, timeout\n</code></pre>"},{"location":"api/processor/utils/embedding_utils/#codemap.processor.utils.embedding_utils.generate_embeddings_batch","title":"generate_embeddings_batch  <code>async</code>","text":"<pre><code>generate_embeddings_batch(\n\ttexts: list[str],\n\tmodel: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; list[list[float]] | None\n</code></pre> <p>Generates embeddings for a batch of texts using the Voyage AI async client.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>A list of text strings to embed.</p> required <code>model</code> <code>str</code> <p>The embedding model to use (defaults to config value).</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[float]] | None</code> <p>Optional[List[List[float]]]: A list of embedding vectors,                          or None if embedding fails after retries.</p> Source code in <code>src/codemap/processor/utils/embedding_utils.py</code> <pre><code>async def generate_embeddings_batch(\n\ttexts: list[str], model: str | None = None, config_loader: ConfigLoader | None = None\n) -&gt; list[list[float]] | None:\n\t\"\"\"\n\tGenerates embeddings for a batch of texts using the Voyage AI async client.\n\n\tArgs:\n\t    texts (List[str]): A list of text strings to embed.\n\t    model (str): The embedding model to use (defaults to config value).\n\t    config_loader: Configuration loader instance.\n\n\tReturns:\n\t    Optional[List[List[float]]]: A list of embedding vectors,\n\t                                 or None if embedding fails after retries.\n\n\t\"\"\"\n\tif not texts:\n\t\tlogger.warning(\"generate_embeddings_batch called with empty list.\")\n\t\treturn []\n\n\t# Create ConfigLoader if not provided\n\tif config_loader is None:\n\t\tconfig_loader = ConfigLoader()\n\n\tembedding_config = config_loader.get(\"embedding\", {})\n\n\t# Use model from parameter or fallback to config\n\tembedding_model = model or embedding_config.get(\"model_name\", \"voyage-code-3\")\n\n\t# Get retry and timeout settings from config\n\tmax_retries, timeout = get_retry_settings(config_loader)\n\n\t# Ensure VOYAGE_API_KEY is available (voyageai client checks this, but explicit check is good)\n\tif \"voyage\" in embedding_model and \"VOYAGE_API_KEY\" not in os.environ:\n\t\tlogger.error(\"VOYAGE_API_KEY environment variable not set, but required for model '%s'\", embedding_model)\n\t\treturn None\n\n\ttry:\n\t\tlogger.info(f\"Generating embeddings for {len(texts)} texts using model: {embedding_model} via voyageai client\")\n\n\t\t# Instantiate the async client with retry and timeout settings\n\t\t# API key is automatically picked up from VOYAGE_API_KEY env var by default\n\t\t# Explicitly reference voyageai.AsyncClient\n\t\t# type: ignore because linter incorrectly flags AsyncClient as not exported\n\t\tvo = voyageai.AsyncClient(max_retries=max_retries, timeout=timeout)  # type: ignore[arg-type]\n\n\t\t# Call the voyageai embed method\n\t\t# Use keyword argument 'texts=' for clarity and future compatibility\n\t\tresult = await vo.embed(texts=texts, model=embedding_model)\n\n\t\t# Check response structure (based on typical patterns, might need adjustment)\n\t\t# Assuming result has an 'embeddings' attribute which is a list of lists of floats\n\t\tif result and hasattr(result, \"embeddings\") and isinstance(result.embeddings, list):\n\t\t\tembeddings = result.embeddings\n\t\t\tif len(embeddings) == len(texts):\n\t\t\t\t# Check if embeddings are valid lists of floats\n\t\t\t\tif all(isinstance(emb, list) and all(isinstance(x, float) for x in emb) for emb in embeddings):\n\t\t\t\t\tlogger.debug(f\"Successfully generated {len(embeddings)} embeddings.\")\n\t\t\t\t\t# Use cast to assure the type checker\n\t\t\t\t\treturn cast(\"list[list[float]]\", embeddings)\n\t\t\t\tlogger.error(\"Generated embeddings list contains non-float or non-list items.\")\n\t\t\t\treturn None\n\t\t\tlogger.error(\n\t\t\t\t\"Mismatch between input texts (%d) and generated embeddings (%d).\", len(texts), len(embeddings)\n\t\t\t)\n\t\t\treturn None  # Indicate partial failure\n\t\tlogger.error(\"Unexpected response structure from voyageai.embed: %s\", result)\n\t\treturn None  # Indicate unexpected response\n\n\texcept Exception:\n\t\t# Catch specific Voyage AI errors (includes API errors, rate limits, etc.)\n\t\t# Catch any unexpected errors during the API call\n\t\t# Use logger.exception without redundant variable (per TRY401)\n\t\tlogger.exception(\"Error during voyageai embedding generation\")\n\t\treturn None\n</code></pre>"},{"location":"api/processor/utils/embedding_utils/#codemap.processor.utils.embedding_utils.generate_embedding","title":"generate_embedding  <code>async</code>","text":"<pre><code>generate_embedding(\n\ttext: str,\n\tmodel: str | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; list[float] | None\n</code></pre> <p>Generates an embedding for a single text using the Voyage AI client.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed.</p> required <code>model</code> <code>str</code> <p>The embedding model to use.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[float] | None</code> <p>Optional[List[float]]: The embedding vector, or None if embedding fails.</p> Source code in <code>src/codemap/processor/utils/embedding_utils.py</code> <pre><code>async def generate_embedding(\n\ttext: str, model: str | None = None, config_loader: ConfigLoader | None = None\n) -&gt; list[float] | None:\n\t\"\"\"\n\tGenerates an embedding for a single text using the Voyage AI client.\n\n\tArgs:\n\t    text (str): The text to embed.\n\t    model (str): The embedding model to use.\n\t    config_loader: Configuration loader instance.\n\n\tReturns:\n\t    Optional[List[float]]: The embedding vector, or None if embedding fails.\n\n\t\"\"\"\n\tif not text:\n\t\tlogger.warning(\"generate_embedding called with empty string.\")\n\t\treturn None\n\n\t# Await the async batch function (now using voyageai client)\n\tembeddings_batch = await generate_embeddings_batch(texts=[text], model=model, config_loader=config_loader)\n\n\tif embeddings_batch and len(embeddings_batch) == 1:\n\t\treturn embeddings_batch[0]\n\n\t# Error logging is now handled within generate_embeddings_batch\n\tlogger.error(\"Failed to generate embedding for single text using voyageai client.\")\n\treturn None\n</code></pre>"},{"location":"api/processor/utils/git_utils/","title":"Git Utils","text":"<p>Utilities for interacting with Git.</p>"},{"location":"api/processor/utils/git_utils/#codemap.processor.utils.git_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/utils/git_utils/#codemap.processor.utils.git_utils.MIN_GIT_LS_FILES_PARTS","title":"MIN_GIT_LS_FILES_PARTS  <code>module-attribute</code>","text":"<pre><code>MIN_GIT_LS_FILES_PARTS = 4\n</code></pre>"},{"location":"api/processor/utils/git_utils/#codemap.processor.utils.git_utils.SHA1_HASH_LENGTH","title":"SHA1_HASH_LENGTH  <code>module-attribute</code>","text":"<pre><code>SHA1_HASH_LENGTH = 40\n</code></pre>"},{"location":"api/processor/utils/git_utils/#codemap.processor.utils.git_utils.get_git_tracked_files","title":"get_git_tracked_files","text":"<pre><code>get_git_tracked_files(\n\trepo_path: Path,\n) -&gt; dict[str, str] | None\n</code></pre> <p>Get all tracked files in the Git repository with their blob hashes.</p> <p>Uses 'git ls-files -s' which shows staged files with mode, hash, stage, path. Respects .gitignore rules by using --exclude-standard flag.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>The path to the root of the Git repository.</p> required <p>Returns:</p> Type Description <code>dict[str, str] | None</code> <p>dict[str, str] | None: A dictionary mapping file paths (relative to repo_path)                   to their Git blob hashes. Returns None on failure.</p> Source code in <code>src/codemap/processor/utils/git_utils.py</code> <pre><code>def get_git_tracked_files(repo_path: Path) -&gt; dict[str, str] | None:\n\t\"\"\"\n\tGet all tracked files in the Git repository with their blob hashes.\n\n\tUses 'git ls-files -s' which shows staged files with mode, hash, stage, path.\n\tRespects .gitignore rules by using --exclude-standard flag.\n\n\tArgs:\n\t    repo_path (Path): The path to the root of the Git repository.\n\n\tReturns:\n\t    dict[str, str] | None: A dictionary mapping file paths (relative to repo_path)\n\t                          to their Git blob hashes. Returns None on failure.\n\n\t\"\"\"\n\tcommand = [\"git\", \"ls-files\", \"-s\", \"--full-name\", \"--exclude-standard\"]\n\tstdout, _, returncode = _run_git_command(command, repo_path)\n\n\tif returncode != 0:\n\t\tlogger.error(f\"'git ls-files -s' failed in {repo_path}\")\n\t\treturn None\n\n\ttracked_files: dict[str, str] = {}\n\tlines = stdout.splitlines()\n\tfor line in lines:\n\t\tif not line:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tparts = line.split()\n\t\t\tif len(parts) &lt; MIN_GIT_LS_FILES_PARTS:\n\t\t\t\tlogger.warning(f\"Skipping malformed line in git ls-files output: {line}\")\n\t\t\t\tcontinue\n\n\t\t\t# Extract mode, hash, stage, and path\n\t\t\t_mode, blob_hash, stage_str = parts[:3]\n\t\t\tfile_path = \" \".join(parts[3:])  # Handle spaces in filenames\n\n\t\t\t# Unquote path if necessary (git ls-files quotes paths with special chars)\n\t\t\tif file_path.startswith('\"') and file_path.endswith('\"'):\n\t\t\t\tfile_path = file_path[1:-1].encode(\"latin-1\").decode(\"unicode_escape\")\n\n\t\t\tstage = int(stage_str)\n\n\t\t\t# We are interested in committed files (stage 0)\n\t\t\tif stage == 0 and not _should_exclude_path(file_path):\n\t\t\t\ttracked_files[file_path] = blob_hash\n\t\texcept ValueError:\n\t\t\tlogger.warning(f\"Could not parse line: {line}\")\n\t\texcept IndexError:\n\t\t\tlogger.warning(f\"Index error parsing line: {line}\")\n\n\tlogger.info(f\"Found {len(tracked_files)} tracked files in Git repository: {repo_path}\")\n\treturn tracked_files\n</code></pre>"},{"location":"api/processor/utils/git_utils/#codemap.processor.utils.git_utils.get_file_git_hash","title":"get_file_git_hash","text":"<pre><code>get_file_git_hash(\n\trepo_path: Path, file_path: str\n) -&gt; str | None\n</code></pre> <p>Get the Git hash (blob ID) for a specific tracked file.</p> <p>Uses 'git hash-object' which computes the hash of the file content as it is on the filesystem currently. This matches the behavior needed for comparing against potentially modified files before staging.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>The path to the root of the Git repository.</p> required <code>file_path</code> <code>str</code> <p>The path to the file relative to the repository root.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The Git blob hash of the file, or None if an error occurs         or the file is not found/tracked.</p> Source code in <code>src/codemap/processor/utils/git_utils.py</code> <pre><code>def get_file_git_hash(repo_path: Path, file_path: str) -&gt; str | None:\n\t\"\"\"\n\tGet the Git hash (blob ID) for a specific tracked file.\n\n\tUses 'git hash-object' which computes the hash of the file content as it is\n\ton the filesystem currently. This matches the behavior needed for comparing\n\tagainst potentially modified files before staging.\n\n\tArgs:\n\t    repo_path (Path): The path to the root of the Git repository.\n\t    file_path (str): The path to the file relative to the repository root.\n\n\tReturns:\n\t    str | None: The Git blob hash of the file, or None if an error occurs\n\t                or the file is not found/tracked.\n\n\t\"\"\"\n\tfull_file_path = repo_path / file_path\n\tif not full_file_path.is_file():\n\t\tlogger.warning(f\"Cannot get git hash: File not found or is not a regular file: {full_file_path}\")\n\t\treturn None\n\n\tcommand = [\"git\", \"hash-object\", str(full_file_path)]\n\tstdout, _, returncode = _run_git_command(command, repo_path)\n\n\tif returncode != 0:\n\t\tlogger.error(f\"'git hash-object {file_path}' failed in {repo_path}\")\n\t\treturn None\n\n\t# stdout should contain just the hash\n\tgit_hash = stdout.strip()\n\tif len(git_hash) == SHA1_HASH_LENGTH:  # Use constant\n\t\treturn git_hash\n\tlogger.error(f\"Unexpected output from 'git hash-object {file_path}': {stdout}\")\n\treturn None\n</code></pre>"},{"location":"api/processor/utils/sync_utils/","title":"Sync Utils","text":"<p>Utilities for synchronization logic.</p>"},{"location":"api/processor/utils/sync_utils/#codemap.processor.utils.sync_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/utils/sync_utils/#codemap.processor.utils.sync_utils.compare_states","title":"compare_states","text":"<pre><code>compare_states(\n\tcurrent_files: Mapping[str, str],\n\tdb_files: Mapping[str, str | set[str]],\n) -&gt; tuple[set[str], set[str], set[str]]\n</code></pre> <p>Compare current file state with database state to find differences.</p> <p>Handles cases where the database might store multiple hashes per file path (e.g., if different chunks of the same file have different source hashes, although typically it should be one hash per file path in the DB state dict).</p> <p>Parameters:</p> Name Type Description Default <code>current_files</code> <code>Mapping[str, str]</code> <p>Dictionary mapping file paths to their                              current hash (e.g., from Git).</p> required <code>db_files</code> <code>Mapping[str, str | set[str]]</code> <p>Dictionary mapping file paths to their                                    hash(es) stored in the database.                                    Values can be single hashes (str) or                                    sets of hashes (set[str]).</p> required <p>Returns:</p> Type Description <code>tuple[set[str], set[str], set[str]]</code> <p>tuple[set[str], set[str], set[str]]: A tuple containing: - files_to_add: Set of file paths present in current_files but not db_files. - files_to_update: Set of file paths present in both, but with different hashes. - files_to_delete: Set of file paths present in db_files but not current_files.</p> Source code in <code>src/codemap/processor/utils/sync_utils.py</code> <pre><code>def compare_states(\n\tcurrent_files: Mapping[str, str],\n\tdb_files: Mapping[str, str | set[str]],\n) -&gt; tuple[set[str], set[str], set[str]]:\n\t\"\"\"\n\tCompare current file state with database state to find differences.\n\n\tHandles cases where the database might store multiple hashes per file path\n\t(e.g., if different chunks of the same file have different source hashes,\n\talthough typically it should be one hash per file path in the DB state dict).\n\n\tArgs:\n\t    current_files (Mapping[str, str]): Dictionary mapping file paths to their\n\t                                     current hash (e.g., from Git).\n\t    db_files (Mapping[str, str | set[str]]): Dictionary mapping file paths to their\n\t                                           hash(es) stored in the database.\n\t                                           Values can be single hashes (str) or\n\t                                           sets of hashes (set[str]).\n\n\tReturns:\n\t    tuple[set[str], set[str], set[str]]: A tuple containing:\n\t        - files_to_add: Set of file paths present in current_files but not db_files.\n\t        - files_to_update: Set of file paths present in both, but with different hashes.\n\t        - files_to_delete: Set of file paths present in db_files but not current_files.\n\n\t\"\"\"\n\tcurrent_paths = set(current_files.keys())\n\tdb_paths = set(db_files.keys())\n\n\t# Files in current state but not in DB -&gt; Add\n\tfiles_to_add = current_paths - db_paths\n\n\t# Files in DB but not in current state -&gt; Delete\n\tfiles_to_delete = db_paths - current_paths\n\n\t# Files in both -&gt; Check hash for updates\n\tfiles_to_update: set[str] = set()\n\tcommon_paths = current_paths.intersection(db_paths)\n\n\tfor path in common_paths:\n\t\tcurrent_hash = current_files[path]\n\t\tdb_hash_or_hashes = db_files[path]\n\n\t\tneeds_update = False\n\t\tif isinstance(db_hash_or_hashes, str):\n\t\t\t# DB stores a single hash for the file\n\t\t\tif current_hash != db_hash_or_hashes:\n\t\t\t\tneeds_update = True\n\t\telif isinstance(db_hash_or_hashes, set):\n\t\t\t# DB stores multiple hashes (e.g., different versions/chunks)\n\t\t\t# Update if the current hash is not among the DB hashes\n\t\t\tif current_hash not in db_hash_or_hashes:\n\t\t\t\tneeds_update = True\n\t\t\t# Optional: Consider updating if the set doesn't *exactly* match\n\t\t\t# (e.g., if DB has extra hashes not in current state -&gt; cleanup?)\n\t\t\t# For now, just check if the current hash exists.\n\t\telse:\n\t\t\tlogger.warning(f\"Unexpected hash type in db_files for path '{path}': {type(db_hash_or_hashes)}\")\n\t\t\t# Treat as needing update to be safe\n\t\t\tneeds_update = True\n\n\t\tif needs_update:\n\t\t\tfiles_to_update.add(path)\n\n\tlogger.debug(\n\t\tf\"State comparison results: Add: {len(files_to_add)}, \"\n\t\tf\"Update: {len(files_to_update)}, Delete: {len(files_to_delete)}\"\n\t)\n\treturn files_to_add, files_to_update, files_to_delete\n</code></pre>"},{"location":"api/processor/vector/","title":"Vector Overview","text":"<p>Vector processing package for CodeMap.</p> <ul> <li>Chunking - Module for chunking source code files using LODGenerator.</li> <li>Qdrant Manager - Module for managing Qdrant vector database collections.</li> <li>Synchronizer - Module for synchronizing HNSW index with Git state.</li> </ul>"},{"location":"api/processor/vector/chunking/","title":"Chunking","text":"<p>Module for chunking source code files using LODGenerator.</p>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata","title":"ChunkMetadata","text":"<p>               Bases: <code>TypedDict</code></p> <p>Metadata associated with a code chunk.</p> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>class ChunkMetadata(TypedDict):\n\t\"\"\"Metadata associated with a code chunk.\"\"\"\n\n\tchunk_id: str\n\tfile_path: str\n\tstart_line: int\n\tend_line: int\n\tentity_type: str  # Name of the EntityType enum\n\tentity_name: str | None\n\tlanguage: str | None\n\tgit_hash: str | None\n\thierarchy_path: str  # Full path in the code hierarchy\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.chunk_id","title":"chunk_id  <code>instance-attribute</code>","text":"<pre><code>chunk_id: str\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.file_path","title":"file_path  <code>instance-attribute</code>","text":"<pre><code>file_path: str\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.start_line","title":"start_line  <code>instance-attribute</code>","text":"<pre><code>start_line: int\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.end_line","title":"end_line  <code>instance-attribute</code>","text":"<pre><code>end_line: int\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.entity_type","title":"entity_type  <code>instance-attribute</code>","text":"<pre><code>entity_type: str\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.entity_name","title":"entity_name  <code>instance-attribute</code>","text":"<pre><code>entity_name: str | None\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.language","title":"language  <code>instance-attribute</code>","text":"<pre><code>language: str | None\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.git_hash","title":"git_hash  <code>instance-attribute</code>","text":"<pre><code>git_hash: str | None\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.ChunkMetadata.hierarchy_path","title":"hierarchy_path  <code>instance-attribute</code>","text":"<pre><code>hierarchy_path: str\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.CodeChunk","title":"CodeChunk","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a chunk of code with its metadata.</p> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>class CodeChunk(TypedDict):\n\t\"\"\"Represents a chunk of code with its metadata.\"\"\"\n\n\tcontent: str\n\tmetadata: ChunkMetadata\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.CodeChunk.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.CodeChunk.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: ChunkMetadata\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker","title":"TreeSitterChunker","text":"<p>Chunks code files based on LODEntity structure generated by LODGenerator.</p> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>class TreeSitterChunker:\n\t\"\"\"Chunks code files based on LODEntity structure generated by LODGenerator.\"\"\"\n\n\tdef __init__(self, lod_generator: LODGenerator | None = None, config_loader: ConfigLoader | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the chunker.\n\n\t\tArgs:\n\t\t    lod_generator: An instance of LODGenerator. If None, creates a new one.\n\t\t    config_loader: Configuration loader instance.\n\n\t\t\"\"\"\n\t\tself.lod_generator = lod_generator or LODGenerator()\n\t\tself.config_loader = config_loader or ConfigLoader()\n\n\t\t# Load configuration values\n\t\tembedding_config = self.config_loader.get(\"embedding\", {})\n\t\tchunking_config = embedding_config.get(\"chunking\", {})\n\n\t\t# Set constants from config with fallbacks\n\t\tself.max_hierarchy_depth = chunking_config.get(\"max_hierarchy_depth\", 2)\n\t\tself.max_file_lines = chunking_config.get(\"max_file_lines\", 1000)\n\n\tdef _get_entity_code_content(self, entity: LODEntity, file_lines: list[str]) -&gt; str | None:\n\t\t\"\"\"Extract the raw code content for an entity using its line numbers.\"\"\"\n\t\tif entity.start_line is None or entity.end_line is None:\n\t\t\treturn None\n\n\t\tstart_idx = entity.start_line - 1\n\t\tend_idx = entity.end_line\n\t\tif 0 &lt;= start_idx &lt; end_idx &lt;= len(file_lines):\n\t\t\treturn \"\\n\".join(file_lines[start_idx:end_idx])\n\t\tlogger.warning(\n\t\t\t\"Invalid line numbers for entity %s in %s: start=%s, end=%s, total_lines=%d\",\n\t\t\tentity.name,\n\t\t\tentity.metadata.get(\"file_path\"),\n\t\t\tentity.start_line,\n\t\t\tentity.end_line,\n\t\t\tlen(file_lines),\n\t\t)\n\t\treturn None\n\n\tdef _build_hierarchy_path(self, entity: LODEntity, parent_path: str = \"\") -&gt; str:\n\t\t\"\"\"\n\t\tBuild a hierarchical path string representing the entity's position in the code.\n\n\t\tArgs:\n\t\t        entity: The current entity\n\t\t        parent_path: Path of parent entities\n\n\t\tReturns:\n\t\t        String representation of the hierarchy path\n\n\t\t\"\"\"\n\t\tentity_name = entity.name or f\"&lt;{entity.entity_type.name.lower()}&gt;\"\n\t\tif not parent_path:\n\t\t\treturn entity_name\n\t\treturn f\"{parent_path}.{entity_name}\"\n\n\tdef _extract_nested_entities(self, entity: LODEntity) -&gt; list[dict[str, Any]]:\n\t\t\"\"\"\n\t\tExtract information about nested entities to enhance chunk context.\n\n\t\tArgs:\n\t\t        entity: The current entity\n\n\t\tReturns:\n\t\t        List of dictionaries containing info about nested entities\n\n\t\t\"\"\"\n\t\tnested_info = []\n\n\t\tdef process_nested(nested_entity: LODEntity, depth: int = 1) -&gt; None:\n\t\t\t\"\"\"Process a nested entity and its children recursively to extract information.\n\n\t\t\tArgs:\n\t\t\t\tnested_entity: The nested entity to process\n\t\t\t\tdepth: Current depth in the hierarchy (default: 1)\n\n\t\t\tReturns:\n\t\t\t\tNone: Modifies nested_info in place by appending entity information\n\t\t\t\"\"\"\n\t\t\t# Skip UNKNOWN entities\n\t\t\tif nested_entity.entity_type == EntityType.UNKNOWN:\n\t\t\t\treturn\n\n\t\t\tentity_info = {\n\t\t\t\t\"type\": nested_entity.entity_type.name,\n\t\t\t\t\"name\": nested_entity.name or f\"&lt;{nested_entity.entity_type.name.lower()}&gt;\",\n\t\t\t\t\"signature\": nested_entity.signature or \"\",\n\t\t\t\t\"depth\": depth,\n\t\t\t\t\"line_range\": f\"{nested_entity.start_line}-{nested_entity.end_line}\"\n\t\t\t\tif nested_entity.start_line and nested_entity.end_line\n\t\t\t\telse \"\",\n\t\t\t}\n\t\t\tnested_info.append(entity_info)\n\n\t\t\t# Process children (limited by configured max hierarchy depth)\n\t\t\tif depth &lt; self.max_hierarchy_depth:\n\t\t\t\tfor child in nested_entity.children:\n\t\t\t\t\tprocess_nested(child, depth + 1)\n\n\t\t# Process all direct children\n\t\tfor child in entity.children:\n\t\t\tprocess_nested(child)\n\n\t\treturn nested_info\n\n\tdef _chunk_entity_recursive(\n\t\tself,\n\t\tentity: LODEntity,\n\t\tfile_path: Path,\n\t\tfile_lines: list[str],\n\t\tgit_hash: str | None,\n\t\tlanguage: str,\n\t\tparent_hierarchy: str = \"\",\n\t\tfile_entity: LODEntity | None = None,\n\t) -&gt; Generator[CodeChunk, None, None]:\n\t\t\"\"\"Recursive helper to generate chunks from the LODEntity tree with hierarchy context.\"\"\"\n\t\t# Decide which entity types are significant enough to become their own chunk\n\t\tprimary_chunkable_types = (\n\t\t\tEntityType.MODULE,\n\t\t\tEntityType.CLASS,\n\t\t\tEntityType.INTERFACE,\n\t\t\tEntityType.STRUCT,\n\t\t)\n\n\t\tsecondary_chunkable_types = (\n\t\t\tEntityType.FUNCTION,\n\t\t\tEntityType.METHOD,\n\t\t)\n\n\t\t# Skip UNKNOWN entities entirely\n\t\tif entity.entity_type == EntityType.UNKNOWN:\n\t\t\treturn\n\n\t\t# Build hierarchy path for this entity\n\t\tentity_hierarchy = self._build_hierarchy_path(entity, parent_hierarchy)\n\n\t\t# For primary entities (modules, classes), create full chunks with all their content\n\t\tif (\n\t\t\tentity.entity_type in primary_chunkable_types\n\t\t\tand entity.start_line is not None\n\t\t\tand entity.end_line is not None\n\t\t):\n\t\t\ttry:\n\t\t\t\t# Get full content including all nested entities\n\t\t\t\tcode_content = self._get_entity_code_content(entity, file_lines)\n\t\t\t\tif code_content:\n\t\t\t\t\t# Extract information about nested entities to enhance context\n\t\t\t\t\tnested_entities = self._extract_nested_entities(entity)\n\n\t\t\t\t\t# Construct rich chunk content with nested entity information\n\t\t\t\t\tcontent_parts = []\n\t\t\t\t\tcontent_parts.append(f\"Type: {entity.entity_type.name}\")\n\t\t\t\t\tcontent_parts.append(f\"Path: {entity_hierarchy}\")\n\t\t\t\t\tif entity.name:\n\t\t\t\t\t\tcontent_parts.append(f\"Name: {entity.name}\")\n\t\t\t\t\tif entity.signature:\n\t\t\t\t\t\tcontent_parts.append(f\"Signature: {entity.signature}\")\n\t\t\t\t\tif entity.docstring:\n\t\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{entity.docstring}\")\n\n\t\t\t\t\t# Add structure overview\n\t\t\t\t\tif nested_entities:\n\t\t\t\t\t\tcontent_parts.append(\"Contains:\")\n\t\t\t\t\t\tfor ne in nested_entities:\n\t\t\t\t\t\t\tindent = \"  \" * ne[\"depth\"]\n\t\t\t\t\t\t\tcontent_parts.append(\n\t\t\t\t\t\t\t\tf\"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})\"\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t# Add the full code\n\t\t\t\t\tcontent_parts.append(f\"Code:\\n```{language}\\n{code_content}\\n```\")\n\n\t\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\t\tcontent_parts.append(f\"Raw:\\n{code_content}\")\n\n\t\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t\t# Reverted path logic: use original file_path\n\t\t\t\t\t# Removed relative path calculation\n\t\t\t\t\tchunk_id = f\"{file_path!s}:{entity.start_line}-{entity.end_line}\"\n\t\t\t\t\tmetadata: ChunkMetadata = {\n\t\t\t\t\t\t\"chunk_id\": chunk_id,\n\t\t\t\t\t\t# Store original (likely absolute) path\n\t\t\t\t\t\t\"file_path\": str(file_path),\n\t\t\t\t\t\t\"start_line\": entity.start_line,\n\t\t\t\t\t\t\"end_line\": entity.end_line,\n\t\t\t\t\t\t\"entity_type\": entity.entity_type.name,\n\t\t\t\t\t\t\"entity_name\": entity.name,\n\t\t\t\t\t\t\"language\": language,\n\t\t\t\t\t\t\"git_hash\": git_hash,\n\t\t\t\t\t\t\"hierarchy_path\": entity_hierarchy,\n\t\t\t\t\t}\n\t\t\t\t\tyield {\"content\": chunk_content, \"metadata\": metadata}\n\n\t\t\texcept (ValueError, TypeError, KeyError, AttributeError):\n\t\t\t\tlogger.exception(\"Error processing LOD entity %s in %s\", entity.name, file_path)\n\n\t\t# For secondary entities (functions, methods), create individual chunks\n\t\telif (\n\t\t\tentity.entity_type in secondary_chunkable_types\n\t\t\tand entity.start_line is not None\n\t\t\tand entity.end_line is not None\n\t\t):\n\t\t\ttry:\n\t\t\t\tcode_content = self._get_entity_code_content(entity, file_lines)\n\t\t\t\tif code_content:\n\t\t\t\t\t# Use file entity if available (for better context)\n\t\t\t\t\tfile_context = \"\"\n\t\t\t\t\tif file_entity and file_entity.entity_type == EntityType.MODULE:\n\t\t\t\t\t\tfile_context = f\"File: {file_entity.name or Path(str(file_path)).name}\\n\"\n\n\t\t\t\t\t# Construct rich chunk content\n\t\t\t\t\tcontent_parts = []\n\t\t\t\t\tcontent_parts.append(f\"{file_context}Type: {entity.entity_type.name}\")\n\t\t\t\t\tcontent_parts.append(f\"Path: {entity_hierarchy}\")\n\t\t\t\t\tif entity.name:\n\t\t\t\t\t\tcontent_parts.append(f\"Name: {entity.name}\")\n\t\t\t\t\tif entity.signature:\n\t\t\t\t\t\tcontent_parts.append(f\"Signature: {entity.signature}\")\n\t\t\t\t\tif entity.docstring:\n\t\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{entity.docstring}\")\n\n\t\t\t\t\t# Add code with any dependencies visible in comments\n\t\t\t\t\tcontent_parts.append(f\"Code:\\n```{language}\\n{code_content}\\n```\")\n\n\t\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\t\tcontent_parts.append(f\"Raw:\\n{code_content}\")\n\n\t\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t\t# Reverted path logic: use original file_path\n\t\t\t\t\t# Removed relative path calculation\n\t\t\t\t\tchunk_id = f\"{file_path!s}:{entity.start_line}-{entity.end_line}\"\n\t\t\t\t\tmetadata: ChunkMetadata = {\n\t\t\t\t\t\t\"chunk_id\": chunk_id,\n\t\t\t\t\t\t# Store original (likely absolute) path\n\t\t\t\t\t\t\"file_path\": str(file_path),\n\t\t\t\t\t\t\"start_line\": entity.start_line,\n\t\t\t\t\t\t\"end_line\": entity.end_line,\n\t\t\t\t\t\t\"entity_type\": entity.entity_type.name,\n\t\t\t\t\t\t\"entity_name\": entity.name,\n\t\t\t\t\t\t\"language\": language,\n\t\t\t\t\t\t\"git_hash\": git_hash,\n\t\t\t\t\t\t\"hierarchy_path\": entity_hierarchy,\n\t\t\t\t\t}\n\t\t\t\t\tyield {\"content\": chunk_content, \"metadata\": metadata}\n\n\t\t\texcept (ValueError, TypeError, KeyError, AttributeError):\n\t\t\t\tlogger.exception(\"Error processing LOD entity %s in %s\", entity.name, file_path)\n\n\t\t# Recursively process children, remove repo_path pass\n\t\tfor child in entity.children:\n\t\t\tyield from self._chunk_entity_recursive(\n\t\t\t\tchild,\n\t\t\t\tfile_path,\n\t\t\t\tfile_lines,\n\t\t\t\tgit_hash,\n\t\t\t\tlanguage,\n\t\t\t\tentity_hierarchy,\n\t\t\t\tfile_entity=file_entity,\n\t\t\t)\n\n\tdef chunk_file(\n\t\tself,\n\t\tfile_path: Path,\n\t\tgit_hash: str | None = None,\n\t\tlod_level: LODLevel = LODLevel.FULL,  # Use FULL for max info, not DETAIL\n\t) -&gt; Generator[CodeChunk, None, None]:\n\t\t\"\"\"\n\t\tGenerates code chunks for a given file using LODGenerator.\n\n\t\tArgs:\n\t\t    file_path: The path to the file to chunk.\n\t\t    git_hash: Optional Git hash of the file content.\n\t\t    lod_level: The level of detail to request from LODGenerator.\n\n\t\tYields:\n\t\t    CodeChunk dictionaries, each representing a semantically rich code chunk.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tcontent = read_file_content(file_path)\n\t\t\tif content is None:\n\t\t\t\tlogger.debug(\"Skipping file %s - file does not exist or cannot be read\", file_path)\n\t\t\t\treturn\n\n\t\t\t# Generate the LODEntity tree for the file using the specified level of detail\n\t\t\troot_entity = self.lod_generator.generate_lod(file_path, lod_level)\n\n\t\t\tif not root_entity:\n\t\t\t\tlogger.debug(\"LODGenerator returned no entity for %s, skipping chunking\", file_path)\n\t\t\t\treturn\n\n\t\t\t# Language should be available in the root entity metadata now\n\t\t\tresolved_language = root_entity.metadata.get(\"language\", \"unknown\")\n\t\t\tfile_lines = content.splitlines()\n\n\t\t\t# First, create a chunk for the entire file if it's small enough\n\t\t\tif len(file_lines) &lt; self.max_file_lines:\n\t\t\t\t# Create a chunk for the entire file\n\t\t\t\ttry:\n\t\t\t\t\twhole_file_content = \"\\n\".join(file_lines)\n\n\t\t\t\t\t# Information about the file as a whole\n\t\t\t\t\tcontent_parts = []\n\t\t\t\t\tcontent_parts.append(\"Type: FILE\")\n\t\t\t\t\tfile_name = Path(file_path).name\n\t\t\t\t\tcontent_parts.append(f\"Path: {file_name}\")\n\t\t\t\t\tcontent_parts.append(f\"Name: {file_name}\")\n\n\t\t\t\t\t# Add docstring if the file has one (module docstring)\n\t\t\t\t\tif root_entity.docstring:\n\t\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{root_entity.docstring}\")\n\n\t\t\t\t\t# Get structure overview\n\t\t\t\t\tnested_entities = self._extract_nested_entities(root_entity)\n\t\t\t\t\tif nested_entities:\n\t\t\t\t\t\tcontent_parts.append(\"Contains:\")\n\t\t\t\t\t\tfor ne in nested_entities:\n\t\t\t\t\t\t\tindent = \"  \" * ne[\"depth\"]\n\t\t\t\t\t\t\tcontent_parts.append(\n\t\t\t\t\t\t\t\tf\"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})\"\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t# Add the full code\n\t\t\t\t\tcontent_parts.append(f\"Code:\\n```{resolved_language}\\n{whole_file_content}\\n```\")\n\n\t\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\t\tcontent_parts.append(f\"Raw:\\n{whole_file_content}\")\n\n\t\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\t\tchunk_id = f\"{file_path!s}:1-{len(file_lines)}\"\n\t\t\t\t\tmetadata: ChunkMetadata = {\n\t\t\t\t\t\t\"chunk_id\": chunk_id,\n\t\t\t\t\t\t\"file_path\": str(file_path),\n\t\t\t\t\t\t\"start_line\": 1,\n\t\t\t\t\t\t\"end_line\": len(file_lines),\n\t\t\t\t\t\t\"entity_type\": \"FILE\",\n\t\t\t\t\t\t\"entity_name\": file_name,\n\t\t\t\t\t\t\"language\": resolved_language,\n\t\t\t\t\t\t\"git_hash\": git_hash,\n\t\t\t\t\t\t\"hierarchy_path\": file_name,\n\t\t\t\t\t}\n\t\t\t\t\tyield {\"content\": chunk_content, \"metadata\": metadata}\n\t\t\t\texcept (ValueError, TypeError, KeyError, AttributeError) as e:\n\t\t\t\t\tlogger.warning(\"Error creating whole-file chunk for %s: %s\", file_path, e)\n\n\t\t\t# Then create more specific chunks for the individual entities\n\t\t\tyield from self._chunk_entity_recursive(\n\t\t\t\troot_entity, file_path, file_lines, git_hash, resolved_language, file_entity=root_entity\n\t\t\t)\n\n\t\texcept (OSError, ValueError, TypeError, KeyError, AttributeError) as e:\n\t\t\tlogger.debug(\"Failed to chunk file %s: %s\", file_path, str(e))\n\t\t\treturn\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.__init__","title":"__init__","text":"<pre><code>__init__(\n\tlod_generator: LODGenerator | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the chunker.</p> <p>Parameters:</p> Name Type Description Default <code>lod_generator</code> <code>LODGenerator | None</code> <p>An instance of LODGenerator. If None, creates a new one.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance.</p> <code>None</code> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>def __init__(self, lod_generator: LODGenerator | None = None, config_loader: ConfigLoader | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize the chunker.\n\n\tArgs:\n\t    lod_generator: An instance of LODGenerator. If None, creates a new one.\n\t    config_loader: Configuration loader instance.\n\n\t\"\"\"\n\tself.lod_generator = lod_generator or LODGenerator()\n\tself.config_loader = config_loader or ConfigLoader()\n\n\t# Load configuration values\n\tembedding_config = self.config_loader.get(\"embedding\", {})\n\tchunking_config = embedding_config.get(\"chunking\", {})\n\n\t# Set constants from config with fallbacks\n\tself.max_hierarchy_depth = chunking_config.get(\"max_hierarchy_depth\", 2)\n\tself.max_file_lines = chunking_config.get(\"max_file_lines\", 1000)\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.lod_generator","title":"lod_generator  <code>instance-attribute</code>","text":"<pre><code>lod_generator = lod_generator or LODGenerator()\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader or ConfigLoader()\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.max_hierarchy_depth","title":"max_hierarchy_depth  <code>instance-attribute</code>","text":"<pre><code>max_hierarchy_depth = get('max_hierarchy_depth', 2)\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.max_file_lines","title":"max_file_lines  <code>instance-attribute</code>","text":"<pre><code>max_file_lines = get('max_file_lines', 1000)\n</code></pre>"},{"location":"api/processor/vector/chunking/#codemap.processor.vector.chunking.TreeSitterChunker.chunk_file","title":"chunk_file","text":"<pre><code>chunk_file(\n\tfile_path: Path,\n\tgit_hash: str | None = None,\n\tlod_level: LODLevel = FULL,\n) -&gt; Generator[CodeChunk, None, None]\n</code></pre> <p>Generates code chunks for a given file using LODGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the file to chunk.</p> required <code>git_hash</code> <code>str | None</code> <p>Optional Git hash of the file content.</p> <code>None</code> <code>lod_level</code> <code>LODLevel</code> <p>The level of detail to request from LODGenerator.</p> <code>FULL</code> <p>Yields:</p> Type Description <code>CodeChunk</code> <p>CodeChunk dictionaries, each representing a semantically rich code chunk.</p> Source code in <code>src/codemap/processor/vector/chunking.py</code> <pre><code>def chunk_file(\n\tself,\n\tfile_path: Path,\n\tgit_hash: str | None = None,\n\tlod_level: LODLevel = LODLevel.FULL,  # Use FULL for max info, not DETAIL\n) -&gt; Generator[CodeChunk, None, None]:\n\t\"\"\"\n\tGenerates code chunks for a given file using LODGenerator.\n\n\tArgs:\n\t    file_path: The path to the file to chunk.\n\t    git_hash: Optional Git hash of the file content.\n\t    lod_level: The level of detail to request from LODGenerator.\n\n\tYields:\n\t    CodeChunk dictionaries, each representing a semantically rich code chunk.\n\n\t\"\"\"\n\ttry:\n\t\tcontent = read_file_content(file_path)\n\t\tif content is None:\n\t\t\tlogger.debug(\"Skipping file %s - file does not exist or cannot be read\", file_path)\n\t\t\treturn\n\n\t\t# Generate the LODEntity tree for the file using the specified level of detail\n\t\troot_entity = self.lod_generator.generate_lod(file_path, lod_level)\n\n\t\tif not root_entity:\n\t\t\tlogger.debug(\"LODGenerator returned no entity for %s, skipping chunking\", file_path)\n\t\t\treturn\n\n\t\t# Language should be available in the root entity metadata now\n\t\tresolved_language = root_entity.metadata.get(\"language\", \"unknown\")\n\t\tfile_lines = content.splitlines()\n\n\t\t# First, create a chunk for the entire file if it's small enough\n\t\tif len(file_lines) &lt; self.max_file_lines:\n\t\t\t# Create a chunk for the entire file\n\t\t\ttry:\n\t\t\t\twhole_file_content = \"\\n\".join(file_lines)\n\n\t\t\t\t# Information about the file as a whole\n\t\t\t\tcontent_parts = []\n\t\t\t\tcontent_parts.append(\"Type: FILE\")\n\t\t\t\tfile_name = Path(file_path).name\n\t\t\t\tcontent_parts.append(f\"Path: {file_name}\")\n\t\t\t\tcontent_parts.append(f\"Name: {file_name}\")\n\n\t\t\t\t# Add docstring if the file has one (module docstring)\n\t\t\t\tif root_entity.docstring:\n\t\t\t\t\tcontent_parts.append(f\"Docstring:\\n{root_entity.docstring}\")\n\n\t\t\t\t# Get structure overview\n\t\t\t\tnested_entities = self._extract_nested_entities(root_entity)\n\t\t\t\tif nested_entities:\n\t\t\t\t\tcontent_parts.append(\"Contains:\")\n\t\t\t\t\tfor ne in nested_entities:\n\t\t\t\t\t\tindent = \"  \" * ne[\"depth\"]\n\t\t\t\t\t\tcontent_parts.append(\n\t\t\t\t\t\t\tf\"{indent}- {ne['type']}: {ne['name']} {ne['signature']} (lines {ne['line_range']})\"\n\t\t\t\t\t\t)\n\n\t\t\t\t# Add the full code\n\t\t\t\tcontent_parts.append(f\"Code:\\n```{resolved_language}\\n{whole_file_content}\\n```\")\n\n\t\t\t\t# Add raw unformatted code at the end\n\t\t\t\tcontent_parts.append(f\"Raw:\\n{whole_file_content}\")\n\n\t\t\t\tchunk_content = \"\\n\\n\".join(content_parts)\n\n\t\t\t\tchunk_id = f\"{file_path!s}:1-{len(file_lines)}\"\n\t\t\t\tmetadata: ChunkMetadata = {\n\t\t\t\t\t\"chunk_id\": chunk_id,\n\t\t\t\t\t\"file_path\": str(file_path),\n\t\t\t\t\t\"start_line\": 1,\n\t\t\t\t\t\"end_line\": len(file_lines),\n\t\t\t\t\t\"entity_type\": \"FILE\",\n\t\t\t\t\t\"entity_name\": file_name,\n\t\t\t\t\t\"language\": resolved_language,\n\t\t\t\t\t\"git_hash\": git_hash,\n\t\t\t\t\t\"hierarchy_path\": file_name,\n\t\t\t\t}\n\t\t\t\tyield {\"content\": chunk_content, \"metadata\": metadata}\n\t\t\texcept (ValueError, TypeError, KeyError, AttributeError) as e:\n\t\t\t\tlogger.warning(\"Error creating whole-file chunk for %s: %s\", file_path, e)\n\n\t\t# Then create more specific chunks for the individual entities\n\t\tyield from self._chunk_entity_recursive(\n\t\t\troot_entity, file_path, file_lines, git_hash, resolved_language, file_entity=root_entity\n\t\t)\n\n\texcept (OSError, ValueError, TypeError, KeyError, AttributeError) as e:\n\t\tlogger.debug(\"Failed to chunk file %s: %s\", file_path, str(e))\n\t\treturn\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/","title":"Qdrant Manager","text":"<p>Module for managing Qdrant vector database collections.</p>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.HTTP_NOT_FOUND","title":"HTTP_NOT_FOUND  <code>module-attribute</code>","text":"<pre><code>HTTP_NOT_FOUND = 404\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager","title":"QdrantManager","text":"<p>Manages interactions with a Qdrant vector database collection.</p> <p>Handles initialization, adding/upserting points (vectors + payload), searching, and retrieving points based on IDs or filters. Uses an AsyncQdrantClient for non-blocking operations.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>class QdrantManager:\n\t\"\"\"\n\tManages interactions with a Qdrant vector database collection.\n\n\tHandles initialization, adding/upserting points (vectors + payload),\n\tsearching, and retrieving points based on IDs or filters. Uses an\n\tAsyncQdrantClient for non-blocking operations.\n\n\t\"\"\"\n\n\tvalue_error_msg = \"Filter condition cannot be None if point_ids are not provided\"\n\n\tdef __init__(\n\t\tself,\n\t\tconfig_loader: ConfigLoader,\n\t\tcollection_name: str | None = None,\n\t\tdim: int | None = None,\n\t\tdistance: Distance | None = None,\n\t\tapi_key: str | None = None,  # For Qdrant Cloud\n\t\turl: str | None = None,  # For self-hosted or Cloud URL\n\t\tprefer_grpc: bool | None = None,\n\t\ttimeout: float | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the QdrantManager.\n\n\t\tArgs:\n\t\t    config_loader: Configuration loader instance.\n\t\t    location: Path for local storage or \":memory:\". Ignored if url is provided.\n\t\t    collection_name: Name of the Qdrant collection to use.\n\t\t    dim: Dimension of the vectors.\n\t\t    distance: Distance metric for vector comparison.\n\t\t    api_key: API key for Qdrant Cloud authentication.\n\t\t    url: URL for connecting to a remote Qdrant instance (overrides location).\n\t\t    prefer_grpc: Whether to prefer gRPC over REST.\n\t\t    timeout: Connection timeout in seconds.\n\n\t\t\"\"\"\n\t\t# Get embedding configuration\n\t\tself.config_loader = config_loader\n\t\tembedding_config = self.config_loader.get(\"embedding\", {})\n\n\t\t# Get distance metric from config or parameter\n\t\tdistance_metric_str = embedding_config.get(\"dimension_metric\", \"cosine\").upper()\n\t\tdefault_distance = (\n\t\t\tgetattr(Distance, distance_metric_str) if hasattr(Distance, distance_metric_str) else Distance.COSINE\n\t\t)\n\n\t\t# Load values from parameters or fall back to config\n\t\tself.collection_name = collection_name or embedding_config.get(\"qdrant_collection_name\", \"codemap_vectors\")\n\t\tself.dim = dim or embedding_config.get(\"dimension\", 1024)\n\t\tself.distance = distance or default_distance\n\n\t\t# Build client args\n\t\tself.client_args = {\n\t\t\t\"api_key\": api_key or embedding_config.get(\"api_key\"),\n\t\t\t\"url\": url or embedding_config.get(\"url\"),\n\t\t\t\"prefer_grpc\": prefer_grpc if prefer_grpc is not None else embedding_config.get(\"prefer_grpc\", True),\n\t\t\t\"timeout\": timeout or embedding_config.get(\"timeout\"),\n\t\t}\n\t\t# Remove None values from args\n\t\tself.client_args = {k: v for k, v in self.client_args.items() if v is not None}\n\n\t\t# Initialize client later in an async context\n\t\tself.client: AsyncQdrantClient | None = None\n\t\tself.is_initialized = False\n\n\tasync def initialize(self) -&gt; None:\n\t\t\"\"\"Asynchronously initialize the Qdrant client and ensure the collection exists.\"\"\"\n\t\tif self.is_initialized:\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(\"Initializing Qdrant client with args: %s\", self.client_args)\n\t\t\tself.client = AsyncQdrantClient(**self.client_args)\n\n\t\t\t# Check if collection exists\n\t\t\tcollections_response = await self.client.get_collections()\n\t\t\tcollection_names = {col.name for col in collections_response.collections}\n\n\t\t\tif self.collection_name not in collection_names:\n\t\t\t\tlogger.info(f\"Collection '{self.collection_name}' not found. Creating...\")\n\t\t\t\tawait self.client.create_collection(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tvectors_config=VectorParams(size=self.dim, distance=self.distance),\n\t\t\t\t)\n\t\t\t\tlogger.info(f\"Collection '{self.collection_name}' created successfully.\")\n\n\t\t\t\t# Create payload indexes for commonly filtered fields\n\t\t\t\tlogger.info(f\"Creating payload indexes for collection '{self.collection_name}'\")\n\t\t\t\t# Common fields used in filters from ChunkMetadata\n\t\t\t\tawait self._create_payload_indexes()\n\t\t\telse:\n\t\t\t\tlogger.info(f\"Using existing Qdrant collection: '{self.collection_name}'\")\n\t\t\t\t# Check if indexes exist, create if missing\n\t\t\t\tawait self._ensure_payload_indexes()\n\n\t\t\tself.is_initialized = True\n\t\t\tlogger.info(\"QdrantManager initialized successfully.\")\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to initialize QdrantManager or collection\")\n\t\t\tself.client = None  # Ensure client is None if init fails\n\t\t\traise  # Re-raise the exception to signal failure\n\n\tasync def _create_payload_indexes(self) -&gt; None:\n\t\t\"\"\"Create payload indexes for commonly used filter fields.\"\"\"\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized before creating indexes\"\n\t\t\traise RuntimeError(msg)\n\n\t\ttry:\n\t\t\t# Create keyword indexes for string fields commonly used in filters\n\t\t\tindex_fields = [\n\t\t\t\t(\"file_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"entity_type\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"language\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"git_hash\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"hierarchy_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"start_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t\t(\"end_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t]\n\n\t\t\tfor field_name, field_type in index_fields:\n\t\t\t\tlogger.info(f\"Creating index for field: {field_name} ({field_type})\")\n\t\t\t\tawait self.client.create_payload_index(\n\t\t\t\t\tcollection_name=self.collection_name, field_name=field_name, field_schema=field_type\n\t\t\t\t)\n\n\t\t\tlogger.info(f\"Created {len(index_fields)} payload indexes successfully\")\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tlogger.warning(f\"Error creating payload indexes: {e}\")\n\t\t\t# Continue even if index creation fails - collection will still work\n\n\tasync def _ensure_payload_indexes(self) -&gt; None:\n\t\t\"\"\"Check if payload indexes exist and create any missing ones.\"\"\"\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized before checking indexes\"\n\t\t\traise RuntimeError(msg)\n\n\t\ttry:\n\t\t\t# Get existing collection info\n\t\t\tcollection_info = await self.client.get_collection(collection_name=self.collection_name)\n\t\t\texisting_schema = collection_info.payload_schema\n\n\t\t\t# List of fields that should be indexed\n\t\t\tindex_fields = [\n\t\t\t\t(\"file_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"entity_type\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"language\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"git_hash\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"hierarchy_path\", models.PayloadSchemaType.KEYWORD),\n\t\t\t\t(\"start_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t\t(\"end_line\", models.PayloadSchemaType.INTEGER),\n\t\t\t]\n\n\t\t\t# Create any missing indexes\n\t\t\tfor field_name, field_type in index_fields:\n\t\t\t\tif field_name not in existing_schema:\n\t\t\t\t\tlogger.info(f\"Creating missing index for field: {field_name} ({field_type})\")\n\t\t\t\t\tawait self.client.create_payload_index(\n\t\t\t\t\t\tcollection_name=self.collection_name, field_name=field_name, field_schema=field_type\n\t\t\t\t\t)\n\n\t\texcept Exception as e:  # noqa: BLE001\n\t\t\tlogger.warning(f\"Error checking or creating payload indexes: {e}\")\n\t\t\t# Continue even if index check fails\n\n\tasync def _ensure_initialized(self) -&gt; None:\n\t\t\"\"\"Ensure the client is initialized before performing operations.\"\"\"\n\t\tif not self.is_initialized or self.client is None:\n\t\t\tawait self.initialize()\n\t\tif not self.client:\n\t\t\t# Should not happen if initialize didn't raise, but check anyway\n\t\t\tmsg = \"Qdrant client is not available after initialization attempt.\"\n\t\t\traise RuntimeError(msg)\n\n\tasync def upsert_points(self, points: list[PointStruct]) -&gt; None:\n\t\t\"\"\"\n\t\tAdd or update points (vectors and payloads) in the collection.\n\n\t\tArgs:\n\t\t    points: A list of Qdrant PointStruct objects.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tif not points:\n\t\t\tlogger.warning(\"upsert_points called with an empty list.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Upserting {len(points)} points into '{self.collection_name}'\")\n\t\t\tawait self.client.upsert(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints=points,\n\t\t\t\twait=True,  # Wait for operation to complete\n\t\t\t)\n\t\t\tlogger.debug(f\"Successfully upserted {len(points)} points.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error upserting points into Qdrant\")\n\t\t\t# Decide if partial failure needs specific handling or re-raising\n\n\tasync def delete_points(self, point_ids: list[str | int | uuid.UUID]) -&gt; None:\n\t\t\"\"\"\n\t\tDelete points from the collection by their IDs.\n\n\t\tArgs:\n\t\t    point_ids: A list of point IDs to delete.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tif not point_ids:\n\t\t\tlogger.warning(\"delete_points called with an empty list.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Deleting {len(point_ids)} points from '{self.collection_name}'\")\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\tawait self.client.delete(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t# Ignore linter complaint about list type compatibility\n\t\t\t\tpoints_selector=models.PointIdsList(points=qdrant_ids),  # type: ignore[arg-type]\n\t\t\t\twait=True,\n\t\t\t)\n\t\t\tlogger.debug(f\"Successfully deleted {len(point_ids)} points.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error deleting points from Qdrant\")\n\t\t\t# Consider error handling strategy\n\n\tasync def search(\n\t\tself,\n\t\tquery_vector: list[float],\n\t\tk: int = 5,\n\t\tquery_filter: models.Filter | None = None,\n\t) -&gt; list[models.ScoredPoint]:\n\t\t\"\"\"\n\t\tPerform a vector search with optional filtering.\n\n\t\tArgs:\n\t\t    query_vector: The vector to search for.\n\t\t    k: The number of nearest neighbors to return.\n\t\t    query_filter: Optional Qdrant filter conditions.\n\n\t\tReturns:\n\t\t    A list of ScoredPoint objects, including ID, score, and payload.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tif not query_vector:\n\t\t\tlogger.error(\"Search called with empty query vector.\")\n\t\t\treturn []\n\n\t\ttry:\n\t\t\tsearch_result = await self.client.search(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tquery_vector=query_vector,\n\t\t\t\tquery_filter=query_filter,\n\t\t\t\tlimit=k,\n\t\t\t\twith_payload=True,  # Always include payload\n\t\t\t\twith_vectors=False,  # Usually not needed in results\n\t\t\t)\n\t\t\tlogger.debug(f\"Search returned {len(search_result)} results.\")\n\t\t\treturn search_result\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error during Qdrant search\")\n\t\t\treturn []\n\n\tasync def get_all_point_ids_with_filter(\n\t\tself, query_filter: models.Filter | None = None\n\t) -&gt; list[str | int | uuid.UUID]:\n\t\t\"\"\"\n\t\tRetrieves all point IDs currently in the collection, optionally filtered.\n\n\t\tUses scrolling API to handle potentially large collections.\n\n\t\tArgs:\n\t\t    query_filter: Optional Qdrant filter to apply.\n\n\t\tReturns:\n\t\t    A list of all point IDs matching the filter.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tall_ids: list[str | int | uuid.UUID] = []\n\t\t# Use Any for offset type hint due to persistent linter issues\n\t\tnext_offset: Any | None = None\n\t\tlimit_per_scroll = 1000\n\n\t\t# Add logging for parameters\n\t\tlogger.info(\n\t\t\tf\"[QdrantManager Get IDs] Fetching all point IDs from collection '{self.collection_name}'%s...\",\n\t\t\tf\" with filter: {query_filter}\" if query_filter else \"\",\n\t\t)\n\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scrolling with offset: {next_offset}\")\n\t\t\t\tscroll_response, next_offset_id = await self.client.scroll(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tscroll_filter=query_filter,\n\t\t\t\t\tlimit=limit_per_scroll,\n\t\t\t\t\toffset=next_offset,\n\t\t\t\t\twith_payload=False,\n\t\t\t\t\twith_vectors=False,\n\t\t\t\t)\n\t\t\t\tbatch_ids: list[ExtendedPointId] = [point.id for point in scroll_response]\n\t\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scroll returned {len(batch_ids)} IDs in this batch.\")\n\t\t\t\tif not batch_ids:\n\t\t\t\t\tlogger.debug(\"[QdrantManager Get IDs] No more IDs returned by scroll. Stopping.\")\n\t\t\t\t\tbreak\n\t\t\t\tall_ids.extend([cast(\"str | int | uuid.UUID\", point_id) for point_id in batch_ids])\n\t\t\t\t# Assign the returned offset ID - type is likely PointId but linter struggles\n\t\t\t\tnext_offset = next_offset_id  # No ignore needed if next_offset is Any\n\t\t\t\tif next_offset is None:\n\t\t\t\t\tbreak\n\n\t\t\texcept UnexpectedResponse as e:\n\t\t\t\t# Qdrant might return 404 if offset points to non-existent ID after deletions\n\t\t\t\tif e.status_code == HTTP_NOT_FOUND and \"Point with id\" in str(e.content):\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"Scroll encountered potentially deleted point ID at offset: {next_offset}. Stopping scroll.\"\n\t\t\t\t\t)\n\t\t\t\t\tbreak\n\t\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\t\traise  # Reraise other unexpected errors\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\t\traise\n\n\t\tlogger.info(f\"Retrieved {len(all_ids)} point IDs from collection '{self.collection_name}'.\")\n\t\treturn all_ids\n\n\tasync def get_payloads_by_ids(self, point_ids: list[str | int | uuid.UUID]) -&gt; dict[str, dict[str, Any]]:\n\t\t\"\"\"\n\t\tRetrieves payloads for specific point IDs.\n\n\t\tArgs:\n\t\t    point_ids: List of point IDs to fetch payloads for.\n\n\t\tReturns:\n\t\t    A dictionary mapping point IDs (as strings) to their payloads.\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\t\tif not point_ids:\n\t\t\treturn {}\n\n\t\ttry:\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\t\t\tpoints_data = await self.client.retrieve(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t# Ignore linter complaint about list type compatibility\n\t\t\t\tids=qdrant_ids,  # type: ignore[arg-type]\n\t\t\t\twith_payload=True,\n\t\t\t\twith_vectors=False,\n\t\t\t)\n\t\t\tpayloads = {str(point.id): point.payload for point in points_data if point.payload}\n\t\t\tlogger.debug(f\"Retrieved payloads for {len(payloads)} points.\")\n\t\t\treturn payloads\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error retrieving payloads from Qdrant\")\n\t\t\treturn {}\n\n\tasync def close(self) -&gt; None:\n\t\t\"\"\"Close the AsyncQdrantClient connection.\"\"\"\n\t\tif self.client:\n\t\t\tlogger.info(\"Closing Qdrant client connection.\")\n\t\t\ttry:\n\t\t\t\tawait self.client.close()\n\t\t\texcept Exception:\n\t\t\t\tlogger.exception(\"Error closing Qdrant client\")\n\t\t\tfinally:\n\t\t\t\tself.client = None\n\t\t\t\tself.is_initialized = False\n\n\tasync def __aenter__(self) -&gt; \"QdrantManager\":\n\t\t\"\"\"Enter the async context manager, initializing the Qdrant client.\"\"\"\n\t\tawait self.initialize()\n\t\treturn self\n\n\tasync def __aexit__(\n\t\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n\t) -&gt; None:\n\t\t\"\"\"Exit the async context manager, closing the Qdrant client.\"\"\"\n\t\tawait self.close()\n\n\tasync def set_payload(\n\t\tself,\n\t\tfilter_condition: models.Filter,\n\t\tpayload: dict[str, Any],\n\t\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\t\tkey: str | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tSet specific payload fields for points without overwriting existing fields.\n\n\t\tArgs:\n\t\t    payload: Dictionary of payload fields to set\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\t\t    key: Optional specific payload key path to modify\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not payload:\n\t\t\tlogger.warning(\"set_payload called with empty payload.\")\n\t\t\treturn\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"set_payload called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Setting payload fields: {list(payload.keys())} for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\t\tawait self.client.set_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\tkey=key,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here because of the initial check\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.set_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\tkey=key,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(f\"Successfully set payload fields: {list(payload.keys())}\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error setting payload in Qdrant\")\n\n\tasync def overwrite_payload(\n\t\tself,\n\t\tpayload: dict[str, Any],\n\t\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\t\tfilter_condition: models.Filter | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tCompletely replace the payload for points with the new payload.\n\n\t\tArgs:\n\t\t    payload: Dictionary of payload fields to set\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not payload:\n\t\t\tlogger.warning(\"overwrite_payload called with empty payload.\")\n\t\t\treturn\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"overwrite_payload called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Overwriting payload for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\t\tawait self.client.overwrite_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here\n\t\t\t\tif filter_condition is None:\n\t\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.overwrite_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpayload=payload,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(\"Successfully overwrote payload\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error overwriting payload in Qdrant\")\n\n\tasync def clear_payload(\n\t\tself, point_ids: list[str | int | uuid.UUID] | None = None, filter_condition: models.Filter | None = None\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tRemove all payload fields from points.\n\n\t\tArgs:\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"clear_payload called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Clearing payload for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)  # type: ignore[arg-type]\n\n\t\t\t\tawait self.client.clear_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpoints_selector=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here\n\t\t\t\tif filter_condition is None:\n\t\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.clear_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tpoints_selector=points_selector,  # type: ignore[arg-type]\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(\"Successfully cleared payload\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error clearing payload in Qdrant\")\n\n\tasync def delete_payload_keys(\n\t\tself,\n\t\tkeys: list[str],\n\t\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\t\tfilter_condition: models.Filter | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tDelete specific payload fields from points.\n\n\t\tArgs:\n\t\t    keys: List of payload field keys to delete\n\t\t    point_ids: Optional list of point IDs to update\n\t\t    filter_condition: Optional filter to select points\n\n\t\t\"\"\"\n\t\tawait self._ensure_initialized()\n\t\tif self.client is None:\n\t\t\tmsg = \"Client should be initialized here\"\n\t\t\traise RuntimeError(msg)\n\n\t\tif not keys:\n\t\t\tlogger.warning(\"delete_payload_keys called with empty keys list.\")\n\t\t\treturn\n\n\t\tif not point_ids and not filter_condition:\n\t\t\tlogger.warning(\"delete_payload_keys called without point_ids or filter.\")\n\t\t\treturn\n\n\t\ttry:\n\t\t\tlogger.info(f\"Deleting payload keys {keys} for points in '{self.collection_name}'\")\n\n\t\t\tif point_ids:\n\t\t\t\t# Convert to list of Qdrant point IDs\n\t\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t\t# Create points selector with IDs\n\t\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\t\tawait self.client.delete_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tkeys=keys,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# We know filter_condition is not None here\n\t\t\t\tif filter_condition is None:\n\t\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\t\tawait self.client.delete_payload(\n\t\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\t\tkeys=keys,\n\t\t\t\t\tpoints=points_selector,\n\t\t\t\t\twait=True,\n\t\t\t\t)\n\n\t\t\tlogger.debug(f\"Successfully deleted payload keys: {keys}\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error deleting payload keys in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.value_error_msg","title":"value_error_msg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value_error_msg = \"Filter condition cannot be None if point_ids are not provided\"\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig_loader: ConfigLoader,\n\tcollection_name: str | None = None,\n\tdim: int | None = None,\n\tdistance: Distance | None = None,\n\tapi_key: str | None = None,\n\turl: str | None = None,\n\tprefer_grpc: bool | None = None,\n\ttimeout: float | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the QdrantManager.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>Configuration loader instance.</p> required <code>location</code> <p>Path for local storage or \":memory:\". Ignored if url is provided.</p> required <code>collection_name</code> <code>str | None</code> <p>Name of the Qdrant collection to use.</p> <code>None</code> <code>dim</code> <code>int | None</code> <p>Dimension of the vectors.</p> <code>None</code> <code>distance</code> <code>Distance | None</code> <p>Distance metric for vector comparison.</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for Qdrant Cloud authentication.</p> <code>None</code> <code>url</code> <code>str | None</code> <p>URL for connecting to a remote Qdrant instance (overrides location).</p> <code>None</code> <code>prefer_grpc</code> <code>bool | None</code> <p>Whether to prefer gRPC over REST.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>Connection timeout in seconds.</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>def __init__(\n\tself,\n\tconfig_loader: ConfigLoader,\n\tcollection_name: str | None = None,\n\tdim: int | None = None,\n\tdistance: Distance | None = None,\n\tapi_key: str | None = None,  # For Qdrant Cloud\n\turl: str | None = None,  # For self-hosted or Cloud URL\n\tprefer_grpc: bool | None = None,\n\ttimeout: float | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the QdrantManager.\n\n\tArgs:\n\t    config_loader: Configuration loader instance.\n\t    location: Path for local storage or \":memory:\". Ignored if url is provided.\n\t    collection_name: Name of the Qdrant collection to use.\n\t    dim: Dimension of the vectors.\n\t    distance: Distance metric for vector comparison.\n\t    api_key: API key for Qdrant Cloud authentication.\n\t    url: URL for connecting to a remote Qdrant instance (overrides location).\n\t    prefer_grpc: Whether to prefer gRPC over REST.\n\t    timeout: Connection timeout in seconds.\n\n\t\"\"\"\n\t# Get embedding configuration\n\tself.config_loader = config_loader\n\tembedding_config = self.config_loader.get(\"embedding\", {})\n\n\t# Get distance metric from config or parameter\n\tdistance_metric_str = embedding_config.get(\"dimension_metric\", \"cosine\").upper()\n\tdefault_distance = (\n\t\tgetattr(Distance, distance_metric_str) if hasattr(Distance, distance_metric_str) else Distance.COSINE\n\t)\n\n\t# Load values from parameters or fall back to config\n\tself.collection_name = collection_name or embedding_config.get(\"qdrant_collection_name\", \"codemap_vectors\")\n\tself.dim = dim or embedding_config.get(\"dimension\", 1024)\n\tself.distance = distance or default_distance\n\n\t# Build client args\n\tself.client_args = {\n\t\t\"api_key\": api_key or embedding_config.get(\"api_key\"),\n\t\t\"url\": url or embedding_config.get(\"url\"),\n\t\t\"prefer_grpc\": prefer_grpc if prefer_grpc is not None else embedding_config.get(\"prefer_grpc\", True),\n\t\t\"timeout\": timeout or embedding_config.get(\"timeout\"),\n\t}\n\t# Remove None values from args\n\tself.client_args = {k: v for k, v in self.client_args.items() if v is not None}\n\n\t# Initialize client later in an async context\n\tself.client: AsyncQdrantClient | None = None\n\tself.is_initialized = False\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.collection_name","title":"collection_name  <code>instance-attribute</code>","text":"<pre><code>collection_name = collection_name or get(\n\t\"qdrant_collection_name\", \"codemap_vectors\"\n)\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.dim","title":"dim  <code>instance-attribute</code>","text":"<pre><code>dim = dim or get('dimension', 1024)\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.distance","title":"distance  <code>instance-attribute</code>","text":"<pre><code>distance = distance or default_distance\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.client_args","title":"client_args  <code>instance-attribute</code>","text":"<pre><code>client_args = {k: _ifor (k, v) in items() if v is not None}\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client: AsyncQdrantClient | None = None\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.is_initialized","title":"is_initialized  <code>instance-attribute</code>","text":"<pre><code>is_initialized = False\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize() -&gt; None\n</code></pre> <p>Asynchronously initialize the Qdrant client and ensure the collection exists.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def initialize(self) -&gt; None:\n\t\"\"\"Asynchronously initialize the Qdrant client and ensure the collection exists.\"\"\"\n\tif self.is_initialized:\n\t\treturn\n\n\ttry:\n\t\tlogger.info(\"Initializing Qdrant client with args: %s\", self.client_args)\n\t\tself.client = AsyncQdrantClient(**self.client_args)\n\n\t\t# Check if collection exists\n\t\tcollections_response = await self.client.get_collections()\n\t\tcollection_names = {col.name for col in collections_response.collections}\n\n\t\tif self.collection_name not in collection_names:\n\t\t\tlogger.info(f\"Collection '{self.collection_name}' not found. Creating...\")\n\t\t\tawait self.client.create_collection(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tvectors_config=VectorParams(size=self.dim, distance=self.distance),\n\t\t\t)\n\t\t\tlogger.info(f\"Collection '{self.collection_name}' created successfully.\")\n\n\t\t\t# Create payload indexes for commonly filtered fields\n\t\t\tlogger.info(f\"Creating payload indexes for collection '{self.collection_name}'\")\n\t\t\t# Common fields used in filters from ChunkMetadata\n\t\t\tawait self._create_payload_indexes()\n\t\telse:\n\t\t\tlogger.info(f\"Using existing Qdrant collection: '{self.collection_name}'\")\n\t\t\t# Check if indexes exist, create if missing\n\t\t\tawait self._ensure_payload_indexes()\n\n\t\tself.is_initialized = True\n\t\tlogger.info(\"QdrantManager initialized successfully.\")\n\n\texcept Exception:\n\t\tlogger.exception(\"Failed to initialize QdrantManager or collection\")\n\t\tself.client = None  # Ensure client is None if init fails\n\t\traise  # Re-raise the exception to signal failure\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.upsert_points","title":"upsert_points  <code>async</code>","text":"<pre><code>upsert_points(points: list[PointStruct]) -&gt; None\n</code></pre> <p>Add or update points (vectors and payloads) in the collection.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list[PointStruct]</code> <p>A list of Qdrant PointStruct objects.</p> required Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def upsert_points(self, points: list[PointStruct]) -&gt; None:\n\t\"\"\"\n\tAdd or update points (vectors and payloads) in the collection.\n\n\tArgs:\n\t    points: A list of Qdrant PointStruct objects.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tif not points:\n\t\tlogger.warning(\"upsert_points called with an empty list.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Upserting {len(points)} points into '{self.collection_name}'\")\n\t\tawait self.client.upsert(\n\t\t\tcollection_name=self.collection_name,\n\t\t\tpoints=points,\n\t\t\twait=True,  # Wait for operation to complete\n\t\t)\n\t\tlogger.debug(f\"Successfully upserted {len(points)} points.\")\n\texcept Exception:\n\t\tlogger.exception(\"Error upserting points into Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.delete_points","title":"delete_points  <code>async</code>","text":"<pre><code>delete_points(point_ids: list[str | int | UUID]) -&gt; None\n</code></pre> <p>Delete points from the collection by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>point_ids</code> <code>list[str | int | UUID]</code> <p>A list of point IDs to delete.</p> required Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def delete_points(self, point_ids: list[str | int | uuid.UUID]) -&gt; None:\n\t\"\"\"\n\tDelete points from the collection by their IDs.\n\n\tArgs:\n\t    point_ids: A list of point IDs to delete.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tif not point_ids:\n\t\tlogger.warning(\"delete_points called with an empty list.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Deleting {len(point_ids)} points from '{self.collection_name}'\")\n\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\tawait self.client.delete(\n\t\t\tcollection_name=self.collection_name,\n\t\t\t# Ignore linter complaint about list type compatibility\n\t\t\tpoints_selector=models.PointIdsList(points=qdrant_ids),  # type: ignore[arg-type]\n\t\t\twait=True,\n\t\t)\n\t\tlogger.debug(f\"Successfully deleted {len(point_ids)} points.\")\n\texcept Exception:\n\t\tlogger.exception(\"Error deleting points from Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.search","title":"search  <code>async</code>","text":"<pre><code>search(\n\tquery_vector: list[float],\n\tk: int = 5,\n\tquery_filter: Filter | None = None,\n) -&gt; list[ScoredPoint]\n</code></pre> <p>Perform a vector search with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>query_vector</code> <code>list[float]</code> <p>The vector to search for.</p> required <code>k</code> <code>int</code> <p>The number of nearest neighbors to return.</p> <code>5</code> <code>query_filter</code> <code>Filter | None</code> <p>Optional Qdrant filter conditions.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ScoredPoint]</code> <p>A list of ScoredPoint objects, including ID, score, and payload.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def search(\n\tself,\n\tquery_vector: list[float],\n\tk: int = 5,\n\tquery_filter: models.Filter | None = None,\n) -&gt; list[models.ScoredPoint]:\n\t\"\"\"\n\tPerform a vector search with optional filtering.\n\n\tArgs:\n\t    query_vector: The vector to search for.\n\t    k: The number of nearest neighbors to return.\n\t    query_filter: Optional Qdrant filter conditions.\n\n\tReturns:\n\t    A list of ScoredPoint objects, including ID, score, and payload.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tif not query_vector:\n\t\tlogger.error(\"Search called with empty query vector.\")\n\t\treturn []\n\n\ttry:\n\t\tsearch_result = await self.client.search(\n\t\t\tcollection_name=self.collection_name,\n\t\t\tquery_vector=query_vector,\n\t\t\tquery_filter=query_filter,\n\t\t\tlimit=k,\n\t\t\twith_payload=True,  # Always include payload\n\t\t\twith_vectors=False,  # Usually not needed in results\n\t\t)\n\t\tlogger.debug(f\"Search returned {len(search_result)} results.\")\n\t\treturn search_result\n\texcept Exception:\n\t\tlogger.exception(\"Error during Qdrant search\")\n\t\treturn []\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.get_all_point_ids_with_filter","title":"get_all_point_ids_with_filter  <code>async</code>","text":"<pre><code>get_all_point_ids_with_filter(\n\tquery_filter: Filter | None = None,\n) -&gt; list[str | int | UUID]\n</code></pre> <p>Retrieves all point IDs currently in the collection, optionally filtered.</p> <p>Uses scrolling API to handle potentially large collections.</p> <p>Parameters:</p> Name Type Description Default <code>query_filter</code> <code>Filter | None</code> <p>Optional Qdrant filter to apply.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str | int | UUID]</code> <p>A list of all point IDs matching the filter.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def get_all_point_ids_with_filter(\n\tself, query_filter: models.Filter | None = None\n) -&gt; list[str | int | uuid.UUID]:\n\t\"\"\"\n\tRetrieves all point IDs currently in the collection, optionally filtered.\n\n\tUses scrolling API to handle potentially large collections.\n\n\tArgs:\n\t    query_filter: Optional Qdrant filter to apply.\n\n\tReturns:\n\t    A list of all point IDs matching the filter.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tall_ids: list[str | int | uuid.UUID] = []\n\t# Use Any for offset type hint due to persistent linter issues\n\tnext_offset: Any | None = None\n\tlimit_per_scroll = 1000\n\n\t# Add logging for parameters\n\tlogger.info(\n\t\tf\"[QdrantManager Get IDs] Fetching all point IDs from collection '{self.collection_name}'%s...\",\n\t\tf\" with filter: {query_filter}\" if query_filter else \"\",\n\t)\n\n\twhile True:\n\t\ttry:\n\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scrolling with offset: {next_offset}\")\n\t\t\tscroll_response, next_offset_id = await self.client.scroll(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tscroll_filter=query_filter,\n\t\t\t\tlimit=limit_per_scroll,\n\t\t\t\toffset=next_offset,\n\t\t\t\twith_payload=False,\n\t\t\t\twith_vectors=False,\n\t\t\t)\n\t\t\tbatch_ids: list[ExtendedPointId] = [point.id for point in scroll_response]\n\t\t\tlogger.debug(f\"[QdrantManager Get IDs] Scroll returned {len(batch_ids)} IDs in this batch.\")\n\t\t\tif not batch_ids:\n\t\t\t\tlogger.debug(\"[QdrantManager Get IDs] No more IDs returned by scroll. Stopping.\")\n\t\t\t\tbreak\n\t\t\tall_ids.extend([cast(\"str | int | uuid.UUID\", point_id) for point_id in batch_ids])\n\t\t\t# Assign the returned offset ID - type is likely PointId but linter struggles\n\t\t\tnext_offset = next_offset_id  # No ignore needed if next_offset is Any\n\t\t\tif next_offset is None:\n\t\t\t\tbreak\n\n\t\texcept UnexpectedResponse as e:\n\t\t\t# Qdrant might return 404 if offset points to non-existent ID after deletions\n\t\t\tif e.status_code == HTTP_NOT_FOUND and \"Point with id\" in str(e.content):\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"Scroll encountered potentially deleted point ID at offset: {next_offset}. Stopping scroll.\"\n\t\t\t\t)\n\t\t\t\tbreak\n\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\traise  # Reraise other unexpected errors\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error scrolling through Qdrant points\")\n\t\t\traise\n\n\tlogger.info(f\"Retrieved {len(all_ids)} point IDs from collection '{self.collection_name}'.\")\n\treturn all_ids\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.get_payloads_by_ids","title":"get_payloads_by_ids  <code>async</code>","text":"<pre><code>get_payloads_by_ids(\n\tpoint_ids: list[str | int | UUID],\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Retrieves payloads for specific point IDs.</p> <p>Parameters:</p> Name Type Description Default <code>point_ids</code> <code>list[str | int | UUID]</code> <p>List of point IDs to fetch payloads for.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping point IDs (as strings) to their payloads.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def get_payloads_by_ids(self, point_ids: list[str | int | uuid.UUID]) -&gt; dict[str, dict[str, Any]]:\n\t\"\"\"\n\tRetrieves payloads for specific point IDs.\n\n\tArgs:\n\t    point_ids: List of point IDs to fetch payloads for.\n\n\tReturns:\n\t    A dictionary mapping point IDs (as strings) to their payloads.\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\tif not point_ids:\n\t\treturn {}\n\n\ttry:\n\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\t\tpoints_data = await self.client.retrieve(\n\t\t\tcollection_name=self.collection_name,\n\t\t\t# Ignore linter complaint about list type compatibility\n\t\t\tids=qdrant_ids,  # type: ignore[arg-type]\n\t\t\twith_payload=True,\n\t\t\twith_vectors=False,\n\t\t)\n\t\tpayloads = {str(point.id): point.payload for point in points_data if point.payload}\n\t\tlogger.debug(f\"Retrieved payloads for {len(payloads)} points.\")\n\t\treturn payloads\n\texcept Exception:\n\t\tlogger.exception(\"Error retrieving payloads from Qdrant\")\n\t\treturn {}\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the AsyncQdrantClient connection.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def close(self) -&gt; None:\n\t\"\"\"Close the AsyncQdrantClient connection.\"\"\"\n\tif self.client:\n\t\tlogger.info(\"Closing Qdrant client connection.\")\n\t\ttry:\n\t\t\tawait self.client.close()\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error closing Qdrant client\")\n\t\tfinally:\n\t\t\tself.client = None\n\t\t\tself.is_initialized = False\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; QdrantManager\n</code></pre> <p>Enter the async context manager, initializing the Qdrant client.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def __aenter__(self) -&gt; \"QdrantManager\":\n\t\"\"\"Enter the async context manager, initializing the Qdrant client.\"\"\"\n\tawait self.initialize()\n\treturn self\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n\texc_type: type[BaseException] | None,\n\texc_val: BaseException | None,\n\texc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit the async context manager, closing the Qdrant client.</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def __aexit__(\n\tself, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None\n) -&gt; None:\n\t\"\"\"Exit the async context manager, closing the Qdrant client.\"\"\"\n\tawait self.close()\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.set_payload","title":"set_payload  <code>async</code>","text":"<pre><code>set_payload(\n\tfilter_condition: Filter,\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tkey: str | None = None,\n) -&gt; None\n</code></pre> <p>Set specific payload fields for points without overwriting existing fields.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict[str, Any]</code> <p>Dictionary of payload fields to set</p> required <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter</code> <p>Optional filter to select points</p> required <code>key</code> <code>str | None</code> <p>Optional specific payload key path to modify</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def set_payload(\n\tself,\n\tfilter_condition: models.Filter,\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\tkey: str | None = None,\n) -&gt; None:\n\t\"\"\"\n\tSet specific payload fields for points without overwriting existing fields.\n\n\tArgs:\n\t    payload: Dictionary of payload fields to set\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\t    key: Optional specific payload key path to modify\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not payload:\n\t\tlogger.warning(\"set_payload called with empty payload.\")\n\t\treturn\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"set_payload called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Setting payload fields: {list(payload.keys())} for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\tawait self.client.set_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\tkey=key,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here because of the initial check\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.set_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\tkey=key,\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(f\"Successfully set payload fields: {list(payload.keys())}\")\n\texcept Exception:\n\t\tlogger.exception(\"Error setting payload in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.overwrite_payload","title":"overwrite_payload  <code>async</code>","text":"<pre><code>overwrite_payload(\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tfilter_condition: Filter | None = None,\n) -&gt; None\n</code></pre> <p>Completely replace the payload for points with the new payload.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict[str, Any]</code> <p>Dictionary of payload fields to set</p> required <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter | None</code> <p>Optional filter to select points</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def overwrite_payload(\n\tself,\n\tpayload: dict[str, Any],\n\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\tfilter_condition: models.Filter | None = None,\n) -&gt; None:\n\t\"\"\"\n\tCompletely replace the payload for points with the new payload.\n\n\tArgs:\n\t    payload: Dictionary of payload fields to set\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not payload:\n\t\tlogger.warning(\"overwrite_payload called with empty payload.\")\n\t\treturn\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"overwrite_payload called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Overwriting payload for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\tawait self.client.overwrite_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here\n\t\t\tif filter_condition is None:\n\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.overwrite_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpayload=payload,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(\"Successfully overwrote payload\")\n\texcept Exception:\n\t\tlogger.exception(\"Error overwriting payload in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.clear_payload","title":"clear_payload  <code>async</code>","text":"<pre><code>clear_payload(\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tfilter_condition: Filter | None = None,\n) -&gt; None\n</code></pre> <p>Remove all payload fields from points.</p> <p>Parameters:</p> Name Type Description Default <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter | None</code> <p>Optional filter to select points</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def clear_payload(\n\tself, point_ids: list[str | int | uuid.UUID] | None = None, filter_condition: models.Filter | None = None\n) -&gt; None:\n\t\"\"\"\n\tRemove all payload fields from points.\n\n\tArgs:\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"clear_payload called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Clearing payload for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)  # type: ignore[arg-type]\n\n\t\t\tawait self.client.clear_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints_selector=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here\n\t\t\tif filter_condition is None:\n\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.clear_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tpoints_selector=points_selector,  # type: ignore[arg-type]\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(\"Successfully cleared payload\")\n\texcept Exception:\n\t\tlogger.exception(\"Error clearing payload in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.QdrantManager.delete_payload_keys","title":"delete_payload_keys  <code>async</code>","text":"<pre><code>delete_payload_keys(\n\tkeys: list[str],\n\tpoint_ids: list[str | int | UUID] | None = None,\n\tfilter_condition: Filter | None = None,\n) -&gt; None\n</code></pre> <p>Delete specific payload fields from points.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>List of payload field keys to delete</p> required <code>point_ids</code> <code>list[str | int | UUID] | None</code> <p>Optional list of point IDs to update</p> <code>None</code> <code>filter_condition</code> <code>Filter | None</code> <p>Optional filter to select points</p> <code>None</code> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>async def delete_payload_keys(\n\tself,\n\tkeys: list[str],\n\tpoint_ids: list[str | int | uuid.UUID] | None = None,\n\tfilter_condition: models.Filter | None = None,\n) -&gt; None:\n\t\"\"\"\n\tDelete specific payload fields from points.\n\n\tArgs:\n\t    keys: List of payload field keys to delete\n\t    point_ids: Optional list of point IDs to update\n\t    filter_condition: Optional filter to select points\n\n\t\"\"\"\n\tawait self._ensure_initialized()\n\tif self.client is None:\n\t\tmsg = \"Client should be initialized here\"\n\t\traise RuntimeError(msg)\n\n\tif not keys:\n\t\tlogger.warning(\"delete_payload_keys called with empty keys list.\")\n\t\treturn\n\n\tif not point_ids and not filter_condition:\n\t\tlogger.warning(\"delete_payload_keys called without point_ids or filter.\")\n\t\treturn\n\n\ttry:\n\t\tlogger.info(f\"Deleting payload keys {keys} for points in '{self.collection_name}'\")\n\n\t\tif point_ids:\n\t\t\t# Convert to list of Qdrant point IDs\n\t\t\tqdrant_ids: list[ExtendedPointId] = [cast(\"ExtendedPointId\", pid) for pid in point_ids]\n\n\t\t\t# Create points selector with IDs\n\t\t\tpoints_selector = models.PointIdsList(points=qdrant_ids)\n\n\t\t\tawait self.client.delete_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tkeys=keys,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\t\telse:\n\t\t\t# We know filter_condition is not None here\n\t\t\tif filter_condition is None:\n\t\t\t\traise ValueError(self.value_error_msg)\n\t\t\tpoints_selector = models.FilterSelector(filter=filter_condition)\n\n\t\t\tawait self.client.delete_payload(\n\t\t\t\tcollection_name=self.collection_name,\n\t\t\t\tkeys=keys,\n\t\t\t\tpoints=points_selector,\n\t\t\t\twait=True,\n\t\t\t)\n\n\t\tlogger.debug(f\"Successfully deleted payload keys: {keys}\")\n\texcept Exception:\n\t\tlogger.exception(\"Error deleting payload keys in Qdrant\")\n</code></pre>"},{"location":"api/processor/vector/qdrant_manager/#codemap.processor.vector.qdrant_manager.create_qdrant_point","title":"create_qdrant_point","text":"<pre><code>create_qdrant_point(\n\tchunk_id: str,\n\tvector: list[float],\n\tpayload: dict[str, Any],\n) -&gt; PointStruct\n</code></pre> <p>Helper function to create a Qdrant PointStruct.</p> <p>Ensures the ID is treated appropriately (UUIDs if possible).</p> Source code in <code>src/codemap/processor/vector/qdrant_manager.py</code> <pre><code>def create_qdrant_point(chunk_id: str, vector: list[float], payload: dict[str, Any]) -&gt; PointStruct:\n\t\"\"\"\n\tHelper function to create a Qdrant PointStruct.\n\n\tEnsures the ID is treated appropriately (UUIDs if possible).\n\n\t\"\"\"\n\tpoint_id: ExtendedPointId\n\ttry:\n\t\t# Validate it's a UUID but use string representation for ExtendedPointId\n\t\tuuid_obj = uuid.UUID(chunk_id)\n\t\tpoint_id = str(uuid_obj)\n\texcept ValueError:\n\t\tpoint_id = chunk_id\n\n\t# Keep type ignore for ExtendedPointId union issue\n\treturn PointStruct(id=point_id, vector=vector, payload=payload)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/processor/vector/synchronizer/","title":"Synchronizer","text":"<p>Module for synchronizing HNSW index with Git state.</p>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.DEFAULT_VOYAGE_TOKEN_LIMIT","title":"DEFAULT_VOYAGE_TOKEN_LIMIT  <code>module-attribute</code>","text":"<pre><code>DEFAULT_VOYAGE_TOKEN_LIMIT = 80000\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer","title":"VectorSynchronizer","text":"<p>Handles asynchronous synchronization between Git repository and Qdrant vector index.</p> Source code in <code>src/codemap/processor/vector/synchronizer.py</code> <pre><code>class VectorSynchronizer:\n\t\"\"\"Handles asynchronous synchronization between Git repository and Qdrant vector index.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\trepo_path: Path,\n\t\tqdrant_manager: QdrantManager,\n\t\tchunker: TreeSitterChunker,\n\t\tembedding_model_name: str,\n\t\tanalyzer: TreeSitterAnalyzer | None = None,\n\t\tconfig_loader: ConfigLoader | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the vector synchronizer.\n\n\t\tArgs:\n\t\t    repo_path: Path to the git repository root.\n\t\t    qdrant_manager: Instance of QdrantManager to handle vector storage.\n\t\t    chunker: Instance of chunker used to create code chunks.\n\t\t    embedding_model_name: Name of the embedding model to use.\n\t\t    analyzer: Optional TreeSitterAnalyzer instance.\n\t\t    config_loader: Configuration loader instance.\n\n\t\t\"\"\"\n\t\tself.repo_path = repo_path\n\t\tself.qdrant_manager = qdrant_manager\n\t\tself.chunker = chunker\n\t\tself.embedding_model_name = embedding_model_name\n\t\tself.analyzer = analyzer or TreeSitterAnalyzer()\n\t\tself.config_loader = config_loader or ConfigLoader()\n\n\t\t# Get configuration values\n\t\tembedding_config = self.config_loader.get(\"embedding\", {})\n\t\tself.batch_size = embedding_config.get(\"batch_size\", 32)\n\t\tself.qdrant_batch_size = embedding_config.get(\"qdrant_batch_size\", 100)\n\t\tself.voyage_token_limit = embedding_config.get(\"voyage_token_limit\", DEFAULT_VOYAGE_TOKEN_LIMIT)\n\n\t\t# Instantiate synchronous VoyageAI client for token counting\n\t\ttry:\n\t\t\t# API key is picked up from environment automatically\n\t\t\t# type: ignore for linter error on Client export\n\t\t\tself.voyage_client = voyageai.Client()  # type: ignore[arg-type]\n\t\t\tlogger.info(\"VoyageAI client initialized for token counting.\")\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Failed to initialize VoyageAI client for token counting. Ensure VOYAGE_API_KEY is set.\")\n\t\t\t# Depending on requirements, might want to raise here or handle gracefully\n\t\t\tself.voyage_client = None\n\n\t\tlogger.info(\n\t\t\tf\"VectorSynchronizer initialized for repo: {repo_path} \"\n\t\t\tf\"using Qdrant collection: '{qdrant_manager.collection_name}' \"\n\t\t\tf\"and embedding model: {embedding_model_name}\"\n\t\t)\n\n\tasync def _get_qdrant_state(self) -&gt; dict[str, set[tuple[str, str]]]:\n\t\t\"\"\"\n\t\tRetrieves the current state from Qdrant, mapping file paths to sets of (chunk_id, git_hash).\n\n\t\tReturns:\n\t\t    A dictionary where keys are file paths relative to the repo root,\n\t\t    and values are sets containing tuples of (chunk_id, git_hash)\n\t\t    for each chunk associated with that file path.\n\n\t\t\"\"\"\n\t\tawait self.qdrant_manager.initialize()\n\t\tlogger.info(\"Retrieving current state from Qdrant collection...\")\n\t\tqdrant_state: dict[str, set[tuple[str, str]]] = defaultdict(set)\n\t\tall_ids = await self.qdrant_manager.get_all_point_ids_with_filter()\n\t\tlogger.info(f\"[State Check] Retrieved {len(all_ids)} point IDs from Qdrant.\")\n\n\t\t# Fetch payloads in batches to avoid overloading retrieve\n\t\tpayloads = {}\n\t\tif all_ids:\n\t\t\tfor i in range(0, len(all_ids), self.qdrant_batch_size):\n\t\t\t\tbatch_ids = all_ids[i : i + self.qdrant_batch_size]\n\t\t\t\tbatch_payloads = await self.qdrant_manager.get_payloads_by_ids(batch_ids)\n\t\t\t\tpayloads.update(batch_payloads)\n\t\tlogger.info(f\"[State Check] Retrieved {len(payloads)} payloads from Qdrant.\")\n\n\t\tprocessed_count = 0\n\t\tfor point_id, payload in payloads.items():\n\t\t\tif payload and \"file_path\" in payload and \"git_hash\" in payload:\n\t\t\t\tfile_path = payload[\"file_path\"]\n\t\t\t\tgit_hash = payload[\"git_hash\"]\n\t\t\t\t# Store chunk_id as string, as Qdrant might return UUID objects\n\t\t\t\tqdrant_state[file_path].add((str(point_id), git_hash))\n\t\t\t\tprocessed_count += 1\n\t\t\telse:\n\t\t\t\tlogger.warning(f\"Point ID {point_id} is missing file_path or git_hash in payload.\")\n\n\t\tlogger.info(f\"Retrieved state for {len(qdrant_state)} files ({processed_count} chunks) from Qdrant.\")\n\t\treturn qdrant_state\n\n\tasync def _compare_states(\n\t\tself,\n\t\tcurrent_git_files: dict[str, str],\n\t\tqdrant_state: dict[str, set[tuple[str, str]]],\n\t) -&gt; tuple[set[str], set[str], set[str]]:\n\t\t\"\"\"\n\t\tCompare current Git state with Qdrant state to find differences.\n\n\t\tArgs:\n\t\t    current_git_files: Dictionary mapping file paths to their current Git hash.\n\t\t    qdrant_state: Dictionary mapping file paths to sets of (chunk_id, git_hash).\n\n\t\tReturns:\n\t\t    tuple[set[str], set[str], set[str]]: A tuple containing:\n\t\t        - files_to_process: Files that are new or have changed hash.\n\t\t        - files_to_delete_chunks_for: Files that no longer exist in Git.\n\t\t        - chunks_to_delete: Specific chunk IDs to delete (e.g., from updated files).\n\n\t\t\"\"\"\n\t\t# current_git_files uses RELATIVE paths\n\t\t# qdrant_state uses ABSOLUTE paths as keys (based on stored metadata)\n\t\tlogger.info(\n\t\t\tf\"[Compare Check] Comparing {len(current_git_files)} Git files with \"\n\t\t\tf\"{len(qdrant_state)} files in Qdrant state.\"\n\t\t)\n\n\t\tgit_relative_paths = set(current_git_files.keys())\n\t\t# qdrant_absolute_paths = set(qdrant_state.keys()) # Removed unused variable F841\n\n\t\tfiles_to_process: set[str] = set()  # Store RELATIVE paths\n\t\tchunks_to_delete: set[str] = set()\n\t\tprocessed_relative_paths: set[str] = set()  # Keep track of relative paths found in Qdrant\n\n\t\t# Iterate through Qdrant state (absolute paths)\n\t\tfor abs_path_str, qdrant_chunks_set in qdrant_state.items():\n\t\t\trelative_path_str = abs_path_str  # Default if conversion fails\n\t\t\ttry:\n\t\t\t\tabs_path = Path(abs_path_str)\n\t\t\t\tif abs_path.is_absolute():\n\t\t\t\t\trelative_path = abs_path.relative_to(self.repo_path)\n\t\t\t\t\trelative_path_str = str(relative_path)\n\t\t\t\telse:\n\t\t\t\t\t# If it's somehow already relative, log a warning but use it\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"[Compare Check] Could not make Qdrant path {abs_path_str} \"\n\t\t\t\t\t\tf\"relative to {self.repo_path}. Skipping comparison for this path.\"\n\t\t\t\t\t)\n\t\t\t\t\tcontinue  # Skip to next item in qdrant_state\n\n\t\t\texcept (ValueError, TypeError):\n\t\t\t\tlogger.warning(\n\t\t\t\t\tf\"[Compare Check] Could not make Qdrant path {abs_path_str} \"\n\t\t\t\t\tf\"relative to {self.repo_path}. Skipping comparison for this path.\"\n\t\t\t\t)\n\t\t\t\tcontinue  # Skip to next item in qdrant_state\n\n\t\t\tprocessed_relative_paths.add(relative_path_str)\n\n\t\t\t# Check if this RELATIVE path exists in the current Git state\n\t\t\tif relative_path_str in git_relative_paths:\n\t\t\t\t# File exists in both: Compare Hashes\n\t\t\t\tcurrent_hash = current_git_files[relative_path_str]\n\t\t\t\tdb_hashes = {git_hash for _, git_hash in qdrant_chunks_set}\n\t\t\t\tdb_chunk_ids = {chunk_id for chunk_id, _ in qdrant_chunks_set}\n\n\t\t\t\tif current_hash not in db_hashes:\n\t\t\t\t\t# File content changed\n\t\t\t\t\tlogger.info(\n\t\t\t\t\t\tf\"[Compare Hash] Mismatch for {relative_path_str}. \"\n\t\t\t\t\t\tf\"Current: {current_hash}, DB: {db_hashes}. Marking for reprocessing and deletion.\"\n\t\t\t\t\t)\n\t\t\t\t\tfiles_to_process.add(relative_path_str)\n\t\t\t\t\tchunks_to_delete.update(db_chunk_ids)\n\t\t\t\telif len(db_hashes) &gt; 1:\n\t\t\t\t\t# File hash matches, but stale entries exist\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf\"[Compare Hash] File '{relative_path_str}' has matching hash '{current_hash}' \"\n\t\t\t\t\t\tf\"but also stale entries in Qdrant. Cleaning up.\"\n\t\t\t\t\t)\n\t\t\t\t\tstale_chunk_ids = {chunk_id for chunk_id, git_hash in qdrant_chunks_set if git_hash != current_hash}\n\t\t\t\t\tchunks_to_delete.update(stale_chunk_ids)\n\t\t\t\t# Else: Hash matches and no stale entries -&gt; Do nothing for this file\n\t\t\telse:\n\t\t\t\t# File exists in Git but not in Qdrant state (handled below)\n\t\t\t\tlogger.info(f\"[Compare Check] File deleted from Git: {relative_path_str}. Marking chunks for deletion.\")\n\t\t\t\tchunks_to_delete.update(chunk_id for chunk_id, _ in qdrant_chunks_set)\n\n\t\t# Now find files that are ONLY in Git (new files)\n\t\tnew_git_files = git_relative_paths - processed_relative_paths\n\t\tfiles_to_process.update(new_git_files)\n\t\tlogger.info(f\"[Compare Check] Found {len(new_git_files)} new files in Git to process.\")\n\n\t\t# We don't use files_to_delete_chunks_for currently, assign to _\n\t\tfiles_to_delete_chunks_for = {rp for rp in processed_relative_paths if rp not in git_relative_paths}\n\n\t\tlogger.info(\n\t\t\tf\"[Compare Check] Result: {len(files_to_process)} relative files to process, \"\n\t\t\t# f\"{len(files_to_delete_chunks_for)} files with all chunks to delete, \"\n\t\t\tf\"{len(chunks_to_delete)} specific chunks to delete.\"\n\t\t)\n\t\t# Return relative paths for processing, and chunk IDs to delete\n\t\treturn files_to_process, files_to_delete_chunks_for, chunks_to_delete\n\n\tasync def _process_and_upsert_batch(self, chunk_batch: list[CodeChunk]) -&gt; int:\n\t\t\"\"\"Process a batch of chunks by generating embeddings and upserting to Qdrant.\n\n\t\tArgs:\n\t\t    chunk_batch: List of CodeChunk objects to process. Each chunk contains content\n\t\t        and metadata about a code segment.\n\n\t\tReturns:\n\t\t    int: Number of points successfully upserted to Qdrant. Returns 0 if:\n\t\t        - Input batch is empty\n\t\t        - Embedding generation fails\n\t\t        - No points are generated from the batch\n\n\t\tRaises:\n\t\t    RuntimeError: If there's a mismatch between input chunks and generated embeddings.\n\t\t\"\"\"\n\t\tif not chunk_batch:\n\t\t\treturn 0\n\n\t\tlogger.info(f\"Processing batch of {len(chunk_batch)} chunks for embedding and upsert.\")\n\t\ttexts_to_embed = [chunk[\"content\"] for chunk in chunk_batch]\n\t\tembeddings = await generate_embeddings_batch(\n\t\t\ttexts_to_embed, model=self.embedding_model_name, config_loader=self.config_loader\n\t\t)\n\n\t\tif embeddings is None or len(embeddings) != len(chunk_batch):\n\t\t\tlogger.error(\n\t\t\t\t\"Embed batch failed: \"\n\t\t\t\tf\"got {len(embeddings) if embeddings else 0}, \"\n\t\t\t\tf\"expected {len(chunk_batch)}. Skipping.\"\n\t\t\t)\n\t\t\t# Log details of the failed batch chunks if possible\n\t\t\tfailed_files = {chunk[\"metadata\"].get(\"file_path\", \"unknown\") for chunk in chunk_batch}\n\t\t\tlogger.error(f\"Failed batch involved files: {failed_files}\")\n\t\t\treturn 0\n\n\t\tpoints_to_upsert = []\n\t\tfor chunk, embedding in zip(chunk_batch, embeddings, strict=True):  # Use strict=True\n\t\t\t# Get the original file path from metadata (likely absolute)\n\t\t\toriginal_file_path_str = chunk[\"metadata\"].get(\"file_path\", \"unknown\")\n\n\t\t\t# No longer converting to relative path here. Let the original path be stored.\n\t\t\t# relative_path_str = original_file_path_str ...\n\n\t\t\tchunk_id = str(uuid.uuid4())\n\t\t\tchunk[\"metadata\"][\"chunk_id\"] = chunk_id\n\t\t\t# Ensure the file_path in metadata remains as it came from the chunker\n\t\t\tchunk[\"metadata\"][\"file_path\"] = original_file_path_str\n\t\t\tpayload: dict[str, Any] = cast(\"dict[str, Any]\", chunk[\"metadata\"])\n\t\t\tpoint = create_qdrant_point(chunk_id, embedding, payload)\n\t\t\tpoints_to_upsert.append(point)\n\n\t\tif points_to_upsert:\n\t\t\tawait self.qdrant_manager.upsert_points(points_to_upsert)\n\t\t\tlogger.info(f\"Successfully upserted {len(points_to_upsert)} points from batch.\")\n\t\t\treturn len(points_to_upsert)\n\t\tlogger.warning(\"No points generated from batch to upsert.\")\n\t\treturn 0\n\n\tasync def sync_index(self, progress: Progress | None = None, task_id: TaskID | None = None) -&gt; bool:\n\t\t\"\"\"\n\t\tAsynchronously synchronize the Qdrant index with the current repository state.\n\n\t\tArgs:\n\t\t    progress: Optional rich Progress instance for UI updates.\n\t\t    task_id: Optional rich TaskID for progress tracking.\n\n\t\tReturns:\n\t\t    True if synchronization completed successfully, False otherwise.\n\n\t\t\"\"\"\n\t\tsync_success = False\n\t\tfinal_message = \"[red]Error:[/red] Index sync failed (initialization error).\"\n\t\ttotal_steps = 5  # Git state, Qdrant state, Compare, Delete, Process\n\t\tcurrent_step = 0\n\t\tprocessed_chunk_count = 0\n\n\t\tdef update_progress(description: str, step_increment: int = 1, processed_chunks: int = 0) -&gt; None:\n\t\t\t\"\"\"Updates the progress bar with current synchronization status.\n\n\t\t\tArgs:\n\t\t\t\tdescription: Main description text to display in progress bar.\n\t\t\t\tstep_increment: Number of steps to increment progress by. Defaults to 1.\n\t\t\t\tprocessed_chunks: Number of chunks processed in this update. Defaults to 0.\n\t\t\t\t\tIf greater than 0, will be appended to description in parentheses.\n\n\t\t\tReturns:\n\t\t\t\tNone\n\t\t\t\"\"\"\n\t\t\tnonlocal current_step, processed_chunk_count\n\t\t\tcurrent_step += step_increment\n\t\t\tprocessed_chunk_count += processed_chunks\n\t\t\tif progress and task_id is not None:\n\t\t\t\t# Add processed chunk count to description if relevant\n\t\t\t\tdesc_with_count = description\n\t\t\t\tif processed_chunk_count &gt; 0:\n\t\t\t\t\tdesc_with_count += f\" ({processed_chunk_count} chunks processed)\"\n\t\t\t\tprogress.update(\n\t\t\t\t\ttask_id,\n\t\t\t\t\tdescription=desc_with_count,\n\t\t\t\t\tcompleted=(current_step / total_steps) * 100,\n\t\t\t\t)\n\n\t\ttry:\n\t\t\tawait self.qdrant_manager.initialize()\n\n\t\t\t# Ensure VoyageAI client for token counting is ready\n\t\t\tif not self.voyage_client:\n\t\t\t\tlogger.error(\"VoyageAI client for token counting is not initialized. Cannot proceed.\")\n\t\t\t\tfinal_message = \"[red]Error:[/red] VoyageAI client init failed.\"\n\t\t\t\tif progress and task_id:\n\t\t\t\t\tprogress.update(task_id, description=final_message, completed=100)\n\t\t\t\treturn False\n\n\t\t\t# 1. Get current Git state (tracked files and hashes)\n\t\t\tupdate_progress(\"[1/5] Reading Git tracked files...\", step_increment=0)\n\t\t\tgit_files_raw = get_git_tracked_files(self.repo_path)\n\t\t\tif git_files_raw is None:\n\t\t\t\tlogger.error(\"Failed to retrieve Git tracked files.\")\n\t\t\t\t# Use the initialized final_message\n\t\t\t\tif progress and task_id:\n\t\t\t\t\tprogress.update(task_id, description=final_message, completed=100)\n\t\t\t\treturn False\n\n\t\t\t# Filter out excluded files\n\t\t\tgit_files = {fp: h for fp, h in git_files_raw.items() if not _should_exclude_path(fp)}\n\t\t\tlogger.info(f\"Found {len(git_files)} tracked files in Git (after exclusion).\")\n\t\t\tupdate_progress(f\"[1/5] Found {len(git_files)} tracked files.\")\n\n\t\t\t# 2. Get current Qdrant state (file paths -&gt; chunk IDs and hashes)\n\t\t\tupdate_progress(\"[2/5] Retrieving existing vector state...\")\n\t\t\tqdrant_state = await self._get_qdrant_state()\n\n\t\t\t# 3. Compare states\n\t\t\tupdate_progress(\"[3/5] Comparing Git state with vector state...\")\n\t\t\t# Use _ for the unused 'files_to_delete_chunks_for' variable\n\t\t\tfiles_to_process, _, chunks_to_delete = await self._compare_states(git_files, qdrant_state)\n\n\t\t\t# 4. Delete outdated chunks\n\t\t\tupdate_progress(f\"[4/5] Deleting {len(chunks_to_delete)} outdated vectors...\")\n\t\t\tif chunks_to_delete:\n\t\t\t\t# Delete in batches\n\t\t\t\tdelete_ids_list = list(chunks_to_delete)\n\t\t\t\tfor i in range(0, len(delete_ids_list), self.qdrant_batch_size):\n\t\t\t\t\tbatch_ids = delete_ids_list[i : i + self.qdrant_batch_size]\n\t\t\t\t\t# Use cast to handle type checking\n\t\t\t\t\tawait self.qdrant_manager.delete_points(cast(\"list[str | int | uuid.UUID]\", batch_ids))\n\t\t\t\t\tlogger.info(f\"Deleted batch of {len(batch_ids)} vectors.\")\n\t\t\t\tlogger.info(f\"Finished deleting {len(chunks_to_delete)} vectors.\")\n\t\t\telse:\n\t\t\t\tlogger.info(\"No vectors to delete.\")\n\n\t\t\t# 5. Process new/updated files - **REFACTORED BATCHING LOGIC**\n\t\t\tupdate_progress(f\"[5/5] Processing {len(files_to_process)} new/updated files...\")\n\t\t\tfiles_processed_count = 0\n\n\t\t\t# Batching variables\n\t\t\tcurrent_chunk_batch: list[CodeChunk] = []\n\t\t\tcurrent_batch_token_count = 0\n\n\t\t\tfor file_path in files_to_process:\n\t\t\t\tgit_hash = git_files.get(file_path)\n\t\t\t\tif git_hash:\n\t\t\t\t\tabsolute_path = self.repo_path / file_path\n\t\t\t\t\ttry:\n\t\t\t\t\t\tlogger.debug(f\"Chunking file: {file_path}\")\n\t\t\t\t\t\t# Use iter_chunk_file if available, otherwise list()\n\t\t\t\t\t\tfile_chunks = list(self.chunker.chunk_file(absolute_path, git_hash))\n\n\t\t\t\t\t\tif not file_chunks:\n\t\t\t\t\t\t\tlogger.debug(f\"No chunks generated for file: {file_path}\")\n\t\t\t\t\t\t\tcontinue  # Skip to next file\n\n\t\t\t\t\t\tfor chunk in file_chunks:\n\t\t\t\t\t\t\t# Estimate token count for the chunk content\n\t\t\t\t\t\t\tchunk_text = chunk[\"content\"]\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t# Note: count_tokens expects a list\n\t\t\t\t\t\t\t\tchunk_token_count = self.voyage_client.count_tokens(\n\t\t\t\t\t\t\t\t\t[chunk_text], model=self.embedding_model_name\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\texcept Exception:\n\t\t\t\t\t\t\t\tlogger.exception(f\"Failed to count tokens for chunk in {file_path}. Skipping chunk.\")\n\t\t\t\t\t\t\t\tcontinue  # Skip this chunk\n\n\t\t\t\t\t\t\t# Check if adding this chunk exceeds the limit\n\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\tcurrent_batch_token_count + chunk_token_count &gt; self.voyage_token_limit\n\t\t\t\t\t\t\t\tand current_chunk_batch\n\t\t\t\t\t\t\t):\n\t\t\t\t\t\t\t\t# Process the current batch before adding the new chunk\n\t\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\t\tf\"Token limit ({self.voyage_token_limit}) reached. \"\n\t\t\t\t\t\t\t\t\tf\"Processing batch ({len(current_chunk_batch)} chunks).\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tupserted_count = await self._process_and_upsert_batch(current_chunk_batch)\n\t\t\t\t\t\t\t\tupdate_progress(\n\t\t\t\t\t\t\t\t\tf\"[5/5] Processing {len(files_to_process)} files...\",\n\t\t\t\t\t\t\t\t\tstep_increment=0,\n\t\t\t\t\t\t\t\t\tprocessed_chunks=upserted_count,\n\t\t\t\t\t\t\t\t)\n\n\t\t\t\t\t\t\t\t# Reset batch\n\t\t\t\t\t\t\t\tcurrent_chunk_batch = []\n\t\t\t\t\t\t\t\tcurrent_batch_token_count = 0\n\n\t\t\t\t\t\t\t# Add the current chunk to the (potentially new) batch\n\t\t\t\t\t\t\t# Ensure chunk_token_count itself isn't over the limit (handle large chunks)\n\t\t\t\t\t\t\tif chunk_token_count &gt; self.voyage_token_limit:\n\t\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\t\tf\"Chunk in {file_path} ({chunk['metadata'].get('start_line', '?')}-...) \"\n\t\t\t\t\t\t\t\t\tf\"exceeds token limit ({chunk_token_count} &gt; {self.voyage_token_limit}). Skipping.\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\tcontinue  # Skip this large chunk\n\n\t\t\t\t\t\t\tcurrent_chunk_batch.append(chunk)\n\t\t\t\t\t\t\tcurrent_batch_token_count += chunk_token_count\n\n\t\t\t\t\texcept Exception:\n\t\t\t\t\t\tlogger.exception(f\"Error processing file {file_path} during sync\")\n\t\t\t\t\t\t# Decide if we should continue with other files or stop\n\t\t\t\t\t\tcontinue  # Continue processing other files\n\n\t\t\t\tfiles_processed_count += 1\n\n\t\t\t# Process any remaining chunks in the last batch\n\t\t\tif current_chunk_batch:\n\t\t\t\tlogger.info(\n\t\t\t\t\tf\"Processing final batch of {len(current_chunk_batch)} chunks ({current_batch_token_count} tokens).\"\n\t\t\t\t)\n\t\t\t\tupserted_count = await self._process_and_upsert_batch(current_chunk_batch)\n\t\t\t\tupdate_progress(\"[5/5] Final batch processed...\", step_increment=0, processed_chunks=upserted_count)\n\n\t\t\tsync_success = True\n\t\t\tfinal_message = (\n\t\t\t\tf\"[green]\u2713[/green] Vector index synchronized. {processed_chunk_count} chunks processed/upserted.\"\n\t\t\t)\n\t\t\tlogger.info(\"Vector index synchronization completed successfully.\")\n\n\t\texcept Exception:\n\t\t\tlogger.exception(\"An unexpected error occurred during index synchronization\")\n\t\t\tfinal_message = \"[red]Error:[/red] Index sync failed unexpectedly.\"\n\t\t\tsync_success = False  # Ensure success is False on exception\n\t\tfinally:\n\t\t\t# Final progress update\n\t\t\tif progress and task_id is not None:\n\t\t\t\tprogress.update(task_id, description=final_message, completed=100)\n\n\t\treturn sync_success\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.__init__","title":"__init__","text":"<pre><code>__init__(\n\trepo_path: Path,\n\tqdrant_manager: QdrantManager,\n\tchunker: TreeSitterChunker,\n\tembedding_model_name: str,\n\tanalyzer: TreeSitterAnalyzer | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the vector synchronizer.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to the git repository root.</p> required <code>qdrant_manager</code> <code>QdrantManager</code> <p>Instance of QdrantManager to handle vector storage.</p> required <code>chunker</code> <code>TreeSitterChunker</code> <p>Instance of chunker used to create code chunks.</p> required <code>embedding_model_name</code> <code>str</code> <p>Name of the embedding model to use.</p> required <code>analyzer</code> <code>TreeSitterAnalyzer | None</code> <p>Optional TreeSitterAnalyzer instance.</p> <code>None</code> <code>config_loader</code> <code>ConfigLoader | None</code> <p>Configuration loader instance.</p> <code>None</code> Source code in <code>src/codemap/processor/vector/synchronizer.py</code> <pre><code>def __init__(\n\tself,\n\trepo_path: Path,\n\tqdrant_manager: QdrantManager,\n\tchunker: TreeSitterChunker,\n\tembedding_model_name: str,\n\tanalyzer: TreeSitterAnalyzer | None = None,\n\tconfig_loader: ConfigLoader | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the vector synchronizer.\n\n\tArgs:\n\t    repo_path: Path to the git repository root.\n\t    qdrant_manager: Instance of QdrantManager to handle vector storage.\n\t    chunker: Instance of chunker used to create code chunks.\n\t    embedding_model_name: Name of the embedding model to use.\n\t    analyzer: Optional TreeSitterAnalyzer instance.\n\t    config_loader: Configuration loader instance.\n\n\t\"\"\"\n\tself.repo_path = repo_path\n\tself.qdrant_manager = qdrant_manager\n\tself.chunker = chunker\n\tself.embedding_model_name = embedding_model_name\n\tself.analyzer = analyzer or TreeSitterAnalyzer()\n\tself.config_loader = config_loader or ConfigLoader()\n\n\t# Get configuration values\n\tembedding_config = self.config_loader.get(\"embedding\", {})\n\tself.batch_size = embedding_config.get(\"batch_size\", 32)\n\tself.qdrant_batch_size = embedding_config.get(\"qdrant_batch_size\", 100)\n\tself.voyage_token_limit = embedding_config.get(\"voyage_token_limit\", DEFAULT_VOYAGE_TOKEN_LIMIT)\n\n\t# Instantiate synchronous VoyageAI client for token counting\n\ttry:\n\t\t# API key is picked up from environment automatically\n\t\t# type: ignore for linter error on Client export\n\t\tself.voyage_client = voyageai.Client()  # type: ignore[arg-type]\n\t\tlogger.info(\"VoyageAI client initialized for token counting.\")\n\texcept Exception:\n\t\tlogger.exception(\"Failed to initialize VoyageAI client for token counting. Ensure VOYAGE_API_KEY is set.\")\n\t\t# Depending on requirements, might want to raise here or handle gracefully\n\t\tself.voyage_client = None\n\n\tlogger.info(\n\t\tf\"VectorSynchronizer initialized for repo: {repo_path} \"\n\t\tf\"using Qdrant collection: '{qdrant_manager.collection_name}' \"\n\t\tf\"and embedding model: {embedding_model_name}\"\n\t)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.repo_path","title":"repo_path  <code>instance-attribute</code>","text":"<pre><code>repo_path = repo_path\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.qdrant_manager","title":"qdrant_manager  <code>instance-attribute</code>","text":"<pre><code>qdrant_manager = qdrant_manager\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.chunker","title":"chunker  <code>instance-attribute</code>","text":"<pre><code>chunker = chunker\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.embedding_model_name","title":"embedding_model_name  <code>instance-attribute</code>","text":"<pre><code>embedding_model_name = embedding_model_name\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.analyzer","title":"analyzer  <code>instance-attribute</code>","text":"<pre><code>analyzer = analyzer or TreeSitterAnalyzer()\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.config_loader","title":"config_loader  <code>instance-attribute</code>","text":"<pre><code>config_loader = config_loader or ConfigLoader()\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = get('batch_size', 32)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.qdrant_batch_size","title":"qdrant_batch_size  <code>instance-attribute</code>","text":"<pre><code>qdrant_batch_size = get('qdrant_batch_size', 100)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.voyage_token_limit","title":"voyage_token_limit  <code>instance-attribute</code>","text":"<pre><code>voyage_token_limit = get(\n\t\"voyage_token_limit\", DEFAULT_VOYAGE_TOKEN_LIMIT\n)\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.voyage_client","title":"voyage_client  <code>instance-attribute</code>","text":"<pre><code>voyage_client = Client()\n</code></pre>"},{"location":"api/processor/vector/synchronizer/#codemap.processor.vector.synchronizer.VectorSynchronizer.sync_index","title":"sync_index  <code>async</code>","text":"<pre><code>sync_index(\n\tprogress: Progress | None = None,\n\ttask_id: TaskID | None = None,\n) -&gt; bool\n</code></pre> <p>Asynchronously synchronize the Qdrant index with the current repository state.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>Progress | None</code> <p>Optional rich Progress instance for UI updates.</p> <code>None</code> <code>task_id</code> <code>TaskID | None</code> <p>Optional rich TaskID for progress tracking.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if synchronization completed successfully, False otherwise.</p> Source code in <code>src/codemap/processor/vector/synchronizer.py</code> <pre><code>async def sync_index(self, progress: Progress | None = None, task_id: TaskID | None = None) -&gt; bool:\n\t\"\"\"\n\tAsynchronously synchronize the Qdrant index with the current repository state.\n\n\tArgs:\n\t    progress: Optional rich Progress instance for UI updates.\n\t    task_id: Optional rich TaskID for progress tracking.\n\n\tReturns:\n\t    True if synchronization completed successfully, False otherwise.\n\n\t\"\"\"\n\tsync_success = False\n\tfinal_message = \"[red]Error:[/red] Index sync failed (initialization error).\"\n\ttotal_steps = 5  # Git state, Qdrant state, Compare, Delete, Process\n\tcurrent_step = 0\n\tprocessed_chunk_count = 0\n\n\tdef update_progress(description: str, step_increment: int = 1, processed_chunks: int = 0) -&gt; None:\n\t\t\"\"\"Updates the progress bar with current synchronization status.\n\n\t\tArgs:\n\t\t\tdescription: Main description text to display in progress bar.\n\t\t\tstep_increment: Number of steps to increment progress by. Defaults to 1.\n\t\t\tprocessed_chunks: Number of chunks processed in this update. Defaults to 0.\n\t\t\t\tIf greater than 0, will be appended to description in parentheses.\n\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\t\tnonlocal current_step, processed_chunk_count\n\t\tcurrent_step += step_increment\n\t\tprocessed_chunk_count += processed_chunks\n\t\tif progress and task_id is not None:\n\t\t\t# Add processed chunk count to description if relevant\n\t\t\tdesc_with_count = description\n\t\t\tif processed_chunk_count &gt; 0:\n\t\t\t\tdesc_with_count += f\" ({processed_chunk_count} chunks processed)\"\n\t\t\tprogress.update(\n\t\t\t\ttask_id,\n\t\t\t\tdescription=desc_with_count,\n\t\t\t\tcompleted=(current_step / total_steps) * 100,\n\t\t\t)\n\n\ttry:\n\t\tawait self.qdrant_manager.initialize()\n\n\t\t# Ensure VoyageAI client for token counting is ready\n\t\tif not self.voyage_client:\n\t\t\tlogger.error(\"VoyageAI client for token counting is not initialized. Cannot proceed.\")\n\t\t\tfinal_message = \"[red]Error:[/red] VoyageAI client init failed.\"\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=final_message, completed=100)\n\t\t\treturn False\n\n\t\t# 1. Get current Git state (tracked files and hashes)\n\t\tupdate_progress(\"[1/5] Reading Git tracked files...\", step_increment=0)\n\t\tgit_files_raw = get_git_tracked_files(self.repo_path)\n\t\tif git_files_raw is None:\n\t\t\tlogger.error(\"Failed to retrieve Git tracked files.\")\n\t\t\t# Use the initialized final_message\n\t\t\tif progress and task_id:\n\t\t\t\tprogress.update(task_id, description=final_message, completed=100)\n\t\t\treturn False\n\n\t\t# Filter out excluded files\n\t\tgit_files = {fp: h for fp, h in git_files_raw.items() if not _should_exclude_path(fp)}\n\t\tlogger.info(f\"Found {len(git_files)} tracked files in Git (after exclusion).\")\n\t\tupdate_progress(f\"[1/5] Found {len(git_files)} tracked files.\")\n\n\t\t# 2. Get current Qdrant state (file paths -&gt; chunk IDs and hashes)\n\t\tupdate_progress(\"[2/5] Retrieving existing vector state...\")\n\t\tqdrant_state = await self._get_qdrant_state()\n\n\t\t# 3. Compare states\n\t\tupdate_progress(\"[3/5] Comparing Git state with vector state...\")\n\t\t# Use _ for the unused 'files_to_delete_chunks_for' variable\n\t\tfiles_to_process, _, chunks_to_delete = await self._compare_states(git_files, qdrant_state)\n\n\t\t# 4. Delete outdated chunks\n\t\tupdate_progress(f\"[4/5] Deleting {len(chunks_to_delete)} outdated vectors...\")\n\t\tif chunks_to_delete:\n\t\t\t# Delete in batches\n\t\t\tdelete_ids_list = list(chunks_to_delete)\n\t\t\tfor i in range(0, len(delete_ids_list), self.qdrant_batch_size):\n\t\t\t\tbatch_ids = delete_ids_list[i : i + self.qdrant_batch_size]\n\t\t\t\t# Use cast to handle type checking\n\t\t\t\tawait self.qdrant_manager.delete_points(cast(\"list[str | int | uuid.UUID]\", batch_ids))\n\t\t\t\tlogger.info(f\"Deleted batch of {len(batch_ids)} vectors.\")\n\t\t\tlogger.info(f\"Finished deleting {len(chunks_to_delete)} vectors.\")\n\t\telse:\n\t\t\tlogger.info(\"No vectors to delete.\")\n\n\t\t# 5. Process new/updated files - **REFACTORED BATCHING LOGIC**\n\t\tupdate_progress(f\"[5/5] Processing {len(files_to_process)} new/updated files...\")\n\t\tfiles_processed_count = 0\n\n\t\t# Batching variables\n\t\tcurrent_chunk_batch: list[CodeChunk] = []\n\t\tcurrent_batch_token_count = 0\n\n\t\tfor file_path in files_to_process:\n\t\t\tgit_hash = git_files.get(file_path)\n\t\t\tif git_hash:\n\t\t\t\tabsolute_path = self.repo_path / file_path\n\t\t\t\ttry:\n\t\t\t\t\tlogger.debug(f\"Chunking file: {file_path}\")\n\t\t\t\t\t# Use iter_chunk_file if available, otherwise list()\n\t\t\t\t\tfile_chunks = list(self.chunker.chunk_file(absolute_path, git_hash))\n\n\t\t\t\t\tif not file_chunks:\n\t\t\t\t\t\tlogger.debug(f\"No chunks generated for file: {file_path}\")\n\t\t\t\t\t\tcontinue  # Skip to next file\n\n\t\t\t\t\tfor chunk in file_chunks:\n\t\t\t\t\t\t# Estimate token count for the chunk content\n\t\t\t\t\t\tchunk_text = chunk[\"content\"]\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t# Note: count_tokens expects a list\n\t\t\t\t\t\t\tchunk_token_count = self.voyage_client.count_tokens(\n\t\t\t\t\t\t\t\t[chunk_text], model=self.embedding_model_name\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\texcept Exception:\n\t\t\t\t\t\t\tlogger.exception(f\"Failed to count tokens for chunk in {file_path}. Skipping chunk.\")\n\t\t\t\t\t\t\tcontinue  # Skip this chunk\n\n\t\t\t\t\t\t# Check if adding this chunk exceeds the limit\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\tcurrent_batch_token_count + chunk_token_count &gt; self.voyage_token_limit\n\t\t\t\t\t\t\tand current_chunk_batch\n\t\t\t\t\t\t):\n\t\t\t\t\t\t\t# Process the current batch before adding the new chunk\n\t\t\t\t\t\t\tlogger.info(\n\t\t\t\t\t\t\t\tf\"Token limit ({self.voyage_token_limit}) reached. \"\n\t\t\t\t\t\t\t\tf\"Processing batch ({len(current_chunk_batch)} chunks).\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\tupserted_count = await self._process_and_upsert_batch(current_chunk_batch)\n\t\t\t\t\t\t\tupdate_progress(\n\t\t\t\t\t\t\t\tf\"[5/5] Processing {len(files_to_process)} files...\",\n\t\t\t\t\t\t\t\tstep_increment=0,\n\t\t\t\t\t\t\t\tprocessed_chunks=upserted_count,\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t\t\t# Reset batch\n\t\t\t\t\t\t\tcurrent_chunk_batch = []\n\t\t\t\t\t\t\tcurrent_batch_token_count = 0\n\n\t\t\t\t\t\t# Add the current chunk to the (potentially new) batch\n\t\t\t\t\t\t# Ensure chunk_token_count itself isn't over the limit (handle large chunks)\n\t\t\t\t\t\tif chunk_token_count &gt; self.voyage_token_limit:\n\t\t\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\t\t\tf\"Chunk in {file_path} ({chunk['metadata'].get('start_line', '?')}-...) \"\n\t\t\t\t\t\t\t\tf\"exceeds token limit ({chunk_token_count} &gt; {self.voyage_token_limit}). Skipping.\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\tcontinue  # Skip this large chunk\n\n\t\t\t\t\t\tcurrent_chunk_batch.append(chunk)\n\t\t\t\t\t\tcurrent_batch_token_count += chunk_token_count\n\n\t\t\t\texcept Exception:\n\t\t\t\t\tlogger.exception(f\"Error processing file {file_path} during sync\")\n\t\t\t\t\t# Decide if we should continue with other files or stop\n\t\t\t\t\tcontinue  # Continue processing other files\n\n\t\t\tfiles_processed_count += 1\n\n\t\t# Process any remaining chunks in the last batch\n\t\tif current_chunk_batch:\n\t\t\tlogger.info(\n\t\t\t\tf\"Processing final batch of {len(current_chunk_batch)} chunks ({current_batch_token_count} tokens).\"\n\t\t\t)\n\t\t\tupserted_count = await self._process_and_upsert_batch(current_chunk_batch)\n\t\t\tupdate_progress(\"[5/5] Final batch processed...\", step_increment=0, processed_chunks=upserted_count)\n\n\t\tsync_success = True\n\t\tfinal_message = (\n\t\t\tf\"[green]\u2713[/green] Vector index synchronized. {processed_chunk_count} chunks processed/upserted.\"\n\t\t)\n\t\tlogger.info(\"Vector index synchronization completed successfully.\")\n\n\texcept Exception:\n\t\tlogger.exception(\"An unexpected error occurred during index synchronization\")\n\t\tfinal_message = \"[red]Error:[/red] Index sync failed unexpectedly.\"\n\t\tsync_success = False  # Ensure success is False on exception\n\tfinally:\n\t\t# Final progress update\n\t\tif progress and task_id is not None:\n\t\t\tprogress.update(task_id, description=final_message, completed=100)\n\n\treturn sync_success\n</code></pre>"},{"location":"api/utils/","title":"Utils Overview","text":"<p>Utility module for CodeMap package.</p> <ul> <li>Cli Utils - Utility functions for CLI operations in CodeMap.</li> <li>Config Loader - Configuration loader for CodeMap.</li> <li>Docker Utils - Utilities for working with Docker containers directly via Python.</li> <li>File Utils - Utility functions for file operations in CodeMap.</li> <li>Log Setup - Logging setup for CodeMap.</li> <li>Path Utils - Utilities for handling paths and file system operations.</li> </ul>"},{"location":"api/utils/cli_utils/","title":"Cli Utils","text":"<p>Utility functions for CLI operations in CodeMap.</p>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState","title":"SpinnerState","text":"<p>Singleton class to track spinner state.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>class SpinnerState:\n\t\"\"\"Singleton class to track spinner state.\"\"\"\n\n\t_instance = None\n\tis_active = False\n\n\tdef __new__(cls) -&gt; Self:\n\t\t\"\"\"\n\t\tCreate or return the singleton instance.\n\n\t\tReturns:\n\t\t    The singleton instance of SpinnerState\n\n\t\t\"\"\"\n\t\tif cls._instance is None:\n\t\t\tcls._instance = super().__new__(cls)\n\t\treturn cls._instance\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.is_active","title":"is_active  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_active = False\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.SpinnerState.__new__","title":"__new__","text":"<pre><code>__new__() -&gt; Self\n</code></pre> <p>Create or return the singleton instance.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The singleton instance of SpinnerState</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def __new__(cls) -&gt; Self:\n\t\"\"\"\n\tCreate or return the singleton instance.\n\n\tReturns:\n\t    The singleton instance of SpinnerState\n\n\t\"\"\"\n\tif cls._instance is None:\n\t\tcls._instance = super().__new__(cls)\n\treturn cls._instance\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(*, is_verbose: bool) -&gt; None\n</code></pre> <p>Configure logging based on verbosity.</p> <p>Parameters:</p> Name Type Description Default <code>is_verbose</code> <code>bool</code> <p>Whether to enable debug logging.</p> required Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def setup_logging(*, is_verbose: bool) -&gt; None:\n\t\"\"\"\n\tConfigure logging based on verbosity.\n\n\tArgs:\n\t    is_verbose: Whether to enable debug logging.\n\n\t\"\"\"\n\t# Suppress certain warnings regardless of verbose mode\n\timport warnings\n\n\t# Suppress SyntaxWarnings from Qdrant client (invalid escape sequences)\n\twarnings.filterwarnings(\n\t\t\"ignore\", message=r\"invalid escape sequence\", category=SyntaxWarning, module=r\"qdrant_client\\..*\"\n\t)\n\n\t# Set log level based on verbosity\n\t# In verbose mode, use DEBUG or the level specified in LOG_LEVEL env var\n\t# In non-verbose mode, only show ERROR and above\n\tlog_level = \"DEBUG\" if is_verbose else os.environ.get(\"LOG_LEVEL\", \"INFO\").upper()\n\n\t# When not in verbose mode, override to only show errors\n\tif not is_verbose:\n\t\tlog_level = \"ERROR\"\n\n\tlogging.basicConfig(\n\t\tlevel=log_level,\n\t\tformat=\"%(message)s\",\n\t\tdatefmt=\"[%X]\",\n\t\thandlers=[RichHandler(rich_tracebacks=True, show_time=True, show_path=True)],\n\t)\n\n\t# Also set specific loggers to ERROR when not in verbose mode\n\tif not is_verbose:\n\t\t# Suppress logs from these packages completely unless in verbose mode\n\t\tlogging.getLogger(\"litellm\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"httpx\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"httpcore\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"requests\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"openai\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"tqdm\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"qdrant_client\").setLevel(logging.ERROR)\n\n\t\t# Set codemap loggers to ERROR level\n\t\tlogging.getLogger(\"codemap\").setLevel(logging.ERROR)\n\n\t\t# Specifically suppress git-related logs\n\t\tlogging.getLogger(\"codemap.utils.git_utils\").setLevel(logging.ERROR)\n\t\tlogging.getLogger(\"codemap.git\").setLevel(logging.ERROR)\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.create_spinner_progress","title":"create_spinner_progress","text":"<pre><code>create_spinner_progress() -&gt; Progress\n</code></pre> <p>Create a spinner progress bar.</p> <p>Returns:</p> Type Description <code>Progress</code> <p>A Progress instance with a spinner</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def create_spinner_progress() -&gt; Progress:\n\t\"\"\"\n\tCreate a spinner progress bar.\n\n\tReturns:\n\t    A Progress instance with a spinner\n\n\t\"\"\"\n\treturn Progress(\n\t\tSpinnerColumn(),\n\t\tTextColumn(\"[progress.description]{task.description}\"),\n\t)\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.loading_spinner","title":"loading_spinner","text":"<pre><code>loading_spinner(\n\tmessage: str = \"Processing...\",\n) -&gt; Iterator[None]\n</code></pre> <p>Display a loading spinner while executing a task.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to display alongside the spinner</p> <code>'Processing...'</code> <p>Yields:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>@contextlib.contextmanager\ndef loading_spinner(message: str = \"Processing...\") -&gt; Iterator[None]:\n\t\"\"\"\n\tDisplay a loading spinner while executing a task.\n\n\tArgs:\n\t    message: Message to display alongside the spinner\n\n\tYields:\n\t    None\n\n\t\"\"\"\n\t# In test environments, don't display a spinner\n\tif os.environ.get(\"PYTEST_CURRENT_TEST\") or os.environ.get(\"CI\"):\n\t\tyield\n\t\treturn\n\n\t# Check if a spinner is already active\n\tspinner_state = SpinnerState()\n\tif spinner_state.is_active:\n\t\t# If there's already an active spinner, don't create a new one\n\t\tyield\n\t\treturn\n\n\t# Only use spinner in interactive environments\n\ttry:\n\t\tspinner_state.is_active = True\n\t\t# Use rich.console.Console.status which is designed for this purpose\n\t\t# and provides the spinner animation\n\t\twith console.status(message):\n\t\t\tyield\n\tfinally:\n\t\tspinner_state.is_active = False\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.ensure_directory_exists","title":"ensure_directory_exists","text":"<pre><code>ensure_directory_exists(directory_path: Path) -&gt; None\n</code></pre> <p>Ensure a directory exists, creating it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>Path</code> <p>Path to ensure exists</p> required Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def ensure_directory_exists(directory_path: Path) -&gt; None:\n\t\"\"\"\n\tEnsure a directory exists, creating it if necessary.\n\n\tArgs:\n\t    directory_path: Path to ensure exists\n\n\t\"\"\"\n\ttry:\n\t\tdirectory_path.mkdir(parents=True, exist_ok=True)\n\texcept (PermissionError, OSError) as e:\n\t\tconsole.print(f\"[red]Unable to create directory {directory_path}: {e!s}\")\n\t\traise\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.progress_indicator","title":"progress_indicator","text":"<pre><code>progress_indicator(\n\tmessage: str,\n\tstyle: str = \"spinner\",\n\ttotal: int | None = None,\n\ttransient: bool = False,\n) -&gt; Iterator[Callable[[int], None]]\n</code></pre> <p>Standardized progress indicator that supports different styles uniformly.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to display with the progress indicator</p> required <code>style</code> <code>str</code> <p>The style of progress indicator - options:    - \"spinner\": Shows an indeterminate spinner    - \"progress\": Shows a determinate progress bar    - \"step\": Shows simple step-by-step progress</p> <code>'spinner'</code> <code>total</code> <code>int | None</code> <p>For determinate progress, the total units of work</p> <code>None</code> <code>transient</code> <code>bool</code> <p>Whether the progress indicator should disappear after completion</p> <code>False</code> <p>Yields:</p> Type Description <code>Callable[[int], None]</code> <p>A callable that accepts an integer amount to advance the progress</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>@contextlib.contextmanager\ndef progress_indicator(\n\tmessage: str,\n\tstyle: str = \"spinner\",\n\ttotal: int | None = None,\n\ttransient: bool = False,\n) -&gt; Iterator[Callable[[int], None]]:\n\t\"\"\"\n\tStandardized progress indicator that supports different styles uniformly.\n\n\tArgs:\n\t    message: The message to display with the progress indicator\n\t    style: The style of progress indicator - options:\n\t           - \"spinner\": Shows an indeterminate spinner\n\t           - \"progress\": Shows a determinate progress bar\n\t           - \"step\": Shows simple step-by-step progress\n\t    total: For determinate progress, the total units of work\n\t    transient: Whether the progress indicator should disappear after completion\n\n\tYields:\n\t    A callable that accepts an integer amount to advance the progress\n\n\t\"\"\"\n\t# Skip visual indicators in testing/CI environments\n\tif os.environ.get(\"PYTEST_CURRENT_TEST\") or os.environ.get(\"CI\"):\n\t\t# Return a no-op advance function\n\t\tyield lambda _: None\n\t\treturn\n\n\t# Check if a spinner is already active\n\tspinner_state = SpinnerState()\n\tif spinner_state.is_active and style == \"spinner\":\n\t\t# If there's already an active spinner, don't create a new one for spinner style\n\t\tyield lambda _: None\n\t\treturn\n\n\ttry:\n\t\t# Mark spinner as active if using spinner style\n\t\tif style == \"spinner\":\n\t\t\tspinner_state.is_active = True\n\n\t\t# Handle different progress styles\n\t\tif style == \"spinner\":\n\t\t\t# Indeterminate spinner using console.status\n\t\t\twith console.status(message):\n\t\t\t\t# Return a no-op advance function since spinners don't advance\n\t\t\t\tyield lambda _: None\n\n\t\telif style == \"progress\":\n\t\t\t# Determinate progress bar using rich.progress.Progress\n\t\t\tprogress = Progress(\n\t\t\t\tSpinnerColumn(),\n\t\t\t\tTextColumn(\"[progress.description]{task.description}\"),\n\t\t\t\ttransient=transient,\n\t\t\t)\n\t\t\twith progress:\n\t\t\t\ttask_id = progress.add_task(message, total=total or 1)\n\t\t\t\t# Return a function that advances the progress\n\t\t\t\tyield lambda amount=1: progress.update(task_id, advance=amount)\n\n\t\telif style == \"step\":\n\t\t\t# Simple step progress like typer.progressbar\n\t\t\tsteps_completed = 0\n\t\t\ttotal_steps = total or 1\n\n\t\t\tconsole.print(f\"{message} [0/{total_steps}]\")\n\n\t\t\t# Function to advance and display steps\n\t\t\tdef advance_step(amount: int = 1) -&gt; None:\n\t\t\t\t\"\"\"Advances the step progress by the specified amount and updates the display.\n\n\t\t\t\tArgs:\n\t\t\t\t\tamount: The number of steps to advance. Defaults to 1.\n\n\t\t\t\tReturns:\n\t\t\t\t\tNone\n\t\t\t\t\"\"\"\n\t\t\t\tnonlocal steps_completed\n\t\t\t\tsteps_completed += amount\n\t\t\t\tsteps_completed = min(steps_completed, total_steps)\n\t\t\t\tconsole.print(f\"{message} [{steps_completed}/{total_steps}]\")\n\n\t\t\tyield advance_step\n\n\t\t\t# Print completion if not transient\n\t\t\tif not transient and steps_completed &gt;= total_steps:\n\t\t\t\tconsole.print(f\"{message} [green]Complete![/green]\")\n\tfinally:\n\t\t# Reset spinner state if we were using spinner style\n\t\tif style == \"spinner\":\n\t\t\tspinner_state.is_active = False\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.show_error","title":"show_error","text":"<pre><code>show_error(\n\tmessage: str, exception: Exception | None = None\n) -&gt; None\n</code></pre> <p>Display an error summary with standardized formatting.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The error message to display</p> required <code>exception</code> <code>Exception | None</code> <p>Optional exception that caused the error</p> <code>None</code> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def show_error(message: str, exception: Exception | None = None) -&gt; None:\n\t\"\"\"\n\tDisplay an error summary with standardized formatting.\n\n\tArgs:\n\t        message: The error message to display\n\t        exception: Optional exception that caused the error\n\n\t\"\"\"\n\terror_text = message\n\tif exception:\n\t\terror_text += f\"\\n\\nDetails: {exception!s}\"\n\t\tlogger.exception(\"Error occurred\", exc_info=exception)\n\n\tdisplay_error_summary(error_text)\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.show_warning","title":"show_warning","text":"<pre><code>show_warning(message: str) -&gt; None\n</code></pre> <p>Display a warning summary with standardized formatting.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The warning message to display</p> required Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def show_warning(message: str) -&gt; None:\n\t\"\"\"\n\tDisplay a warning summary with standardized formatting.\n\n\tArgs:\n\t        message: The warning message to display\n\n\t\"\"\"\n\tdisplay_warning_summary(message)\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.exit_with_error","title":"exit_with_error","text":"<pre><code>exit_with_error(\n\tmessage: str,\n\texit_code: int = 1,\n\texception: Exception | None = None,\n) -&gt; None\n</code></pre> <p>Display an error message and exit.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message to display</p> required <code>exit_code</code> <code>int</code> <p>Exit code to use</p> <code>1</code> <code>exception</code> <code>Exception | None</code> <p>Optional exception that caused the error</p> <code>None</code> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def exit_with_error(message: str, exit_code: int = 1, exception: Exception | None = None) -&gt; None:\n\t\"\"\"\n\tDisplay an error message and exit.\n\n\tArgs:\n\t        message: Error message to display\n\t        exit_code: Exit code to use\n\t        exception: Optional exception that caused the error\n\n\t\"\"\"\n\tshow_error(message, exception)\n\t# No need to check for exception is None, typer.Exit handles it\n\traise typer.Exit(exit_code) from exception\n</code></pre>"},{"location":"api/utils/cli_utils/#codemap.utils.cli_utils.handle_keyboard_interrupt","title":"handle_keyboard_interrupt","text":"<pre><code>handle_keyboard_interrupt() -&gt; None\n</code></pre> <p>Handles KeyboardInterrupt by printing a message and exiting cleanly.</p> Source code in <code>src/codemap/utils/cli_utils.py</code> <pre><code>def handle_keyboard_interrupt() -&gt; None:\n\t\"\"\"Handles KeyboardInterrupt by printing a message and exiting cleanly.\"\"\"\n\tconsole.print(\"\\n[yellow]Operation cancelled by user.[/yellow]\")\n\traise typer.Exit(130)  # Standard exit code for SIGINT\n</code></pre>"},{"location":"api/utils/config_loader/","title":"Config Loader","text":"<p>Configuration loader for CodeMap.</p> <p>This module provides functionality for loading and managing configuration settings.</p>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.MIN_ENV_VAR_PARTS","title":"MIN_ENV_VAR_PARTS  <code>module-attribute</code>","text":"<pre><code>MIN_ENV_VAR_PARTS = 2\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigValue","title":"ConfigValue  <code>module-attribute</code>","text":"<pre><code>ConfigValue = (\n\tstr\n\t| int\n\t| float\n\t| bool\n\t| dict[str, Any]\n\t| list[Any]\n\t| None\n)\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigError","title":"ConfigError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for configuration errors.</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>class ConfigError(Exception):\n\t\"\"\"Exception raised for configuration errors.\"\"\"\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader","title":"ConfigLoader","text":"<p>Loads and manages configuration for CodeMap.</p> <p>This class handles loading configuration from files, environment variables, and default values, with proper error handling and path resolution.</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>class ConfigLoader:\n\t\"\"\"\n\tLoads and manages configuration for CodeMap.\n\n\tThis class handles loading configuration from files, environment\n\tvariables, and default values, with proper error handling and path\n\tresolution.\n\n\t\"\"\"\n\n\t_instance = None  # For singleton pattern\n\n\t@classmethod\n\tdef get_instance(\n\t\tcls, config_file: str | None = None, reload: bool = False, repo_root: Path | None = None\n\t) -&gt; \"ConfigLoader\":\n\t\t\"\"\"\n\t\tGet the singleton instance of ConfigLoader.\n\n\t\tArgs:\n\t\t        config_file: Path to configuration file (optional)\n\t\t        reload: Whether to reload config even if already loaded\n\t\t        repo_root: Repository root path (optional)\n\n\t\tReturns:\n\t\t        ConfigLoader: Singleton instance\n\n\t\t\"\"\"\n\t\tif cls._instance is None or reload:\n\t\t\tcls._instance = cls(config_file, repo_root=repo_root)\n\t\treturn cls._instance\n\n\tdef __init__(self, config_file: str | None = None, repo_root: Path | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the configuration loader.\n\n\t\tArgs:\n\t\t        config_file: Path to configuration file (optional)\n\t\t        repo_root: Repository root path (optional)\n\n\t\t\"\"\"\n\t\tself.config: dict[str, Any] = {}\n\t\tself.repo_root = repo_root\n\t\tself.config_file = self._resolve_config_file(config_file)\n\t\tself.load_config()\n\n\tdef _resolve_config_file(self, config_file: str | None = None) -&gt; Path | None:\n\t\t\"\"\"\n\t\tResolve the configuration file path.\n\n\t\tIf a config file is specified, use that. Otherwise, look in standard locations:\n\t\t1. ./.codemap.yml in the current directory\n\t\t2. $XDG_CONFIG_HOME/codemap/config.yml\n\t\t3. ~/.config/codemap/config.yml (fallback if XDG_CONFIG_HOME not set)\n\n\t\tArgs:\n\t\t        config_file: Explicitly provided config file path (optional)\n\n\t\tReturns:\n\t\t        Optional[Path]: Resolved config file path or None if no suitable file found\n\n\t\t\"\"\"\n\t\tif config_file:\n\t\t\tpath = Path(config_file).expanduser().resolve()\n\t\t\tif path.exists():\n\t\t\t\treturn path\n\t\t\tlogger.warning(\"Specified config file not found: %s\", path)\n\t\t\treturn path  # Return it anyway, we'll handle the missing file in load_config\n\n\t\t# Try current directory\n\t\tlocal_config = Path(\".codemap.yml\")\n\t\tif local_config.exists():\n\t\t\treturn local_config\n\n\t\t# Try XDG config path\n\t\txdg_config_dir = Path(xdg_config_home) / \"codemap\"\n\t\txdg_config_file = xdg_config_dir / \"config.yml\"\n\t\tif xdg_config_file.exists():\n\t\t\treturn xdg_config_file\n\n\t\t# As a last resort, try the legacy ~/.codemap location\n\t\tlegacy_config = Path.home() / \".codemap\" / \"config.yml\"\n\t\tif legacy_config.exists():\n\t\t\treturn legacy_config\n\n\t\t# If we get here, no config file was found\n\t\treturn None\n\n\tdef load_config(self) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tLoad configuration from file and apply environment variable overrides.\n\n\t\tReturns:\n\t\t        Dict[str, Any]: Loaded configuration\n\n\t\tRaises:\n\t\t        ConfigError: If configuration file exists but cannot be loaded\n\n\t\t\"\"\"\n\t\t# Start with default configuration\n\t\tself.config = DEFAULT_CONFIG.copy()\n\n\t\t# Try to load from file if available\n\t\tif self.config_file:\n\t\t\ttry:\n\t\t\t\tif self.config_file.exists():\n\t\t\t\t\twith self.config_file.open(encoding=\"utf-8\") as f:\n\t\t\t\t\t\tfile_config = yaml.safe_load(f)\n\t\t\t\t\t\tif file_config:\n\t\t\t\t\t\t\tself._merge_configs(self.config, file_config)\n\t\t\t\t\tlogger.info(\"Loaded configuration from %s\", self.config_file)\n\t\t\t\telse:\n\t\t\t\t\tlogger.warning(\"Configuration file not found: %s\", self.config_file)\n\t\t\texcept (OSError, yaml.YAMLError) as e:\n\t\t\t\terror_msg = f\"Error loading configuration from {self.config_file}: {e}\"\n\t\t\t\tlogger.exception(error_msg)\n\t\t\t\traise ConfigError(error_msg) from e\n\n\t\treturn self.config\n\n\tdef _merge_configs(self, base: dict[str, Any], override: dict[str, Any]) -&gt; None:\n\t\t\"\"\"\n\t\tRecursively merge two configuration dictionaries.\n\n\t\tArgs:\n\t\t        base: Base configuration dictionary to merge into\n\t\t        override: Override configuration to apply\n\n\t\t\"\"\"\n\t\tfor key, value in override.items():\n\t\t\tif isinstance(value, dict) and key in base and isinstance(base[key], dict):\n\t\t\t\tself._merge_configs(base[key], value)\n\t\t\telse:\n\t\t\t\tbase[key] = value\n\n\tdef get(self, key: str, default: T = None) -&gt; T:\n\t\t\"\"\"\n\t\tGet a configuration value, optionally with a section.\n\n\t\tExamples:\n\t\t        # Get a top-level key\n\t\t        config.get(\"daemon\")\n\n\t\t        # Get a nested key with dot notation\n\t\t        config.get(\"daemon.host\")\n\n\t\tArgs:\n\t\t        key: Configuration key, can include dots for nested access\n\t\t        default: Default value if key not found\n\n\t\tReturns:\n\t\t        T: Configuration value or default\n\n\t\t\"\"\"\n\t\tparts = key.split(\".\")\n\n\t\t# Start with the whole config\n\t\tcurrent = self.config\n\n\t\t# Traverse the parts\n\t\tfor part in parts:\n\t\t\tif isinstance(current, dict) and part in current:\n\t\t\t\tcurrent = current[part]\n\t\t\telse:\n\t\t\t\treturn default\n\n\t\treturn cast(\"T\", current)\n\n\tdef set(self, key: str, value: ConfigValue) -&gt; None:\n\t\t\"\"\"\n\t\tSet a configuration value.\n\n\t\tArgs:\n\t\t        key: Configuration key, can include dots for nested access\n\t\t        value: Value to set\n\n\t\t\"\"\"\n\t\tparts = key.split(\".\")\n\n\t\t# Start with the whole config\n\t\tcurrent = self.config\n\n\t\t# Traverse to the parent of the leaf\n\t\tfor _i, part in enumerate(parts[:-1]):\n\t\t\tif part not in current:\n\t\t\t\tcurrent[part] = {}\n\t\t\telif not isinstance(current[part], dict):\n\t\t\t\t# Convert to dict if it wasn't already\n\t\t\t\tcurrent[part] = {}\n\t\t\tcurrent = current[part]\n\n\t\t# Set the leaf value\n\t\tcurrent[parts[-1]] = value\n\n\tdef save(self, config_file: str | None = None) -&gt; None:\n\t\t\"\"\"\n\t\tSave the current configuration to a file.\n\n\t\tArgs:\n\t\t        config_file: Path to save configuration to (optional, defaults to current config_file)\n\n\t\tRaises:\n\t\t        ConfigError: If configuration cannot be saved\n\n\t\t\"\"\"\n\t\tsave_path = Path(config_file) if config_file else self.config_file\n\n\t\tif not save_path:\n\t\t\terror_msg = \"No configuration file specified for saving\"\n\t\t\tlogger.error(error_msg)\n\t\t\traise ConfigError(error_msg)\n\n\t\t# Ensure parent directory exists\n\t\tsave_path.parent.mkdir(parents=True, exist_ok=True)\n\n\t\ttry:\n\t\t\twith save_path.open(\"w\", encoding=\"utf-8\") as f:\n\t\t\t\tyaml.dump(self.config, f, default_flow_style=False)\n\t\t\tlogger.info(\"Configuration saved to %s\", save_path)\n\t\texcept OSError as e:\n\t\t\terror_msg = f\"Error saving configuration to {save_path}: {e}\"\n\t\t\tlogger.exception(error_msg)\n\t\t\traise ConfigError(error_msg) from e\n\n\tdef get_bypass_hooks(self) -&gt; bool:\n\t\t\"\"\"\n\t\tGet whether to bypass git hooks.\n\n\t\tReturns:\n\t\t        bool: True if git hooks should be bypassed, False otherwise\n\n\t\t\"\"\"\n\t\treturn self.get(\"git.bypass_hooks\", self.get(\"commit.bypass_hooks\", False))\n\n\tdef get_commit_convention(self) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tGet commit convention configuration.\n\n\t\tReturns:\n\t\t        Dict[str, Any]: Commit convention configuration\n\n\t\t\"\"\"\n\t\tconvention = self.get(\"commit.convention\", {})\n\n\t\t# Ensure 'types' is always present with a default value if missing\n\t\tif \"types\" not in convention:\n\t\t\tconvention[\"types\"] = DEFAULT_CONFIG[\"commit\"][\"convention\"][\"types\"]\n\n\t\treturn convention\n\n\tdef get_workflow_strategy(self) -&gt; str:\n\t\t\"\"\"\n\t\tGet the workflow strategy configuration.\n\n\t\tReturns:\n\t\t        str: Workflow strategy name\n\n\t\t\"\"\"\n\t\treturn self.get(\"git.workflow_strategy\", \"github-flow\")\n\n\tdef get_pr_config(self) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tGet PR configuration.\n\n\t\tReturns:\n\t\t        Dict[str, Any]: PR configuration\n\n\t\t\"\"\"\n\t\treturn self.get(\"git.pr\", {})\n\n\tdef get_content_generation_config(self) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tGet content generation configuration.\n\n\t\tReturns:\n\t\t        Dict[str, Any]: Content generation configuration\n\n\t\t\"\"\"\n\t\treturn self.get(\"generation.content\", {})\n\n\tdef get_llm_config(self) -&gt; dict[str, Any]:\n\t\t\"\"\"\n\t\tGet LLM configuration.\n\n\t\tReturns:\n\t\t        Dict[str, Any]: LLM configuration\n\n\t\t\"\"\"\n\t\t# Get the LLM config from the top-level section\n\t\tllm_config = self.get(\"llm\", {})\n\n\t\t# If empty, use the default config\n\t\tif not llm_config:\n\t\t\tllm_config = DEFAULT_CONFIG[\"llm\"]\n\t\t\tlogger.debug(\"Using default LLM config from DEFAULT_CONFIG\")\n\n\t\t# Ensure we have the proper model format with a provider\n\t\tmodel = llm_config.get(\"model\")\n\t\tif model:\n\t\t\tlogger.debug(\"Using model from config: %s\", model)\n\t\t\tif \"/\" not in model:\n\t\t\t\t# Add openai/ prefix if provider is missing\n\t\t\t\tllm_config[\"model\"] = f\"openai/{model}\"\n\t\t\t\tlogger.debug(\"Added openai/ prefix to model: %s\", llm_config[\"model\"])\n\t\t\telse:\n\t\t\t\t# Extract provider from model string to make it accessible in config\n\t\t\t\tprovider = model.split(\"/\")[0].lower()\n\t\t\t\t# Set provider explicitly in the config\n\t\t\t\tllm_config[\"provider\"] = provider\n\t\t\t\tlogger.debug(\"Extracted provider '%s' from model '%s'\", provider, model)\n\n\t\treturn llm_config\n\n\tdef get_api_key_for_model(self, model_name: str) -&gt; str:\n\t\t\"\"\"\n\t\tGet the API key for a specific model.\n\n\t\tArgs:\n\t\t    model_name: The model name, which may include the provider prefix (e.g., 'openai/gpt-4')\n\n\t\tReturns:\n\t\t    str: The API key for the specified model\n\n\t\tRaises:\n\t\t    ConfigError: If API key cannot be determined\n\n\t\t\"\"\"\n\t\tif not model_name:\n\t\t\tmsg = \"Model name is required to determine API key\"\n\t\t\traise ConfigError(msg)\n\n\t\t# Get the LLM config\n\t\tllm_config = self.get_llm_config()\n\n\t\t# Extract provider from model string if present\n\t\tprovider = model_name.split(\"/\")[0].lower() if \"/\" in model_name else \"openai\"\n\n\t\t# Check if we have an environment variable specified for this provider\n\t\tapi_key_env_var = llm_config.get(f\"{provider}_api_key_env\")\n\t\tif api_key_env_var:\n\t\t\timport os\n\n\t\t\tapi_key = os.environ.get(api_key_env_var)\n\t\t\tif api_key:\n\t\t\t\treturn api_key\n\n\t\t# Check if we have a direct API key in the config\n\t\tapi_key = llm_config.get(f\"{provider}_api_key\") or llm_config.get(\"api_key\")\n\t\tif api_key:\n\t\t\treturn api_key\n\n\t\t# If still no API key, raise an error\n\t\tmsg = f\"Could not determine API key for model {model_name} (provider: {provider})\"\n\t\traise ConfigError(msg)\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_instance","title":"get_instance  <code>classmethod</code>","text":"<pre><code>get_instance(\n\tconfig_file: str | None = None,\n\treload: bool = False,\n\trepo_root: Path | None = None,\n) -&gt; ConfigLoader\n</code></pre> <p>Get the singleton instance of ConfigLoader.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str | None</code> <p>Path to configuration file (optional)</p> <code>None</code> <code>reload</code> <code>bool</code> <p>Whether to reload config even if already loaded</p> <code>False</code> <code>repo_root</code> <code>Path | None</code> <p>Repository root path (optional)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ConfigLoader</code> <code>ConfigLoader</code> <p>Singleton instance</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>@classmethod\ndef get_instance(\n\tcls, config_file: str | None = None, reload: bool = False, repo_root: Path | None = None\n) -&gt; \"ConfigLoader\":\n\t\"\"\"\n\tGet the singleton instance of ConfigLoader.\n\n\tArgs:\n\t        config_file: Path to configuration file (optional)\n\t        reload: Whether to reload config even if already loaded\n\t        repo_root: Repository root path (optional)\n\n\tReturns:\n\t        ConfigLoader: Singleton instance\n\n\t\"\"\"\n\tif cls._instance is None or reload:\n\t\tcls._instance = cls(config_file, repo_root=repo_root)\n\treturn cls._instance\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.__init__","title":"__init__","text":"<pre><code>__init__(\n\tconfig_file: str | None = None,\n\trepo_root: Path | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the configuration loader.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str | None</code> <p>Path to configuration file (optional)</p> <code>None</code> <code>repo_root</code> <code>Path | None</code> <p>Repository root path (optional)</p> <code>None</code> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def __init__(self, config_file: str | None = None, repo_root: Path | None = None) -&gt; None:\n\t\"\"\"\n\tInitialize the configuration loader.\n\n\tArgs:\n\t        config_file: Path to configuration file (optional)\n\t        repo_root: Repository root path (optional)\n\n\t\"\"\"\n\tself.config: dict[str, Any] = {}\n\tself.repo_root = repo_root\n\tself.config_file = self._resolve_config_file(config_file)\n\tself.load_config()\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: dict[str, Any] = {}\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.repo_root","title":"repo_root  <code>instance-attribute</code>","text":"<pre><code>repo_root = repo_root\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.config_file","title":"config_file  <code>instance-attribute</code>","text":"<pre><code>config_file = _resolve_config_file(config_file)\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.load_config","title":"load_config","text":"<pre><code>load_config() -&gt; dict[str, Any]\n</code></pre> <p>Load configuration from file and apply environment variable overrides.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: Loaded configuration</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If configuration file exists but cannot be loaded</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def load_config(self) -&gt; dict[str, Any]:\n\t\"\"\"\n\tLoad configuration from file and apply environment variable overrides.\n\n\tReturns:\n\t        Dict[str, Any]: Loaded configuration\n\n\tRaises:\n\t        ConfigError: If configuration file exists but cannot be loaded\n\n\t\"\"\"\n\t# Start with default configuration\n\tself.config = DEFAULT_CONFIG.copy()\n\n\t# Try to load from file if available\n\tif self.config_file:\n\t\ttry:\n\t\t\tif self.config_file.exists():\n\t\t\t\twith self.config_file.open(encoding=\"utf-8\") as f:\n\t\t\t\t\tfile_config = yaml.safe_load(f)\n\t\t\t\t\tif file_config:\n\t\t\t\t\t\tself._merge_configs(self.config, file_config)\n\t\t\t\tlogger.info(\"Loaded configuration from %s\", self.config_file)\n\t\t\telse:\n\t\t\t\tlogger.warning(\"Configuration file not found: %s\", self.config_file)\n\t\texcept (OSError, yaml.YAMLError) as e:\n\t\t\terror_msg = f\"Error loading configuration from {self.config_file}: {e}\"\n\t\t\tlogger.exception(error_msg)\n\t\t\traise ConfigError(error_msg) from e\n\n\treturn self.config\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get","title":"get","text":"<pre><code>get(key: str, default: T = None) -&gt; T\n</code></pre> <p>Get a configuration value, optionally with a section.</p> <p>Examples:</p>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get--get-a-top-level-key","title":"Get a top-level key","text":"<p>config.get(\"daemon\")</p>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get--get-a-nested-key-with-dot-notation","title":"Get a nested key with dot notation","text":"<p>config.get(\"daemon.host\")</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key, can include dots for nested access</p> required <code>default</code> <code>T</code> <p>Default value if key not found</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>Configuration value or default</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get(self, key: str, default: T = None) -&gt; T:\n\t\"\"\"\n\tGet a configuration value, optionally with a section.\n\n\tExamples:\n\t        # Get a top-level key\n\t        config.get(\"daemon\")\n\n\t        # Get a nested key with dot notation\n\t        config.get(\"daemon.host\")\n\n\tArgs:\n\t        key: Configuration key, can include dots for nested access\n\t        default: Default value if key not found\n\n\tReturns:\n\t        T: Configuration value or default\n\n\t\"\"\"\n\tparts = key.split(\".\")\n\n\t# Start with the whole config\n\tcurrent = self.config\n\n\t# Traverse the parts\n\tfor part in parts:\n\t\tif isinstance(current, dict) and part in current:\n\t\t\tcurrent = current[part]\n\t\telse:\n\t\t\treturn default\n\n\treturn cast(\"T\", current)\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.set","title":"set","text":"<pre><code>set(key: str, value: ConfigValue) -&gt; None\n</code></pre> <p>Set a configuration value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key, can include dots for nested access</p> required <code>value</code> <code>ConfigValue</code> <p>Value to set</p> required Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def set(self, key: str, value: ConfigValue) -&gt; None:\n\t\"\"\"\n\tSet a configuration value.\n\n\tArgs:\n\t        key: Configuration key, can include dots for nested access\n\t        value: Value to set\n\n\t\"\"\"\n\tparts = key.split(\".\")\n\n\t# Start with the whole config\n\tcurrent = self.config\n\n\t# Traverse to the parent of the leaf\n\tfor _i, part in enumerate(parts[:-1]):\n\t\tif part not in current:\n\t\t\tcurrent[part] = {}\n\t\telif not isinstance(current[part], dict):\n\t\t\t# Convert to dict if it wasn't already\n\t\t\tcurrent[part] = {}\n\t\tcurrent = current[part]\n\n\t# Set the leaf value\n\tcurrent[parts[-1]] = value\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.save","title":"save","text":"<pre><code>save(config_file: str | None = None) -&gt; None\n</code></pre> <p>Save the current configuration to a file.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str | None</code> <p>Path to save configuration to (optional, defaults to current config_file)</p> <code>None</code> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If configuration cannot be saved</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def save(self, config_file: str | None = None) -&gt; None:\n\t\"\"\"\n\tSave the current configuration to a file.\n\n\tArgs:\n\t        config_file: Path to save configuration to (optional, defaults to current config_file)\n\n\tRaises:\n\t        ConfigError: If configuration cannot be saved\n\n\t\"\"\"\n\tsave_path = Path(config_file) if config_file else self.config_file\n\n\tif not save_path:\n\t\terror_msg = \"No configuration file specified for saving\"\n\t\tlogger.error(error_msg)\n\t\traise ConfigError(error_msg)\n\n\t# Ensure parent directory exists\n\tsave_path.parent.mkdir(parents=True, exist_ok=True)\n\n\ttry:\n\t\twith save_path.open(\"w\", encoding=\"utf-8\") as f:\n\t\t\tyaml.dump(self.config, f, default_flow_style=False)\n\t\tlogger.info(\"Configuration saved to %s\", save_path)\n\texcept OSError as e:\n\t\terror_msg = f\"Error saving configuration to {save_path}: {e}\"\n\t\tlogger.exception(error_msg)\n\t\traise ConfigError(error_msg) from e\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_bypass_hooks","title":"get_bypass_hooks","text":"<pre><code>get_bypass_hooks() -&gt; bool\n</code></pre> <p>Get whether to bypass git hooks.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if git hooks should be bypassed, False otherwise</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get_bypass_hooks(self) -&gt; bool:\n\t\"\"\"\n\tGet whether to bypass git hooks.\n\n\tReturns:\n\t        bool: True if git hooks should be bypassed, False otherwise\n\n\t\"\"\"\n\treturn self.get(\"git.bypass_hooks\", self.get(\"commit.bypass_hooks\", False))\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_commit_convention","title":"get_commit_convention","text":"<pre><code>get_commit_convention() -&gt; dict[str, Any]\n</code></pre> <p>Get commit convention configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: Commit convention configuration</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get_commit_convention(self) -&gt; dict[str, Any]:\n\t\"\"\"\n\tGet commit convention configuration.\n\n\tReturns:\n\t        Dict[str, Any]: Commit convention configuration\n\n\t\"\"\"\n\tconvention = self.get(\"commit.convention\", {})\n\n\t# Ensure 'types' is always present with a default value if missing\n\tif \"types\" not in convention:\n\t\tconvention[\"types\"] = DEFAULT_CONFIG[\"commit\"][\"convention\"][\"types\"]\n\n\treturn convention\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_workflow_strategy","title":"get_workflow_strategy","text":"<pre><code>get_workflow_strategy() -&gt; str\n</code></pre> <p>Get the workflow strategy configuration.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Workflow strategy name</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get_workflow_strategy(self) -&gt; str:\n\t\"\"\"\n\tGet the workflow strategy configuration.\n\n\tReturns:\n\t        str: Workflow strategy name\n\n\t\"\"\"\n\treturn self.get(\"git.workflow_strategy\", \"github-flow\")\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_pr_config","title":"get_pr_config","text":"<pre><code>get_pr_config() -&gt; dict[str, Any]\n</code></pre> <p>Get PR configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: PR configuration</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get_pr_config(self) -&gt; dict[str, Any]:\n\t\"\"\"\n\tGet PR configuration.\n\n\tReturns:\n\t        Dict[str, Any]: PR configuration\n\n\t\"\"\"\n\treturn self.get(\"git.pr\", {})\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_content_generation_config","title":"get_content_generation_config","text":"<pre><code>get_content_generation_config() -&gt; dict[str, Any]\n</code></pre> <p>Get content generation configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: Content generation configuration</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get_content_generation_config(self) -&gt; dict[str, Any]:\n\t\"\"\"\n\tGet content generation configuration.\n\n\tReturns:\n\t        Dict[str, Any]: Content generation configuration\n\n\t\"\"\"\n\treturn self.get(\"generation.content\", {})\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_llm_config","title":"get_llm_config","text":"<pre><code>get_llm_config() -&gt; dict[str, Any]\n</code></pre> <p>Get LLM configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: LLM configuration</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get_llm_config(self) -&gt; dict[str, Any]:\n\t\"\"\"\n\tGet LLM configuration.\n\n\tReturns:\n\t        Dict[str, Any]: LLM configuration\n\n\t\"\"\"\n\t# Get the LLM config from the top-level section\n\tllm_config = self.get(\"llm\", {})\n\n\t# If empty, use the default config\n\tif not llm_config:\n\t\tllm_config = DEFAULT_CONFIG[\"llm\"]\n\t\tlogger.debug(\"Using default LLM config from DEFAULT_CONFIG\")\n\n\t# Ensure we have the proper model format with a provider\n\tmodel = llm_config.get(\"model\")\n\tif model:\n\t\tlogger.debug(\"Using model from config: %s\", model)\n\t\tif \"/\" not in model:\n\t\t\t# Add openai/ prefix if provider is missing\n\t\t\tllm_config[\"model\"] = f\"openai/{model}\"\n\t\t\tlogger.debug(\"Added openai/ prefix to model: %s\", llm_config[\"model\"])\n\t\telse:\n\t\t\t# Extract provider from model string to make it accessible in config\n\t\t\tprovider = model.split(\"/\")[0].lower()\n\t\t\t# Set provider explicitly in the config\n\t\t\tllm_config[\"provider\"] = provider\n\t\t\tlogger.debug(\"Extracted provider '%s' from model '%s'\", provider, model)\n\n\treturn llm_config\n</code></pre>"},{"location":"api/utils/config_loader/#codemap.utils.config_loader.ConfigLoader.get_api_key_for_model","title":"get_api_key_for_model","text":"<pre><code>get_api_key_for_model(model_name: str) -&gt; str\n</code></pre> <p>Get the API key for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model name, which may include the provider prefix (e.g., 'openai/gpt-4')</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The API key for the specified model</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If API key cannot be determined</p> Source code in <code>src/codemap/utils/config_loader.py</code> <pre><code>def get_api_key_for_model(self, model_name: str) -&gt; str:\n\t\"\"\"\n\tGet the API key for a specific model.\n\n\tArgs:\n\t    model_name: The model name, which may include the provider prefix (e.g., 'openai/gpt-4')\n\n\tReturns:\n\t    str: The API key for the specified model\n\n\tRaises:\n\t    ConfigError: If API key cannot be determined\n\n\t\"\"\"\n\tif not model_name:\n\t\tmsg = \"Model name is required to determine API key\"\n\t\traise ConfigError(msg)\n\n\t# Get the LLM config\n\tllm_config = self.get_llm_config()\n\n\t# Extract provider from model string if present\n\tprovider = model_name.split(\"/\")[0].lower() if \"/\" in model_name else \"openai\"\n\n\t# Check if we have an environment variable specified for this provider\n\tapi_key_env_var = llm_config.get(f\"{provider}_api_key_env\")\n\tif api_key_env_var:\n\t\timport os\n\n\t\tapi_key = os.environ.get(api_key_env_var)\n\t\tif api_key:\n\t\t\treturn api_key\n\n\t# Check if we have a direct API key in the config\n\tapi_key = llm_config.get(f\"{provider}_api_key\") or llm_config.get(\"api_key\")\n\tif api_key:\n\t\treturn api_key\n\n\t# If still no API key, raise an error\n\tmsg = f\"Could not determine API key for model {model_name} (provider: {provider})\"\n\traise ConfigError(msg)\n</code></pre>"},{"location":"api/utils/docker_utils/","title":"Docker Utils","text":"<p>Utilities for working with Docker containers directly via Python.</p>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.DEFAULT_TIMEOUT","title":"DEFAULT_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TIMEOUT = 60\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.HTTP_OK","title":"HTTP_OK  <code>module-attribute</code>","text":"<pre><code>HTTP_OK = 200\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_IMAGE","title":"QDRANT_IMAGE  <code>module-attribute</code>","text":"<pre><code>QDRANT_IMAGE = 'qdrant/qdrant:latest'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_CONTAINER_NAME","title":"QDRANT_CONTAINER_NAME  <code>module-attribute</code>","text":"<pre><code>QDRANT_CONTAINER_NAME = 'codemap-qdrant'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_HOST_PORT","title":"QDRANT_HOST_PORT  <code>module-attribute</code>","text":"<pre><code>QDRANT_HOST_PORT = 6333\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_HTTP_PORT","title":"QDRANT_HTTP_PORT  <code>module-attribute</code>","text":"<pre><code>QDRANT_HTTP_PORT = 6333\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_GRPC_PORT","title":"QDRANT_GRPC_PORT  <code>module-attribute</code>","text":"<pre><code>QDRANT_GRPC_PORT = 6334\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.QDRANT_STORAGE_PATH","title":"QDRANT_STORAGE_PATH  <code>module-attribute</code>","text":"<pre><code>QDRANT_STORAGE_PATH = './codemap_cache/qdrant_storage'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_IMAGE","title":"POSTGRES_IMAGE  <code>module-attribute</code>","text":"<pre><code>POSTGRES_IMAGE = 'postgres:latest'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_CONTAINER_NAME","title":"POSTGRES_CONTAINER_NAME  <code>module-attribute</code>","text":"<pre><code>POSTGRES_CONTAINER_NAME = 'codemap-postgres'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_HOST_PORT","title":"POSTGRES_HOST_PORT  <code>module-attribute</code>","text":"<pre><code>POSTGRES_HOST_PORT = 5432\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_ENV","title":"POSTGRES_ENV  <code>module-attribute</code>","text":"<pre><code>POSTGRES_ENV = {\n\t\"POSTGRES_PASSWORD\": \"postgres\",\n\t\"POSTGRES_USER\": \"postgres\",\n\t\"POSTGRES_DB\": \"codemap\",\n}\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.POSTGRES_STORAGE_PATH","title":"POSTGRES_STORAGE_PATH  <code>module-attribute</code>","text":"<pre><code>POSTGRES_STORAGE_PATH = './codemap_cache/postgres_data'\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.is_docker_running","title":"is_docker_running  <code>async</code>","text":"<pre><code>is_docker_running() -&gt; bool\n</code></pre> <p>Check if the Docker daemon is running.</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def is_docker_running() -&gt; bool:\n\t\"\"\"Check if the Docker daemon is running.\"\"\"\n\tclient = None\n\ttry:\n\t\tclient = docker.from_env()\n\t\tclient.ping()\n\t\treturn True\n\texcept DockerException:\n\t\treturn False\n\tfinally:\n\t\tif client is not None:\n\t\t\tclient.close()\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.is_container_running","title":"is_container_running  <code>async</code>","text":"<pre><code>is_container_running(container_name: str) -&gt; bool\n</code></pre> <p>Check if a specific Docker container is running.</p> <p>Parameters:</p> Name Type Description Default <code>container_name</code> <code>str</code> <p>Name of the container to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the container is running, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def is_container_running(container_name: str) -&gt; bool:\n\t\"\"\"\n\tCheck if a specific Docker container is running.\n\n\tArgs:\n\t    container_name: Name of the container to check\n\n\tReturns:\n\t    True if the container is running, False otherwise\n\n\t\"\"\"\n\tclient = None\n\ttry:\n\t\tclient = docker.from_env()\n\t\ttry:\n\t\t\tcontainer = client.containers.get(container_name)\n\t\t\treturn container.status == \"running\"\n\t\texcept NotFound:\n\t\t\treturn False\n\texcept DockerException:\n\t\tlogger.exception(\"Docker error while checking container status\")\n\t\treturn False\n\tfinally:\n\t\tif client is not None:\n\t\t\tclient.close()\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.pull_image_if_needed","title":"pull_image_if_needed  <code>async</code>","text":"<pre><code>pull_image_if_needed(image_name: str) -&gt; bool\n</code></pre> <p>Pull a Docker image if it's not already available locally.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of the image to pull</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def pull_image_if_needed(image_name: str) -&gt; bool:\n\t\"\"\"\n\tPull a Docker image if it's not already available locally.\n\n\tArgs:\n\t    image_name: Name of the image to pull\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\tclient = None\n\ttry:\n\t\tclient = docker.from_env()\n\t\ttry:\n\t\t\tclient.images.get(image_name)\n\t\t\tlogger.info(f\"Image {image_name} already exists locally\")\n\t\t\treturn True\n\t\texcept ImageNotFound:\n\t\t\tlogger.info(f\"Pulling image {image_name}...\")\n\t\t\ttry:\n\t\t\t\tclient.images.pull(image_name)\n\t\t\t\tlogger.info(f\"Successfully pulled image {image_name}\")\n\t\t\t\treturn True\n\t\t\texcept APIError:\n\t\t\t\tlogger.exception(f\"Failed to pull image {image_name}\")\n\t\t\t\treturn False\n\texcept DockerException:\n\t\tlogger.exception(\"Docker error\")\n\t\treturn False\n\tfinally:\n\t\tif client is not None:\n\t\t\tclient.close()\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.ensure_volume_path_exists","title":"ensure_volume_path_exists  <code>async</code>","text":"<pre><code>ensure_volume_path_exists(path: str) -&gt; None\n</code></pre> <p>Ensure the host path for a volume exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to ensure exists</p> required Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def ensure_volume_path_exists(path: str) -&gt; None:\n\t\"\"\"\n\tEnsure the host path for a volume exists.\n\n\tArgs:\n\t    path: Path to ensure exists\n\n\t\"\"\"\n\tPath(path).mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.start_qdrant_container","title":"start_qdrant_container  <code>async</code>","text":"<pre><code>start_qdrant_container() -&gt; bool\n</code></pre> <p>Start the Qdrant container.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def start_qdrant_container() -&gt; bool:\n\t\"\"\"\n\tStart the Qdrant container.\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\tclient = None\n\ttry:\n\t\tclient = docker.from_env()\n\n\t\t# Ensure image is available\n\t\tif not await pull_image_if_needed(QDRANT_IMAGE):\n\t\t\treturn False\n\n\t\t# Ensure storage directory exists\n\t\tawait ensure_volume_path_exists(QDRANT_STORAGE_PATH)\n\n\t\t# Check if container already exists\n\t\ttry:\n\t\t\tcontainer = client.containers.get(QDRANT_CONTAINER_NAME)\n\t\t\tif container.status == \"running\":\n\t\t\t\tlogger.info(f\"Container {QDRANT_CONTAINER_NAME} is already running\")\n\t\t\t\treturn True\n\n\t\t\t# If container exists but is not running, start it\n\t\t\tlogger.info(f\"Starting existing container {QDRANT_CONTAINER_NAME}\")\n\t\t\tcontainer.start()\n\t\t\tlogger.info(f\"Started container {QDRANT_CONTAINER_NAME}\")\n\t\t\treturn True\n\n\t\texcept NotFound:\n\t\t\t# Container doesn't exist, create and start it\n\t\t\tabs_storage_path = str(Path(QDRANT_STORAGE_PATH).absolute())\n\n\t\t\tlogger.info(f\"Creating and starting container {QDRANT_CONTAINER_NAME}\")\n\n\t\t\t# Define volume binding in Docker SDK format\n\t\t\tvolumes: list[str] = [f\"{abs_storage_path}:/qdrant/storage:rw\"]\n\n\t\t\t# Define port mapping\n\t\t\tports: dict[str, int | list[int] | tuple[str, int] | None] = {\n\t\t\t\tf\"{QDRANT_HTTP_PORT}/tcp\": QDRANT_HOST_PORT,\n\t\t\t\tf\"{QDRANT_GRPC_PORT}/tcp\": QDRANT_GRPC_PORT,\n\t\t\t}\n\n\t\t\trestart_policy = {\"Name\": \"always\"}\n\n\t\t\tclient.containers.run(\n\t\t\t\timage=QDRANT_IMAGE,\n\t\t\t\tname=QDRANT_CONTAINER_NAME,\n\t\t\t\tports=ports,\n\t\t\t\tvolumes=volumes,\n\t\t\t\tdetach=True,\n\t\t\t\trestart_policy=restart_policy,  # type: ignore[arg-type]\n\t\t\t)\n\t\t\tlogger.info(f\"Created and started container {QDRANT_CONTAINER_NAME}\")\n\t\t\treturn True\n\n\texcept DockerException:\n\t\tlogger.exception(\"Docker error while starting Qdrant container\")\n\t\treturn False\n\tfinally:\n\t\tif client is not None:\n\t\t\tclient.close()\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.start_postgres_container","title":"start_postgres_container  <code>async</code>","text":"<pre><code>start_postgres_container() -&gt; bool\n</code></pre> <p>Start the PostgreSQL container.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def start_postgres_container() -&gt; bool:\n\t\"\"\"\n\tStart the PostgreSQL container.\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\tclient = None\n\ttry:\n\t\tclient = docker.from_env()\n\n\t\t# Ensure image is available\n\t\tif not await pull_image_if_needed(POSTGRES_IMAGE):\n\t\t\treturn False\n\n\t\t# Ensure storage directory exists\n\t\tawait ensure_volume_path_exists(POSTGRES_STORAGE_PATH)\n\n\t\t# Check if container already exists\n\t\ttry:\n\t\t\tcontainer = client.containers.get(POSTGRES_CONTAINER_NAME)\n\t\t\tif container.status == \"running\":\n\t\t\t\tlogger.info(f\"Container {POSTGRES_CONTAINER_NAME} is already running\")\n\t\t\t\treturn True\n\n\t\t\t# If container exists but is not running, start it\n\t\t\tlogger.info(f\"Starting existing container {POSTGRES_CONTAINER_NAME}\")\n\t\t\tcontainer.start()\n\t\t\tlogger.info(f\"Started container {POSTGRES_CONTAINER_NAME}\")\n\t\t\treturn True\n\n\t\texcept NotFound:\n\t\t\t# Container doesn't exist, create and start it\n\t\t\tabs_storage_path = str(Path(POSTGRES_STORAGE_PATH).absolute())\n\n\t\t\tlogger.info(f\"Creating and starting container {POSTGRES_CONTAINER_NAME}\")\n\n\t\t\t# Define volume binding in Docker SDK format\n\t\t\tvolumes: list[str] = [f\"{abs_storage_path}:/var/lib/postgresql/data:rw\"]\n\n\t\t\t# Define port mapping\n\t\t\tports: dict[str, int | list[int] | tuple[str, int] | None] = {\"5432/tcp\": POSTGRES_HOST_PORT}\n\n\t\t\trestart_policy = {\"Name\": \"always\"}\n\n\t\t\tclient.containers.run(\n\t\t\t\timage=POSTGRES_IMAGE,\n\t\t\t\tname=POSTGRES_CONTAINER_NAME,\n\t\t\t\tports=ports,\n\t\t\t\tvolumes=volumes,\n\t\t\t\tenvironment=POSTGRES_ENV,\n\t\t\t\tdetach=True,\n\t\t\t\trestart_policy=restart_policy,  # type: ignore[arg-type]\n\t\t\t)\n\t\t\tlogger.info(f\"Created and started container {POSTGRES_CONTAINER_NAME}\")\n\t\t\treturn True\n\n\texcept DockerException:\n\t\tlogger.exception(\"Docker error while starting PostgreSQL container\")\n\t\treturn False\n\tfinally:\n\t\tif client is not None:\n\t\t\tclient.close()\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.check_qdrant_health","title":"check_qdrant_health  <code>async</code>","text":"<pre><code>check_qdrant_health(\n\turl: str = f\"http://localhost:{QDRANT_HOST_PORT}\",\n) -&gt; bool\n</code></pre> <p>Check if Qdrant service is healthy and ready to accept connections.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Base URL of the Qdrant service</p> <code>f'http://localhost:{QDRANT_HOST_PORT}'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if Qdrant is healthy, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def check_qdrant_health(url: str = f\"http://localhost:{QDRANT_HOST_PORT}\") -&gt; bool:\n\t\"\"\"\n\tCheck if Qdrant service is healthy and ready to accept connections.\n\n\tArgs:\n\t    url: Base URL of the Qdrant service\n\n\tReturns:\n\t    True if Qdrant is healthy, False otherwise\n\n\t\"\"\"\n\thealth_url = f\"{url}/healthz\"\n\tstart_time = time.time()\n\n\tasync with httpx.AsyncClient() as client:\n\t\ttry:\n\t\t\tasync with asyncio.timeout(DEFAULT_TIMEOUT):\n\t\t\t\twhile True:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresponse = await client.get(health_url)\n\t\t\t\t\t\tif response.status_code == HTTP_OK:\n\t\t\t\t\t\t\tlogger.info(\"Qdrant service is healthy (responded 200 OK)\")\n\t\t\t\t\t\t\treturn True\n\t\t\t\t\texcept httpx.RequestError:\n\t\t\t\t\t\tpass\n\n\t\t\t\t\tif time.time() - start_time &gt;= DEFAULT_TIMEOUT:\n\t\t\t\t\t\tbreak\n\n\t\t\t\t\t# Wait before trying again\n\t\t\t\t\tawait asyncio.sleep(1)\n\t\texcept TimeoutError:\n\t\t\tpass\n\n\tlogger.error(f\"Qdrant service did not become healthy within {DEFAULT_TIMEOUT} seconds\")\n\treturn False\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.ensure_qdrant_running","title":"ensure_qdrant_running  <code>async</code>","text":"<pre><code>ensure_qdrant_running(\n\twait_for_health: bool = True,\n\tqdrant_url: str = f\"http://localhost:{QDRANT_HOST_PORT}\",\n) -&gt; tuple[bool, str]\n</code></pre> <p>Ensure the Qdrant container is running, starting it if needed.</p> <p>Parameters:</p> Name Type Description Default <code>wait_for_health</code> <code>bool</code> <p>Whether to wait for Qdrant to be healthy</p> <code>True</code> <code>qdrant_url</code> <code>str</code> <p>URL of the Qdrant service</p> <code>f'http://localhost:{QDRANT_HOST_PORT}'</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple of (success, message)</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def ensure_qdrant_running(\n\twait_for_health: bool = True, qdrant_url: str = f\"http://localhost:{QDRANT_HOST_PORT}\"\n) -&gt; tuple[bool, str]:\n\t\"\"\"\n\tEnsure the Qdrant container is running, starting it if needed.\n\n\tArgs:\n\t    wait_for_health: Whether to wait for Qdrant to be healthy\n\t    qdrant_url: URL of the Qdrant service\n\n\tReturns:\n\t    Tuple of (success, message)\n\n\t\"\"\"\n\tif not await is_docker_running():\n\t\treturn False, \"Docker daemon is not running\"\n\n\t# Check if Qdrant service is already running\n\tqdrant_running = False\n\n\ttry:\n\t\t# Try a direct HTTP request first to see if Qdrant is up\n\t\tasync with httpx.AsyncClient(timeout=3.0) as client:\n\t\t\ttry:\n\t\t\t\tresponse = await client.get(f\"{qdrant_url}/health\")\n\t\t\t\tif response.status_code == HTTP_OK:\n\t\t\t\t\tlogger.info(\"Qdrant is already available via HTTP\")\n\t\t\t\t\tqdrant_running = True\n\t\t\texcept httpx.RequestError:\n\t\t\t\t# HTTP request failed, now check if it's running in Docker\n\t\t\t\tqdrant_running = await is_container_running(QDRANT_CONTAINER_NAME)\n\texcept (httpx.RequestError, httpx.HTTPError, ConnectionError, OSError) as e:\n\t\tlogger.warning(f\"Error checking Qdrant service: {e}\")\n\n\t# Start services if needed\n\tif not qdrant_running:\n\t\tlogger.info(\"Qdrant service is not running, starting container...\")\n\t\tstarted = await start_qdrant_container()\n\t\tif not started:\n\t\t\treturn False, \"Failed to start Qdrant container\"\n\n\t\tif wait_for_health:\n\t\t\t# Wait for Qdrant to be healthy\n\t\t\tlogger.info(f\"Waiting for Qdrant service to be healthy (timeout: {DEFAULT_TIMEOUT}s)...\")\n\t\t\thealthy = await check_qdrant_health(qdrant_url)\n\t\t\tif not healthy:\n\t\t\t\treturn False, \"Qdrant service failed to become healthy within the timeout period\"\n\n\treturn True, \"Qdrant container is running\"\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.ensure_postgres_running","title":"ensure_postgres_running  <code>async</code>","text":"<pre><code>ensure_postgres_running() -&gt; tuple[bool, str]\n</code></pre> <p>Ensure the PostgreSQL container is running, starting it if needed.</p> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple of (success, message)</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def ensure_postgres_running() -&gt; tuple[bool, str]:\n\t\"\"\"\n\tEnsure the PostgreSQL container is running, starting it if needed.\n\n\tReturns:\n\t    Tuple of (success, message)\n\n\t\"\"\"\n\tif not await is_docker_running():\n\t\treturn False, \"Docker daemon is not running\"\n\n\t# Check if PostgreSQL container is already running\n\tpostgres_running = await is_container_running(POSTGRES_CONTAINER_NAME)\n\n\t# Start container if needed\n\tif not postgres_running:\n\t\tlogger.info(\"PostgreSQL service is not running, starting container...\")\n\t\tstarted = await start_postgres_container()\n\t\tif not started:\n\t\t\treturn False, \"Failed to start PostgreSQL container\"\n\n\treturn True, \"PostgreSQL container is running\"\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.stop_container","title":"stop_container  <code>async</code>","text":"<pre><code>stop_container(container_name: str) -&gt; bool\n</code></pre> <p>Stop a Docker container.</p> <p>Parameters:</p> Name Type Description Default <code>container_name</code> <code>str</code> <p>Name of the container to stop</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def stop_container(container_name: str) -&gt; bool:\n\t\"\"\"\n\tStop a Docker container.\n\n\tArgs:\n\t    container_name: Name of the container to stop\n\n\tReturns:\n\t    True if successful, False otherwise\n\n\t\"\"\"\n\tclient = None\n\ttry:\n\t\tclient = docker.from_env()\n\t\ttry:\n\t\t\tcontainer = client.containers.get(container_name)\n\t\t\tif container.status == \"running\":\n\t\t\t\tlogger.info(f\"Stopping container {container_name}\")\n\t\t\t\tcontainer.stop(timeout=10)  # Wait up to 10 seconds for clean shutdown\n\t\t\t\tlogger.info(f\"Stopped container {container_name}\")\n\t\t\treturn True\n\t\texcept NotFound:\n\t\t\tlogger.info(f\"Container {container_name} does not exist\")\n\t\t\treturn True\n\texcept DockerException:\n\t\tlogger.exception(\"Docker error while stopping container\")\n\t\treturn False\n\tfinally:\n\t\tif client is not None:\n\t\t\tclient.close()\n</code></pre>"},{"location":"api/utils/docker_utils/#codemap.utils.docker_utils.stop_all_codemap_containers","title":"stop_all_codemap_containers  <code>async</code>","text":"<pre><code>stop_all_codemap_containers() -&gt; tuple[bool, str]\n</code></pre> <p>Stop all CodeMap containers.</p> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple of (success, message)</p> Source code in <code>src/codemap/utils/docker_utils.py</code> <pre><code>async def stop_all_codemap_containers() -&gt; tuple[bool, str]:\n\t\"\"\"\n\tStop all CodeMap containers.\n\n\tReturns:\n\t    Tuple of (success, message)\n\n\t\"\"\"\n\tcontainers = [QDRANT_CONTAINER_NAME, POSTGRES_CONTAINER_NAME]\n\tsuccess = True\n\n\tfor container_name in containers:\n\t\tif not await stop_container(container_name):\n\t\t\tsuccess = False\n\n\tif success:\n\t\treturn True, \"All CodeMap containers stopped successfully\"\n\treturn False, \"Failed to stop some CodeMap containers\"\n</code></pre>"},{"location":"api/utils/file_utils/","title":"File Utils","text":"<p>Utility functions for file operations in CodeMap.</p>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.count_tokens","title":"count_tokens","text":"<pre><code>count_tokens(file_path: Path) -&gt; int\n</code></pre> <p>Rough estimation of tokens in a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to count tokens in.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated number of tokens in the file.</p> Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def count_tokens(file_path: Path) -&gt; int:\n\t\"\"\"\n\tRough estimation of tokens in a file.\n\n\tArgs:\n\t    file_path: Path to the file to count tokens in.\n\n\tReturns:\n\t    Estimated number of tokens in the file.\n\n\t\"\"\"\n\ttry:\n\t\twith file_path.open(encoding=\"utf-8\") as f:\n\t\t\tcontent = f.read()\n\t\t\t# Simple tokenization by whitespace\n\t\t\treturn len(content.split())\n\texcept (OSError, UnicodeDecodeError):\n\t\treturn 0\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.read_file_content","title":"read_file_content","text":"<pre><code>read_file_content(file_path: Path | str) -&gt; str | None\n</code></pre> <p>Read content from a file with proper error handling.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Content of the file as string, or None if the file cannot be read</p> Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def read_file_content(file_path: Path | str) -&gt; str | None:\n\t\"\"\"\n\tRead content from a file with proper error handling.\n\n\tArgs:\n\t    file_path: Path to the file to read\n\n\tReturns:\n\t    Content of the file as string, or None if the file cannot be read\n\n\t\"\"\"\n\tpath_obj = Path(file_path)\n\ttry:\n\t\twith path_obj.open(\"r\", encoding=\"utf-8\") as f:\n\t\t\treturn f.read()\n\texcept FileNotFoundError:\n\t\t# Handle case where file was tracked but has been deleted\n\t\tlogger.debug(f\"File not found: {path_obj} - possibly deleted since last tracked\")\n\t\treturn None\n\texcept UnicodeDecodeError:\n\t\t# Try to read as binary and then decode with error handling\n\t\tlogger.warning(\"File %s contains non-UTF-8 characters, attempting to decode with errors='replace'\", path_obj)\n\t\ttry:\n\t\t\twith path_obj.open(\"rb\") as f:\n\t\t\t\tcontent = f.read()\n\t\t\t\treturn content.decode(\"utf-8\", errors=\"replace\")\n\t\texcept (OSError, FileNotFoundError):\n\t\t\tlogger.debug(f\"Unable to read file as binary: {path_obj}\")\n\t\t\treturn None\n\texcept OSError as e:\n\t\t# Handle other file access errors\n\t\tlogger.debug(f\"Error reading file {path_obj}: {e}\")\n\t\treturn None\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.ensure_directory_exists","title":"ensure_directory_exists","text":"<pre><code>ensure_directory_exists(dir_path: Path) -&gt; None\n</code></pre> <p>Ensure that a directory exists, creating it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>The path to the directory.</p> required Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def ensure_directory_exists(dir_path: Path) -&gt; None:\n\t\"\"\"\n\tEnsure that a directory exists, creating it if necessary.\n\n\tArgs:\n\t    dir_path (Path): The path to the directory.\n\n\t\"\"\"\n\tif not dir_path.exists():\n\t\tlogger.info(f\"Creating directory: {dir_path}\")\n\t\ttry:\n\t\t\tdir_path.mkdir(parents=True, exist_ok=True)\n\t\texcept OSError:\n\t\t\tlogger.exception(f\"Failed to create directory {dir_path}\")\n\t\t\traise\n\telif not dir_path.is_dir():\n\t\tlogger.error(f\"Path exists but is not a directory: {dir_path}\")\n\t\tmsg = f\"Path exists but is not a directory: {dir_path}\"\n\t\traise NotADirectoryError(msg)\n</code></pre>"},{"location":"api/utils/file_utils/#codemap.utils.file_utils.is_binary_file","title":"is_binary_file","text":"<pre><code>is_binary_file(file_path: Path) -&gt; bool\n</code></pre> <p>Check if a file is binary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file is binary, False otherwise</p> Source code in <code>src/codemap/utils/file_utils.py</code> <pre><code>def is_binary_file(file_path: Path) -&gt; bool:\n\t\"\"\"\n\tCheck if a file is binary.\n\n\tArgs:\n\t        file_path: Path to the file\n\n\tReturns:\n\t        True if the file is binary, False otherwise\n\n\t\"\"\"\n\t# Skip files larger than 10 MB\n\ttry:\n\t\tif file_path.stat().st_size &gt; 10 * 1024 * 1024:\n\t\t\treturn True\n\n\t\t# Try to read as text\n\t\twith file_path.open(encoding=\"utf-8\") as f:\n\t\t\tchunk = f.read(1024)\n\t\t\treturn \"\\0\" in chunk\n\texcept UnicodeDecodeError:\n\t\treturn True\n\texcept (OSError, PermissionError):\n\t\treturn True\n</code></pre>"},{"location":"api/utils/log_setup/","title":"Log Setup","text":"<p>Logging setup for CodeMap.</p> <p>This module configures logging for different parts of the CodeMap application, ensuring logs are stored in the appropriate directories.</p>"},{"location":"api/utils/log_setup/#codemap.utils.log_setup.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/utils/log_setup/#codemap.utils.log_setup.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(\n\tis_verbose: bool = False, log_to_console: bool = True\n) -&gt; None\n</code></pre> <p>Set up logging configuration.</p> <p>Parameters:</p> Name Type Description Default <code>log_type</code> <p>Type of log ('daemon', 'cli', 'error')</p> required <code>log_name</code> <p>Specific name for the log file (default: based on log_type)</p> required <code>is_verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>log_to_file</code> <p>Whether to log to a file</p> required <code>log_to_console</code> <code>bool</code> <p>Whether to log to the console</p> <code>True</code> Source code in <code>src/codemap/utils/log_setup.py</code> <pre><code>def setup_logging(\n\tis_verbose: bool = False,\n\tlog_to_console: bool = True,\n) -&gt; None:\n\t\"\"\"\n\tSet up logging configuration.\n\n\tArgs:\n\t    log_type: Type of log ('daemon', 'cli', 'error')\n\t    log_name: Specific name for the log file (default: based on log_type)\n\t    is_verbose: Enable verbose logging\n\t    log_to_file: Whether to log to a file\n\t    log_to_console: Whether to log to the console\n\n\t\"\"\"\n\t# Determine log level\n\tlog_level = logging.DEBUG if is_verbose else logging.INFO\n\n\t# Root logger configuration\n\troot_logger = logging.getLogger()\n\troot_logger.setLevel(log_level)\n\n\t# Clear existing handlers\n\tfor handler in root_logger.handlers[:]:\n\t\troot_logger.removeHandler(handler)\n\n\t# Setup console logging if requested\n\tif log_to_console:\n\t\tconsole_handler = RichHandler(\n\t\t\tlevel=log_level,\n\t\t\trich_tracebacks=True,\n\t\t\tshow_time=True,\n\t\t\tshow_path=is_verbose,\n\t\t)\n\t\tformatter = logging.Formatter(\"%(message)s\")\n\t\tconsole_handler.setFormatter(formatter)\n\t\troot_logger.addHandler(console_handler)\n</code></pre>"},{"location":"api/utils/log_setup/#codemap.utils.log_setup.log_environment_info","title":"log_environment_info","text":"<pre><code>log_environment_info() -&gt; None\n</code></pre> <p>Log information about the execution environment.</p> Source code in <code>src/codemap/utils/log_setup.py</code> <pre><code>def log_environment_info() -&gt; None:\n\t\"\"\"Log information about the execution environment.\"\"\"\n\tlogger = logging.getLogger(__name__)\n\n\ttry:\n\t\timport platform\n\n\t\tfrom codemap import __version__\n\n\t\tlogger.info(\"CodeMap version: %s\", __version__)\n\t\tlogger.info(\"Python version: %s\", platform.python_version())\n\t\tlogger.info(\"Platform: %s\", platform.platform())\n\n\texcept Exception:\n\t\t# logger.exception automatically handles exception info\n\t\tlogger.exception(\"Error logging environment info:\")\n</code></pre>"},{"location":"api/utils/log_setup/#codemap.utils.log_setup.display_error_summary","title":"display_error_summary","text":"<pre><code>display_error_summary(error_message: str) -&gt; None\n</code></pre> <p>Display an error summary with a divider and a title.</p> <p>Parameters:</p> Name Type Description Default <code>error_message</code> <code>str</code> <p>The error message to display</p> required Source code in <code>src/codemap/utils/log_setup.py</code> <pre><code>def display_error_summary(error_message: str) -&gt; None:\n\t\"\"\"\n\tDisplay an error summary with a divider and a title.\n\n\tArgs:\n\t        error_message: The error message to display\n\n\t\"\"\"\n\ttitle = Text(\"Error Summary\", style=\"bold red\")\n\n\tconsole.print()\n\tconsole.print(Rule(title, style=\"red\"))\n\tconsole.print(f\"\\n{error_message}\\n\")\n\tconsole.print(Rule(style=\"red\"))\n\tconsole.print()\n</code></pre>"},{"location":"api/utils/log_setup/#codemap.utils.log_setup.display_warning_summary","title":"display_warning_summary","text":"<pre><code>display_warning_summary(warning_message: str) -&gt; None\n</code></pre> <p>Display a warning summary with a divider and a title.</p> <p>Parameters:</p> Name Type Description Default <code>warning_message</code> <code>str</code> <p>The warning message to display</p> required Source code in <code>src/codemap/utils/log_setup.py</code> <pre><code>def display_warning_summary(warning_message: str) -&gt; None:\n\t\"\"\"\n\tDisplay a warning summary with a divider and a title.\n\n\tArgs:\n\t        warning_message: The warning message to display\n\n\t\"\"\"\n\ttitle = Text(\"Warning Summary\", style=\"bold yellow\")\n\n\tconsole.print()\n\tconsole.print(Rule(title, style=\"yellow\"))\n\tconsole.print(f\"\\n{warning_message}\\n\")\n\tconsole.print(Rule(style=\"yellow\"))\n\tconsole.print()\n</code></pre>"},{"location":"api/utils/path_utils/","title":"Path Utils","text":"<p>Utilities for handling paths and file system operations.</p>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.filter_paths_by_gitignore","title":"filter_paths_by_gitignore","text":"<pre><code>filter_paths_by_gitignore(\n\tpaths: Sequence[Path], repo_root: Path\n) -&gt; list[Path]\n</code></pre> <p>Filter paths based on .gitignore patterns.</p> <p>This function filters a list of paths to exclude those that match patterns in a .gitignore file, while preserving the directory structure.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Sequence[Path]</code> <p>Sequence of paths to filter</p> required <code>repo_root</code> <code>Path</code> <p>Root directory of the repository</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of paths that don't match any gitignore patterns</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def filter_paths_by_gitignore(paths: Sequence[Path], repo_root: Path) -&gt; list[Path]:\n\t\"\"\"\n\tFilter paths based on .gitignore patterns.\n\n\tThis function filters a list of paths to exclude those that match\n\tpatterns in a .gitignore file, while preserving the directory structure.\n\n\tArgs:\n\t    paths: Sequence of paths to filter\n\t    repo_root: Root directory of the repository\n\n\tReturns:\n\t    List of paths that don't match any gitignore patterns\n\n\t\"\"\"\n\ttry:\n\t\timport pathspec\n\t\tfrom pathspec.patterns.gitwildmatch import GitWildMatchPattern\n\texcept ImportError:\n\t\tlogger.warning(\"pathspec package not installed, gitignore filtering disabled\")\n\t\treturn list(paths)\n\n\t# Read .gitignore if it exists\n\tgitignore_path = repo_root / \".gitignore\"\n\tgitignore_patterns = []\n\n\tif gitignore_path.exists():\n\t\t# Parse gitignore patterns\n\t\twith gitignore_path.open(\"r\", encoding=\"utf-8\") as f:\n\t\t\tgitignore_content = f.read()\n\t\tgitignore_patterns = gitignore_content.splitlines()\n\n\t# Add default patterns for common directories that should be ignored\n\tdefault_ignore_patterns = [\n\t\t\"__pycache__/\",\n\t\t\"*.py[cod]\",\n\t\t\"*$py.class\",\n\t\t\".git/\",\n\t\t\".pytest_cache/\",\n\t\t\".coverage\",\n\t\t\"htmlcov/\",\n\t\t\".tox/\",\n\t\t\".nox/\",\n\t\t\".hypothesis/\",\n\t\t\".mypy_cache/\",\n\t\t\".ruff_cache/\",\n\t\t\"dist/\",\n\t\t\"build/\",\n\t\t\"*.so\",\n\t\t\"*.egg\",\n\t\t\"*.egg-info/\",\n\t\t\".env/\",\n\t\t\"venv/\",\n\t\t\".venv/\",\n\t\t\"env/\",\n\t\t\"ENV/\",\n\t\t\"node_modules/\",\n\t]\n\n\t# Combine patterns with existing ones, avoiding duplicates\n\tall_patterns = gitignore_patterns + [p for p in default_ignore_patterns if p not in gitignore_patterns]\n\n\t# Create path spec with direct import\n\tspec = pathspec.PathSpec.from_lines(GitWildMatchPattern, all_patterns)\n\n\t# Filter paths\n\tfiltered_paths = []\n\n\t# Process files first\n\tfile_paths = [p for p in paths if p.is_file()]\n\tfor path in file_paths:\n\t\ttry:\n\t\t\trel_path = path.relative_to(repo_root)\n\t\t\tif not spec.match_file(str(rel_path)):\n\t\t\t\tfiltered_paths.append(path)\n\t\texcept ValueError:\n\t\t\t# Path is not relative to repo_root\n\t\t\tfiltered_paths.append(path)\n\n\t# Process directories\n\tdir_paths = [p for p in paths if p.is_dir()]\n\n\t# First check which directories are included according to gitignore patterns\n\tincluded_dirs = []\n\tfor dir_path in dir_paths:\n\t\ttry:\n\t\t\trel_path = dir_path.relative_to(repo_root)\n\t\t\trel_path_str = str(rel_path) + \"/\"  # Add trailing slash for directory patterns\n\n\t\t\t# Skip the directory if it matches a gitignore pattern\n\t\t\tif spec.match_file(rel_path_str):\n\t\t\t\tlogger.debug(f\"Skipping ignored directory: {rel_path}\")\n\t\t\t\tcontinue\n\n\t\t\t# Check if any parent directory is already ignored\n\t\t\tparent_ignored = False\n\t\t\tfor parent in rel_path.parents:\n\t\t\t\tparent_str = str(parent) + \"/\"\n\t\t\t\tif spec.match_file(parent_str):\n\t\t\t\t\tparent_ignored = True\n\t\t\t\t\tlogger.debug(f\"Skipping directory with ignored parent: {parent}\")\n\t\t\t\t\tbreak\n\n\t\t\tif not parent_ignored:\n\t\t\t\tincluded_dirs.append(dir_path)\n\n\t\texcept ValueError:\n\t\t\t# Path is not relative to repo_root\n\t\t\tincluded_dirs.append(dir_path)\n\n\t# Include all directories at all levels to preserve hierarchy\n\t# Directories with no content might still be needed for the tree visualization\n\tfiltered_paths.extend(included_dirs)\n\n\tlogger.debug(f\"Filtered {len(paths)} paths down to {len(filtered_paths)} after applying gitignore patterns\")\n\treturn filtered_paths\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.normalize_path","title":"normalize_path","text":"<pre><code>normalize_path(path: str | Path) -&gt; Path\n</code></pre> <p>Normalize a path to an absolute Path object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path string or object</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Normalized absolute Path</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def normalize_path(path: str | Path) -&gt; Path:\n\t\"\"\"\n\tNormalize a path to an absolute Path object.\n\n\tArgs:\n\t    path: Path string or object\n\n\tReturns:\n\t    Normalized absolute Path\n\n\t\"\"\"\n\tif isinstance(path, str):\n\t\tpath = Path(path)\n\treturn path.expanduser().resolve()\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.get_relative_path","title":"get_relative_path","text":"<pre><code>get_relative_path(path: Path, base_path: Path) -&gt; Path\n</code></pre> <p>Get path relative to base_path if possible, otherwise return absolute path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to make relative</p> required <code>base_path</code> <code>Path</code> <p>The base path to make it relative to</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Relative path if possible, otherwise absolute path</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def get_relative_path(path: Path, base_path: Path) -&gt; Path:\n\t\"\"\"\n\tGet path relative to base_path if possible, otherwise return absolute path.\n\n\tArgs:\n\t    path: The path to make relative\n\t    base_path: The base path to make it relative to\n\n\tReturns:\n\t    Relative path if possible, otherwise absolute path\n\n\t\"\"\"\n\ttry:\n\t\treturn path.relative_to(base_path)\n\texcept ValueError:\n\t\treturn path.absolute()\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.find_project_root","title":"find_project_root","text":"<pre><code>find_project_root(start_path: Path | None = None) -&gt; Path\n</code></pre> <p>Determine the project root directory (typically the Git repository root).</p> <p>Searches upwards from the starting path (or current working directory if start_path is None) for the '.git' directory.</p> <p>Parameters:</p> Name Type Description Default <code>start_path</code> <code>Path | None</code> <p>The path to start searching from.                                 Defaults to the current working directory.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The absolute path to the project root (containing .git).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the project root cannot be determined based on                the presence of the '.git' directory.</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def find_project_root(start_path: Path | None = None) -&gt; Path:\n\t\"\"\"\n\tDetermine the project root directory (typically the Git repository root).\n\n\tSearches upwards from the starting path (or current working directory if\n\tstart_path is None) for the '.git' directory.\n\n\tArgs:\n\t    start_path (Path | None, optional): The path to start searching from.\n\t                                        Defaults to the current working directory.\n\n\tReturns:\n\t    Path: The absolute path to the project root (containing .git).\n\n\tRaises:\n\t    FileNotFoundError: If the project root cannot be determined based on\n\t                       the presence of the '.git' directory.\n\n\t\"\"\"\n\tif start_path is None:\n\t\tcurrent_dir = Path.cwd()\n\t\tsearch_origin_display = \"current working directory\"\n\telse:\n\t\t# Ensure start_path is absolute for consistent parent traversal\n\t\tcurrent_dir = start_path.resolve()\n\t\tsearch_origin_display = f\"'{start_path}'\"\n\n\t# Check the starting directory itself and its parents\n\tfor parent in [current_dir, *current_dir.parents]:\n\t\t# Check for .git directory\n\t\tis_git_root = (parent / \".git\").is_dir()\n\n\t\tif is_git_root:\n\t\t\tlogger.debug(f\"Project root (Git repository root) found at: {parent}\")\n\t\t\treturn parent\n\n\t# If loop completes without finding a root\n\tmsg = f\"Could not determine project root searching upwards from {search_origin_display}. No '.git' directory found.\"\n\traise FileNotFoundError(msg)\n</code></pre>"},{"location":"api/utils/path_utils/#codemap.utils.path_utils.get_cache_path","title":"get_cache_path","text":"<pre><code>get_cache_path(\n\tcomponent_name: str | None = None,\n\tworkspace_root: Path | None = None,\n) -&gt; Path\n</code></pre> <p>Get the cache path for a specific component or the root cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>component_name</code> <code>str | None</code> <p>The name of the component requiring a                                    cache directory (e.g., 'graph', 'vector').                                    If None, the root cache directory path                                    is returned. Defaults to None.</p> <code>None</code> <code>workspace_root</code> <code>Path | None</code> <p>The workspace root path.                                     If None, it will be determined automatically                                     using <code>find_project_root()</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The absolute path to the component's cache directory if <code>component_name</code>   is provided, otherwise the absolute path to the root cache directory.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>workspace_root</code> is None and <code>find_project_root()</code> fails.</p> <code># ConfigError</code> <p>Config related errors might also be raised by ConfigLoader</p> Source code in <code>src/codemap/utils/path_utils.py</code> <pre><code>def get_cache_path(component_name: str | None = None, workspace_root: Path | None = None) -&gt; Path:\n\t\"\"\"\n\tGet the cache path for a specific component or the root cache directory.\n\n\tArgs:\n\t    component_name (str | None, optional): The name of the component requiring a\n\t                                           cache directory (e.g., 'graph', 'vector').\n\t                                           If None, the root cache directory path\n\t                                           is returned. Defaults to None.\n\t    workspace_root (Path | None, optional): The workspace root path.\n\t                                            If None, it will be determined automatically\n\t                                            using `find_project_root()`. Defaults to None.\n\n\tReturns:\n\t    Path: The absolute path to the component's cache directory if `component_name`\n\t          is provided, otherwise the absolute path to the root cache directory.\n\n\tRaises:\n\t    FileNotFoundError: If `workspace_root` is None and `find_project_root()` fails.\n\t    # ConfigError: Config related errors might also be raised by ConfigLoader\n\n\t\"\"\"\n\tif workspace_root is None:\n\t\tworkspace_root = find_project_root()\n\n\t# Get ConfigLoader instance, potentially passing the repo_root\n\t# Ensure ConfigLoader handles initialization correctly if called multiple times\n\tconfig_loader = ConfigLoader.get_instance(repo_root=workspace_root)\n\n\t# Get cache directory name from config, falling back to default\n\t# Ensure DEFAULT_CONFIG is accessible here\n\tcache_dir_name = config_loader.get(\"cache_dir\", DEFAULT_CONFIG.get(\"cache_dir\", \".codemap_cache\"))\n\n\tcache_root = workspace_root / cache_dir_name\n\n\tif component_name is None:\n\t\t# Ensure the root cache directory exists\n\t\tcache_root.mkdir(parents=True, exist_ok=True)\n\t\tlogger.debug(f\"Root cache path: {cache_root}\")\n\t\treturn cache_root\n\n\t# Calculate the specific component's cache path\n\tcomponent_cache_path = cache_root / component_name\n\t# Ensure the component cache directory exists\n\tcomponent_cache_path.mkdir(parents=True, exist_ok=True)\n\tlogger.debug(f\"Cache path for component '{component_name}': {component_cache_path}\")\n\treturn component_cache_path\n</code></pre>"},{"location":"api/watcher/","title":"Watcher Overview","text":"<p>Watcher module for CodeMap.</p> <ul> <li>File Watcher - File watcher module for CodeMap.</li> </ul>"},{"location":"api/watcher/file_watcher/","title":"File Watcher","text":"<p>File watcher module for CodeMap.</p>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler","title":"FileChangeHandler","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Handles file system events and triggers a callback.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>class FileChangeHandler(FileSystemEventHandler):\n\t\"\"\"Handles file system events and triggers a callback.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tcallback: Callable[[], Coroutine[None, None, None]],\n\t\tdebounce_delay: float = 1.0,\n\t\tevent_loop: asyncio.AbstractEventLoop | None = None,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the handler.\n\n\t\tArgs:\n\t\t    callback: An async function to call when changes are detected.\n\t\t    debounce_delay: Minimum time (seconds) between callback triggers.\n\t\t    event_loop: The asyncio event loop to use, or None to use the current one.\n\n\t\t\"\"\"\n\t\tself.callback = callback\n\t\tself.debounce_delay = debounce_delay\n\t\tself._last_event_time: float = 0\n\t\tself._debounce_task: asyncio.Task | None = None\n\t\t# Set up a thread-safe way to communicate with the event loop\n\t\tself._event_queue = queue.Queue()\n\t\tself._event_processed = threading.Event()\n\t\t# Store or get the event loop\n\t\tself._event_loop = event_loop or asyncio.get_event_loop()\n\t\t# Flag to track if we're in the process of handling an event\n\t\tself._processing = False\n\n\tdef _schedule_callback(self) -&gt; None:\n\t\t\"\"\"Schedule the callback execution from a thread-safe context.\"\"\"\n\t\t# If processing is already in progress, just return\n\t\tif self._processing:\n\t\t\treturn\n\n\t\t# Put the event in the queue for the event loop to process\n\t\tself._event_processed.clear()\n\t\tself._event_queue.put_nowait(\"file_change\")\n\n\t\t# Schedule the task in the event loop\n\t\tasyncio.run_coroutine_threadsafe(self._process_events(), self._event_loop)\n\n\tasync def _process_events(self) -&gt; None:\n\t\t\"\"\"Process events from the queue in the event loop's context.\"\"\"\n\t\tif self._processing:\n\t\t\treturn\n\n\t\tself._processing = True\n\t\ttry:\n\t\t\t# Get an event from the queue\n\t\t\twhile not self._event_queue.empty():\n\t\t\t\t_ = self._event_queue.get_nowait()\n\n\t\t\t\t# Cancel any existing debounce task\n\t\t\t\tif self._debounce_task and not self._debounce_task.done():\n\t\t\t\t\tself._debounce_task.cancel()\n\t\t\t\t\tlogger.debug(\"Cancelled existing debounce task due to new event.\")\n\n\t\t\t\t# Create a new debounce task within the event loop's context\n\t\t\t\tlogger.debug(f\"Scheduling new debounced callback with {self.debounce_delay}s delay.\")\n\t\t\t\tself._debounce_task = self._event_loop.create_task(self._debounced_callback())\n\t\tfinally:\n\t\t\tself._processing = False\n\t\t\tself._event_processed.set()\n\n\tasync def _debounced_callback(self) -&gt; None:\n\t\t\"\"\"Wait for the debounce period and then execute the callback.\"\"\"\n\t\ttry:\n\t\t\tawait asyncio.sleep(self.debounce_delay)\n\t\t\tlogger.info(\"Debounce delay finished, triggering sync callback.\")\n\t\t\tawait self.callback()\n\t\t\tself._last_event_time = time.monotonic()  # Update time after successful execution\n\t\t\tlogger.debug(\"Watcher callback executed successfully.\")\n\t\texcept asyncio.CancelledError:\n\t\t\tlogger.debug(\"Debounce task cancelled before execution.\")\n\t\t\t# Do not run the callback if cancelled\n\t\texcept Exception:\n\t\t\tlogger.exception(\"Error executing watcher callback\")\n\t\tfinally:\n\t\t\t# Clear the task reference once it's done\n\t\t\tself._debounce_task = None\n\n\tdef on_any_event(self, event: FileSystemEvent) -&gt; None:\n\t\t\"\"\"\n\t\tCatch all events and schedule the callback after debouncing.\n\n\t\tArgs:\n\t\t    event: The file system event.\n\n\t\t\"\"\"\n\t\tif event.is_directory:\n\t\t\treturn  # Ignore directory events for now, focus on file changes\n\n\t\t# Log the specific event detected\n\t\tevent_type = event.event_type\n\t\tsrc_path = getattr(event, \"src_path\", \"N/A\")\n\t\tdest_path = getattr(event, \"dest_path\", \"N/A\")  # For moved events\n\n\t\tif event_type == \"moved\":\n\t\t\tlogger.debug(f\"Detected file {event_type}: {src_path} -&gt; {dest_path}\")\n\t\telse:\n\t\t\tlogger.debug(f\"Detected file {event_type}: {src_path}\")\n\n\t\t# Schedule the callback in a thread-safe way\n\t\tself._schedule_callback()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.__init__","title":"__init__","text":"<pre><code>__init__(\n\tcallback: Callable[[], Coroutine[None, None, None]],\n\tdebounce_delay: float = 1.0,\n\tevent_loop: AbstractEventLoop | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the handler.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[], Coroutine[None, None, None]]</code> <p>An async function to call when changes are detected.</p> required <code>debounce_delay</code> <code>float</code> <p>Minimum time (seconds) between callback triggers.</p> <code>1.0</code> <code>event_loop</code> <code>AbstractEventLoop | None</code> <p>The asyncio event loop to use, or None to use the current one.</p> <code>None</code> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def __init__(\n\tself,\n\tcallback: Callable[[], Coroutine[None, None, None]],\n\tdebounce_delay: float = 1.0,\n\tevent_loop: asyncio.AbstractEventLoop | None = None,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the handler.\n\n\tArgs:\n\t    callback: An async function to call when changes are detected.\n\t    debounce_delay: Minimum time (seconds) between callback triggers.\n\t    event_loop: The asyncio event loop to use, or None to use the current one.\n\n\t\"\"\"\n\tself.callback = callback\n\tself.debounce_delay = debounce_delay\n\tself._last_event_time: float = 0\n\tself._debounce_task: asyncio.Task | None = None\n\t# Set up a thread-safe way to communicate with the event loop\n\tself._event_queue = queue.Queue()\n\tself._event_processed = threading.Event()\n\t# Store or get the event loop\n\tself._event_loop = event_loop or asyncio.get_event_loop()\n\t# Flag to track if we're in the process of handling an event\n\tself._processing = False\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.callback","title":"callback  <code>instance-attribute</code>","text":"<pre><code>callback = callback\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.debounce_delay","title":"debounce_delay  <code>instance-attribute</code>","text":"<pre><code>debounce_delay = debounce_delay\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.FileChangeHandler.on_any_event","title":"on_any_event","text":"<pre><code>on_any_event(event: FileSystemEvent) -&gt; None\n</code></pre> <p>Catch all events and schedule the callback after debouncing.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>FileSystemEvent</code> <p>The file system event.</p> required Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def on_any_event(self, event: FileSystemEvent) -&gt; None:\n\t\"\"\"\n\tCatch all events and schedule the callback after debouncing.\n\n\tArgs:\n\t    event: The file system event.\n\n\t\"\"\"\n\tif event.is_directory:\n\t\treturn  # Ignore directory events for now, focus on file changes\n\n\t# Log the specific event detected\n\tevent_type = event.event_type\n\tsrc_path = getattr(event, \"src_path\", \"N/A\")\n\tdest_path = getattr(event, \"dest_path\", \"N/A\")  # For moved events\n\n\tif event_type == \"moved\":\n\t\tlogger.debug(f\"Detected file {event_type}: {src_path} -&gt; {dest_path}\")\n\telse:\n\t\tlogger.debug(f\"Detected file {event_type}: {src_path}\")\n\n\t# Schedule the callback in a thread-safe way\n\tself._schedule_callback()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher","title":"Watcher","text":"<p>Monitors a directory for changes and triggers a callback.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>class Watcher:\n\t\"\"\"Monitors a directory for changes and triggers a callback.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tpath_to_watch: str | Path,\n\t\ton_change_callback: Callable[[], Coroutine[None, None, None]],\n\t\tdebounce_delay: float = 1.0,\n\t) -&gt; None:\n\t\t\"\"\"\n\t\tInitialize the watcher.\n\n\t\tArgs:\n\t\t    path_to_watch: The directory path to monitor.\n\t\t    on_change_callback: Async function to call upon detecting changes.\n\t\t    debounce_delay: Delay in seconds to avoid rapid firing of callbacks.\n\n\t\t\"\"\"\n\t\tself.observer = Observer()\n\t\tself.path_to_watch = Path(path_to_watch).resolve()\n\t\tif not self.path_to_watch.is_dir():\n\t\t\tmsg = f\"Path to watch must be a directory: {self.path_to_watch}\"\n\t\t\traise ValueError(msg)\n\n\t\t# Save the current event loop to use for callbacks\n\t\ttry:\n\t\t\tself._event_loop = asyncio.get_event_loop()\n\t\texcept RuntimeError:\n\t\t\t# If we're not in an event loop context, create a new one\n\t\t\tself._event_loop = asyncio.new_event_loop()\n\t\t\tasyncio.set_event_loop(self._event_loop)\n\n\t\tself.event_handler = FileChangeHandler(on_change_callback, debounce_delay, event_loop=self._event_loop)\n\t\tself._stop_event = anyio.Event()  # Initialize the event\n\n\tasync def start(self) -&gt; None:\n\t\t\"\"\"Start monitoring the directory.\"\"\"\n\t\tif not self.path_to_watch.exists():\n\t\t\tlogger.warning(f\"Watch path {self.path_to_watch} does not exist. Creating it.\")\n\t\t\tself.path_to_watch.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n\n\t\tself.observer.schedule(self.event_handler, str(self.path_to_watch), recursive=True)\n\t\tself.observer.start()\n\t\tlogger.info(f\"Started watching directory: {self.path_to_watch}\")\n\t\ttry:\n\t\t\t# Wait until the stop event is set\n\t\t\tawait self._stop_event.wait()\n\t\texcept KeyboardInterrupt:\n\t\t\tlogger.info(\"Watcher stopped by user (KeyboardInterrupt).\")\n\t\tfinally:\n\t\t\t# Ensure stop is called regardless of how wait() exits\n\t\t\tself.stop()\n\n\tdef stop(self) -&gt; None:\n\t\t\"\"\"Stop monitoring the directory.\"\"\"\n\t\tif self.observer.is_alive():\n\t\t\tself.observer.stop()\n\t\t\tself.observer.join()  # Wait for observer thread to finish\n\t\t\tlogger.info(\"Watchdog observer stopped.\")\n\t\t# Set the event to signal the start method to exit\n\t\tself._stop_event.set()\n\t\tlogger.info(\"Watcher stop event set.\")\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.__init__","title":"__init__","text":"<pre><code>__init__(\n\tpath_to_watch: str | Path,\n\ton_change_callback: Callable[\n\t\t[], Coroutine[None, None, None]\n\t],\n\tdebounce_delay: float = 1.0,\n) -&gt; None\n</code></pre> <p>Initialize the watcher.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_watch</code> <code>str | Path</code> <p>The directory path to monitor.</p> required <code>on_change_callback</code> <code>Callable[[], Coroutine[None, None, None]]</code> <p>Async function to call upon detecting changes.</p> required <code>debounce_delay</code> <code>float</code> <p>Delay in seconds to avoid rapid firing of callbacks.</p> <code>1.0</code> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def __init__(\n\tself,\n\tpath_to_watch: str | Path,\n\ton_change_callback: Callable[[], Coroutine[None, None, None]],\n\tdebounce_delay: float = 1.0,\n) -&gt; None:\n\t\"\"\"\n\tInitialize the watcher.\n\n\tArgs:\n\t    path_to_watch: The directory path to monitor.\n\t    on_change_callback: Async function to call upon detecting changes.\n\t    debounce_delay: Delay in seconds to avoid rapid firing of callbacks.\n\n\t\"\"\"\n\tself.observer = Observer()\n\tself.path_to_watch = Path(path_to_watch).resolve()\n\tif not self.path_to_watch.is_dir():\n\t\tmsg = f\"Path to watch must be a directory: {self.path_to_watch}\"\n\t\traise ValueError(msg)\n\n\t# Save the current event loop to use for callbacks\n\ttry:\n\t\tself._event_loop = asyncio.get_event_loop()\n\texcept RuntimeError:\n\t\t# If we're not in an event loop context, create a new one\n\t\tself._event_loop = asyncio.new_event_loop()\n\t\tasyncio.set_event_loop(self._event_loop)\n\n\tself.event_handler = FileChangeHandler(on_change_callback, debounce_delay, event_loop=self._event_loop)\n\tself._stop_event = anyio.Event()  # Initialize the event\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.observer","title":"observer  <code>instance-attribute</code>","text":"<pre><code>observer = Observer()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.path_to_watch","title":"path_to_watch  <code>instance-attribute</code>","text":"<pre><code>path_to_watch = resolve()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.event_handler","title":"event_handler  <code>instance-attribute</code>","text":"<pre><code>event_handler = FileChangeHandler(\n\ton_change_callback,\n\tdebounce_delay,\n\tevent_loop=_event_loop,\n)\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start monitoring the directory.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>async def start(self) -&gt; None:\n\t\"\"\"Start monitoring the directory.\"\"\"\n\tif not self.path_to_watch.exists():\n\t\tlogger.warning(f\"Watch path {self.path_to_watch} does not exist. Creating it.\")\n\t\tself.path_to_watch.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n\n\tself.observer.schedule(self.event_handler, str(self.path_to_watch), recursive=True)\n\tself.observer.start()\n\tlogger.info(f\"Started watching directory: {self.path_to_watch}\")\n\ttry:\n\t\t# Wait until the stop event is set\n\t\tawait self._stop_event.wait()\n\texcept KeyboardInterrupt:\n\t\tlogger.info(\"Watcher stopped by user (KeyboardInterrupt).\")\n\tfinally:\n\t\t# Ensure stop is called regardless of how wait() exits\n\t\tself.stop()\n</code></pre>"},{"location":"api/watcher/file_watcher/#codemap.watcher.file_watcher.Watcher.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop monitoring the directory.</p> Source code in <code>src/codemap/watcher/file_watcher.py</code> <pre><code>def stop(self) -&gt; None:\n\t\"\"\"Stop monitoring the directory.\"\"\"\n\tif self.observer.is_alive():\n\t\tself.observer.stop()\n\t\tself.observer.join()  # Wait for observer thread to finish\n\t\tlogger.info(\"Watchdog observer stopped.\")\n\t# Set the event to signal the start method to exit\n\tself._stop_event.set()\n\tlogger.info(\"Watcher stop event set.\")\n</code></pre>"},{"location":"contributing/","title":"Development Setup","text":"<p>Before contributing, please read our Code of Conduct and Contributing Guidelines.</p> <ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/SarthakMishra/codemap.git\ncd codemap\n</code></pre> </li> <li> <p>Install Prerequisites:</p> <ul> <li>Task: Follow the official installation guide: https://taskfile.dev/installation/</li> <li> <p>uv: Install the <code>uv</code> package manager. We recommend using <code>pipx</code>:</p> <pre><code># Using pipx (recommended)\npipx install uv\n\n# Or using pip\n# pip install uv\n</code></pre> </li> <li> <p>Python: Ensure you have Python 3.12 or later installed.</p> </li> </ul> </li> <li> <p>Set up the Virtual Environment: <pre><code># Create a virtual environment using uv (creates .venv directory)\nuv venv\n\n# Activate the virtual environment\n# On Linux/macOS (bash/zsh):\nsource .venv/bin/activate\n# On Windows (Command Prompt):\n# .venv\\Scripts\\activate.bat\n# On Windows (PowerShell):\n# .venv\\Scripts\\Activate.ps1\n</code></pre></p> </li> <li> <p>Install Dependencies:     Install project dependencies, including development tools, using <code>uv</code>:     <pre><code># Installs dependencies from pyproject.toml including the 'dev' group\nuv sync --dev\n</code></pre></p> </li> <li> <p>Verify Setup:     You can list available development tasks using Task:     <pre><code>task -l\n</code></pre>     To run all checks and tests (similar to CI):     <pre><code>task ci\n</code></pre></p> </li> </ol> <p>For detailed contribution guidelines, branching strategy, and coding standards, please refer to our Contributing Guide. </p>"},{"location":"contributing/code-of-conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributing/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address, without their explicit permission</li> <li>Contacting individual members, contributors, or leaders privately, outside designated community mechanisms, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"contributing/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project maintainers. Please refer to the Contributing Guide for contact information if needed. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations. </p>"},{"location":"contributing/guidelines/","title":"Contributing to CodeMap","text":"<p>First off, thank you for considering contributing to CodeMap! We welcome contributions from everyone, and we're excited to see how you can help make this AI-powered developer toolkit even better.</p> <p>This document provides guidelines for contributing to the project. Please read it carefully to ensure a smooth and effective contribution process.</p>"},{"location":"contributing/guidelines/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Contributing to CodeMap</li> <li>Table of Contents</li> <li>How Can I Contribute?<ul> <li>Reporting Bugs</li> <li>Suggesting Enhancements</li> <li>Code Contributions</li> </ul> </li> <li>Getting Started</li> <li>Branching Strategy (Simplified Git Flow)<ul> <li>Core Branches</li> <li>Supporting Branches</li> <li>Workflow Examples</li> </ul> </li> <li>Code Contribution Workflow</li> <li>Coding Standards</li> <li>Testing</li> <li>Commit Message Guidelines</li> <li>Pull Request Process</li> <li>Release Process<ul> <li>Automatic Releases</li> <li>Release Preparation</li> <li>Hotfix Process</li> </ul> </li> <li>Questions?</li> </ul>"},{"location":"contributing/guidelines/#how-can-i-contribute","title":"How Can I Contribute?","text":""},{"location":"contributing/guidelines/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you encounter a bug, please help us by reporting it!</p> <ol> <li>Check Existing Issues: Before creating a new issue, please search the GitHub Issues to see if the bug has already been reported.</li> <li>Create a New Issue: If the bug hasn't been reported, create a new issue. Please include:<ul> <li>A clear and descriptive title.</li> <li>Your operating system and Python version.</li> <li>Steps to reproduce the bug reliably.</li> <li>What you expected to happen.</li> <li>What actually happened (including any error messages or tracebacks).</li> <li>Screenshots or code snippets if relevant.</li> </ul> </li> </ol>"},{"location":"contributing/guidelines/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>We welcome suggestions for new features or improvements to existing ones.</p> <ol> <li>Check Existing Issues/Discussions: Search the GitHub Issues and Discussions to see if your idea has already been proposed.</li> <li>Create a New Issue/Discussion: If not, open a new issue or start a discussion thread. Describe:<ul> <li>The enhancement you're proposing.</li> <li>The problem it solves or the use case it addresses.</li> <li>Any potential implementation ideas (optional).</li> </ul> </li> </ol>"},{"location":"contributing/guidelines/#code-contributions","title":"Code Contributions","text":"<p>If you'd like to contribute code (bug fixes, new features), please follow the workflow outlined below.</p>"},{"location":"contributing/guidelines/#getting-started","title":"Getting Started","text":"<p>Before you start coding, make sure you have set up your development environment correctly by following the Development Setup Guide.</p>"},{"location":"contributing/guidelines/#branching-strategy-simplified-git-flow","title":"Branching Strategy (Simplified Git Flow)","text":"<p>We use a simplified Git Flow model to manage branches and releases, with automated releases powered by Python Semantic Release.</p> <pre><code>gitGraph\n    commit\n    branch dev\n    checkout dev\n    commit\n\n    branch feature/new-feature\n    checkout feature/new-feature\n    commit\n    commit\n    checkout dev\n    merge feature/new-feature tag: \"v0.2.0-next.1\"\n\n    branch feature/another-feature\n    checkout feature/another-feature\n    commit\n    checkout dev\n    merge feature/another-feature tag: \"v0.2.0-next.2\"\n\n    branch release/v0.2.0\n    checkout release/v0.2.0\n    commit\n    checkout main\n    merge release/v0.2.0 tag: \"v0.2.0\"\n    checkout dev\n    merge main\n\n    branch hotfix/critical-fix\n    checkout hotfix/critical-fix\n    commit\n    checkout main\n    merge hotfix/critical-fix tag: \"v0.2.1\"\n    checkout dev\n    merge main\n</code></pre>"},{"location":"contributing/guidelines/#core-branches","title":"Core Branches","text":"<ul> <li><code>main</code>:<ul> <li>Represents the latest stable production-ready release.</li> <li>Pushes to <code>main</code> trigger automatic stable version releases.</li> <li>Protected branch with required reviews. Changes come via approved PRs from <code>release/*</code> or <code>hotfix/*</code> branches.</li> </ul> </li> <li><code>dev</code>:<ul> <li>The primary integration branch for ongoing development and upcoming features.</li> <li>Pushes to <code>dev</code> trigger automatic pre-release versions with the <code>-next</code> tag.</li> <li>All feature branches are merged into <code>dev</code>.</li> <li>Continuously tested via CI.</li> </ul> </li> </ul>"},{"location":"contributing/guidelines/#supporting-branches","title":"Supporting Branches","text":"<ul> <li>Feature branches (<code>feature/*</code>):<ul> <li>Branched off <code>dev</code>.</li> <li>Used for developing new features or significant changes.</li> <li>Named descriptively (e.g., <code>feature/add-pr-update-command</code>).</li> <li>Merged back into <code>dev</code> via Pull Requests (PRs).</li> </ul> </li> <li>Release branches (<code>release/*</code>):<ul> <li>Branched off <code>dev</code> when preparing for a new stable release.</li> <li>Used for final testing, documentation updates, and version stabilization.</li> <li>Format: <code>release/vX.Y.0</code> (e.g., <code>release/v1.2.0</code>).</li> <li>Merged into <code>main</code> via PR, which triggers automatic release.</li> <li>No need for manual version bumping as this is handled by semantic-release.</li> </ul> </li> <li>Hotfix branches (<code>hotfix/*</code>):<ul> <li>Branched off <code>main</code>.</li> <li>Used for critical bug fixes needed in the production version.</li> <li>Merged into <code>main</code> via PR, triggering automatic patch release.</li> <li>Also merged back into <code>dev</code> (usually by merging the updated <code>main</code>).</li> </ul> </li> </ul>"},{"location":"contributing/guidelines/#workflow-examples","title":"Workflow Examples","text":"<ol> <li> <p>New Feature Development:</p> <pre><code># Start from the dev branch\ngit checkout dev\ngit pull origin dev\n\n# Create your feature branch\ngit checkout -b feature/your-feature-name\n\n# --- Make your changes ---\n\n# Push your feature branch\ngit push -u origin feature/your-feature-name\n\n# Open a Pull Request to merge `feature/your-feature-name` into `dev`\n# When merged, a new pre-release version may be created automatically\n</code></pre> </li> <li> <p>Release Preparation:</p> <pre><code>git checkout dev\ngit pull origin dev\n\n# Create a release branch (no need to bump versions manually)\ngit checkout -b release/v1.3.0\n\n# Make any final adjustments, documentation updates, etc.\n# Push the release branch\ngit push -u origin release/v1.3.0\n\n# Create a PR from release/v1.3.0 to main\n# When the PR is approved and merged:\n# 1. A new release will be automatically created\n# 2. The package will be built and published to PyPI\n# 3. Main should be merged back to dev to sync the version changes\ngit checkout dev\ngit pull origin dev\ngit merge origin/main\ngit push origin dev\n</code></pre> </li> <li> <p>Hotfix Process:     <pre><code>git checkout main\ngit pull origin main\n\n# Create a hotfix branch\ngit checkout -b hotfix/critical-bug-fix\n\n# Fix the bug and commit using conventional commit format\n# (preferably using `codemap commit`)\n\n# Push the hotfix branch\ngit push -u origin hotfix/critical-bug-fix\n\n# Create a PR from hotfix/critical-bug-fix to main\n# When merged, a patch release will be automatically created\n\n# After the hotfix is released, sync changes back to dev\ngit checkout dev\ngit pull origin dev\ngit merge origin/main\ngit push origin dev\n</code></pre></p> </li> </ol>"},{"location":"contributing/guidelines/#code-contribution-workflow","title":"Code Contribution Workflow","text":"<ol> <li>Fork &amp; Clone: Fork the repository on GitHub and clone your fork locally.     <pre><code>git clone https://github.com/YOUR_USERNAME/codemap.git\ncd codemap\ngit remote add upstream https://github.com/SarthakMishra/codemap.git\n</code></pre></li> <li>Setup: Follow the Development Setup instructions.</li> <li>Branch: Create a new branch based on the correct base branch (<code>dev</code> for features/improvements, <code>main</code> only for agreed-upon hotfixes).     <pre><code># For features/improvements\ngit checkout dev\ngit pull upstream dev # Keep dev up-to-date\ngit checkout -b feature/your-descriptive-name\n\n# For hotfixes (usually maintainers)\n# git checkout main\n# git pull upstream main\n# git checkout -b hotfix/your-fix-name\n</code></pre></li> <li>Code: Make your changes. Write clean, well-commented code. Add or update tests as necessary.</li> <li>Format &amp; Lint: Ensure your code adheres to the project's style guidelines.     <pre><code>task format\ntask lint\n# Or run all checks\ntask ci\n</code></pre></li> <li>Test: Run the test suite to ensure your changes haven't broken anything.     <pre><code>task test\n# Check coverage\ntask coverage\n</code></pre></li> <li>Commit: Commit your changes using meaningful commit messages. We strongly encourage using the <code>codemap commit</code> command to generate conventional commit messages.     <pre><code># Stage your changes\ngit add .\n# Use the interactive commit tool\ncodemap commit\n# Or if you prefer manual commits, follow conventional commit format\n# git commit -m \"feat(cli): add option for custom output format\"\n</code></pre></li> <li>Push: Push your branch to your fork.     <pre><code>git push -u origin feature/your-descriptive-name\n</code></pre></li> <li>Pull Request: Open a Pull Request (PR) from your fork's branch to the <code>upstream/dev</code> branch (or <code>upstream/main</code> for hotfixes). Provide a clear description of your changes.</li> </ol>"},{"location":"contributing/guidelines/#coding-standards","title":"Coding Standards","text":"<ul> <li>Follow PEP 8 for Python code.</li> <li>Use type hints (<code>typing</code> module).</li> <li>Write docstrings for public modules, classes, and functions (see project docs rules).</li> <li>Use <code>ruff</code> for linting and formatting (<code>task format</code>, <code>task lint</code>).</li> </ul>"},{"location":"contributing/guidelines/#testing","title":"Testing","text":"<ul> <li>Write tests using <code>pytest</code>.</li> <li>Aim for good test coverage (<code>task coverage</code>).</li> <li>Ensure all tests pass (<code>task test</code>) before submitting a PR.</li> </ul>"},{"location":"contributing/guidelines/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>We follow the Conventional Commits specification.</p> <ul> <li>Format: <code>&lt;type&gt;[optional scope]: &lt;description&gt;</code></li> <li>Example: <code>feat(commit): add semantic diff splitting strategy</code></li> <li>Use <code>codemap commit</code>: The easiest way to ensure compliance is to use the built-in <code>codemap commit</code> command.</li> </ul>"},{"location":"contributing/guidelines/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure all CI checks (linting, testing) pass.</li> <li>Provide a clear title and description for your PR.</li> <li>Link any related issues.</li> <li>Request reviews from maintainers.</li> <li>Address any feedback promptly.</li> <li>Once approved, a maintainer will merge the PR.</li> </ol>"},{"location":"contributing/guidelines/#release-process","title":"Release Process","text":"<p>Releases are managed automatically using Python Semantic Release.</p>"},{"location":"contributing/guidelines/#automatic-releases","title":"Automatic Releases","text":"<ul> <li>Merging a PR into <code>dev</code> may trigger a pre-release (e.g., <code>v1.2.0-next.1</code>).</li> <li>Merging a PR from a <code>release/*</code> or <code>hotfix/*</code> branch into <code>main</code> will trigger a stable release (e.g., <code>v1.2.0</code> or <code>v1.2.1</code>).</li> <li>The release process includes:<ul> <li>Bumping the version based on commit messages.</li> <li>Generating a changelog.</li> <li>Tagging the commit in Git.</li> <li>Creating a GitHub Release.</li> <li>Building the package.</li> <li>Publishing to PyPI.</li> </ul> </li> </ul>"},{"location":"contributing/guidelines/#release-preparation","title":"Release Preparation","text":"<p>Maintainers will create <code>release/*</code> branches off <code>dev</code> when ready to stabilize for a release. This branch allows for final testing and documentation updates before merging to <code>main</code>.</p>"},{"location":"contributing/guidelines/#hotfix-process","title":"Hotfix Process","text":"<p>Critical bugs in <code>main</code> are fixed using <code>hotfix/*</code> branches, which are merged directly back into <code>main</code> to trigger a patch release.</p>"},{"location":"contributing/guidelines/#questions","title":"Questions?","text":"<p>If you have questions, feel free to open an issue or start a discussion on GitHub. </p>"},{"location":"usage/","title":"Usage Overview","text":"<p>This section covers the main commands provided by CodeMap:</p> <ul> <li>Generate Docs (<code>gen</code>): Learn how to create optimized documentation for your codebase.</li> <li>Smart Commit (<code>commit</code>): Discover how to use AI assistance for crafting meaningful Git commit messages.</li> <li>Pull Requests (<code>pr</code>): See how CodeMap helps streamline the creation and management of pull requests. </li> </ul>"},{"location":"usage/commit/","title":"Smart Commit (<code>commit</code>)","text":"<p>Create intelligent Git commits with AI-assisted message generation. The tool analyzes your changes, splits them into logical chunks, and generates meaningful commit messages using LLMs.</p>"},{"location":"usage/commit/#basic-usage","title":"Basic Usage","text":"<pre><code># Basic usage with default settings (interactive, semantic splitting)\ncodemap commit\n# Or using the alias:\ncm commit\n\n# Commit with a specific message (skips AI generation)\ncodemap commit -m \"feat: add new feature\"\n\n# Commit all changes (including untracked files)\ncodemap commit -a\n\n# Use a specific LLM model\ncodemap commit --model groq/llama-3.1-8b-instant\n\n# Bypass git hooks (e.g., pre-commit)\ncodemap commit --bypass-hooks\n</code></pre>"},{"location":"usage/commit/#command-options","title":"Command Options","text":"<pre><code>codemap commit [PATH] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATH</code>: Path to repository or specific file to commit (defaults to current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--message</code>, <code>-m</code>: Specify a commit message directly (skips AI generation)</li> <li><code>--all</code>, <code>-a</code>: Commit all changes (stages untracked files)</li> <li><code>--model</code>: LLM model to use for message generation (default: <code>openai/gpt-4o-mini</code>). Overrides config (<code>commit.llm.model</code>).</li> <li><code>--strategy</code>, <code>-s</code>: Strategy for splitting diffs (default: <code>semantic</code>). Options: <code>file</code>, <code>hunk</code>, <code>semantic</code>. Overrides config (<code>commit.strategy</code>).</li> <li><code>--non-interactive</code>: Run in non-interactive mode (accepts all generated messages)</li> <li><code>--bypass-hooks</code>: Bypass git hooks with <code>--no-verify</code> (overrides config <code>commit.bypass_hooks</code>).</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/commit/#interactive-workflow","title":"Interactive Workflow","text":"<p>The commit command provides an interactive workflow that: 1. Analyzes your changes and splits them into logical chunks 2. Generates AI-powered commit messages for each chunk 3. Allows you to:    - Accept the generated message    - Edit the message before committing    - Regenerate the message    - Skip the chunk    - Exit the process</p>"},{"location":"usage/commit/#commit-linting-feature","title":"Commit Linting Feature","text":"<p>CodeMap includes automatic commit message linting to ensure your commit messages follow conventions:</p> <ol> <li>Automatic Validation: Generated commit messages are automatically validated against conventional commit standards.</li> <li>Linting Rules: Configurable in <code>.codemap.yml</code> (see Configuration).</li> <li>Auto-remediation: If a generated message fails linting, CodeMap attempts to regenerate a compliant message.</li> <li>Fallback Mechanism: If regeneration fails, the last message is used with linting status indicated.</li> </ol>"},{"location":"usage/commit/#commit-strategy","title":"Commit Strategy","text":"<p>The tool uses semantic analysis to group related changes together based on: - File relationships - Code content similarity - Directory structure - Common file patterns</p> <p>Note</p> <p>The semantic strategy utilizes a custom, distilled version of the <code>Qodo/Qodo-Embed-1-1.5B</code> model, named <code>Qodo-Embed-M-1-1.5B-M2V-Distilled</code>. This Model2Vec distilled model is significantly smaller (233MB vs 5.9GB) and faster (~112x) than the original while retaining ~85% of its performance. Find more details here.</p>"},{"location":"usage/commit/#environment-variables","title":"Environment Variables","text":"<p>Refer to the LLM Support page for relevant environment variables.</p>"},{"location":"usage/commit/#examples","title":"Examples","text":"<pre><code># Basic interactive commit\ncodemap commit\n\n# Commit specific files\ncodemap commit path/to/file.py\n\n# Use a specific model with custom strategy\ncodemap commit --model anthropic/claude-3-sonnet --strategy semantic\n\n# Non-interactive commit with all changes\ncodemap commit -a --non-interactive\n\n# Commit with verbose logging\ncodemap commit -v\n\n# Demonstrate automatic linting and regeneration\ncodemap commit --verbose  # Will show linting feedback and regeneration attempts\n</code></pre>"},{"location":"usage/configuration/","title":"Configuration","text":"<p>Create a <code>.codemap.yml</code> file in your project root to customize the behavior. Below are all available configuration options with their default values:</p> <pre><code># LLM configuration (applies globally unless overridden by command-specific LLM config)\nllm:\n  model: openai/gpt-4o-mini  # Default LLM model (provider/model_name format)\n  api_base: null             # Custom API base URL (e.g., for local LLMs or proxies)\n\n# Documentation Generation Settings ('gen' command)\ngen:\n  max_content_length: 5000       # Max content length per file (0 = unlimited)\n  use_gitignore: true            # Respect .gitignore patterns\n  output_dir: documentation       # Directory for generated docs (Note: mkdocs uses 'docs/')\n  include_tree: true             # Include directory tree in output\n  include_entity_graph: true     # Include Mermaid entity relationship graph\n  semantic_analysis: true        # Enable semantic analysis using LSP\n  lod_level: docs                # Level of Detail: signatures, structure, docs, full\n  mermaid_entities:              # Entity types for Mermaid graph\n    - module\n    - class\n    - function\n    - method\n    - constant\n    - variable\n    - import\n  mermaid_relationships:         # Relationship types for Mermaid graph\n    - declares\n    - imports\n    - calls\n  mermaid_show_legend: true      # Show legend in Mermaid diagram\n  mermaid_remove_unconnected: false # Remove unconnected nodes in Mermaid diagram\n\n# Processor configuration (background analysis - currently unused)\nprocessor:\n  enabled: true\n  max_workers: 4\n  ignored_patterns:\n    - \"**/.git/**\"\n    - \"**/__pycache__/**\"\n    - \"**/.venv/**\"\n    - \"**/node_modules/**\"\n    - \"**/*.pyc\"\n    - \"**/dist/**\"\n    - \"**/build/**\"\n  default_lod_level: signatures\n\n# Commit Feature Configuration ('commit' command)\ncommit:\n  strategy: semantic             # Diff splitting strategy: file, hunk, semantic\n  bypass_hooks: false            # Default for --bypass-hooks flag (--no-verify)\n\n  convention:                    # Commit convention settings (based on Conventional Commits)\n    types:                       # Allowed commit types\n      - feat\n      - fix\n      - docs\n      - style\n      - refactor\n      - perf\n      - test\n      - build\n      - ci\n      - chore\n    scopes: []                   # Optional scopes (can be auto-derived if empty)\n    max_length: 72               # Max length for commit subject line\n\n  lint:                          # Commitlint rule configuration (see https://commitlint.js.org/#/reference-rules)\n    # Example rules (full list in README)\n    header_max_length: { level: ERROR, rule: always, value: 100 }\n    type_enum: { level: ERROR, rule: always } # Uses types from commit.convention.types\n    type_case: { level: ERROR, rule: always, value: lower-case }\n    subject_empty: { level: ERROR, rule: never }\n    subject_full_stop: { level: ERROR, rule: never, value: . }\n\n# Pull Request Configuration ('pr' command)\npr:\n  defaults:\n    base_branch: null            # Default base branch (null = repo default)\n    feature_prefix: \"feature/\"   # Default prefix for feature branches\n\n  strategy: github-flow          # Git workflow: github-flow, gitflow, trunk-based\n\n  branch_mapping:                # Branch base/prefix mapping (primarily for GitFlow)\n    feature: { base: develop, prefix: \"feature/\" }\n    release: { base: main, prefix: \"release/\" }\n    hotfix: { base: main, prefix: \"hotfix/\" }\n    bugfix: { base: develop, prefix: \"bugfix/\" }\n\n  generate:                      # Content generation settings\n    title_strategy: commits      # How to generate title: commits, llm, template\n    description_strategy: commits # How to generate description: commits, llm, template\n    use_workflow_templates: true # Use built-in templates based on workflow/branch type?\n    # Template used if description_strategy is 'template' AND use_workflow_templates is false\n    description_template: |\n      ## Changes\n      {changes}\n\n      ## Testing\n      {testing_instructions}\n\n      ## Screenshots\n      {screenshots}\n</code></pre>"},{"location":"usage/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>The configuration is loaded in the following order (later sources override earlier ones):</p> <ol> <li>Default configuration from the package</li> <li><code>.codemap.yml</code> in the project root</li> <li>Custom config file specified with <code>--config</code></li> <li>Command-line arguments</li> </ol>"},{"location":"usage/configuration/#configuration-tips","title":"Configuration Tips","text":"<p>Refer to the main README section for detailed tips on configuring:</p> <ul> <li>Token Limits (Deprecated) &amp; Content Length</li> <li>Git Integration (<code>use_gitignore</code>, <code>convention.scopes</code>, <code>bypass_hooks</code>)</li> <li>LLM Settings (<code>llm.model</code>, <code>llm.api_base</code>, <code>--model</code> flag)</li> <li>Commit Conventions &amp; Linting (<code>commit.convention</code>, <code>commit.lint</code>)</li> <li>PR Workflow Settings (<code>pr.strategy</code>, <code>pr.defaults</code>, <code>pr.branch_mapping</code>, <code>pr.generate</code>)</li> <li>Documentation Generation (<code>gen.*</code> settings and flags)</li> </ul>"},{"location":"usage/configuration/#environment-variables","title":"Environment Variables","text":"<p>LLM API keys and optional base URLs can be set via environment variables. See the LLM Support page for details. </p>"},{"location":"usage/generate/","title":"Generate Markdown Docs (<code>gen</code>)","text":"<p>Generate optimized markdown documentation and directory structures for your project:</p>"},{"location":"usage/generate/#command-options","title":"Command Options","text":"<pre><code>codemap gen [PATH] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATH</code>: Path to the codebase to analyze (defaults to current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--output</code>, <code>-o</code>: Output file path for the documentation (overrides config)</li> <li><code>--config</code>, <code>-c</code>: Path to custom configuration file</li> <li><code>--max-content-length</code>: Maximum content length for file display (set to 0 for unlimited, overrides config)</li> <li><code>--lod</code>: Level of Detail for code analysis (signatures, structure, docs, full). Default: <code>docs</code>. Overrides config.</li> <li><code>--semantic</code>/<code>--no-semantic</code>: Enable/disable semantic analysis using LSP. Default: enabled. Overrides config.</li> <li><code>--tree</code>/<code>--no-tree</code>: Include/exclude directory tree in output. Overrides config (<code>gen.include_tree</code>).</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> <li><code>--process</code>/<code>--no-process</code>: Process the codebase before generation. Default: enabled.</li> <li><code>--entity-graph</code>/<code>--no-entity-graph</code>: Include/exclude entity relationship graph (Mermaid) in output. Overrides config (<code>gen.include_entity_graph</code>).</li> <li><code>--mermaid-entities</code>: Comma-separated list of entity types (e.g., 'module,class,function'). Overrides config (<code>gen.mermaid_entities</code>).</li> <li><code>--mermaid-relationships</code>: Comma-separated list of relationship types (e.g., 'declares,imports,calls'). Overrides config (<code>gen.mermaid_relationships</code>).</li> <li><code>--mermaid-legend</code>/<code>--no-mermaid-legend</code>: Show/hide the legend in the Mermaid diagram. Overrides config (<code>gen.mermaid_show_legend</code>).</li> <li><code>--mermaid-unconnected</code>/<code>--no-mermaid-unconnected</code>: Remove/keep nodes with no connections in the Mermaid diagram. Overrides config (<code>gen.mermaid_remove_unconnected</code>).</li> </ul>"},{"location":"usage/generate/#examples","title":"Examples","text":"<pre><code># Generate documentation for current directory using defaults\ncodemap gen\n# Or using the alias:\ncm gen\n\n# Generate for a specific path with full detail and no semantic analysis\ncodemap gen /path/to/project --lod full --no-semantic\n\n# Generate docs with signatures only and custom Mermaid settings\ncm gen --lod signatures --mermaid-entities \"class,function\" --mermaid-relationships \"calls\"\n\n# Generate only directory tree (implicitly disables entity graph)\ncodemap gen --tree --no-entity-graph\n\n# Custom output location and content length\ncodemap gen -o ./docs/codebase.md --max-content-length 1500\n\n# Use custom configuration file\ncodemap gen --config custom-config.yml\n\n# Verbose mode for debugging\ncodemap gen -v\n</code></pre>"},{"location":"usage/generate/#output-structure","title":"Output Structure","text":"<p>The generated documentation includes: 1. Project overview and structure 2. Directory tree visualization 3. Token-optimized code summaries 4. File relationships and dependencies 5. Rich markdown formatting with syntax highlighting</p>"},{"location":"usage/generate/#file-processing","title":"File Processing","text":"<p>The generator: - Respects <code>.gitignore</code> patterns by default - Intelligently analyzes code structure - Optimizes content for token limits - Generates well-structured markdown - Handles various file types and languages </p>"},{"location":"usage/llm-support/","title":"LLM Provider Support","text":"<p>CodeMap supports multiple LLM providers through LiteLLM.</p> <p>You can specify the desired model using the <code>--model</code> option in the <code>commit</code> and <code>pr</code> commands, or set a default in the Configuration.</p>"},{"location":"usage/llm-support/#examples","title":"Examples","text":"<pre><code># Using OpenAI (default)\ncodemap commit --model openai/gpt-4o-mini\n# Or using the alias:\ncm commit --model openai/gpt-4o-mini\n\n# Using Anthropic\ncodemap commit --model anthropic/claude-3-sonnet-20240229\n\n# Using Groq (recommended for speed)\ncodemap commit --model groq/llama-3.1-8b-instant\n\n# Using OpenRouter\ncodemap commit --model openrouter/meta-llama/llama-3-8b-instruct\n</code></pre>"},{"location":"usage/llm-support/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables are needed to authenticate with the respective LLM providers. You can set these in your system environment or place them in a <code>.env</code> or <code>.env.local</code> file in your project root.</p> <pre><code># LLM Provider API Keys\nOPENAI_API_KEY=your_key_here\nANTHROPIC_API_KEY=your_key_here\nGROQ_API_KEY=your_key_here\nMISTRAL_API_KEY=your_key_here\nCOHERE_API_KEY=your_key_here\nTOGETHER_API_KEY=your_key_here\nOPENROUTER_API_KEY=your_key_here\n\n# Optional: Custom API Base URLs (for proxies or self-hosted models)\nOPENAI_API_BASE=your_custom_url\nANTHROPIC_API_BASE=your_custom_url\n# ... add others as needed ...\n</code></pre>"},{"location":"usage/pr/","title":"Pull Requests (<code>pr</code>)","text":"<p>The <code>codemap pr</code> command helps you create and manage pull requests with ease. It integrates with the existing <code>codemap commit</code> command to provide a seamless workflow from code changes to pull request creation.</p>"},{"location":"usage/pr/#pr-command-features","title":"PR Command Features","text":"<ul> <li>Create branches with intelligent naming based on your current changes</li> <li>Support for multiple Git workflow strategies (GitHub Flow, GitFlow, Trunk-Based)</li> <li>Rich branch visualization with metadata and relationships</li> <li>Smart base branch selection based on branch type</li> <li>Automatic content generation for different PR types (feature, release, hotfix)</li> <li>Workflow-specific PR templates based on branch type</li> <li>Interactive PR content editing with previews</li> <li>Update existing PRs with new commits</li> <li>Configurable via <code>.codemap.yml</code> for team-wide settings (see Configuration)</li> </ul>"},{"location":"usage/pr/#pr-command-requirements","title":"PR Command Requirements","text":"<ul> <li>Git repository with a remote named <code>origin</code></li> <li>GitHub CLI (<code>gh</code>) installed for PR creation and management</li> <li>Valid GitHub authentication for the <code>gh</code> CLI</li> </ul>"},{"location":"usage/pr/#creating-a-pr","title":"Creating a PR","text":"<pre><code>codemap pr create [PATH] [OPTIONS]\n# Or using the alias:\ncm pr create [PATH] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATH</code>: Path to the codebase to analyze (defaults to current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--branch</code>, <code>-b</code>: Target branch name</li> <li><code>--type</code>, <code>-t</code>: Branch type (e.g., feature, release, hotfix, bugfix). Valid types depend on workflow strategy.</li> <li><code>--base</code>: Base branch for the PR (defaults to repo default or workflow-defined default)</li> <li><code>--title</code>: Pull request title</li> <li><code>--desc</code>, <code>-d</code>: Pull request description (file path or text)</li> <li><code>--no-commit</code>: Skip the commit process before creating PR</li> <li><code>--force-push</code>, <code>-f</code>: Force push the branch</li> <li><code>--workflow</code>, <code>-w</code>: Git workflow strategy (github-flow, gitflow, trunk-based). Overrides config (<code>pr.strategy</code>).</li> <li><code>--non-interactive</code>: Run in non-interactive mode</li> <li><code>--model</code>, <code>-m</code>: LLM model for content generation (overrides config <code>llm.model</code>).</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul>"},{"location":"usage/pr/#updating-a-pr","title":"Updating a PR","text":"<pre><code>codemap pr update [PATH] [OPTIONS]\n# Or using the alias:\ncm pr update [PATH] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATH</code>: Path to the codebase to analyze (defaults to current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--pr</code>: PR number to update (required if not updating PR for current branch)</li> <li><code>--title</code>: New PR title</li> <li><code>--desc</code>, <code>-d</code>: New PR description (file path or text)</li> <li><code>--force-push</code>, <code>-f</code>: Force push the branch (use with caution)</li> <li><code>--non-interactive</code>: Run in non-interactive mode</li> <li><code>--verbose</code>, <code>-v</code>: Enable verbose logging</li> </ul> <p>Warning</p> <p>--no-commit is NOT an option for 'update'</p>"},{"location":"usage/pr/#git-workflow-strategies","title":"Git Workflow Strategies","text":"<p>The PR command supports multiple Git workflow strategies:</p> <ol> <li>GitHub Flow (default)</li> <li>Simple, linear workflow</li> <li> <p>Feature branches merge directly to main</p> </li> <li> <p>GitFlow</p> </li> <li>Feature branches \u2192 develop</li> <li>Release branches \u2192 main</li> <li> <p>Hotfix branches \u2192 main (with back-merge to develop)</p> </li> <li> <p>Trunk-Based Development</p> </li> <li>Short-lived feature branches</li> <li>Emphasizes small, frequent PRs</li> </ol>"},{"location":"usage/pr/#pr-template-system","title":"PR Template System","text":"<p>CodeMap includes a robust PR template system that automatically generates appropriate titles and descriptions based on the selected workflow strategy, branch type, and changes being made. See the Configuration page for details on customizing templates.</p>"},{"location":"usage/pr/#examples","title":"Examples","text":"<pre><code># Create PR using workflow-specific templates (GitFlow)\ncodemap pr create --workflow gitflow --type feature\n\n# Create PR with custom title but workflow-based description\ncodemap pr create --title \"My Custom Title\" --workflow trunk-based\n\n# Override both the workflow template and use custom description\ncodemap pr create --desc \"Custom description with **markdown** support\"\n\n# Non-interactive PR creation with defined template usage\ncodemap pr create --non-interactive --workflow gitflow --type release\n</code></pre>"}]}