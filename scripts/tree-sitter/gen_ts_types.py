"""Generate literals.py with lists for supported languages, node types, and all supported extensions."""

import json
import sys
from collections import defaultdict
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

from rich import print as rprint
from rich.console import Console
from tree_sitter_language_pack import SupportedLanguage

# Add the project root to path so we can import from scripts
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))
from scripts.utils import escape_string

NODE_TYPES_DIR = Path("src/codemap/processor/tree_sitter/schema/languages/json/languages")
EXT_MAP_PATH = Path("src/codemap/processor/tree_sitter/schema/languages/json/extension-map.json")
LITERALS_PY_PATH = Path("src/codemap/processor/tree_sitter/schema/languages/literals.py")
SCRIPT_PATH = "scripts/analyze_node_types.py"

console = Console()


def extract_root_types(json_path: Path) -> set[str]:
	"""Extract unique root node types from a node-types.json file."""
	with json_path.open() as f:
		data = json.load(f)
	return {entry["type"] for entry in data if "type" in entry}


def main() -> None:
	"""Analyze and print a summary of node types and skipped files among all node-types.json files."""
	all_files = list(NODE_TYPES_DIR.glob("*.json"))
	valid_langs = {normalize_language_name(lang) for lang in SupportedLanguage.__args__}
	processed = []
	skipped = []
	root_types: dict[str, set[str]] = {}
	type_to_files = defaultdict(set)
	for path in all_files:
		orig_lang = path.stem
		norm_lang = normalize_language_name(orig_lang)
		if norm_lang not in valid_langs:
			skipped.append(orig_lang)
			continue
		types = extract_root_types(path)
		processed.append(orig_lang)
		root_types[orig_lang] = types
		for t in types:
			type_to_files[t].add(orig_lang)
	rprint(f"Processed {len(processed)} files: {', '.join(processed)}")
	rprint(f"Skipped {len(skipped)} files: {', '.join(skipped)}")
	rprint(f"Extracted {len(set().union(*root_types.values()))} unique node types.")


def normalize_language_name(name: str) -> str:
	"""Normalize a language name to lowercase, remove non-alphanumerics, and handle common aliases (e.g., c-sharp -> csharp)."""
	name = name.lower().replace("-", "")
	return "".join(c for c in name if c.isalnum())


def extract_supported_languages() -> list[str]:
	"""Extract supported languages from node-types.json files, filtered by SupportedLanguage."""
	all_langs = sorted([normalize_language_name(p.stem) for p in NODE_TYPES_DIR.glob("*.json")])
	valid_langs = set(SupportedLanguage.__args__)
	return [lang for lang in all_langs if lang in valid_langs]


def extract_all_node_types() -> list[str]:
	"""Extract all node types from all node-types.json files and return a sorted list of unique types."""
	node_types = set()
	for path in NODE_TYPES_DIR.glob("*.json"):
		with path.open() as f:
			data = json.load(f)

		def collect_types(obj: Any):  # noqa: ANN401
			if isinstance(obj, dict):
				if "type" in obj and isinstance(obj["type"], str):
					node_types.add(obj["type"])
				for v in obj.values():
					collect_types(v)
			elif isinstance(obj, list):
				for item in obj:
					collect_types(item)

		collect_types(data)
	return sorted(node_types)


def extract_language_extensions(supported: set[str]) -> dict[str, set[str]]:
	"""Extract language extensions from extension-map.json, filtered by SupportedLanguage."""
	with EXT_MAP_PATH.open() as f:
		ext_map = json.load(f)
	lang_exts = defaultdict(set)
	for entry in ext_map:
		if entry.get("type") != "programming":
			continue
		name = entry.get("name", "")
		normalized = normalize_language_name(name)
		if normalized in supported:
			for ext in entry.get("extensions", []):
				lang_exts[normalized].add(ext)
	return lang_exts


def generate_literals_py() -> None:
	"""Generate literals.py with lists for supported languages, node types, and all supported extensions."""
	now = datetime.now(UTC).strftime("%Y-%m-%d %H:%M:%S")
	supported = set(extract_supported_languages())
	node_types = extract_all_node_types()
	lang_exts = extract_language_extensions(supported)
	# Collect all unique extensions
	all_exts = set()
	for exts in lang_exts.values():
		all_exts.update(exts)
	# Remove leading period from each extension
	all_exts = sorted(ext.lstrip(".") for ext in all_exts)
	with LITERALS_PY_PATH.open("w", encoding="utf-8") as f:
		f.write(
			f'"""\n'
			f"Auto-generated file. DO NOT EDIT MANUALLY.\n"
			f"\n"
			f"Generated by {SCRIPT_PATH} on {now}.\n"
			f"\n"
			f"Contains literals for supported languages, node types, and language extensions.\n"
			f'"""\n\n'
		)
		f.write("# ruff: noqa: RUF001\n")
		f.write("from typing import Literal\n\n")
		# Supported languages
		f.write("SupportedLanguages = Literal[\n")
		f.write("    # Supported programming languages (auto-generated)\n")
		for lang in sorted(supported):
			f.write(f"    {escape_string(lang)},\n")
		f.write("]\n\n")
		# Node types
		f.write("NodeTypes = Literal[\n")
		f.write("    # All node types from all grammars (auto-generated)\n")
		for t in node_types:
			f.write(f"    {escape_string(t)},\n")
		f.write("]\n\n")
		# All supported extensions (without leading period)
		f.write("SupportedExtensions = Literal[\n")
		f.write("    # All unique file extensions for supported languages (auto-generated)\n")
		for ext in all_exts:
			f.write(f"    {escape_string(ext)},\n")
		f.write("]\n\n")


if __name__ == "__main__":
	main()
	generate_literals_py()
